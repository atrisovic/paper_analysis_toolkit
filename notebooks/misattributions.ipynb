{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "from typing import Literal\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import tiktoken\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def ollamaResponse(content):\n",
    "    response = ollama.chat(model='llama3', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': content,\n",
    "    },\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "def openAIReponse(content, model = 'gpt-3.5-turbo'):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    response: bool\n",
    "    \n",
    "class Classification(BaseModel):\n",
    "    response: str# Literal['context', 'uses', 'extends']\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    citation: str# Literal['context', 'uses', 'extends']\n",
    "    \n",
    "    \n",
    "def mask_apply(series, mask, function, fill_none = True):\n",
    "    mask = mask if mask is not None else [True for _ in range(len(series))]\n",
    "    result = []\n",
    "    for ind, element in tqdm(zip(mask, series), total = len(mask)):\n",
    "        result.append(function(element) if ind else element if fill_none else None)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/Alex/Desktop/2.FutureTech/OSFM/uniform_sample/results/9-13-and-8-28-with-confidence.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "context_fp = ((df['classification'] == 'uses') | (df['classification'] == 'extends')) & (df['alex'] == 'context')\n",
    "misattribution_mask = df['misattribution'].notna()\n",
    "ignore_mask = (df['multisentence'].apply(lambda s: s.find('\\\\begin{tab') != -1)) | (df['strippedModelKey'] == 'ase')\n",
    "ambiguity_mask = df['alex_confidence'] <= 5\n",
    "\n",
    "print(f\"False positives: {(context_fp & misattribution_mask & ~ignore_mask).sum()}\")\n",
    "assert((context_fp & misattribution_mask).sum() == context_fp.sum()), f\"Missing misattribution label for some false positives\"\n",
    "(context_fp & (df['misattribution'] == 1) & ~ignore_mask & ~ambiguity_mask).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extends_or_use_mask = (df['classification'] == 'extends') | (df['classification'] == 'uses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"The following sentences as from an academic paper, and cites a foundation model indicated by these <cite> citation brackets </cite>. These sentences have been identified as explicitly using the model, either by fine-tuning or using it out of the box, but we'd like to identify whether the authors were the ones doing so. Pretend you are the authors of the paper from which these sentences are pulled. Based on the following sentences, respond in the JSON format (without using ```json```) as {{\\\"response\\\": bool}} whether you were the one who used the model (true) or whether someone else did (false). Give one response for the whole set of sentences, without explanation. The sentence is as follows: {sentence}\"\n",
    "\n",
    "n = 5\n",
    "model = partial(openAIReponse, model = 'gpt-4o-mini')\n",
    "#model = ollamaResponse\n",
    "def correct_attribution(sentence, n = n) -> List[bool]:\n",
    "    responses = [model(prompt1.format(sentence = sentence)).lower() for i in range(n)]\n",
    "    models = map(Response.model_validate_json, responses)\n",
    "    total = np.array([model.response for model in models]).sum()\n",
    "    return total/n \n",
    "\n",
    "def try_correct_attribution(*args, **kwargs):\n",
    "    try:\n",
    "        return correct_attribution(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "column = f'gpt4omini_prompt1_{n}'\n",
    "df[column] = mask_apply(df['multisentence'], extends_or_use_mask & ~ignore_mask, try_correct_attribution, fill_none = False)\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-4o-mini'\n",
    "cost_per_token = .15/1e6\n",
    "token_counter = partial(num_tokens_from_string, model_name = model_name)\n",
    "\n",
    "df['token_count'] = df['multisentence'].apply(lambda s: token_counter(prompt1.format(sentence = s)))\n",
    "df['token_count'][extends_or_use_mask & ~ignore_mask].sum() * cost_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/Alex/Desktop/2.FutureTech/OSFM/data/premicrosoft_meeting_classified.csv'\n",
    "big_df = pd.read_csv(path)\n",
    "extends_mask = big_df['classification'].apply(lambda s: s in {'extends', 'uses'})\n",
    "ignore_mask = (big_df['multisentence'].apply(lambda s: s.find('\\\\begin{tab') != -1)) | (big_df['strippedModelKey'] == 'ase')\n",
    "big_df_postprocess = big_df[extends_mask & ~ignore_mask]\n",
    "big_df_postprocess['token_count'] = big_df_postprocess['multisentence'].apply(lambda s: token_counter(prompt1.format(sentence = s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df_postprocess['token_count'].sum() * cost_per_token * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['misattribution'].notna() & df[column].notna()  & ~ignore_mask & context_fp]\n",
    "cm = confusion_matrix(1-df_filtered['misattribution'].astype(int), (df_filtered[column] >= .8).astype(int))\n",
    "labels = ['Incorrect', 'Correct']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=ListedColormap(['lightgrey']),linewidths=0.5, xticklabels=labels, yticklabels=labels, cbar = False,annot_kws={\"size\": 30})\n",
    "plt.ylabel('Manual')\n",
    "plt.xlabel(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((df_filtered['misattribution'].astype(int)) & (df_filtered[column] > .5).astype(int)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing_promp1 = \"You are a research scientist working on artificial intelligence. You wrote the following sentences in your paper, where you cite a foundation model indicated by these <cite> citation brackets </cite>. For each set of sentences, you will respond with a JSON format that determines whether you used the model, fine-tuned it, or only referenced others' work. The response will contain a field \\\"citation\\\" which is either \\\"used\\\" if you used the model directly, \\\"fine_tuned\\\" if you fine-tuned the model or a \\\"referenced\\\" if you only referenced others' work. Here is the sentence: {sentence}\"\n",
    "postprocessing_prompt2 = \"You are a research scientist working on artificial intelligence, and you wrote the following sentences in a research paper. In these sentences, you reference a foundation model, indicated by these <cite></cite> citation brackets. For sentences provided, your task is to respond in JSON format, that should include a field called \\\"citation\\\" with one of the following values: \\\"uses\\\" – if you directly used, tested or run evaluation on the cited model in your research. \\\"extends\\\" – if you extended, fine-tuned, adapted or modified the cited model for your research. \\\"background\\\" – if you only referenced others' work that involved the cited model, without directly using or modifying it. Please analyze the following sentences accordingly: {sentence}\"\n",
    "\n",
    "model = partial(openAIReponse, model = 'gpt-4o-mini')\n",
    "df['postprocess_reclass'] = mask_apply(df['multisentence'], None, lambda s: model(postprocessing_prompt2.format(sentence = s)), fill_none=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .9\n",
    "\n",
    "df['corrected_classification'] = mask_apply(df['classification'], (df['correct_attribution'] >= threshold) & (df['correct_attribution'].notna()), lambda s: 'context')\n",
    "\n",
    "cm = confusion_matrix(df['alex'], df['corrected_classification'])\n",
    "uses_fp = 1 - (cm[2,2]/(cm[:, 2].sum()))\n",
    "extends_fp = 1 - (cm[1,1]/(cm[:, 1].sum()))\n",
    "\n",
    "print(uses_fp, extends_fp)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_json(s):\n",
    "    try:\n",
    "        return Citation.model_validate_json(s).citation\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "df['ollama']  = df['ollama_raw_response'].apply(lambda s: try_json(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
