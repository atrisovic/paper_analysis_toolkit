{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_copy = 'post_classification_random_sample_8-28'\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from utils import *\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn\n",
    "from math import floor, ceil\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "right_now = datetime.now().replace(microsecond=0, second=0)\n",
    "label_col = 'answer_vector'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "seed = 42 # np.randint(0, 110)\n",
    "\n",
    "mapping = lambda s: {'background': 'context',\n",
    "                     'future_work': 'context',\n",
    "                    'differences': 'context',\n",
    "                     'future work': 'context',\n",
    "                     'motivation': 'context',\n",
    "                     'similarities': 'context',\n",
    "                     'extends': 'extends',\n",
    "                     'uses': 'uses',\n",
    "                     '*': 'context'\n",
    "                     }.get(s) or s\n",
    "\n",
    "\n",
    "vector_from_string = lambda s: np.array(list(\n",
    "                                map(float,\n",
    "                                filter(None, \n",
    "                                map( str.strip, \n",
    "                                    s[1:-1].split(' ')\n",
    "                    )))))\n",
    "\n",
    "vector_from_string_bool = lambda s: np.array(list(\n",
    "                                map(lambda s: {'true': 1, 'false': 0, '1': 1, '0': 0, '1.':1, '0.':0}.get(s.lower()),\n",
    "                                filter(None, \n",
    "                                map( str.strip, \n",
    "                                    s[1:-1].split(' ')\n",
    "                    )))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load existing or default dataframe, query LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_path = f'~/Desktop/2.FutureTech/uniform_sample/results/{lazy_copy.replace('/',':')}.csv'\n",
    "df = pd.read_csv(other_path) #update_labels(other_path, save = True)\n",
    "\n",
    "\n",
    "df = df.sort_values(by = 'multisentence').reset_index(drop = True)\n",
    "df = df.sample(frac=1, random_state = seed)\n",
    "#df.rename(columns={'json_booleans':'json_response'}, inplace = True)\n",
    "print(len(df))\n",
    "df.head()\n",
    "\n",
    "df['answer_vector'] = df['answer_vector'].apply(lambda s: vector_from_string_bool(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_questions = df['answer_vector'].apply(len).max()\n",
    "assert(total_questions == df['answer_vector'].apply(len).min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_split = 2/3\n",
    "split = floor(tt_split * len(df))\n",
    "print(f\"Test-train split: {split}, {len(df) - split}\")\n",
    "\n",
    "df_train, df_test = df.iloc[:split], df.iloc[split:]\n",
    "\n",
    " \n",
    "label_mask = lambda s: df_train['alex'].apply(mapping) == s\n",
    "labels = sorted(list({mapping(item) for item in ['background', 'extends', 'uses']}))\n",
    "per_label_samples = max([len(df_train[label_mask(label)]) for label in labels])\n",
    "per_label_multipliers = [ceil(per_label_samples/len(df_train[label_mask(label)])) for label in labels]\n",
    "\n",
    "\n",
    "#comment out this line to use the original distribution\n",
    "#df_train = pd.concat([df_train[label_mask(label)] for label, multiplier in zip(labels, per_label_multipliers) for _ in range(0, multiplier)])\n",
    "print(f\"Training size {len(df_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = (np.stack(df_train['answer_vector']), \n",
    "                        np.array(df_train['alex'].apply(mapping).apply(lambda s: {'context': 0, 'uses': 1, 'extends': 2}.get(s)))\n",
    "                )\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype = torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype = torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "X_test = torch.tensor(np.stack(df_test['answer_vector']), dtype = torch.float32)\n",
    "y_test = torch.tensor(np.array(df_test['alex'].apply(mapping).apply(lambda s: {'context': 0, 'uses': 1, 'extends': 2}.get(s))), dtype = torch.float32).reshape(-1, 1)\n",
    "print(\"Training shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "c = Counter([el.item() for el in y_test])\n",
    "background_composition = c[0]/len(df_test)\n",
    "print(f\"Background composition: {background_composition}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(question_frequency_distribution(X_train, y_train, 0))\n",
    "plt.plot(question_frequency_distribution(X_train, y_train, 1))\n",
    "plt.plot(question_frequency_distribution(X_train, y_train, 2))\n",
    "plt.xticks(ticks = [i for i in range(X_train.shape[1]) if i % 2 == 0])\n",
    "\n",
    "plt.legend([\"Context\", \"Uses\", \"Extends\"])\n",
    "\n",
    "print(get_highest_relative_questions(X_train, y_train, target_index = 0, reference_index = 1, n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_questions = 30\n",
    "#best_uses_questions = get_highest_relative_questions(X_train, y_train, target_index = 1, reference_index = 0, n = top_n_questions)\n",
    "#best_extends_questions = get_highest_relative_questions(X_train, y_train, target_index = 2, reference_index = 1, n = top_n_questions)\n",
    "\n",
    "best_uses_questions = [10, 0, 13, 11, 23, 20, 8, 28, 3, 18, 15, 1, 2, 21, 17, 22, 4, 9, 14, 5, 16, 25, 19, 6, 26, 29, 27, 24, 52, 12]\n",
    "best_extends_questions = [50, 30, 40, 31, 41, 51, 38, 42, 35, 32, 52, 48, 54, 58, 53, 33, 34, 44, 36, 37, 47, 56, 43, 1, 8, 12, 18, 22, 57, 3]\n",
    "\n",
    "def heuristic_model(X, \n",
    "                    best_uses_questions = best_uses_questions, \n",
    "                    best_extends_questions = best_extends_questions, \n",
    "                    uses_threshold = 3/10,\n",
    "                    extends_threshold = 3/10):\n",
    "    uses = np.array([1 if answer else 0 for answer in (X[:, best_uses_questions].sum(axis = 1)/len(best_uses_questions) > uses_threshold)]).reshape(-1,)\n",
    "    extends = np.array([2 if answer else 1 for answer in (X[:, best_extends_questions].sum(axis = 1)/len(best_extends_questions) > extends_threshold)]).reshape(-1,)\n",
    "    return uses * extends\n",
    "\n",
    "def get_acc(uses_t, extends_t, X = X_train, y = y_train):\n",
    "    y_pred = heuristic_model(X, uses_threshold=uses_t, extends_threshold=extends_t)\n",
    "    acc = (y_pred == y.cpu().detach().numpy().reshape(-1, )).sum()/len(X)\n",
    "    return acc.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:, best_uses_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [[get_acc(uses_t = i/10, extends_t = j/10, X = X_train, y = y_train) for j in range(1,20)] for i in range(1,20)]\n",
    "sns.heatmap(results)\n",
    "plt.xlabel('Extends threshold')\n",
    "plt.ylabel('Uses threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generateModel(\n",
    "            input_size = total_questions,\n",
    "            classes = len(labels),\n",
    "            dropout_rate=0.7,\n",
    "            depth = 15,\n",
    "            base = 1.1\n",
    ")\n",
    "#model = model.to(device) # device = 'mps'\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight = torch.Tensor([1,5]).to(device))\n",
    "\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, verbose = False, iters = 1000, lr = 1e-4):\n",
    "    lr = lr * 10 # hacky and stupid\n",
    "    learning_rate_horizons = {0, 500, 1000}\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(iters):\n",
    "        if (epoch in learning_rate_horizons):\n",
    "            lr = lr / 10\n",
    "            print(f\"lr: {lr}\")\n",
    "            optimizer = torch.optim.Adam(params = model.parameters(), lr = lr)\n",
    "            \n",
    "        sample = random.sample([i for i in range(len(X_train))], ceil(len(X_train) * .1))\n",
    "        X_sample, y_sample = X_train[sample], y_train[sample]\n",
    "            \n",
    "\n",
    "        output = model(X_sample)\n",
    "        loss = loss_fn(output, y_sample.reshape(-1,))\n",
    "        \n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_accuracy = (output.argmax(dim = 1) == y_sample.reshape(-1, )).sum().item()/len(y_sample)\n",
    "        test_accuracy = (model(X_test).argmax(dim = 1) == y_test.reshape(-1,)).sum().item()/len(y_test)\n",
    "        \n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        if (epoch % 100 == 0 and verbose):\n",
    "            if (np.abs(test_accuracy - background_composition) <= .0001):\n",
    "                print(\"WARNING, YOU ARE CLASSIFYING EVERYTHING AS BACKGROUND\")\n",
    "            print(f\"Epoch: {epoch}, train loss: {loss.item()}, train acc: {train_accuracy}, test ac {test_accuracy}\")\n",
    "            \n",
    "    \n",
    "    return train_accuracies, test_accuracies\n",
    "\n",
    "\n",
    "#train_accuracies, test_accuracies = train_model(model, verbose=True, iters = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, v, threshold =  0.80):\n",
    "    if v is None:\n",
    "        return None\n",
    "    mapping = {0: 'context', 1:'uses', 2:'extends'}\n",
    "    \n",
    "    ten = torch.Tensor(v).to(device).reshape(1, -1)\n",
    "    rankings = model(ten)\n",
    "    \n",
    "    return mapping.get(rankings.item())\n",
    "\n",
    "    if (type(rankings) == torch.Tensor):  \n",
    "        rankings = rankings.cpu().detach().numpy().reshape(-1, )\n",
    "        \n",
    "    classification = rankings.argmax()\n",
    "    \n",
    "    if (rankings[classification] >= threshold):\n",
    "        return mapping.get(classification) \n",
    "    \n",
    "    return 'context'\n",
    "\n",
    "df['learned_classification'] = df['answer_vector'].apply(lambda v: run_model(heuristic_model, v))\n",
    "fp, fn = get_fp_fn(y_true=y_test, y_pred=df['learned_classification'].iloc[split:], verbose=False)\n",
    "\n",
    "df.to_csv(other_path, index = False)\n",
    "print(f\"False positive {fp}, False negative {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Last test acc: {test_accuracies[-1]}\")\n",
    "print(Counter(df['learned_classification']))\n",
    "print(f\"Test labels hash: {hash_dataframe(list(df_test['alex']))}\")\n",
    "print(f\"Test labels hash: {hash_dataframe(list(df['learned_classification'].iloc[split:]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_plot(X, y, start = .5, end = 1, num = 10, verbose = False, test_or_train = \"Test\"):\n",
    "    fps, fns = [], []\n",
    "    thresholds = np.geomspace(start, end, num = num)\n",
    "    print(thresholds)\n",
    "    \n",
    "    for t in tqdm(thresholds):\n",
    "        pred = np.array([run_model(X[v,:], threshold = t) for v in range(len(X))])\n",
    "        \n",
    "        fp, fn = get_fp_fn(y, pred, positive_label = 'uses', positive_index=1, verbose = verbose)\n",
    "    \n",
    "        fps.append(fp)\n",
    "        fns.append(fn)\n",
    "    \n",
    "    plt.title(f\"{test_or_train} FN vs. FP\")\n",
    "    plt.plot(thresholds * 100,fps, 'xr-')\n",
    "    plt.plot(thresholds * 100,fns, 'xb-')\n",
    "    plt.legend(['False Positive', 'False Negative'])\n",
    "    plt.xlabel(\"Confience threshold %\")\n",
    "    \n",
    "#pareto_plot(X_train, y_train, verbose = False, test_or_train = \"Train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_plot(X_test, y_test, test_or_train = \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
