{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_copy = 'sample_results'\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from math import floor\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "from utils import prompt, update_labels, master_path, get_answer_vector, hash_dataframe\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mapping = lambda s: {'background': 'context',\n",
    "                     'future_work': 'context',\n",
    "                    'differences': 'context',\n",
    "                     'future work': 'context',\n",
    "                     'motivation': 'context',\n",
    "                     'similarities': 'context',\n",
    "                     'extends': 'extends',\n",
    "                     '*': 'context'\n",
    "                     }.get(s) or s\n",
    "\n",
    "seed = 42 #np.random.randint(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load existing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "other_path = f'~/Desktop/2.FutureTech/uniform_sample/results/{lazy_copy.replace('/',':')}.csv'\n",
    "\n",
    "df = pd.read_csv(other_path)\n",
    "df = df.sort_values(by = 'multisentence').reset_index(drop = True)\n",
    "\n",
    "df = df.sample(frac=1, random_state = seed)\n",
    "\n",
    "df = df[df.index < 400]\n",
    "\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df['alex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse outputs, generate confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_mask = np.array([1,1,1,1,1,   0,0,0,0,0,   0,0,0,0,0,  1,1])\n",
    "uses_mask = np.array([0,0,0,0,0,   0,1,1,1,1,   1,1,0,0,0,   0,0])\n",
    "extends_mask = np.array([0,0,0,0,0,   0,0,0,0,0,   0,0,1,1,0,   0,0])\n",
    "\n",
    "\n",
    "def get_truth(r, verbose = False, background_mask = background_mask, uses_mask = uses_mask, extends_mask = extends_mask):\n",
    "    assert(len(background_mask) == len(uses_mask) and len(uses_mask) == len(extends_mask))\n",
    "    answer_vector = get_answer_vector(r, length = len(background_mask))\n",
    "    \n",
    "    if answer_vector is None:\n",
    "        return None\n",
    "    \n",
    "    resolve = lambda mask: np.array([{(0,1):0, (0,0): 0, (0, -1): 1, (1,1):1, (1,0) : 0, (1, -1): 0}[(x,y)] for x,y in zip(answer_vector, mask)]).sum()\n",
    "    \n",
    "    background =  resolve(background_mask)\n",
    "    uses = resolve(uses_mask)\n",
    "    fine_tune = resolve(extends_mask)\n",
    "\n",
    "    \n",
    "    if (verbose):\n",
    "        print(answer_vector)\n",
    "        print(f\"bg: {background}, uses: {uses}, extends: {fine_tune}\", flush = True)\n",
    "        \n",
    "    if answer_vector[3]:\n",
    "        return 'background'\n",
    "\n",
    "    if fine_tune:\n",
    "        return 'extends'\n",
    "    \n",
    "    if uses:\n",
    "        return 'uses'\n",
    "    \n",
    "    return 'background'\n",
    "\n",
    "#df['gpt-vector'] = df[label_col].apply(get_truth)\n",
    "#print(set(df['gpt-vector']))\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    continue\n",
    "    if (row['gpt-vector'] is None):\n",
    "        print(row['multisentence'])\n",
    "        print(row['gpt-vector'])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label, x_label = 'alex', 'learned_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_split = 2/3\n",
    "split = floor(tt_split * len(df))\n",
    "test_mode = False  \n",
    "\n",
    "true = df[y_label][split:] if test_mode else df[y_label][:split]\n",
    "pred = df[x_label][split:] if test_mode else df[x_label][:split]\n",
    "\n",
    "print(f\"Labels hash: {hash_dataframe(list(true))}\")\n",
    "print(f\"Class hash: {hash_dataframe(list(pred))}\")\n",
    "\n",
    "\n",
    "true, pred = list(map(mapping, true)), list(map(mapping, pred))\n",
    "\n",
    "\n",
    "\n",
    "cm = confusion_matrix(true, pred)\n",
    "print(set(true), set(pred))\n",
    "labels = sorted(list({mapping(item) for item in ['background', 'extends', 'uses']}))\n",
    "\n",
    "diagonal = cm * np.identity(len(labels))\n",
    "false_positives = (cm - diagonal).sum(axis = 0)/cm.sum(axis = 0)\n",
    "false_negatives = (cm - diagonal).sum(axis = 1)/cm.sum(axis = 1)\n",
    "accuracy = diagonal.sum() / cm.sum()\n",
    "\n",
    "print(f\"Accuracy {accuracy}\\n\")\n",
    "print(f\"Uses false positive: {false_positives[-1]}\")\n",
    "print(f\"Uses false negative: {false_negatives[-1]}\\n\")\n",
    "print(f\"Extends false positive: {false_positives[1]}\")\n",
    "print(f\"Extends false negative: {false_negatives[1]}\")\n",
    "\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Test Set Confusion Matrix with Fixed Threshold\")\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Confusion Matrix\n",
    "## Get samples, option to relabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(y_value, x_value):\n",
    "    mask = (df[y_label].apply(mapping) == y_value) & (df[x_label].apply(mapping) == x_value) \n",
    "    return df[mask]\n",
    "\n",
    "idx = -1\n",
    "samples = get_examples(y_value='context', x_value = 'uses')\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = samples.index[idx]\n",
    "print(idx, index)\n",
    "print(f\"Drawing from {len(samples)} samples\")\n",
    "print(samples['modelKey'].iloc[idx])\n",
    "\n",
    "print(samples['multisentence'].iloc[idx])\n",
    "\n",
    "vector = [float(el) for el in samples['answer_vector'].iloc[idx][1:-1].split(' ')]\n",
    "print(vector)\n",
    "one_indices = [i for i in range(len(vector)) if vector[i] == 1]\n",
    "one_idx = 1\n",
    "print(one_indices[one_idx], json.loads(samples['answer_string'].iloc[idx])[one_indices[one_idx]])\n",
    "\n",
    "print(samples[y_label].iloc[idx])\n",
    "print(samples[x_label].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv(master_path + '.csv')\n",
    "assert(df_temp['multisentence'].loc[index] == df['multisentence'].loc[index])\n",
    "print(df_temp['multisentence'].loc[index])\n",
    "reclass = input()\n",
    "\n",
    "if (len(reclass) > 0):\n",
    "    df_temp['alex2'].loc[index] = reclass\n",
    "    df_temp.to_csv(master_path + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reprompt on samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_sample = True\n",
    "if (retry_sample):\n",
    "    for idx in tqdm(samples.index):    \n",
    "        df[label_col].at[idx] = prompt(\n",
    "                                    MULTIPROMPT.format(\n",
    "                                            input = df['multisentence'].loc[idx], \n",
    "                                            modelKey = df['modelKeyStriped'].loc[idx]))\n",
    "CONNECTION_ON = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([df['gpt-vector'].loc[i] for i in samples.index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
