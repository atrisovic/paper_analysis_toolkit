{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_per_token = .15/1e6\n",
    "model_name = 'gpt-4o-mini'\n",
    "queries_per_sentence = 5\n",
    "\n",
    "prompt1 = \"The following sentences as from an academic paper, and cites a foundation model indicated by these <cite> citation brackets </cite>. These sentences have been identified as explicitly using the model, either by fine-tuning or using it out of the box, but we'd like to identify whether the authors were the ones doing so. Pretend you are the authors of the paper from which these sentences are pulled. Based on the following sentences, respond in the JSON format (without using ```json```) as {{\\\"response\\\": bool}} whether you were the one who used the model (true) or whether someone else did (false). Give one response for the whole set of sentences, without explanation. The sentence is as follows: {sentence}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/Alex/Desktop/2.FutureTech/OSFM/data/premicrosoft_meeting_classified.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "extends_uses_mask = df['classification'].apply(lambda s: s in {'uses', 'extends'})\n",
    "ignore_mask = (df['multisentence'].apply(lambda s: s.find('\\\\begin{tab') != -1)) | (df['strippedModelKey'] == 'ase')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_total_tokens = lambda s: num_tokens_from_string(prompt1.format(sentence = s), model_name=model_name) \n",
    "total_tokens = df[extends_uses_mask & ~ignore_mask]['multisentence'].apply(count_total_tokens).sum()\n",
    "total_cost = cost_per_token * queries_per_sentence * total_tokens\n",
    "print(f\"${round(total_cost, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
