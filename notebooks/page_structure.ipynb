{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/6j/9qz49mv9637bsngvd04d4pf00000gp/T/ipykernel_15017/3414380311.py:19: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  return f'(#+\\s*(?:\\d*|[ivx]*)\\.?\\s*{header})'\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def splitContentBySections(content: str):\n",
    "    sections = [{'content': '# ' + section} for section in filter(None, re.split(\"\\n##(?=[^#])\", content))]\n",
    "    \n",
    "    for section in sections:\n",
    "        section['labels'] = labelSection(section['content'])\n",
    "        \n",
    "    sections[0]['labels'].append('first')\n",
    "        \n",
    "    return sections\n",
    "\n",
    "def findRegexHeadings(header):\n",
    "    return f'(#+\\s*(?:\\d*|[ivx]*)\\.?\\s*{header})'\n",
    "\n",
    "def getGenericHeadingCheckerFunction(*args):    \n",
    "    return (lambda s: np.array([bool(re.findall(findRegexHeadings(header),s)) for header in args]).any())\n",
    "    \n",
    "\n",
    "def labelSection(content: str):    \n",
    "    mappings = {\n",
    "                        'reference': getGenericHeadingCheckerFunction('references', 'citations'),\n",
    "                        'method': getGenericHeadingCheckerFunction('methodology', 'method', 'approach', 'experiment'),\n",
    "                        'abstract': getGenericHeadingCheckerFunction('abstract'),\n",
    "                        'appendix': getGenericHeadingCheckerFunction('appendix'),\n",
    "                        'background': getGenericHeadingCheckerFunction('introduction', 'related work', 'background'),\n",
    "                        'conclusion': getGenericHeadingCheckerFunction('conclusion', 'discussion'),\n",
    "                        'results': getGenericHeadingCheckerFunction('results')\n",
    "    }\n",
    "    \n",
    "    labels = [label for label, func in mappings.items() if func(content)]\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669\n",
      "de519a1976d0f5005ffac09f2560b1b61b37603c.mmd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': \"# # chatlaw: open-source legal large language model with integrated external knowledge bases\\n\\n jiaxi cui\\n\\npeking university\\n\\njiaxicui@chatlaw.cloud\\n\\nequal contribution.\\n\\nzongjian li\\n\\npeking university\\n\\nchestnutlzj@chatlaw.cloud\\n\\nyang yan\\n\\npeking university\\n\\nyyang@stu.pku.edu.cn\\n\\n&bohua chen\\n\\npeking university\\n\\nbohua@chatlaw.cloud\\n\\n&li yuan\\n\\npeking university\\n\\nyuanli-ece@pku.edu.cn\\n\\ncorresponding author\\n\\n###### abstract\\n\\nlarge language models (llms) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. however, unlike proprietary models such as bloomberggpt and fingpt, which have leveraged their unique data accumulations to make strides in the finance domain, there hasn't not many similar large language models in the chinese legal domain to facilitate its digital transformation.\\n\\nin this paper, we propose an open-source legal large language model named chatlaw. due to the importance of data quality, we carefully designed a legal domain\\n\\nfigure 1: chatlaw frameworkfine-tuning dataset. additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models. we also open-sourced our model and part of the data at [https://github.com/pku-yuangroup/chatlaw](https://github.com/pku-yuangroup/chatlaw).\\n\",\n",
       "  'labels': ['abstract', 'first']},\n",
       " {'content': '#  1 introduction\\n\\nthe continuous expansion and development of artificial intelligence have provided a fertile ground for the proliferation of large-scale language models. models such as chatgpt, gpt4 [5], llama [7], falcon [1], vicuna [2], and chatglm [12] have demonstrated remarkable performance in various conventional tasks, unleashing tremendous potential for the field of law. however, it is evident that acquiring high-quality, relevant, and up-to-date data is a crucial factor in the development of large language models. therefore, the development of effective and efficient open-source legal language models has become of paramount importance.\\n\\nin the realm of artificial intelligence, the development of large-scale models has permeated various domains such as healthcare, education, and finance: bloomberggpt [9], fingpt [10], huatuo [8], chatmed [14], these models have demonstrated their utility and impact in tackling complex tasks and generating valuable insights. however, the field of law, with its inherent importance and demand for accuracy, stands as a domain that necessitates dedicated research and development of a specialized legal model.\\n\\nlaw plays a pivotal role in shaping societies, governing human interactions, and upholding justice. legal professionals rely on accurate and up-to-date information to make informed decisions, interpret laws, and provide legal counsel. the complexities of legal language, nuanced interpretations, and the ever-evolving nature of legislation present unique challenges that require tailored solutions.\\n\\nhowever, when it comes to legal issues, there is often a phenomenon of hallucination and nonsensical outputs, even with the most advanced model like gpt4. people tend to believe that fine-tuning a model with specific domain knowledge would yield satisfactory results. however, in reality, this is not the case with early legal llm (lawgpt), as there are still many instances of hallucination and unreliable outputs.\\n\\nwe initially recognized the need for a chinese legal llm. however, at the time, there were no commercially available chinese models surpassing the scale of 13 billion parameters. therefore, we built upon the foundation of openllama, a commercially viable model, by expanding the chinese vocabulary and incorporating training data from sources like moss. this allowed us to create a foundational chinese language model. subsequently, we incorporated legal-specific data to train our legal model---chatlaw.\\n\\nthe key contributions of this paper are as follows:\\n\\n1. **effective approach to mitigate hallucination:** we propose an approach to address hallucination by enhancing the model\\'s training process and incorporating four modules during inference: \"consult,\" \"reference\", \"self-suggestion\" and \"response.\" by integrating vertical models and knowledge bases through the reference module, we inject domain-specific knowledge into the model and leverage accurate information from the knowledge base, reducing the occurrence of hallucinations.\\n2. **legal feature word extraction model based on llm:** we train a model that extracts legal feature words from users\\' everyday language. this model identifies words with legal significance, enabling efficient identification and analysis of legal contexts within user input.\\n3. **legal text similarity calculation model based on bert:** we train a model to measure the similarity between users\\' everyday language and a dataset consisting of 930,000 relevant legal case texts. this enables the creation of a vector database for efficient retrieval of similar legal texts, facilitating further analysis and reference.\\n4. **construction of a chinese legal exam testing dataset:** we curate a dataset specifically designed for testing legal domain knowledge in chinese. additionally, we design an elo arena scoring mechanism to compare the performance of different models in legal multiple-choice questions.\\n\\nfurthermore, we observed that a single general-purpose legal llm may not perform optimally across all tasks in this domain. therefore, we trained different models for various scenarios, such as multiple-choice questions, keyword extraction, and question-answering. to handle the selection and deployment of these models, we employed a big llm as a controller using the methodology provided by hugginggpt [6]. this controller model dynamically determines which specific model to invoke based on each user\\'s request, ensuring the most suitable model is utilized for the given task.\\n',\n",
       "  'labels': ['background']},\n",
       " {'content': \"#  2 dataset\\n\\nin constructing the dataset, we employed several approaches to ensure its comprehensiveness and diversity. the dataset composition methods are as follows:\\n\\n**collection of a vast amount of original legal data:** this includes gathering legal news, social media content, and discussions from legal industry forums. these sources provide a diverse range of real-world legal text, offering insights into various legal topics and discussions.\\n\\n**construction based on legal regulations and judicial interpretations:** to ensure comprehensive coverage of legal knowledge, we incorporate relevant legal regulations and judicial interpretations into the dataset. this ensures that the dataset reflects the legal framework and provides accurate and up-to-date information.\\n\\n**construction of multiple-choice questions for the bar exam:** we create a set of multiple-choice questions specifically designed for the bar exam. these questions cover various legal topics and test users' understanding and application of legal principles.\\n\\nby incorporating data from these diverse sources and construction methods, our dataset encompasses a wide range of legal contexts, ensuring that the developed model is capable of effectively understanding and addressing various legal scenarios.\\n\\nonce these data components are collected, the dataset undergoes a rigorous cleaning process. this involves filtering out short and incoherent responses, ensuring that only high-quality and meaningful text is included. additionally, to enhance the dataset, we leverage the chatgpt api for assisted construction, allowing us to generate supplementary data based on the existing dataset.\\n\",\n",
       "  'labels': []},\n",
       " {'content': \"#  3 training process\\n\\nthe keyword llm is a language model that extracts keywords from abstract consulting problems raised by users. the law llm, on the other hand, extracts legal terminology that may be involved in user consultations. the chatlaw llm is the ultimate language model that outputs responses to users. it refers to relevant legal clauses and utilizes its own summarization and q&a function to generate advice for users in their consultations.\\n\\n### chatlaw llm\\n\\nto train chatlaw, we fine-tuned it on the basis of ziya-llama-13b [11] using low-rank adaptation (lora) [3]. additionally, we introduced the self-suggestion role to further alleviate model hallucination issues. the training process was carried out on multiple a100 gpus and the training costs were further reduced with the help of deepspeed.\\n\\n### keyword llm\\n\\ncreating chatlaw product by combining vertical-specific llm with a knowledge base, it is crucial to retrieve relevant information from the knowledge base based on user queries. we initially tried traditional software development methods such as mysql and elasticsearch for retrieval, but the results were unsatisfactory. therefore, we attempted to use a pre-trained bert model for embedding,followed by methods such as faiss [4] to calculate cosine similarity and extract the top k legal regulations related to user queries. however, this method often yields suboptimal results when the user's question is vague. therefore, we aim to extract key information from user queries and use the vector embedding of this information to design an algorithm to improve matching accuracy.\\n\\ndue to the significant advantages of large models in understanding user queries, we fine-tuned an llm to extract the keywords from user queries. after obtaining multiple keywords, we adopted **algorithm 1** to retrieve relevant legal provisions.\\n\\nfigure 2: result of keyword llm and law llm\\n\\n### law llm\\n\\nwe trained a bert model using a dataset of 937k national case law examples to extract corresponding legal provisions and judicial interpretations from user queries. this law llm model forms an essential component of the chatlaw product.\\n\",\n",
       "  'labels': []},\n",
       " {'content': \"#  4 experiment and analysis\\n\\nevaluating the performance of the large language model (llm) has always been a challenge. for this purpose, we have collected national judicial examination questions over a decade and compiled a test dataset containing 2000 questions with their standard answers to measure the models' ability to handle legal multiple-choice questions.\\n\\nhowever, we found that the accuracy rates of the models are generally quite low. under these circumstances, simply comparing accuracy rates seems to hold little significance. therefore, we have established an evaluation mechanism for model competition for elo points, inspired by the matchmaking mechanism in e-sports and the design of chatbot arena [13], to more effectively assess the models' abilities to handle legal multiple-choice questions.\\n\\nthrough the analysis of the above experimental results, we can make the following observations:\\n\\n(1) the introduction of legal-related q&a and statute data can to some extent improve the model's performance on multiple-choice questions;\\n\\n(2) the addition of specific task types for training significantly improves the model's performance on such tasks. for example, the reason why the chatlaw model outperforms gpt-4 is that we used a large number of multiple-choice questions as training data;\\n\\n(3) legal multiple-choice questions require complex logical reasoning, so models with a larger number of parameters usually perform better.\\n\\nconclusions\\n\\nin this paper, we proposed chatlaw, a legal large language model(llm) developed using legal domain knowledge. we propose a novel approach that combines llm with vector knowledge databases, which significantly alleviates the hallucination problem commonly seen in llm. our stable model handling strategies enable the resolution of various legal domain problems. additionally, we release a dataset for legal multiple-choice questions and design an elo model ranking mechanism.\\n\\nhowever, our limitations arise due to the scale of the base model. our performance in tasks such as logical reasoning and deduction is not optimal. additionally, after incorporating a large amount of domain-specific data, further research is required to improve the generalization of chatlaw for generic tasks. there are potential social risks on chatlaw, and we advise users to make use of our method for proper purposes.\\n\",\n",
       "  'labels': ['method']},\n",
       " {'content': '#  references\\n\\n* [1]e. almazrouei, h. alobeidli, a. alshamsi, a. cappelli, r. cojocaru, m. debbah, e. goffinet, d. heslow, j. launay, q. malartic, b. noune, b. pannier, and g. penedo (2023) falcon-40b: an open large language model with state-of-the-art performance. external links: 2303.03016 cited by: ss1.\\n* [2]w. chiang, z. li, z. lin, y. sheng, z. wu, h. zhang, l. zheng, s. zhuang, y. zhuang, j. e. gonzalez, i. stoica, and e. p. xing (2023-04) vicuna: an open-source chatbot impressing gpt-4 with 90%* chatgrp quality. external links: 2303.03016 cited by: ss1.\\n* [3]e. j. hu, y. shen, p. wallis, z. allen-zhu, y. li, s. wang, l. wang, and w. chen (2022) lora: low-rank adaptation of large language models. in international conference on learning representations, cited by: ss1.\\n* [4]j. johnson, m. douze, and h. jegou (2019) billion-scale similarity search with gpus. ieee transactions on big data7 (3), pp. 535-547. external links: document, issn 1078-1268 cited by: ss1.\\n* [5]openai (2023) gpt-4 technical report. external links: 2302.13971 cited by: ss1.\\n* [6]y. shen, k. song, x. tan, d. li, w. lu, and y. zhuang (2023) hugginggpt: solving ai tasks with chatgrp and its friends in hugging face. external links: 2302.13971 cited by: ss1.\\n* [7]h. wang, c. liu, n. xi, z. qiang, s. zhao, b. qin, and t. liu (2023) huatuo: tuning llama model with chinese medical knowledge. external links: 2302.13971 cited by: ss1.\\n* [8]h. wang, c. liu, n. xi, z. qiang, s. zhao, b. qin, and t. liu (2023) huatuo: tuning llama model with chinese medical knowledge. external links: 2302.13971 cited by: ss1.\\n* [9]s. wu, o. irsoy, s. lu, v. dabravolski, m. dredze, s. gehrmann, p. kambadur, d. rosenberg, and g. mann (2023) bloomberggpt: a large language model for finance. external links: 2302.13971 cited by: ss1.\\n* [10]h. yang, x. liu, and c. dan wang (2023) fingpt: open-source financial large language models. external links: 2302.13971 cited by: ss1.\\n* [11]p. yang, j. wang, r. gan, x. zhu, l. zhang, z. wu, x. gao, j. zhang, and t. sakai (2022) zero-shot learners for natural language understanding via a unified multiple choice perspective. external links: 2202.13971 cited by: ss1.\\n* [12]a. zeng, x. liu, z. du, z. wang, h. lai, m. ding, z. yang, y. xu, w. zheng, x. xia, w. lam tam, z. ma, y. xue, j. zhai, w. chen, z. liu, p. zhang, y. dong, and j. tang (2023) glm-130b: an open bilingual pre-trained model. in the eleventh international conference on learning representations (iclr), external links: 2302.13971 cited by: ss1.\\n* [13]l. zheng, w. chiang, y. sheng, s. zhuang, z. wu, y. zhuang, z. lin, z. li, d. li, e. p. xing, h. zhang, j. e. gonzalez, and i. stoica (2023) judging llm-as-a-judge with mt-bench and chatbot arena. external links: 2302.13971 cited by: ss1.\\n* [14]w. zhu and x. wang (2023) chatmed: a chinese medical large language model. note: [https://github.com/michael-wzhu/chatmed](https://github.com/michael-wzhu/chatmed) cited by: ss1.',\n",
       "  'labels': ['reference']}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_path = '../data/Markdown'\n",
    "all_markdowns = listdir(markdown_path)\n",
    "\n",
    "num = randint(1, len(all_markdowns)-1)\n",
    "print(num)\n",
    "sample = all_markdowns[num]\n",
    "print(sample)\n",
    "with open(join(markdown_path, sample), 'r') as f:\n",
    "    content = f.read().lower()\n",
    "sections = splitContentBySections(content)\n",
    "sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_sections = []\n",
    "percentages = []\n",
    "for markdown in all_markdowns:\n",
    "    with open(join(markdown_path, markdown), 'r') as f:\n",
    "        content = f.read().lower()\n",
    "    \n",
    "    sections = splitContentBySections(content)\n",
    "    has_method = len([sec for sec in sections if 'method' in sec['labels']]) > 0\n",
    "    markdown_sections.append((sections, has_method))\n",
    "    \n",
    "    if has_method:\n",
    "        method_length = np.array([len(section['content']) for section in sections if 'method' in section['labels']]).sum()\n",
    "        all_length = np.array([len(section['content']) for section in sections]).sum()\n",
    "        percentages.append(method_length/all_length)\n",
    "        \n",
    "print(f\"Average content percent as method: {np.array(percentages).mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([sections for sections, has_method in markdown_sections if not has_method])/len(all_markdowns)\n",
    "\n",
    "i += 1\n",
    "[sections for sections, has_method in markdown_sections if not has_method][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
