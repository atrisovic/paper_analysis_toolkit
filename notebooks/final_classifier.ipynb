{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import plot_cm\n",
    "import matplotlib.pyplot as plt\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/Alex/Desktop/2.FutureTech/OSFM/uniform_sample/results/9-13-and-8-28-with-confidence.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "uses_questions = [10, 0, 13, 11, 23, 20, 8, 28, 3, 18, 15, 1, 2, 21, 17, 22, 4, 9, 14, 5, 16, 25, 19, 6, 26, 29, 27, 24, 52, 12]\n",
    "extends_questions = [50, 30, 40, 31, 41, 51, 38, 42, 35, 32, 52, 48, 54, 58, 53, 33, 34, 44, 36, 37, 47, 56, 43, 1, 8, 12, 18, 22, 57, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer_vector_np'] = df['answer_vector'].apply(lambda s: np.array(s[1:-1].split(' ')))\n",
    "df['extends_score'] = df['answer_vector_np'].apply(lambda s: (s[extends_questions]).astype(int).sum()/len(extends_questions))\n",
    "df['uses_score'] = df['answer_vector_np'].apply(lambda s: (s[uses_questions]).astype(int).sum()/len(uses_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Citation(BaseModel):\n",
    "    citation: str# Literal['context', 'uses', 'extends']\n",
    "def try_json(s):\n",
    "    s = s.replace('```','').replace('json', '')\n",
    "    try:\n",
    "        return Citation.model_validate_json(s).citation\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "df['postprocess_reclass_stripped'] =df['postprocess_reclass'].apply(try_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_column = 'gpt4omini_prompt1_10'\n",
    "df['manual_classification'] = ['extends' if e_score >= .3 else 'uses' if u_score >= .3 else 'context' for e_score, u_score in zip(df['extends_score'], df['uses_score'])]\n",
    "df['corrected_classification'] = [reclass if c in {'uses','extends'} else c for reclass, c in zip(df['postprocess_reclass_stripped'], df['manual_classification'])]\n",
    "df['corrected_classification'] = df['corrected_classification'].apply(lambda s: {'background':'context'}.get(s) or s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_mask = (df['multisentence'].apply(lambda s: s.find('\\\\begin{tab') != -1)) | (df['strippedModelKey'] == 'ase') | df['corrected_classification'].isna()\n",
    "df_valid = df[~ignore_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(df_valid['alex'], df_valid['corrected_classification'])\n",
    "#plt.title(model_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gpt4o_prompt1_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "column = 'llama31_prompt1_10'\n",
    "plt.hist(df[column])\n",
    "plt.title(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
