{
    "0_sparse_digit_recognition_svm": {
        "paperId": "1503cbd400cd902f9751995e27973b9286dfd320",
        "externalIds": {
            "MAG": "2149270675",
            "DBLP": "journals/tnn/LabuschBM08",
            "DOI": "10.1109/TNN.2008.2005830",
            "CorpusId": 14358161,
            "PubMed": "19000969"
        },
        "corpusId": 14358161,
        "publicationVenue": {
            "id": "2ac50919-507e-41c7-93a8-721c4b804757",
            "name": "IEEE Transactions on Neural Networks",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Neural Netw"
            ],
            "issn": "1045-9227",
            "alternate_issns": [
                "1941-0093"
            ],
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=72"
        },
        "url": "https://www.semanticscholar.org/paper/1503cbd400cd902f9751995e27973b9286dfd320",
        "title": "Simple Method for High-Performance Digit Recognition Based on Sparse Coding",
        "abstract": "In this brief paper, we propose a method of feature extraction for digit recognition that is inspired by vision research: a sparse-coding strategy and a local maximum operation. We show that our method, despite its simplicity, yields state-of-the-art classification results on a highly competitive digit-recognition benchmark. We first employ the unsupervised Sparsenet algorithm to learn a basis for representing patches of handwritten digit images. We then use this basis to extract local coefficients. In a second step, we apply a local maximum operation to implement local shift invariance. Finally, we train a support vector machine (SVM) on the resulting feature vectors and obtain state-of-the-art classification performance in the digit recognition task defined by the MNIST benchmark. We compare the different classification performances obtained with sparse coding, Gabor wavelets, and principal component analysis (PCA). We conclude that the learning of a sparse representation of local image patches combined with a local maximum operation for feature extraction can significantly improve recognition performance.",
        "venue": "IEEE Transactions on Neural Networks",
        "year": 2008,
        "referenceCount": 35,
        "citationCount": 105,
        "influentialCitationCount": 4,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is concluded that the learning of a sparse representation of local image patches combined with a local maximum operation for feature extraction can significantly improve recognition performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2008-11-01",
        "journal": {
            "name": "IEEE Transactions on Neural Networks",
            "pages": "1985-1989",
            "volume": "19"
        },
        "citationStyles": {
            "bibtex": "@Article{Labusch2008SimpleMF,\n author = {Kai Labusch and E. Barth and T. Martinetz},\n booktitle = {IEEE Transactions on Neural Networks},\n journal = {IEEE Transactions on Neural Networks},\n pages = {1985-1989},\n title = {Simple Method for High-Performance Digit Recognition Based on Sparse Coding},\n volume = {19},\n year = {2008}\n}\n"
        }
    },
    "2_dot(s)-rnn": {
        "paperId": "533ee188324b833e059cb59b654e6160776d5812",
        "externalIds": {
            "DBLP": "journals/corr/PascanuGCB13",
            "MAG": "1889624880",
            "ArXiv": "1312.6026",
            "CorpusId": 1870512
        },
        "corpusId": 1870512,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/533ee188324b833e059cb59b654e6160776d5812",
        "title": "How to Construct Deep Recurrent Neural Networks",
        "abstract": "In this paper, we explore different ways to extend a recurrent neural network (RNN) to a \\textit{deep} RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs benefit from the depth and outperform the conventional, shallow RNNs.",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "referenceCount": 48,
        "citationCount": 939,
        "influentialCitationCount": 67,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Two novel architectures of a deep RNN are proposed which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build aDeep RNN, and an alternative interpretation is provided using a novel framework based on neural operators."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2013-12-20",
        "journal": {
            "name": "CoRR",
            "volume": "abs/1312.6026"
        },
        "citationStyles": {
            "bibtex": "@Article{Pascanu2013HowTC,\n author = {Razvan Pascanu and \u00c7aglar G\u00fcl\u00e7ehre and Kyunghyun Cho and Yoshua Bengio},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {How to Construct Deep Recurrent Neural Networks},\n volume = {abs/1312.6026},\n year = {2013}\n}\n"
        }
    },
    "3_kepler": {
        "paperId": "56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
        "externalIds": {
            "MAG": "3151929433",
            "DBLP": "journals/tacl/WangGZZLLT21",
            "ArXiv": "1911.06136",
            "DOI": "10.1162/tacl_a_00360",
            "CorpusId": 208006241
        },
        "corpusId": 208006241,
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
        "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
        "abstract": "Abstract Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG/KEPLER.",
        "venue": "Transactions of the Association for Computational Linguistics",
        "year": 2019,
        "referenceCount": 77,
        "citationCount": 451,
        "influentialCitationCount": 100,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00360/1923927/tacl_a_00360.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs is proposed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-11-13",
        "journal": {
            "name": "Transactions of the Association for Computational Linguistics",
            "pages": "176-194",
            "volume": "9"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2019KEPLERAU,\n author = {Xiaozhi Wang and Tianyu Gao and Zhaocheng Zhu and Zhiyuan Liu and Juan-Zi Li and Jian Tang},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {176-194},\n title = {KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation},\n volume = {9},\n year = {2019}\n}\n"
        }
    },
    "4_mv-rnn": {
        "paperId": "27e38351e48fe4b7da2775bf94341738bc4da07e",
        "externalIds": {
            "DBLP": "conf/emnlp/SocherHMN12",
            "MAG": "1889268436",
            "ACL": "D12-1110",
            "CorpusId": 806709
        },
        "corpusId": 806709,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/27e38351e48fe4b7da2775bf94341738bc4da07e",
        "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces",
        "abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2012,
        "referenceCount": 44,
        "citationCount": 1359,
        "influentialCitationCount": 128,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A recursive neural network model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length and can learn the meaning of operators in propositional logic and natural language is introduced."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "publicationDate": "2012-07-12",
        "journal": {
            "pages": "1201-1211"
        },
        "citationStyles": {
            "bibtex": "@Article{Socher2012SemanticCT,\n author = {R. Socher and Brody Huval and Christopher D. Manning and A. Ng},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1201-1211},\n title = {Semantic Compositionality through Recursive Matrix-Vector Spaces},\n year = {2012}\n}\n"
        }
    },
    "5_n-gram_lm": {
        "paperId": "fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87",
        "externalIds": {
            "MAG": "1549285799",
            "DBLP": "conf/interspeech/ClarksonR97",
            "DOI": "10.21437/Eurospeech.1997-683",
            "CorpusId": 13988648
        },
        "corpusId": 13988648,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87",
        "title": "Statistical language modeling using the CMU-cambridge toolkit",
        "abstract": "The CMU Statistical Language Modeling toolkit was released in 1994 in order to facilitate the construction and testing of bigram and trigram language models. It is currently in use in over 40 academic, government and industrial laboratories in over 12 countries. This paper presents a new version of the toolkit. We outline the conventional language modeling technology, as implemented in the toolkit, and describe the extra e(cid:14)ciency and functionality that the new toolkit provides as compared to previous software for this task. Finally, we give an example of the use of the toolkit in constructing and testing a simple language model.",
        "venue": "EUROSPEECH",
        "year": 1997,
        "referenceCount": 18,
        "citationCount": 715,
        "influentialCitationCount": 38,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://figshare.com/articles/journal_contribution/Statistical_Language_Modeling_using_the_CMU-Cambridge_Toolkit/6609881/1/files/12102020.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The conventional language modeling technology, as implemented in the toolkit, is outlined, and the extra e(cid:14)ciency and functionality that the new toolkit provides as compared to previous software for this task is described."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1997-09-22",
        "journal": {
            "pages": "2707-2710"
        },
        "citationStyles": {
            "bibtex": "@Article{Clarkson1997StatisticalLM,\n author = {P. Clarkson and R. Rosenfeld},\n booktitle = {EUROSPEECH},\n pages = {2707-2710},\n title = {Statistical language modeling using the CMU-cambridge toolkit},\n year = {1997}\n}\n"
        }
    },
    "7_igpt-l": {
        "paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "externalIds": {
            "MAG": "3034445277",
            "DBLP": "conf/icml/ChenRC0JLS20",
            "CorpusId": 219781060
        },
        "corpusId": 219781060,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "title": "Generative Pretraining From Pixels",
        "abstract": "Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we \ufb01nd that a GPT-2 scale model learns strong image representations as measured by linear probing, \ufb01ne-tuning, and low-data classi\ufb01cation. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full \ufb01ne-tuning, matching the top supervised pre-trained models. An even larger model trained on a mix-ture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "referenceCount": 79,
        "citationCount": 1132,
        "influentialCitationCount": 93,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that a GPT-2 scale model learns strong image representations as measured by linear probing, \ufb01ne-tuning, and low-data classi\ufb01cation, despite training on low-resolution ImageNet without labels."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-07-12",
        "journal": {
            "pages": "1691-1703"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2020GenerativePF,\n author = {Mark Chen and Alec Radford and Jeff Wu and Heewoo Jun and Prafulla Dhariwal and D. Luan and I. Sutskever},\n booktitle = {International Conference on Machine Learning},\n pages = {1691-1703},\n title = {Generative Pretraining From Pixels},\n year = {2020}\n}\n"
        }
    },
    "8_vd-lstm+real_large": {
        "paperId": "424aef7340ee618132cc3314669400e23ad910ba",
        "externalIds": {
            "MAG": "2951403397",
            "DBLP": "conf/iclr/InanKS17",
            "ArXiv": "1611.01462",
            "CorpusId": 7443908
        },
        "corpusId": 7443908,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/424aef7340ee618132cc3314669400e23ad910ba",
        "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
        "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "referenceCount": 36,
        "citationCount": 363,
        "influentialCitationCount": 34,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces a novel theoretical framework that facilitates better learning in language modeling, and shows that this framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-11-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1611.01462"
        },
        "citationStyles": {
            "bibtex": "@Article{Inan2016TyingWV,\n author = {Hakan Inan and Khashayar Khosravi and R. Socher},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling},\n volume = {abs/1611.01462},\n year = {2016}\n}\n"
        }
    },
    "9_deeplab_(2017)": {
        "paperId": "cab372bc3824780cce20d9dd1c22d4df39ed081a",
        "externalIds": {
            "ArXiv": "1606.00915",
            "MAG": "2412782625",
            "DBLP": "journals/corr/ChenPK0Y16",
            "DOI": "10.1109/TPAMI.2017.2699184",
            "CorpusId": 3429309,
            "PubMed": "28463186"
        },
        "corpusId": 3429309,
        "publicationVenue": {
            "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
            "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Pattern Anal Mach Intell"
            ],
            "issn": "0162-8828",
            "url": "http://www.computer.org/tpami/",
            "alternate_urls": [
                "http://www.computer.org/portal/web/tpami",
                "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/cab372bc3824780cce20d9dd1c22d4df39ed081a",
        "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",
        "abstract": "In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or \u2018atrous\u00a0convolution\u2019, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous\u00a0spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed \u201cDeepLab\u201d system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "year": 2016,
        "referenceCount": 107,
        "citationCount": 14768,
        "influentialCitationCount": 1606,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1606.00915",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work addresses the task of semantic image segmentation with Deep Learning and proposes atrous\u00a0spatial pyramid pooling (ASPP), which is proposed to robustly segment objects at multiple scales, and improves the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-06-02",
        "journal": {
            "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "pages": "834-848",
            "volume": "40"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2016DeepLabSI,\n author = {Liang-Chieh Chen and G. Papandreou and Iasonas Kokkinos and K. Murphy and A. Yuille},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {834-848},\n title = {DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},\n volume = {40},\n year = {2016}\n}\n"
        }
    },
    "12_deeply-recursive_convnet": {
        "paperId": "06c06885fd53b2cbd407704cf14f658842ed48e5",
        "externalIds": {
            "MAG": "2214802144",
            "ArXiv": "1511.04491",
            "DBLP": "journals/corr/KimLL15a",
            "DOI": "10.1109/CVPR.2016.181",
            "CorpusId": 206593506
        },
        "corpusId": 206593506,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/06c06885fd53b2cbd407704cf14f658842ed48e5",
        "title": "Deeply-Recursive Convolutional Network for Image Super-Resolution",
        "abstract": "We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/ vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "referenceCount": 32,
        "citationCount": 2257,
        "influentialCitationCount": 306,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1511.04491",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN) with two extensions: recursive-supervision and skip-connection, which outperforms previous methods by a large margin."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-11-14",
        "journal": {
            "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "1637-1645"
        },
        "citationStyles": {
            "bibtex": "@Article{Kim2015DeeplyRecursiveCN,\n author = {Jiwon Kim and Jung Kwon Lee and Kyoung Mu Lee},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1637-1645},\n title = {Deeply-Recursive Convolutional Network for Image Super-Resolution},\n year = {2015}\n}\n"
        }
    },
    "13_gcnn-14": {
        "paperId": "88caa4a0253a8b0076176745ebc072864eab66e1",
        "externalIds": {
            "DBLP": "conf/icml/DauphinFAG17",
            "ArXiv": "1612.08083",
            "MAG": "2567070169",
            "CorpusId": 16119010
        },
        "corpusId": 16119010,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/88caa4a0253a8b0076176745ebc072864eab66e1",
        "title": "Language Modeling with Gated Convolutional Networks",
        "abstract": "The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "referenceCount": 36,
        "citationCount": 1912,
        "influentialCitationCount": 186,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens, is developed and is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-12-23",
        "journal": {
            "pages": "933-941"
        },
        "citationStyles": {
            "bibtex": "@Article{Dauphin2016LanguageMW,\n author = {Yann Dauphin and Angela Fan and Michael Auli and David Grangier},\n booktitle = {International Conference on Machine Learning},\n pages = {933-941},\n title = {Language Modeling with Gated Convolutional Networks},\n year = {2016}\n}\n"
        }
    },
    "14_contextnet": {
        "paperId": "1bd7d2932de819ed1087b6453ef2c0be9f781ac1",
        "externalIds": {
            "DBLP": "journals/corr/abs-2005-03191",
            "MAG": "3095173472",
            "ArXiv": "2005.03191",
            "DOI": "10.21437/interspeech.2020-2059",
            "CorpusId": 218538000
        },
        "corpusId": 218538000,
        "publicationVenue": {
            "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
            "name": "Interspeech",
            "type": "conference",
            "alternate_names": [
                "Conf Int Speech Commun Assoc",
                "INTERSPEECH",
                "Conference of the International Speech Communication Association"
            ],
            "issn": "2308-457X",
            "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
            "alternate_urls": [
                "http://www.isca-speech.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/1bd7d2932de819ed1087b6453ef2c0be9f781ac1",
        "title": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context",
        "abstract": "Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6% without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.",
        "venue": "Interspeech",
        "year": 2020,
        "referenceCount": 39,
        "citationCount": 204,
        "influentialCitationCount": 20,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2005.03191",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy and demonstrates that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate of 2.1%/4.6%."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-05-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2005.03191"
        },
        "citationStyles": {
            "bibtex": "@Article{Han2020ContextNetIC,\n author = {Wei Han and Zhengdong Zhang and Yu Zhang and Jiahui Yu and Chung-Cheng Chiu and James Qin and Anmol Gulati and Ruoming Pang and Yonghui Wu},\n booktitle = {Interspeech},\n journal = {ArXiv},\n title = {ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context},\n volume = {abs/2005.03191},\n year = {2020}\n}\n"
        }
    },
    "16_kohonen_network": {
        "paperId": "3bd14b435399ef4c41ee3499e8cbd4b475daff4e",
        "externalIds": {
            "MAG": "65738273",
            "DOI": "10.1007/BF00337288",
            "CorpusId": 206775459
        },
        "corpusId": 206775459,
        "publicationVenue": {
            "id": "57cada26-a03e-494e-929e-a71ac35f2ad0",
            "name": "Biological cybernetics",
            "type": "journal",
            "alternate_names": [
                "Biological cybern",
                "Biological Cybern",
                "Biological Cybernetics"
            ],
            "issn": "0340-1200",
            "url": "http://link.springer.com/journal/422",
            "alternate_urls": [
                "https://link.springer.com/journal/volumesAndIssues/422"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/3bd14b435399ef4c41ee3499e8cbd4b475daff4e",
        "title": "Self-organized formation of topologically correct feature maps",
        "abstract": null,
        "venue": "Biological cybernetics",
        "year": 2004,
        "referenceCount": 22,
        "citationCount": 7659,
        "influentialCitationCount": 550,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "In a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "Biological Cybernetics",
            "pages": "59-69",
            "volume": "43"
        },
        "citationStyles": {
            "bibtex": "@Article{Kohonen2004SelforganizedFO,\n author = {T. Kohonen},\n booktitle = {Biological cybernetics},\n journal = {Biological Cybernetics},\n pages = {59-69},\n title = {Self-organized formation of topologically correct feature maps},\n volume = {43},\n year = {2004}\n}\n"
        }
    },
    "17_resnet-110_(cifar-10)": {
        "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "externalIds": {
            "DBLP": "conf/cvpr/HeZRS16",
            "MAG": "2949650786",
            "ArXiv": "1512.03385",
            "DOI": "10.1109/cvpr.2016.90",
            "CorpusId": 206594692
        },
        "corpusId": 206594692,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "title": "Deep Residual Learning for Image Recognition",
        "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "referenceCount": 53,
        "citationCount": 154626,
        "influentialCitationCount": 27115,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://repositorio.unal.edu.co/bitstream/unal/81443/1/98670607.2022.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-12-10",
        "journal": {
            "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "770-778"
        },
        "citationStyles": {
            "bibtex": "@Article{He2015DeepRL,\n author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {770-778},\n title = {Deep Residual Learning for Image Recognition},\n year = {2015}\n}\n"
        }
    },
    "18_gpt": {
        "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
        "externalIds": {
            "MAG": "2965425874",
            "CorpusId": 49313245
        },
        "corpusId": 49313245,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
        "title": "Improving Language Understanding by Generative Pre-Training",
        "abstract": "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classi\ufb01cation. Although large unlabeled text corpora are abundant, labeled data for learning these speci\ufb01c tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative \ufb01ne-tuning on each speci\ufb01c task. In contrast to previous approaches, we make use of task-aware input transformations during \ufb01ne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speci\ufb01cally crafted for each task, signi\ufb01cantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).",
        "venue": "",
        "year": 2018,
        "referenceCount": 73,
        "citationCount": 7850,
        "influentialCitationCount": 971,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The general task-agnostic model outperforms discriminatively trained models that use architectures speci\ufb01cally crafted for each task, improving upon the state of the art in 9 out of the 12 tasks studied."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Radford2018ImprovingLU,\n author = {Alec Radford and Karthik Narasimhan},\n title = {Improving Language Understanding by Generative Pre-Training},\n year = {2018}\n}\n"
        }
    },
    "19_alexnet_+_coordinating_filters": {
        "paperId": "75419af744219b9d12c3b6f7fc2a904296d16c23",
        "externalIds": {
            "DBLP": "journals/corr/WenXWWCL17",
            "MAG": "2952588839",
            "ArXiv": "1703.09746",
            "DOI": "10.1109/ICCV.2017.78",
            "CorpusId": 6863796
        },
        "corpusId": 6863796,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/75419af744219b9d12c3b6f7fc2a904296d16c23",
        "title": "Coordinating Filters for Faster Deep Neural Networks",
        "abstract": "Very large-scale Deep Neural Networks (DNNs) have achieved remarkable successes in a large variety of computer vision tasks. However, the high computation intensity of DNNs makes it challenging to deploy these models on resource-limited systems. Some studies used low-rank approaches that approximate the filters by low-rank basis to accelerate the testing. Those works directly decomposed the pre-trained DNNs by Low-Rank Approximations (LRA). How to train DNNs toward lower-rank space for more efficient DNNs, however, remains as an open area. To solve the issue, in this work, we propose Force Regularization, which uses attractive forces to enforce filters so as to coordinate more weight information into lower-rank space1. We mathematically and empirically verify that after applying our technique, standard LRA methods can reconstruct filters using much lower basis and thus result in faster DNNs. The effectiveness of our approach is comprehensively evaluated in ResNets, AlexNet, and GoogLeNet. In AlexNet, for example, Force Regularization gains 2\u00d7 speedup on modern GPU without accuracy loss and 4:05\u00d7 speedup on CPU by paying small accuracy degradation. Moreover, Force Regularization better initializes the low-rank DNNs such that the fine-tuning can converge faster toward higher accuracy. The obtained lower-rank DNNs can be further sparsified, proving that Force Regularization can be integrated with state-of-the-art sparsity-based acceleration methods.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "referenceCount": 30,
        "citationCount": 127,
        "influentialCitationCount": 9,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1703.09746",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Force Regularization, which uses attractive forces to enforce filters so as to coordinate more weight information into lower-rank space, is proposed and mathematically and empirically verified that after applying this technique, standard LRA methods can reconstruct filters using much lower basis and thus result in faster DNNs."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-03-28",
        "journal": {
            "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
            "pages": "658-666"
        },
        "citationStyles": {
            "bibtex": "@Article{Wen2017CoordinatingFF,\n author = {W. Wen and Cong Xu and Chunpeng Wu and Yandan Wang and Yiran Chen and Hai Helen Li},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {658-666},\n title = {Coordinating Filters for Faster Deep Neural Networks},\n year = {2017}\n}\n"
        }
    },
    "20_deepspeech2_(english)": {
        "paperId": "13497bd108d4412d02050e646235f456568cf822",
        "externalIds": {
            "MAG": "2949640717",
            "DBLP": "journals/corr/AmodeiABCCCCCCD15",
            "ArXiv": "1512.02595",
            "CorpusId": 11590585
        },
        "corpusId": 11590585,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/13497bd108d4412d02050e646235f456568cf822",
        "title": "Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin",
        "abstract": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech-two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "referenceCount": 75,
        "citationCount": 2738,
        "influentialCitationCount": 241,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech-two vastly different languages, and is competitive with the transcription of human workers when benchmarked on standard datasets."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-12-08",
        "journal": {
            "pages": "173-182"
        },
        "citationStyles": {
            "bibtex": "@Article{Amodei2015DeepS2,\n author = {Dario Amodei and S. Ananthanarayanan and Rishita Anubhai and Jin Bai and Eric Battenberg and Carl Case and J. Casper and Bryan Catanzaro and Jingdong Chen and Mike Chrzanowski and Adam Coates and G. Diamos and Erich Elsen and Jesse Engel and Linxi (Jim) Fan and Christopher Fougner and Awni Y. Hannun and Billy Jun and T. Han and P. LeGresley and Xiangang Li and Libby Lin and Sharan Narang and A. Ng and Sherjil Ozair and R. Prenger and Sheng Qian and Jonathan Raiman and S. Satheesh and David Seetapun and Shubho Sengupta and Anuroop Sriram and Chong-Jun Wang and Yi Wang and Zhiqian Wang and Bo Xiao and Yan Xie and Dani Yogatama and J. Zhan and Zhenyao Zhu},\n booktitle = {International Conference on Machine Learning},\n pages = {173-182},\n title = {Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin},\n year = {2015}\n}\n"
        }
    },
    "21_lstm(large)+sememe+cell": {
        "paperId": "a2c6c1c840ea8bbd1568bd9ae80bdf1b6fd74b11",
        "externalIds": {
            "DBLP": "journals/taslp/QinQOLYWLS20",
            "MAG": "3046008773",
            "DOI": "10.1109/TASLP.2020.3012060",
            "CorpusId": 221179413
        },
        "corpusId": 221179413,
        "publicationVenue": {
            "id": "309e00f7-4bbd-461f-ab37-a90cd14ef21d",
            "name": "IEEE/ACM Transactions on Audio Speech and Language Processing",
            "alternate_names": [
                "IEEE/ACM Trans Audio Speech Lang Process"
            ],
            "issn": "2329-9290",
            "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=6570655",
            "alternate_urls": [
                "https://signalprocessingsociety.org/publications-resources/ieeeacm-transactions-audio-speech-and-language-processing/ieeeacm"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/a2c6c1c840ea8bbd1568bd9ae80bdf1b6fd74b11",
        "title": "Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes",
        "abstract": "Sememes, the minimum semantic units of human languages, have been successfully utilized in various natural language processing applications. However, most existing studies exploit sememes in specific tasks and few efforts are made to utilize sememes more fundamentally. In this paper, we propose to incorporate sememes into recurrent neural networks (RNNs) to improve their sequence modeling ability, which is beneficial to all kinds of downstream tasks. We design three different sememe incorporation methods and employ them in typical RNNs including LSTM, GRU and their bidirectional variants. In evaluation, we use several benchmark datasets involving PTB and WikiText-2 for language modeling, SNLI for natural language inference and another two datasets for sentiment analysis and paraphrase detection. Experimental results show evident and consistent improvement of our sememe-incorporated models compared with vanilla RNNs, which proves the effectiveness of our sememe incorporation methods. Moreover, we find the sememe-incorporated models have higher robustness and outperform adversarial training in defending adversarial attack. All the code and data of this work can be obtained at https://github.com/thunlp/SememeRNN.",
        "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "year": 2019,
        "referenceCount": 54,
        "citationCount": 19,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1910.08910",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes to incorporate sememes into recurrent neural networks (RNNs) to improve their sequence modeling ability, which is beneficial to all kinds of downstream tasks and finds the sememe-incorporated models have higher robustness and outperform adversarial training in defending adversarial attack."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-10-20",
        "journal": {
            "name": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
            "pages": "2364-2373",
            "volume": "28"
        },
        "citationStyles": {
            "bibtex": "@Article{Qin2019ImprovingSM,\n author = {Yujia Qin and Fanchao Qi and Sicong Ouyang and Zhiyuan Liu and Cheng Yang and Yasheng Wang and Qun Liu and Maosong Sun},\n booktitle = {IEEE/ACM Transactions on Audio Speech and Language Processing},\n journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},\n pages = {2364-2373},\n title = {Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes},\n volume = {28},\n year = {2019}\n}\n"
        }
    },
    "22_adaptive_inputs_+_layerdrop": {
        "paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
        "externalIds": {
            "MAG": "2996159613",
            "DBLP": "conf/iclr/FanGJ20",
            "ArXiv": "1909.11556",
            "CorpusId": 202750230
        },
        "corpusId": 202750230,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
        "title": "Reducing Transformer Depth on Demand with Structured Dropout",
        "abstract": "Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 69,
        "citationCount": 461,
        "influentialCitationCount": 64,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "LayerDrop, a form of structured dropout, is explored, which has a regularization effect during training and allows for efficient pruning at inference time, and shows that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-09-25",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1909.11556"
        },
        "citationStyles": {
            "bibtex": "@Article{Fan2019ReducingTD,\n author = {Angela Fan and Edouard Grave and Armand Joulin},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Reducing Transformer Depth on Demand with Structured Dropout},\n volume = {abs/1909.11556},\n year = {2019}\n}\n"
        }
    },
    "24_neural_cache_model_(size=2000)_(300m)": {
        "paperId": "2d7782c225e0fc123d6e227f2cb253e58279ac73",
        "externalIds": {
            "DBLP": "conf/iclr/GraveJU17",
            "MAG": "2571859396",
            "ArXiv": "1612.04426",
            "CorpusId": 8693672
        },
        "corpusId": 8693672,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/2d7782c225e0fc123d6e227f2cb253e58279ac73",
        "title": "Improving Neural Language Models with a Continuous Cache",
        "abstract": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "referenceCount": 53,
        "citationCount": 284,
        "influentialCitationCount": 41,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation, which is very efficient and scales to very large memory sizes."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-11-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1612.04426"
        },
        "citationStyles": {
            "bibtex": "@Article{Grave2016ImprovingNL,\n author = {Edouard Grave and Armand Joulin and Nicolas Usunier},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Improving Neural Language Models with a Continuous Cache},\n volume = {abs/1612.04426},\n year = {2016}\n}\n"
        }
    },
    "25_gpt-sw3": {
        "paperId": "a3837000307940d66b28b617735c59aea585c360",
        "externalIds": {
            "ACL": "2022.lrec-1.376",
            "DBLP": "conf/lrec/EkgrenGGHVOCS22",
            "CorpusId": 252386188
        },
        "corpusId": 252386188,
        "publicationVenue": {
            "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
            "name": "International Conference on Language Resources and Evaluation",
            "type": "conference",
            "alternate_names": [
                "LREC",
                "Int Conf Lang Resour Evaluation"
            ],
            "url": "http://www.lrec-conf.org/"
        },
        "url": "https://www.semanticscholar.org/paper/a3837000307940d66b28b617735c59aea585c360",
        "title": "Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish",
        "abstract": "We present GTP-SW3, a 3.5 billion parameter autoregressive language model, trained on a newly created 100 GB Swedish corpus. This paper provides insights with regards to data collection and training, while highlights the challenges of proper model evaluation. The results of quantitive evaluation through perplexity indicate that GPT-SW3 is a competent model in comparison with existing autoregressive models of similar size. Additionally, we perform an extensive prompting study which reveals the good text generation capabilities of GTP-SW3.",
        "venue": "International Conference on Language Resources and Evaluation",
        "year": 2022,
        "referenceCount": 29,
        "citationCount": 8,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The results of quantitive evaluation through perplexity indicate that GPT-SW3 is a competent model in comparison with existing autoregressive models of similar size."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "3509-3518"
        },
        "citationStyles": {
            "bibtex": "@Article{Ekgren2022LessonsLF,\n author = {Ariel Ekgren and Amaru Cuba Gyllensten and Evangelia Gogoulou and Alice Heiman and S. Verlinden and Joey \u00d6hman and Fredrik Carlsson and Magnus Sahlgren},\n booktitle = {International Conference on Language Resources and Evaluation},\n pages = {3509-3518},\n title = {Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish},\n year = {2022}\n}\n"
        }
    },
    "26_make-a-video": {
        "paperId": "1e33716e8820b867d5a8aaebab44c2d3135ea4ac",
        "externalIds": {
            "ArXiv": "2209.14792",
            "DBLP": "journals/corr/abs-2209-14792",
            "CorpusId": 252595919
        },
        "corpusId": 252595919,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/1e33716e8820b867d5a8aaebab44c2d3135ea4ac",
        "title": "Make-A-Video: Text-to-Video Generation without Text-Video Data",
        "abstract": "We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 51,
        "citationCount": 555,
        "influentialCitationCount": 52,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-09-29",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2209.14792"
        },
        "citationStyles": {
            "bibtex": "@Article{Singer2022MakeAVideoTG,\n author = {Uriel Singer and Adam Polyak and Thomas Hayes and Xiaoyue Yin and Jie An and Songyang Zhang and Qiyuan Hu and Harry Yang and Oron Ashual and Oran Gafni and Devi Parikh and Sonal Gupta and Yaniv Taigman},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Make-A-Video: Text-to-Video Generation without Text-Video Data},\n volume = {abs/2209.14792},\n year = {2022}\n}\n"
        }
    },
    "27_alstm(depth-2)+recurrentpolicy_(wt2)": {
        "paperId": "a07b76ed236dc23b4437ab33d751057f043d0f5e",
        "externalIds": {
            "MAG": "2804255934",
            "DBLP": "conf/nips/FlennerhagYKE18",
            "ArXiv": "1805.08574",
            "CorpusId": 44135919
        },
        "corpusId": 44135919,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/a07b76ed236dc23b4437ab33d751057f043d0f5e",
        "title": "Breaking the Activation Function Bottleneck through Adaptive Parameterization",
        "abstract": "Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and WikiText-2 word-modeling tasks while using fewer parameters and converging in less than half as many iterations.",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "referenceCount": 56,
        "citationCount": 11,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An adaptive LSTM is presented that advances the state of the art for the Penn Treebank and WikiText-2 word-modeling tasks while using fewer parameters and converging in less than half as many iterations."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-05-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1805.08574"
        },
        "citationStyles": {
            "bibtex": "@Article{Flennerhag2018BreakingTA,\n author = {Sebastian Flennerhag and Hujun Yin and J. Keane and M. Elliot},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Breaking the Activation Function Bottleneck through Adaptive Parameterization},\n volume = {abs/1805.08574},\n year = {2018}\n}\n"
        }
    },
    "28_menace": {
        "paperId": "69d7108c6c9daace884e3d2d533ee7dfcedad375",
        "externalIds": {
            "DBLP": "journals/cj/Michie63",
            "MAG": "2318460192",
            "DOI": "10.1093/COMJNL/6.3.232",
            "CorpusId": 62514450
        },
        "corpusId": 62514450,
        "publicationVenue": {
            "id": "aa746a02-d187-42c2-bdf9-df6a5d4e648c",
            "name": "Computer/law journal",
            "type": "journal",
            "alternate_names": [
                "Computer journal",
                "The Computer Journal",
                "Comput j",
                "Comput J"
            ],
            "issn": "0164-8756",
            "alternate_issns": [
                "0010-4620"
            ],
            "url": "https://repository.jmls.edu/jitpl/all_issues.html",
            "alternate_urls": [
                "http://comjnl.oxfordjournals.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/69d7108c6c9daace884e3d2d533ee7dfcedad375",
        "title": "Experiments on the Mechanization of Game-Learning Part I. Characterization of the Model and its parameters",
        "abstract": "This paper describes a trial-and-error device which learns to play the game of Noughts and Crosses. It was initially constructed front matchboxes and coloured beads and subsequently simulated in essentials by a program for a Pegasus 2 computer. The parameters governing the adaptive behaviour of this automaton are described and preliminary observations on its performance are briefly reported.",
        "venue": "Computer/law journal",
        "year": 1963,
        "referenceCount": 0,
        "citationCount": 28,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://academic.oup.com/comjnl/article-pdf/6/3/232/1030939/6-3-232.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1963-11-01",
        "journal": {
            "name": "Comput. J.",
            "pages": "232-236",
            "volume": "6"
        },
        "citationStyles": {
            "bibtex": "@Article{Michie1963ExperimentsOT,\n author = {D. Michie},\n booktitle = {Computer/law journal},\n journal = {Comput. J.},\n pages = {232-236},\n title = {Experiments on the Mechanization of Game-Learning Part I. Characterization of the Model and its parameters},\n volume = {6},\n year = {1963}\n}\n"
        }
    },
    "29_gshard_(600b)": {
        "paperId": "1882f194cb43828852cc052887671e55a80f945a",
        "externalIds": {
            "MAG": "3040573126",
            "DBLP": "conf/iclr/LepikhinLXCFHKS21",
            "ArXiv": "2006.16668",
            "CorpusId": 220265858
        },
        "corpusId": 220265858,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/1882f194cb43828852cc052887671e55a80f945a",
        "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "abstract": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "referenceCount": 99,
        "citationCount": 612,
        "influentialCitationCount": 103,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding and it is demonstrated that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-06-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2006.16668"
        },
        "citationStyles": {
            "bibtex": "@Article{Lepikhin2020GShardSG,\n author = {Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and M. Krikun and Noam M. Shazeer and Z. Chen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},\n volume = {abs/2006.16668},\n year = {2020}\n}\n"
        }
    },
    "30_bayesian_starcraft": {
        "paperId": "7aa844e49fa7f67ff3172137ca57b18fb7b25219",
        "externalIds": {
            "DBLP": "conf/cig/SynnaeveB11",
            "MAG": "2013907475",
            "DOI": "10.1109/CIG.2011.6032006",
            "CorpusId": 215811566
        },
        "corpusId": 215811566,
        "publicationVenue": {
            "id": "6fc6b436-90f0-4b58-a339-85aed71b82f7",
            "name": "IEEE Conference on Computational Intelligence and Games",
            "type": "conference",
            "alternate_names": [
                "CIG",
                "IEEE Conf Comput Intell Game",
                "Comput Intell Game",
                "Computational Intelligence and Games"
            ],
            "url": "http://www.ieee-cig.org/"
        },
        "url": "https://www.semanticscholar.org/paper/7aa844e49fa7f67ff3172137ca57b18fb7b25219",
        "title": "A Bayesian model for RTS units control applied to StarCraft",
        "abstract": "In real-time strategy games (RTS), the player must reason about high-level strategy and planning while having effective tactics and even individual units micro-management. Enabling an artificial agent to deal with such a task entails breaking down the complexity of this environment. For that, we propose to control units locally in the Bayesian sensory motor robot fashion, with higher level orders integrated as perceptions. As complete inference encompassing global strategy down to individual unit needs is intractable, we embrace incompleteness through a hierarchical model able to deal with uncertainty. We developed and applied our approach on a StarCraft1 AI.",
        "venue": "IEEE Conference on Computational Intelligence and Games",
        "year": 2011,
        "referenceCount": 26,
        "citationCount": 68,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://hal.archives-ouvertes.fr/hal-00607281/file/BayesianUnit.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes to control units locally in the Bayesian sensory motor robot fashion, with higher level orders integrated as perceptions, through a hierarchical model able to deal with uncertainty in real-time strategy games."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2011-09-29",
        "journal": {
            "name": "2011 IEEE Conference on Computational Intelligence and Games (CIG'11)",
            "pages": "190-196"
        },
        "citationStyles": {
            "bibtex": "@Article{Synnaeve2011ABM,\n author = {Gabriel Synnaeve and P. Bessi\u00e8re},\n booktitle = {IEEE Conference on Computational Intelligence and Games},\n journal = {2011 IEEE Conference on Computational Intelligence and Games (CIG'11)},\n pages = {190-196},\n title = {A Bayesian model for RTS units control applied to StarCraft},\n year = {2011}\n}\n"
        }
    },
    "31_conformer": {
        "paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e",
        "externalIds": {
            "MAG": "3025165719",
            "DBLP": "journals/corr/abs-2005-08100",
            "ArXiv": "2005.08100",
            "DOI": "10.21437/interspeech.2020-3015",
            "CorpusId": 218674528
        },
        "corpusId": 218674528,
        "publicationVenue": {
            "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
            "name": "Interspeech",
            "type": "conference",
            "alternate_names": [
                "Conf Int Speech Commun Assoc",
                "INTERSPEECH",
                "Conference of the International Speech Communication Association"
            ],
            "issn": "2308-457X",
            "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
            "alternate_urls": [
                "http://www.isca-speech.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/0170fc76e934ee643f869df18fb617d5357e8b4e",
        "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
        "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
        "venue": "Interspeech",
        "year": 2020,
        "referenceCount": 36,
        "citationCount": 2043,
        "influentialCitationCount": 318,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2005.08100",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes the convolution-augmented transformer for speech recognition, named Conformer, which significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-05-16",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2005.08100"
        },
        "citationStyles": {
            "bibtex": "@Article{Gulati2020ConformerCT,\n author = {Anmol Gulati and James Qin and Chung-Cheng Chiu and Niki Parmar and Yu Zhang and Jiahui Yu and Wei Han and Shibo Wang and Zhengdong Zhang and Yonghui Wu and Ruoming Pang},\n booktitle = {Interspeech},\n journal = {ArXiv},\n title = {Conformer: Convolution-augmented Transformer for Speech Recognition},\n volume = {abs/2005.08100},\n year = {2020}\n}\n"
        }
    },
    "34_seer": {
        "paperId": "0f8aa47ff8c6c49a347e192debe20ce4e5a4caea",
        "externalIds": {
            "ArXiv": "2103.01988",
            "DBLP": "journals/corr/abs-2103-01988",
            "CorpusId": 232104826
        },
        "corpusId": 232104826,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0f8aa47ff8c6c49a347e192debe20ce4e5a4caea",
        "title": "Self-supervised Pretraining of Visual Features in the Wild",
        "abstract": "Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by 1% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9% top-1 with access to only 10% of ImageNet. Code: https://github.com/facebookresearch/vissl",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 58,
        "citationCount": 212,
        "influentialCitationCount": 19,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work explores if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision, and observes that self- supervised models are good few-shot learners."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-03-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2103.01988"
        },
        "citationStyles": {
            "bibtex": "@Article{Goyal2021SelfsupervisedPO,\n author = {Priya Goyal and Mathilde Caron and Benjamin Lefaudeux and Min Xu and Pengchao Wang and Vivek Pai and Mannat Singh and Vitaliy Liptchinsky and Ishan Misra and Armand Joulin and Piotr Bojanowski},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Self-supervised Pretraining of Visual Features in the Wild},\n volume = {abs/2103.01988},\n year = {2021}\n}\n"
        }
    },
    "35_ggnn": {
        "paperId": "f16164799a6eb3abe414d922dbb37c3ae314bea1",
        "externalIds": {
            "PubMedCentral": "10457366",
            "ArXiv": "2212.03447",
            "DOI": "10.1038/s42003-023-05133-1",
            "CorpusId": 254366396,
            "PubMed": "37626165"
        },
        "corpusId": 254366396,
        "publicationVenue": {
            "id": "069e05e7-ca35-41d0-a8c7-bdc9ec6a82af",
            "name": "Communications Biology",
            "alternate_names": [
                "Commun Biology"
            ],
            "issn": "2399-3642",
            "url": "http://www.nature.com/commsbio/"
        },
        "url": "https://www.semanticscholar.org/paper/f16164799a6eb3abe414d922dbb37c3ae314bea1",
        "title": "Integration of pre-trained protein language models into geometric deep learning networks",
        "abstract": null,
        "venue": "Communications Biology",
        "year": 2022,
        "referenceCount": 71,
        "citationCount": 9,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.nature.com/articles/s42003-023-05133-1.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Strong evidence indicates that the incorporation of protein language models\u2019 knowledge enhances geometric networks\u2019 capacity by a significant margin and can be generalized to complex tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-12-07",
        "journal": {
            "name": "Communications Biology",
            "volume": "6"
        },
        "citationStyles": {
            "bibtex": "@Article{Wu2022IntegrationOP,\n author = {Fang Wu and Yujun Tao and Dragomir R. Radev and Jinbo Xu},\n booktitle = {Communications Biology},\n journal = {Communications Biology},\n title = {Integration of pre-trained protein language models into geometric deep learning networks},\n volume = {6},\n year = {2022}\n}\n"
        }
    },
    "36_ctc-trained_lstm": {
        "paperId": "261a056f8b21918e8616a429b2df6e1d5d33be41",
        "externalIds": {
            "CorpusId": 9901844
        },
        "corpusId": 9901844,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/261a056f8b21918e8616a429b2df6e1d5d33be41",
        "title": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
        "abstract": "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un-segmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.",
        "venue": "",
        "year": null,
        "referenceCount": 18,
        "citationCount": 4538,
        "influentialCitationCount": 760,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": null,
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a novel method for training RNNs to label un-segmented sequences directly, thereby solving both problems of sequence learning and post-processing."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": null,
        "citationStyles": {
            "bibtex": "@Misc{None,\n author = {Alex Graves and Santiago Fern\u00b4andez and Faustino J. Gomez and J\u00a8urgen Schmidhuber},\n title = {Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks}\n}\n"
        }
    },
    "38_megatron-lm_(8.3b)": {
        "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "externalIds": {
            "MAG": "2973727699",
            "ArXiv": "1909.08053",
            "DBLP": "journals/corr/abs-1909-08053",
            "CorpusId": 202660670
        },
        "corpusId": 202660670,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
        "abstract": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 62,
        "citationCount": 1112,
        "influentialCitationCount": 170,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters and shows that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-09-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1909.08053"
        },
        "citationStyles": {
            "bibtex": "@Article{Shoeybi2019MegatronLMTM,\n author = {M. Shoeybi and M. Patwary and Raul Puri and P. LeGresley and J. Casper and Bryan Catanzaro},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},\n volume = {abs/1909.08053},\n year = {2019}\n}\n"
        }
    },
    "39_pattern_recognition_and_reading_by_machine": {
        "paperId": "f6e7311b9e560f3a12c895b751d275bac161b31b",
        "externalIds": {
            "DBLP": "conf/aieeire/BledsoeB59",
            "MAG": "1995426156",
            "DOI": "10.1145/1460299.1460326",
            "CorpusId": 15672245
        },
        "corpusId": 15672245,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/f6e7311b9e560f3a12c895b751d275bac161b31b",
        "title": "Pattern recognition and reading by machine",
        "abstract": "Many efforts have been made to discriminate, categorize, and quantitate patterns, and to reduce them into a usable machine language. The results have ordinarily been methods or devices with a high degree of specificity. For example, some devices require a special type font; others can read only one type font; still others require magnetic ink.",
        "venue": "IRE-AIEE-ACM '59 (Eastern)",
        "year": 1959,
        "referenceCount": 0,
        "citationCount": 407,
        "influentialCitationCount": 15,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.1145/1460299.1460326",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Many efforts have been made to discriminate, categorize, and quantitate patterns, and to reduce them into a usable machine language, and the results have ordinarily been methods or devices with a high degree of specificity."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1959-12-01",
        "journal": {
            "pages": "225-232"
        },
        "citationStyles": {
            "bibtex": "@Article{Bledsoe1959PatternRA,\n author = {W. Bledsoe and I. Browning},\n booktitle = {IRE-AIEE-ACM '59 (Eastern)},\n pages = {225-232},\n title = {Pattern recognition and reading by machine},\n year = {1959}\n}\n"
        }
    },
    "40_progen": {
        "paperId": "c5f7074a264356c9a022a8dff24df79d1db8c3d3",
        "externalIds": {
            "ArXiv": "2004.03497",
            "MAG": "3015531900",
            "DBLP": "journals/corr/abs-2004-03497",
            "DOI": "10.1101/2020.03.07.982272",
            "CorpusId": 214725226
        },
        "corpusId": 214725226,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/c5f7074a264356c9a022a8dff24df79d1db8c3d3",
        "title": "ProGen: Language Modeling for Protein Generation",
        "abstract": "Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on \u223c280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.",
        "venue": "bioRxiv",
        "year": 2020,
        "referenceCount": 53,
        "citationCount": 214,
        "influentialCitationCount": 9,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2020/03/13/2020.03.07.982272.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Biology",
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            },
            {
                "category": "Materials Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work poses protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations and trains a 1.2B-parameter language model, ProGen, on \u223c280M protein sequences conditioned on taxonomic and keyword tags."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-03-08",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Madani2020ProGenLM,\n author = {Ali Madani and Bryan McCann and N. Naik and N. Keskar and N. Anand and Raphael R. Eguchi and Po-Ssu Huang and R. Socher},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {ProGen: Language Modeling for Protein Generation},\n year = {2020}\n}\n"
        }
    },
    "41_adaptive_input_transformer_+_rd": {
        "paperId": "520bd2331cca8d5a9c032c186a2a0f7704ead6ff",
        "externalIds": {
            "ArXiv": "2106.14448",
            "DBLP": "journals/corr/abs-2106-14448",
            "CorpusId": 235658346
        },
        "corpusId": 235658346,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/520bd2331cca8d5a9c032c186a2a0f7704ead6ff",
        "title": "R-Drop: Regularized Dropout for Neural Networks",
        "abstract": "Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on $\\bf{5}$ widely used deep learning tasks ($\\bf{18}$ datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English$\\to$German translation ($\\bf{30.91}$ BLEU) and WMT14 English$\\to$French translation ($\\bf{43.95}$ BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub{\\url{https://github.com/dropreg/R-Drop}}.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 101,
        "citationCount": 279,
        "influentialCitationCount": 42,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "R-Drop is introduced, which forces the output distributions of different sub models generated by dropout to be consistent with each other in model training, and yields substantial improvements when applied to fine-tune large-scale pre-trained models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-06-28",
        "journal": {
            "pages": "10890-10905"
        },
        "citationStyles": {
            "bibtex": "@Article{Liang2021RDropRD,\n author = {Xiaobo Liang and Lijun Wu and Juntao Li and Yue Wang and Qi Meng and Tao Qin and Wei Chen and M. Zhang and Tie-Yan Liu},\n booktitle = {Neural Information Processing Systems},\n pages = {10890-10905},\n title = {R-Drop: Regularized Dropout for Neural Networks},\n year = {2021}\n}\n"
        }
    },
    "43_selfish-rnn_(snt-asgd)_stacked_lstms": {
        "paperId": "4df2175c0daadf630623a505f623fe41a386853d",
        "externalIds": {
            "ArXiv": "2101.09048",
            "DBLP": "conf/icml/LiuMPP21",
            "CorpusId": 231693152
        },
        "corpusId": 231693152,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/4df2175c0daadf630623a505f623fe41a386853d",
        "title": "Selfish Sparse RNN Training",
        "abstract": "Sparse neural networks have been widely applied to reduce the computational demands of training and deploying over-parameterized deep neural networks. For inference acceleration, methods that discover a sparse network from a pre-trained dense network (dense-to-sparse training) work effectively. Recently, dynamic sparse training (DST) has been proposed to train sparse neural networks without pre-training a dense model (sparse-to-sparse training), so that the training process can also be accelerated. However, previous sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs), failing to match the performance of dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In this paper, we propose an approach to train intrinsically sparse RNNs with a fixed parameter count in one single run, without compromising performance. During training, we allow RNN layers to have a non-uniform redistribution across cell gates for better regularization. Further, we propose SNT-ASGD, a novel variant of the averaged stochastic gradient optimizer, which significantly improves the performance of all sparse training methods for RNNs. Using these strategies, we achieve state-of-the-art sparse training results, better than the dense-to-sparse methods, with various types of RNNs on Penn TreeBank and Wikitext-2 datasets. Our codes are available at https://github.com/Shiweiliuiiiiiii/Selfish-RNN.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 77,
        "citationCount": 34,
        "influentialCitationCount": 7,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes SNT-ASGD, a novel variant of the averaged stochastic gradient optimizer, which significantly improves the performance of all sparse training methods for RNNs, and achieves state-of-the-art sparse training results, better than the dense-to-sparse methods."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-01-22",
        "journal": {
            "pages": "6893-6904"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2021SelfishSR,\n author = {Shiwei Liu and D. Mocanu and Yulong Pei and Mykola Pechenizkiy},\n booktitle = {International Conference on Machine Learning},\n pages = {6893-6904},\n title = {Selfish Sparse RNN Training},\n year = {2021}\n}\n"
        }
    },
    "45_punish_reward": {
        "paperId": "6587a531da06b9cef73e93b6b7627e466ad51d1b",
        "externalIds": {
            "MAG": "2084378698",
            "DBLP": "journals/tsmc/WidrowGM73",
            "DOI": "10.1109/TSMC.1973.4309272",
            "CorpusId": 1487792
        },
        "corpusId": 1487792,
        "publicationVenue": {
            "id": "336446b6-e859-4f7b-9121-d2d40357fe0a",
            "name": "IEEE Transactions on Systems, Man and Cybernetics",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Syst Man Cybern",
                "IEEE Transactions on Systems, Man, and Cybernetics"
            ],
            "issn": "0018-9472",
            "alternate_issns": [
                "2168-2909"
            ],
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=21"
        },
        "url": "https://www.semanticscholar.org/paper/6587a531da06b9cef73e93b6b7627e466ad51d1b",
        "title": "Punish/Reward: Learning with a Critic in Adaptive Threshold Systems",
        "abstract": "An adaptive threshold element is able to \"learn\" a strategy of play for the game blackjack (twenty-one) with a performance close to that of the Thorp optimal strategy although the adaptive system has no prior knowledge of the game and of the objective of play. After each winning game the decisions of the adaptive system are \"rewarded.\" After each losing game the decisions are \"punished.\" Reward is accomplished by adapting while accepting the actual decision as the desired response. Punishment is accomplished by adapting while taking the desired response to be the opposite of that of the actual decision. This learning scheme is unlike \"learning with a teacher\" and unlike \"unsupervised learning.\" It involves \"bootstrap adaptation\" or \"learning with a critic.\" The critic rewards decisions which are members of successful chains of decisions and punishes other decisions. A general analytical model for learning with a critic is formulated and analyzed. The model represents bootstrap learning per se. Although the hypotheses on which the model is based do not perfectly fit blackjack learning, it is applied heuristically to predict adaptation rates with good experimental success. New applications are being explored for bootstrap learning in adaptive controls and multilayered adaptive systems.",
        "venue": "IEEE Transactions on Systems, Man and Cybernetics",
        "year": 1973,
        "referenceCount": 34,
        "citationCount": 305,
        "influentialCitationCount": 6,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An adaptive threshold element is able to \"learn\" a strategy of play for the game blackjack (twenty-one) with a performance close to that of the Thorp optimal strategy although the adaptive system has no prior knowledge of the game and of the objective of play."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1973-09-01",
        "journal": {
            "name": "IEEE Trans. Syst. Man Cybern.",
            "pages": "455-465",
            "volume": "3"
        },
        "citationStyles": {
            "bibtex": "@Article{Widrow1973PunishRewardLW,\n author = {B. Widrow and Narendra K. Gupta and S. Maitra},\n booktitle = {IEEE Transactions on Systems, Man and Cybernetics},\n journal = {IEEE Trans. Syst. Man Cybern.},\n pages = {455-465},\n title = {Punish/Reward: Learning with a Critic in Adaptive Threshold Systems},\n volume = {3},\n year = {1973}\n}\n"
        }
    },
    "46_rfa-gate-gaussian-stateful_big": {
        "paperId": "9ed25f101f19ea735ca300848948ed64064b97ca",
        "externalIds": {
            "ArXiv": "2103.02143",
            "DBLP": "journals/corr/abs-2103-02143",
            "CorpusId": 232105052
        },
        "corpusId": 232105052,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9ed25f101f19ea735ca300848948ed64064b97ca",
        "title": "Random Feature Attention",
        "abstract": "Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "referenceCount": 79,
        "citationCount": 254,
        "influentialCitationCount": 24,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, is proposed and explored, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-03-03",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2103.02143"
        },
        "citationStyles": {
            "bibtex": "@Article{Peng2021RandomFA,\n author = {Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah A. Smith and Lingpeng Kong},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Random Feature Attention},\n volume = {abs/2103.02143},\n year = {2021}\n}\n"
        }
    },
    "47_diffstk-mrnn": {
        "paperId": "110116481848d8d2e371652a11874635024426f0",
        "externalIds": {
            "DBLP": "journals/corr/abs-2004-07623",
            "MAG": "3017057183",
            "ArXiv": "2004.07623",
            "CorpusId": 215786191
        },
        "corpusId": 215786191,
        "publicationVenue": {
            "id": "ccb36bb0-2502-400e-b912-bc274eefc49b",
            "name": "International Conference on Graphics and Interaction",
            "type": "conference",
            "alternate_names": [
                "International Conference on Grammatical Inference",
                "Int Colloq Gramm Inference",
                "International Colloquium on Grammatical Inference",
                "Int Conf Gramm Inference",
                "ICGI",
                "Int Conf Graph Interact"
            ],
            "url": "http://eurise.univ-st-etienne.fr/gi/"
        },
        "url": "https://www.semanticscholar.org/paper/110116481848d8d2e371652a11874635024426f0",
        "title": "Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack",
        "abstract": "Recurrent neural networks (RNNs) are a widely used deep architecture for sequence modeling, generation, and prediction. Despite success in applications such as machine translation and voice recognition, these stateful models have several critical shortcomings. Specifically, RNNs generalize poorly over very long sequences, which limits their applicability to many important temporal processing and time series forecasting problems. For example, RNNs struggle in recognizing complex context free languages (CFLs), never reaching 100% accuracy on training. One way to address these shortcomings is to couple an RNN with an external, differentiable memory structure, such as a stack. However, differentiable memories in prior work have neither been extensively studied on CFLs nor tested on sequences longer than those seen in training. The few efforts that have studied them have shown that continuous differentiable memory structures yield poor generalization for complex CFLs, making the RNN less interpretable. In this paper, we improve the memory-augmented RNN with important architectural and state updating mechanisms that ensure that the model learns to properly balance the use of its latent states with external memory. Our improved RNN models exhibit better generalization performance and are able to classify long strings generated by complex hierarchical context free grammars (CFGs). We evaluate our models on CGGs, including the Dyck languages, as well as on the Penn Treebank language modelling task, and achieve stable, robust performance across these benchmarks. Furthermore, we show that only our memory-augmented networks are capable of retaining memory for a longer duration up to strings of length 160.",
        "venue": "International Conference on Graphics and Interaction",
        "year": 2020,
        "referenceCount": 87,
        "citationCount": 12,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper improves the memory-augmented RNN with important architectural and state updating mechanisms that ensure that the model learns to properly balance the use of its latent states with external memory, and exhibits better generalization performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-04-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2004.07623"
        },
        "citationStyles": {
            "bibtex": "@Article{Mali2020RecognizingLG,\n author = {A. Mali and Alexander Ororbia and Daniel Kifer and C. Lee Giles},\n booktitle = {International Conference on Graphics and Interaction},\n journal = {ArXiv},\n title = {Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack},\n volume = {abs/2004.07623},\n year = {2020}\n}\n"
        }
    },
    "48_dncon2": {
        "paperId": "2a3662e271d9cbe403e6a4f696ebe63f53b6c6b7",
        "externalIds": {
            "PubMedCentral": "5925776",
            "MAG": "2951621517",
            "DBLP": "journals/bioinformatics/AdhikariHC18",
            "DOI": "10.1093/bioinformatics/btx781",
            "CorpusId": 13710673,
            "PubMed": "29228185"
        },
        "corpusId": 13710673,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/2a3662e271d9cbe403e6a4f696ebe63f53b6c6b7",
        "title": "DNCON2: improved protein contact prediction using two-level deep convolutional neural networks",
        "abstract": "Motivation Significant improvements in the prediction of protein residue-residue contacts are observed in the recent years. These contacts, predicted using a variety of coevolution-based and machine learning methods, are the key contributors to the recent progress in ab initio protein structure prediction, as demonstrated in the recent CASP experiments. Continuing the development of new methods to reliably predict contact maps is essential to further improve ab initio structure prediction. Results In this paper we discuss DNCON2, an improved protein contact map predictor based on two-level deep convolutional neural networks. It consists of six convolutional neural networks \u2013 the first five predict contacts at 6, 7.5, 8, 8.5, and 10 \u00c5 distance thresholds, and the last one uses these five predictions as additional features to predict final contact maps. On the free-modeling datasets in CASP10, 11, and 12 experiments, DNCON2 achieves mean precisions of 35%, 50%, and 53.4%, respectively, higher than 30.6% by MetaPSICOV on CASP10 dataset, 34% by MetaPSICOV on CASP11 dataset, and 46.3% by Raptor-X on CASP12 dataset, when top L/5 long-range contacts are evaluated. We attribute the improved performance of DNCON2 to the inclusion of short- and medium-range contacts into training, two-level approach to prediction, use of the state-of-the-art optimization and activation functions, and a novel deep learning architecture that allows each filter in a convolutional layer to access all the input features of a protein of arbitrary length. Availability The web server of DNCON2 is at http://sysbio.rnet.missouri.edu/dncon2/ where training and testing datasets as well as the predictions for CASP10, 11, and 12 free-modeling datasets can also be downloaded. Its source code is available at https://github.com/multicom-toolbox/DNCON2/. Contact chengji@missouri.edu Supplementary information Supplementary data are available online.",
        "venue": "bioRxiv",
        "year": 2017,
        "referenceCount": 33,
        "citationCount": 132,
        "influentialCitationCount": 11,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2017/11/21/222893.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Biology",
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The improved performance of DNCON2 is attributed to the inclusion of short- and medium-range contacts into training, two-level approach to prediction, use of the state-of-the-art optimization and activation functions, and a novel deep learning architecture that allows each filter in a convolutional layer to access all the input features of a protein of arbitrary length."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-11-21",
        "journal": {
            "name": "Bioinformatics",
            "pages": "1466 - 1472",
            "volume": "34"
        },
        "citationStyles": {
            "bibtex": "@Article{Adhikari2017DNCON2IP,\n author = {B. Adhikari and Jie Hou and Jianlin Cheng},\n booktitle = {bioRxiv},\n journal = {Bioinformatics},\n pages = {1466 - 1472},\n title = {DNCON2: improved protein contact prediction using two-level deep convolutional neural networks},\n volume = {34},\n year = {2017}\n}\n"
        }
    },
    "49_rnn+lda+kn5+cache": {
        "paperId": "d1275b2a2ab53013310e759e5c6878b96df643d4",
        "externalIds": {
            "DBLP": "conf/slt/MikolovZ12",
            "MAG": "1999965501",
            "DOI": "10.1109/SLT.2012.6424228",
            "CorpusId": 11383176
        },
        "corpusId": 11383176,
        "publicationVenue": {
            "id": "d8dfb5ba-9312-410c-a361-8ad05f945939",
            "name": "Spoken Language Technology Workshop",
            "type": "conference",
            "alternate_names": [
                "SLT",
                "Spok Lang Technol Workshop"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/d1275b2a2ab53013310e759e5c6878b96df643d4",
        "title": "Context dependent recurrent neural network language model",
        "abstract": "Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe improvements in word-error-rate.",
        "venue": "Spoken Language Technology Workshop",
        "year": 2012,
        "referenceCount": 40,
        "citationCount": 587,
        "influentialCitationCount": 61,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper improves recurrent neural network language models performance by providing a contextual real-valued input vector in association with each word to convey contextual information about the sentence being modeled by performing Latent Dirichlet Allocation using a block of preceding text."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2012-12-01",
        "journal": {
            "name": "2012 IEEE Spoken Language Technology Workshop (SLT)",
            "pages": "234-239"
        },
        "citationStyles": {
            "bibtex": "@Article{Mikolov2012ContextDR,\n author = {Tomas Mikolov and G. Zweig},\n booktitle = {Spoken Language Technology Workshop},\n journal = {2012 IEEE Spoken Language Technology Workshop (SLT)},\n pages = {234-239},\n title = {Context dependent recurrent neural network language model},\n year = {2012}\n}\n"
        }
    },
    "50_big-little_net": {
        "paperId": "425032e4fdd531cd7a5d3e7effc12d16f9c076d9",
        "externalIds": {
            "MAG": "2949431202",
            "DBLP": "conf/iclr/ChenFMSF19",
            "ArXiv": "1807.03848",
            "CorpusId": 49671490
        },
        "corpusId": 49671490,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/425032e4fdd531cd7a5d3e7effc12d16f9c076d9",
        "title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
        "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at this https URL",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 62,
        "citationCount": 80,
        "influentialCitationCount": 8,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a novel Convolutional Neural Network architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy by using a multi-branch network, which has different computational complexity at different branches."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-07-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1807.03848"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2018BigLittleNA,\n author = {Chun-Fu Chen and Quanfu Fan and Neil Rohit Mallinar and Tom Sercu and R. Feris},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\n volume = {abs/1807.03848},\n year = {2018}\n}\n"
        }
    },
    "51_hyena-3-slim": {
        "paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "externalIds": {
            "DBLP": "journals/corr/abs-2302-10866",
            "ArXiv": "2302.10866",
            "DOI": "10.48550/arXiv.2302.10866",
            "CorpusId": 257050308
        },
        "corpusId": 257050308,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
        "abstract": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "referenceCount": 65,
        "citationCount": 89,
        "influentialCitationCount": 16,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.10866",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating, and sets a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-02-21",
        "journal": {
            "pages": "28043-28078"
        },
        "citationStyles": {
            "bibtex": "@Article{Poli2023HyenaHT,\n author = {Michael Poli and Stefano Massaroli and Eric Q. Nguyen and Daniel Y. Fu and Tri Dao and S. Baccus and Y. Bengio and Stefano Ermon and Christopher R\u00e9},\n booktitle = {International Conference on Machine Learning},\n pages = {28043-28078},\n title = {Hyena Hierarchy: Towards Larger Convolutional Language Models},\n year = {2023}\n}\n"
        }
    },
    "52_ernie-vilg": {
        "paperId": "a02a3e4e3f8c1f185954af9b401f7100a45075a2",
        "externalIds": {
            "ArXiv": "2112.15283",
            "DBLP": "journals/corr/abs-2112-15283",
            "CorpusId": 245634812
        },
        "corpusId": 245634812,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/a02a3e4e3f8c1f185954af9b401f7100a45075a2",
        "title": "ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation",
        "abstract": "Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. Based on the image quantization models, we formulate both image generation and text generation as autoregressive generative tasks conditioned on the text/image input. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. For the text-to-image generation process, we further propose an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 54,
        "citationCount": 42,
        "influentialCitationCount": 4,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model, and proposes an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-12-31",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2112.15283"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2021ERNIEViLGUG,\n author = {Han Zhang and Weichong Yin and Yewei Fang and Lanxin Li and Boqiang Duan and Zhihua Wu and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation},\n volume = {abs/2112.15283},\n year = {2021}\n}\n"
        }
    },
    "53_meta_pseudo_labels": {
        "paperId": "43497fe8aa7c730e075b08facc2aa560a6d4dd85",
        "externalIds": {
            "DBLP": "conf/cvpr/PhamDXL21",
            "MAG": "3014034447",
            "ArXiv": "2003.10580",
            "DOI": "10.1109/CVPR46437.2021.01139",
            "CorpusId": 214623169
        },
        "corpusId": 214623169,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/43497fe8aa7c730e075b08facc2aa560a6d4dd85",
        "title": "Meta Pseudo Labels",
        "abstract": "We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art [16]. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student\u2019s performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student.1",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "referenceCount": 106,
        "citationCount": 519,
        "influentialCitationCount": 56,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state of the art [16]."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-03-23",
        "journal": {
            "name": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "11552-11563"
        },
        "citationStyles": {
            "bibtex": "@Article{Pham2020MetaPL,\n author = {Hieu Pham and Qizhe Xie and Zihang Dai and Quoc V. Le},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {11552-11563},\n title = {Meta Pseudo Labels},\n year = {2020}\n}\n"
        }
    },
    "54_rt-1": {
        "paperId": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
        "externalIds": {
            "DBLP": "conf/rss/BrohanBCCDFGHHH23",
            "ArXiv": "2212.06817",
            "DOI": "10.48550/arXiv.2212.06817",
            "CorpusId": 254591260
        },
        "corpusId": 254591260,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
        "title": "RT-1: Robotics Transformer for Real-World Control at Scale",
        "abstract": "By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io",
        "venue": "Robotics: Science and Systems",
        "year": 2022,
        "referenceCount": 72,
        "citationCount": 345,
        "influentialCitationCount": 30,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2212.06817",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties and verify the conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-12-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2212.06817"
        },
        "citationStyles": {
            "bibtex": "@Article{Brohan2022RT1RT,\n author = {Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Joseph Dabis and Chelsea Finn and K. Gopalakrishnan and Karol Hausman and Alexander Herzog and Jasmine Hsu and Julian Ibarz and Brian Ichter and A. Irpan and Tomas Jackson and Sally Jesmonth and Nikhil Joshi and Ryan C. Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal and Kuang-Huei Lee and S. Levine and Yao Lu and U. Malla and D. Manjunath and Igor Mordatch and Ofir Nachum and Carolina Parada and Jodilyn Peralta and Emily Perez and Karl Pertsch and Jornell Quiambao and Kanishka Rao and M. Ryoo and Grecia Salazar and Pannag R. Sanketi and Kevin Sayed and Jaspiar Singh and S. Sontakke and Austin Stone and Clayton Tan and Huong Tran and Vincent Vanhoucke and Steve Vega and Q. Vuong and F. Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich},\n booktitle = {Robotics: Science and Systems},\n journal = {ArXiv},\n title = {RT-1: Robotics Transformer for Real-World Control at Scale},\n volume = {abs/2212.06817},\n year = {2022}\n}\n"
        }
    },
    "55_true-regularization+finetune+dynamic-eval": {
        "paperId": "5a2304ba4e4401db2e0df8188a5f761646b52480",
        "externalIds": {
            "MAG": "2953242772",
            "DBLP": "journals/corr/abs-1904-04163",
            "ArXiv": "1904.04163",
            "DOI": "10.1109/ICASSP.2019.8683533",
            "CorpusId": 89606968
        },
        "corpusId": 89606968,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/5a2304ba4e4401db2e0df8188a5f761646b52480",
        "title": "Knowledge Distillation for Recurrent Neural Network Language Modeling with Trust Regularization",
        "abstract": "Recurrent Neural Networks (RNNs) have dominated language modeling because of their superior performance over traditional N-gram based models. In many applications, a large Recurrent Neural Network language model (RNNLM) or an ensemble of several RNNLMs is used. These models have large memory footprints and require heavy computation. In this paper, we examine the effect of applying knowledge distillation in reducing the model size for RNNLMs. In addition, we propose a trust regularization method to improve the knowledge distillation training for RNNLMs. Using knowledge distillation with trust regularization, we reduce the parameter size to a third of that of the previously published best model while maintaining the state-of-the-art perplexity result on Penn Treebank data. In a speech recognition N-best rescoring task, we reduce the RNNLM model size to 18.5% of the baseline system, with no degradation in word error rate (WER) performance on Wall Street Journal data set.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2019,
        "referenceCount": 25,
        "citationCount": 23,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.04163",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper reduces the RNNLM model size to 18.5% of the baseline system, with no degradation in word error rate (WER) performance on Wall Street Journal data set, and proposes a trust regularization method to improve the knowledge distillation training for RNNLMs."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-04-08",
        "journal": {
            "name": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "pages": "7230-7234"
        },
        "citationStyles": {
            "bibtex": "@Article{Shi2019KnowledgeDF,\n author = {Yangyang Shi and M. Hwang and X. Lei and Haoyu Sheng},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {7230-7234},\n title = {Knowledge Distillation for Recurrent Neural Network Language Modeling with Trust Regularization},\n year = {2019}\n}\n"
        }
    },
    "56_m6-10b": {
        "paperId": "290bb3cafa82192c4d30661b370c1e4e4b1f03db",
        "externalIds": {
            "DBLP": "journals/corr/abs-2103-00823",
            "ArXiv": "2103.00823",
            "CorpusId": 232075617
        },
        "corpusId": 232075617,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/290bb3cafa82192c4d30661b370c1e4e4b1f03db",
        "title": "M6: A Chinese Multimodal Pretrainer",
        "abstract": "In this work, we construct the largest dataset for multimodal pretraining in Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide range of domains. We propose a cross-modal pretraining method called M6, referring to Multi-Modality to Multi-Modality Multitask Mega-transformer, for unified pretraining on the data of single modality and multiple modalities. We scale the model size up to 10 billion and 100 billion parameters, and build the largest pretrained model in Chinese. We apply the model to a series of downstream applications, and demonstrate its outstanding performance in comparison with strong baselines. Furthermore, we specifically design a downstream task of text-guided image generation, and show that the finetuned M6 can create high-quality images with high resolution and abundant details.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 51,
        "citationCount": 112,
        "influentialCitationCount": 13,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-03-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2103.00823"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2021M6AC,\n author = {Junyang Lin and Rui Men and An Yang and Chan Zhou and Ming Ding and Yichang Zhang and Peng Wang and Ang Wang and Le Jiang and Xianyan Jia and J. Zhang and Jianwei Zhang and Xu Zou and Zhikang Li and X. Deng and Jie Liu and J. Xue and Huiling Zhou and Jianxin Ma and Jin Yu and Yong Li and Wei Lin and Jingren Zhou and J. Tang and Hongxia Yang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {M6: A Chinese Multimodal Pretrainer},\n volume = {abs/2103.00823},\n year = {2021}\n}\n"
        }
    },
    "57_wizardlm_70b": {
        "paperId": "131f499e4d3503da93022d07fcf804a18483bea9",
        "externalIds": {
            "ArXiv": "2304.12244",
            "DBLP": "journals/corr/abs-2304-12244",
            "DOI": "10.48550/arXiv.2304.12244",
            "CorpusId": 258298159
        },
        "corpusId": 258298159,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/131f499e4d3503da93022d07fcf804a18483bea9",
        "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
        "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 43,
        "citationCount": 353,
        "influentialCitationCount": 64,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2304.12244",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs, and it is demonstrated that outputs from the authors' WizardLM are preferred to outputs from OpenAI ChatGPT."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-04-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2304.12244"
        },
        "citationStyles": {
            "bibtex": "@Article{Xu2023WizardLMEL,\n author = {Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {WizardLM: Empowering Large Language Models to Follow Complex Instructions},\n volume = {abs/2304.12244},\n year = {2023}\n}\n"
        }
    },
    "58_alphafold-multimer": {
        "paperId": "2556e820cba6bda75f6f31b76bc74d9e36d72cb3",
        "externalIds": {
            "DOI": "10.1101/2021.10.04.463034",
            "CorpusId": 238413014
        },
        "corpusId": 238413014,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/2556e820cba6bda75f6f31b76bc74d9e36d72cb3",
        "title": "Protein complex prediction with AlphaFold-Multimer",
        "abstract": "While the vast majority of well-structured single protein chains can now be predicted to high accuracy due to the recent AlphaFold [1] model, the prediction of multi-chain protein complexes remains a challenge in many cases. In this work, we demonstrate that an AlphaFold model trained specifically for multimeric inputs of known stoichiometry, which we call AlphaFold-Multimer, significantly increases accuracy of predicted multimeric interfaces over input-adapted single-chain AlphaFold while maintaining high intra-chain accuracy. On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] \u2265 0.49) on 13 targets and high accuracy (DockQ \u2265 0.8) on 7 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2]). We also predict structures for a large dataset of 4,446 recent protein complexes, from which we score all non-redundant interfaces with low template identity. For heteromeric interfaces we successfully predict the interface (DockQ \u2265 0.23) in 70% of cases, and produce high accuracy predictions (DockQ \u2265 0.8) in 26% of cases, an improvement of +27 and +14 percentage points over the flexible linker modification of AlphaFold [4] respectively. For homomeric inter-faces we successfully predict the interface in 72% of cases, and produce high accuracy predictions in 36% of cases, an improvement of +8 and +7 percentage points respectively.",
        "venue": "bioRxiv",
        "year": 2021,
        "referenceCount": 46,
        "citationCount": 1273,
        "influentialCitationCount": 169,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2022/03/10/2021.10.04.463034.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work demonstrates that an AlphaFold model trained specifically for multimeric inputs of known stoichiometry, which it is called AlphaFolding-Multimer, significantly increases accuracy of predicted multimerics interfaces over input-adapted single-chain AlphaFolds while maintaining high intra-chain accuracy."
        },
        "publicationTypes": null,
        "publicationDate": "2021-10-04",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Evans2021ProteinCP,\n author = {Richard Evans and Michael O\u2019Neill and A. Pritzel and Natasha Antropova and Andrew Senior and Tim Green and Augustin Z\u00eddek and Russ Bates and Sam Blackwell and Jason Yim and O. Ronneberger and S. Bodenstein and Michal Zielinski and Alex Bridgland and Anna Potapenko and Andrew Cowie and Kathryn Tunyasuvunakool and Rishub Jain and Ellen Clancy and Pushmeet Kohli and J. Jumper and D. Hassabis},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {Protein complex prediction with AlphaFold-Multimer},\n year = {2021}\n}\n"
        }
    },
    "60_fmmformer_(2-kernel_fast_weight_+_band20)": {
        "paperId": "37abe53ed31caa23ae833b2e67bb4aa1892e8d25",
        "externalIds": {
            "ArXiv": "2108.02347",
            "DBLP": "journals/corr/abs-2108-02347",
            "CorpusId": 236924765
        },
        "corpusId": 236924765,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/37abe53ed31caa23ae833b2e67bb4aa1892e8d25",
        "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention",
        "abstract": "We propose FMMformers, a class of efficient and flexible transformers inspired by the celebrated fast multipole method (FMM) for accelerating interacting particle simulation. FMM decomposes particle-particle interaction into near-field and far-field components and then performs direct and coarse-grained computation, respectively. Similarly, FMMformers decompose the attention into near-field and far-field attention, modeling the near-field attention by a banded matrix and the far-field attention by a low-rank matrix. Computing the attention matrix for FMMformers requires linear complexity in computational time and memory footprint with respect to the sequence length. In contrast, standard transformers suffer from quadratic complexity. We analyze and validate the advantage of FMMformers over the standard transformer on the Long Range Arena and language modeling benchmarks. FMMformers can even outperform the standard transformer in terms of accuracy by a significant margin. For instance, FMMformers achieve an average classification accuracy of $60.74\\%$ over the five Long Range Arena tasks, which is significantly better than the standard transformer's average accuracy of $58.70\\%$.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 77,
        "citationCount": 28,
        "influentialCitationCount": 2,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Physics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "FMMformers is a class of efficient and flexible transformers inspired by the celebrated fast multipole method for accelerating interacting particle simulation that can even outperform the standard transformer in terms of accuracy by a significant margin."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-08-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2108.02347"
        },
        "citationStyles": {
            "bibtex": "@Article{Nguyen2021FMMformerEA,\n author = {T. Nguyen and Vai Suliafu and S. Osher and Long Chen and Bao Wang},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention},\n volume = {abs/2108.02347},\n year = {2021}\n}\n"
        }
    },
    "63_edsr": {
        "paperId": "7ba5d3808e117e7a68dc40331ce1d483ceeedcb2",
        "externalIds": {
            "MAG": "2735224642",
            "DBLP": "journals/corr/LimSKNL17",
            "ArXiv": "1707.02921",
            "DOI": "10.1109/CVPRW.2017.151",
            "CorpusId": 6540453
        },
        "corpusId": 6540453,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/7ba5d3808e117e7a68dc40331ce1d483ceeedcb2",
        "title": "Enhanced Deep Residual Networks for Single Image Super-Resolution",
        "abstract": "Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge[26].",
        "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
        "year": 2017,
        "referenceCount": 37,
        "citationCount": 4646,
        "influentialCitationCount": 1042,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1707.02921",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper develops an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods, and proposes a new multi-scale deepsuper-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-07-10",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "pages": "1132-1140"
        },
        "citationStyles": {
            "bibtex": "@Article{Lim2017EnhancedDR,\n author = {Bee Lim and Sanghyun Son and Heewon Kim and Seungjun Nah and Kyoung Mu Lee},\n booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n pages = {1132-1140},\n title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},\n year = {2017}\n}\n"
        }
    },
    "64_llama-65b": {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "externalIds": {
            "ArXiv": "2302.13971",
            "DBLP": "journals/corr/abs-2302-13971",
            "DOI": "10.48550/arXiv.2302.13971",
            "CorpusId": 257219404
        },
        "corpusId": 257219404,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models",
        "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 80,
        "citationCount": 4186,
        "influentialCitationCount": 671,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.13971",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "LLaMA, a collection of foundation language models ranging from 7B to 65B parameters, is introduced and it is shown that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-02-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2302.13971"
        },
        "citationStyles": {
            "bibtex": "@Article{Touvron2023LLaMAOA,\n author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth\u00e9e Lacroix and Baptiste Rozi\u00e8re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {LLaMA: Open and Efficient Foundation Language Models},\n volume = {abs/2302.13971},\n year = {2023}\n}\n"
        }
    },
    "65_openai_five_rerun": {
        "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
        "externalIds": {
            "MAG": "2996037775",
            "DBLP": "journals/corr/abs-1912-06680",
            "ArXiv": "1912.06680",
            "CorpusId": 209376771
        },
        "corpusId": 209376771,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/b19729b27a1b4c24b52f87308c907653300afa7f",
        "title": "Dota 2 with Large Scale Deep Reinforcement Learning",
        "abstract": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 50,
        "citationCount": 1400,
        "influentialCitationCount": 76,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-12-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1912.06680"
        },
        "citationStyles": {
            "bibtex": "@Article{Berner2019Dota2W,\n author = {Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemyslaw Debiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Christopher Hesse and R. J\u00f3zefowicz and S. Gray and Catherine Olsson and J. Pachocki and Michael Petrov and Henrique Pond\u00e9 de Oliveira Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and I. Sutskever and Jie Tang and Filip Wolski and Susan Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dota 2 with Large Scale Deep Reinforcement Learning},\n volume = {abs/1912.06680},\n year = {2019}\n}\n"
        }
    },
    "66_neat_in_neuroevolution": {
        "paperId": "d03c916d49268d48fde3b76a68e64af7761835e7",
        "externalIds": {
            "MAG": "2148872333",
            "DBLP": "journals/ec/StanleyM02",
            "DOI": "10.1162/106365602320169811",
            "CorpusId": 498161,
            "PubMed": "12180173"
        },
        "corpusId": 498161,
        "publicationVenue": {
            "id": "41b9fb21-e0f4-4304-8584-f1541bdd7efc",
            "name": "Evolutionary Computation",
            "type": "journal",
            "alternate_names": [
                "Evol Comput"
            ],
            "issn": "1063-6560",
            "url": "http://cognet.mit.edu/library/journals/journal?issn=10636560",
            "alternate_urls": [
                "http://www.mitpressjournals.org/loi/evco",
                "https://www.mitpressjournals.org/loi/evco"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/d03c916d49268d48fde3b76a68e64af7761835e7",
        "title": "Evolving Neural Networks through Augmenting Topologies",
        "abstract": "An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signicantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.",
        "venue": "Evolutionary Computation",
        "year": 2002,
        "referenceCount": 57,
        "citationCount": 3294,
        "influentialCitationCount": 444,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A method is presented, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task and shows how it is possible for evolution to both optimize and complexify solutions simultaneously."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2002-06-01",
        "journal": {
            "name": "Evolutionary Computation",
            "pages": "99-127",
            "volume": "10"
        },
        "citationStyles": {
            "bibtex": "@Article{Stanley2002EvolvingNN,\n author = {Kenneth O. Stanley and R. Miikkulainen},\n booktitle = {Evolutionary Computation},\n journal = {Evolutionary Computation},\n pages = {99-127},\n title = {Evolving Neural Networks through Augmenting Topologies},\n volume = {10},\n year = {2002}\n}\n"
        }
    },
    "68_calm": {
        "paperId": "e51f71c171e69c3631ef520851e8f63a65d55b19",
        "externalIds": {
            "DBLP": "conf/siggraph/TesslerKGMCP23",
            "ArXiv": "2305.02195",
            "DOI": "10.1145/3588432.3591541",
            "CorpusId": 258461220
        },
        "corpusId": 258461220,
        "publicationVenue": {
            "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
            "name": "International Conference on Computer Graphics and Interactive Techniques",
            "type": "conference",
            "alternate_names": [
                "Int Conf Comput Graph Interact Tech",
                "SIGGRAPH"
            ],
            "url": "http://www.siggraph.org/"
        },
        "url": "https://www.semanticscholar.org/paper/e51f71c171e69c3631ef520851e8f63a65d55b19",
        "title": "CALM: Conditional Adversarial Latent Models\u00a0 for Directable Virtual Characters",
        "abstract": "In this work, we present Conditional Adversarial Latent Models (CALM), an approach for generating diverse and directable behaviors for user-controlled interactive virtual characters. Using imitation learning, CALM learns a representation of movement that captures the complexity and diversity of human motion, and enables direct control over character movements. The approach jointly learns a control policy and a motion encoder that reconstructs key characteristics of a given motion without merely replicating it. The results show that CALM learns a semantic motion representation, enabling control over the generated motions and style-conditioning for higher-level task training. Once trained, the character can be controlled using intuitive interfaces, akin to those found in video games.",
        "venue": "International Conference on Computer Graphics and Interactive Techniques",
        "year": 2023,
        "referenceCount": 33,
        "citationCount": 13,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.02195",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The results show that CALM learns a semantic motion representation, enabling control over the generated motions and style-conditioning for higher-level task training."
        },
        "publicationTypes": [
            "Book",
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-05-02",
        "journal": {
            "name": "ACM SIGGRAPH 2023 Conference Proceedings"
        },
        "citationStyles": {
            "bibtex": "@Book{Tessler2023CALMCA,\n author = {Chen Tessler and Yoni Kasten and Yunrong Guo and Shie Mannor and Gal Chechik and X. B. Peng},\n booktitle = {International Conference on Computer Graphics and Interactive Techniques},\n journal = {ACM SIGGRAPH 2023 Conference Proceedings},\n title = {CALM: Conditional Adversarial Latent Models\u00a0 for Directable Virtual Characters},\n year = {2023}\n}\n"
        }
    },
    "71_bloomberggpt": {
        "paperId": "83edcfbb206ddad38a971d605da09390604248ea",
        "externalIds": {
            "ArXiv": "2303.17564",
            "DBLP": "journals/corr/abs-2303-17564",
            "DOI": "10.48550/arXiv.2303.17564",
            "CorpusId": 257833842
        },
        "corpusId": 257833842,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/83edcfbb206ddad38a971d605da09390604248ea",
        "title": "BloombergGPT: A Large Language Model for Finance",
        "abstract": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 138,
        "citationCount": 281,
        "influentialCitationCount": 24,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.17564",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Economics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Economics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Business",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data, and constructs a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-03-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2303.17564"
        },
        "citationStyles": {
            "bibtex": "@Article{Wu2023BloombergGPTAL,\n author = {Shijie Wu and Ozan Irsoy and Steven Lu and Vadim Dabravolski and Mark Dredze and Sebastian Gehrmann and P. Kambadur and D. Rosenberg and Gideon Mann},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {BloombergGPT: A Large Language Model for Finance},\n volume = {abs/2303.17564},\n year = {2023}\n}\n"
        }
    },
    "72_ernie-gen_(large)": {
        "paperId": "6191a5122d67dfbab421bc89540d264822dd8173",
        "externalIds": {
            "MAG": "3035359363",
            "ArXiv": "2001.11314",
            "DBLP": "conf/ijcai/XiaoZLST0W20",
            "DOI": "10.24963/ijcai.2020/553",
            "CorpusId": 210966163
        },
        "corpusId": 210966163,
        "publicationVenue": {
            "id": "67f7f831-711a-43c8-8785-1e09005359b5",
            "name": "International Joint Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "Int Jt Conf Artif Intell",
                "IJCAI"
            ],
            "url": "http://www.ijcai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/6191a5122d67dfbab421bc89540d264822dd8173",
        "title": "ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation",
        "abstract": "Current pre-training works in natural language generation pay little attention to the problem of exposure bias on downstream tasks. To address this issue, we propose an enhanced multi-flow sequence to sequence pre-training and fine-tuning framework named ERNIE-GEN, which bridges the discrepancy between training and inference with an infilling generation mechanism and a noise-aware generation method. To make generation closer to human writing patterns, this framework introduces a span-by-span generation flow that trains the model to predict semantically-complete spans consecutively rather than predicting word by word. Unlike existing pre-training methods, ERNIE-GEN incorporates multi-granularity target sampling to construct pre-training data, which enhances the correlation between encoder and decoder. Experimental results demonstrate that ERNIE-GEN achieves state-of-the-art results with a much smaller amount of pre-training data and parameters on a range of language generation tasks, including abstractive summarization (Gigaword and CNN/DailyMail), question generation (SQuAD), dialogue generation (Persona-Chat) and generative question answering (CoQA). The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE/ernie-gen.",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2020,
        "referenceCount": 30,
        "citationCount": 107,
        "influentialCitationCount": 17,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2020/0553.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An enhanced multi-flow sequence to sequence pre-training and fine-tuning framework named ERNIE-GEN, which bridges the discrepancy between training and inference with an infilling generation mechanism and a noise-aware generation method to make generation closer to human writing patterns."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-01-26",
        "journal": {
            "pages": "3997-4003"
        },
        "citationStyles": {
            "bibtex": "@Article{Xiao2020ERNIEGENAE,\n author = {Dongling Xiao and Han Zhang and Yukun Li and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {3997-4003},\n title = {ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation},\n year = {2020}\n}\n"
        }
    },
    "75_cd-grab_(wt103)": {
        "paperId": "4bb9909949a8d28c815cde6df8937586b554b353",
        "externalIds": {
            "ArXiv": "2302.00845",
            "DBLP": "conf/nips/CooperGPYRLS23",
            "CorpusId": 258967770
        },
        "corpusId": 258967770,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/4bb9909949a8d28c815cde6df8937586b554b353",
        "title": "CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training",
        "abstract": "Recent research on online Gradient Balancing (GraB) has revealed that there exist permutation-based example orderings for SGD that are guaranteed to outperform random reshuffling (RR). Whereas RR arbitrarily permutes training examples, GraB leverages stale gradients from prior epochs to order examples -- achieving a provably faster convergence rate than RR. However, GraB is limited by design: while it demonstrates an impressive ability to scale-up training on centralized data, it does not naturally extend to modern distributed ML workloads. We therefore propose Coordinated Distributed GraB (CD-GraB), which uses insights from prior work on kernel thinning to translate the benefits of provably faster permutation-based example ordering to distributed settings. With negligible overhead, CD-GraB exhibits a linear speedup in convergence rate over centralized GraB and outperforms distributed RR on a variety of benchmark tasks.",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "referenceCount": 53,
        "citationCount": 2,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Coordinated Distributed GraB (CD-GraB) is proposed, which uses insights from prior work on kernel thinning to translate the benefits of provably faster permutation-based example ordering to distributed settings and exhibits a linear speedup in convergence rate over centralized GraB."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-02-02",
        "journal": null,
        "citationStyles": {
            "bibtex": "@Article{Cooper2023CDGraBCD,\n author = {A. F. Cooper and Wentao Guo and Khiem Pham and Tiancheng Yuan and Charlie F. Ruan and Yucheng Lu and Chris De Sa},\n booktitle = {Neural Information Processing Systems},\n title = {CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training},\n year = {2023}\n}\n"
        }
    },
    "76_base_lm+gnn+knn": {
        "paperId": "7d1e859fefee1eaac430c38d01cd35003604288b",
        "externalIds": {
            "ArXiv": "2110.08743",
            "DBLP": "journals/corr/abs-2110-08743",
            "CorpusId": 239016943
        },
        "corpusId": 239016943,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7d1e859fefee1eaac430c38d01cd35003604288b",
        "title": "GNN-LM: Language Modeling based on Global Contexts via GNN",
        "abstract": "Inspired by the notion that ``{\\it to copy is easier than to memorize}``, in this work, we introduce GNN-LM, which extends the vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. \\footnote{The code can be found at https://github.com/ShannonAI/GNN-LM",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "referenceCount": 68,
        "citationCount": 31,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "GNN-LM, which extends the vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus, is introduced, which builds a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2110.08743"
        },
        "citationStyles": {
            "bibtex": "@Article{Meng2021GNNLMLM,\n author = {Yuxian Meng and Shi Zong and Xiaoya Li and Xiaofei Sun and Tianwei Zhang and Fei Wu and Jiwei Li},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {GNN-LM: Language Modeling based on Global Contexts via GNN},\n volume = {abs/2110.08743},\n year = {2021}\n}\n"
        }
    },
    "78_fast": {
        "paperId": "e0408181bccb7e3754dd5e6785ec47d8beb8b6bd",
        "externalIds": {
            "MAG": "2584333262",
            "DBLP": "conf/eccv/RostenD06",
            "DOI": "10.1007/11744023_34",
            "CorpusId": 1388140
        },
        "corpusId": 1388140,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/e0408181bccb7e3754dd5e6785ec47d8beb8b6bd",
        "title": "Machine Learning for High-Speed Corner Detection",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2006,
        "referenceCount": 38,
        "citationCount": 4551,
        "influentialCitationCount": 455,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/11744023_34.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7% of the available processing time."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2006-05-07",
        "journal": {
            "pages": "430-443"
        },
        "citationStyles": {
            "bibtex": "@Article{Rosten2006MachineLF,\n author = {E. Rosten and T. Drummond},\n booktitle = {European Conference on Computer Vision},\n pages = {430-443},\n title = {Machine Learning for High-Speed Corner Detection},\n year = {2006}\n}\n"
        }
    },
    "81_megasyn": {
        "paperId": "d7eef9b5bb65feda6647440e7727bbcdf0edaebc",
        "externalIds": {
            "DBLP": "journals/natmi/UrbinaLIE22",
            "DOI": "10.1038/s42256-022-00465-9",
            "CorpusId": 247302391,
            "PubMed": "36211133"
        },
        "corpusId": 247302391,
        "publicationVenue": {
            "id": "6457124b-39bf-4d02-bff4-73752ff21562",
            "name": "Nature Machine Intelligence",
            "type": "journal",
            "alternate_names": [
                "Nat Mach Intell"
            ],
            "issn": "2522-5839",
            "url": "https://www.nature.com/natmachintell/"
        },
        "url": "https://www.semanticscholar.org/paper/d7eef9b5bb65feda6647440e7727bbcdf0edaebc",
        "title": "Dual use of artificial-intelligence-powered drug discovery",
        "abstract": null,
        "venue": "Nature Machine Intelligence",
        "year": 2022,
        "referenceCount": 10,
        "citationCount": 129,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An international security conference explored how artificial intelligence technologies for drug discovery could be misused for de novo design of biochemical weapons."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-03-01",
        "journal": {
            "name": "Nature Machine Intelligence",
            "pages": "189 - 191",
            "volume": "4"
        },
        "citationStyles": {
            "bibtex": "@Article{Urbina2022DualUO,\n author = {Fabio Urbina and Filippa Lentzos and C\u00e9dric Invernizzi and S. Ekins},\n booktitle = {Nature Machine Intelligence},\n journal = {Nature Machine Intelligence},\n pages = {189 - 191},\n title = {Dual use of artificial-intelligence-powered drug discovery},\n volume = {4},\n year = {2022}\n}\n"
        }
    },
    "82_m6-100b": {
        "paperId": "290bb3cafa82192c4d30661b370c1e4e4b1f03db",
        "externalIds": {
            "DBLP": "journals/corr/abs-2103-00823",
            "ArXiv": "2103.00823",
            "CorpusId": 232075617
        },
        "corpusId": 232075617,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/290bb3cafa82192c4d30661b370c1e4e4b1f03db",
        "title": "M6: A Chinese Multimodal Pretrainer",
        "abstract": "In this work, we construct the largest dataset for multimodal pretraining in Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide range of domains. We propose a cross-modal pretraining method called M6, referring to Multi-Modality to Multi-Modality Multitask Mega-transformer, for unified pretraining on the data of single modality and multiple modalities. We scale the model size up to 10 billion and 100 billion parameters, and build the largest pretrained model in Chinese. We apply the model to a series of downstream applications, and demonstrate its outstanding performance in comparison with strong baselines. Furthermore, we specifically design a downstream task of text-guided image generation, and show that the finetuned M6 can create high-quality images with high resolution and abundant details.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 51,
        "citationCount": 112,
        "influentialCitationCount": 13,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-03-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2103.00823"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2021M6AC,\n author = {Junyang Lin and Rui Men and An Yang and Chan Zhou and Ming Ding and Yichang Zhang and Peng Wang and Ang Wang and Le Jiang and Xianyan Jia and J. Zhang and Jianwei Zhang and Xu Zou and Zhikang Li and X. Deng and Jie Liu and J. Xue and Huiling Zhou and Jianxin Ma and Jin Yu and Yong Li and Wei Lin and Jingren Zhou and J. Tang and Hongxia Yang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {M6: A Chinese Multimodal Pretrainer},\n volume = {abs/2103.00823},\n year = {2021}\n}\n"
        }
    },
    "84_unirep": {
        "paperId": "06eb3c3ccae16fced2222f8a45877906f54f2164",
        "externalIds": {
            "MAG": "2980789587",
            "DOI": "10.1038/s41592-019-0598-1",
            "CorpusId": 108777959,
            "PubMed": "31636460"
        },
        "corpusId": 108777959,
        "publicationVenue": {
            "id": "099483df-e8f2-4bee-805d-8a69f07b6cbf",
            "name": "Nature Methods",
            "type": "journal",
            "alternate_names": [
                "Nat Method"
            ],
            "issn": "1548-7091",
            "url": "http://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nmeth/index.html",
                "https://www.nature.com/nmeth/",
                "http://www.nature.com/nmeth/authors/index.html#aims"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/06eb3c3ccae16fced2222f8a45877906f54f2164",
        "title": "Unified rational protein engineering with sequence-based deep representation learning",
        "abstract": null,
        "venue": "Nature Methods",
        "year": 2019,
        "referenceCount": 88,
        "citationCount": 668,
        "influentialCitationCount": 59,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Deep learning is applied to unlabeled amino-acid sequences to distill the fundamental features of a protein into a statistical representation that is semantically rich and structurally, evolutionarily and biophysically grounded and broadly applicable to unseen regions of sequence space."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-10-21",
        "journal": {
            "name": "Nature Methods",
            "pages": "1315 - 1322",
            "volume": "16"
        },
        "citationStyles": {
            "bibtex": "@Article{Alley2019UnifiedRP,\n author = {E. C. Alley and Grigory Khimulya and Surojit Biswas and Mohammed Alquraishi and G. Church},\n booktitle = {Nature Methods},\n journal = {Nature Methods},\n pages = {1315 - 1322},\n title = {Unified rational protein engineering with sequence-based deep representation learning},\n volume = {16},\n year = {2019}\n}\n"
        }
    },
    "85_semvec": {
        "paperId": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
        "externalIds": {
            "ACL": "N13-1090",
            "DBLP": "conf/naacl/MikolovYZ13",
            "MAG": "2141599568",
            "CorpusId": 7478738
        },
        "corpusId": 7478738,
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "url": "https://www.semanticscholar.org/paper/c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
        "title": "Linguistic Regularities in Continuous Space Word Representations",
        "abstract": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, \u201cKing Man + Woman\u201d results in a vector very close to \u201cQueen.\u201d We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2013,
        "referenceCount": 23,
        "citationCount": 3432,
        "influentialCitationCount": 557,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The vector-space word representations that are implicitly learned by the input-layer weights are found to be surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2013-05-27",
        "journal": {
            "pages": "746-751"
        },
        "citationStyles": {
            "bibtex": "@Article{Mikolov2013LinguisticRI,\n author = {Tomas Mikolov and Wen-tau Yih and G. Zweig},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {746-751},\n title = {Linguistic Regularities in Continuous Space Word Representations},\n year = {2013}\n}\n"
        }
    },
    "87_longt5": {
        "paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "externalIds": {
            "ArXiv": "2112.07916",
            "DBLP": "conf/naacl/GuoAUONSY22",
            "DOI": "10.18653/v1/2022.findings-naacl.55",
            "CorpusId": 245144820
        },
        "corpusId": 245144820,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences",
        "abstract": "Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.",
        "venue": "NAACL-HLT",
        "year": 2021,
        "referenceCount": 43,
        "citationCount": 172,
        "influentialCitationCount": 39,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.findings-naacl.55.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new model, called LongT5, is presented, with which the effects of scaling both the input length and model size at the same time are explored, which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-12-15",
        "journal": {
            "pages": "724-736"
        },
        "citationStyles": {
            "bibtex": "@Article{Guo2021LongT5ET,\n author = {Mandy Guo and J. Ainslie and David C. Uthus and Santiago Onta\u00f1\u00f3n and Jianmo Ni and Yun-Hsuan Sung and Yinfei Yang},\n booktitle = {NAACL-HLT},\n pages = {724-736},\n title = {LongT5: Efficient Text-To-Text Transformer for Long Sequences},\n year = {2021}\n}\n"
        }
    },
    "88_bigbigan": {
        "paperId": "cde35c87aaabbc617d38f9cfaa2721a2e166d750",
        "externalIds": {
            "DBLP": "journals/corr/abs-1907-02544",
            "ArXiv": "1907.02544",
            "MAG": "2970241862",
            "CorpusId": 195820291
        },
        "corpusId": 195820291,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/cde35c87aaabbc617d38f9cfaa2721a2e166d750",
        "title": "Large Scale Adversarial Representation Learning",
        "abstract": "Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models -- including image generators and encoders -- are available on TensorFlow Hub (https://tfhub.dev/s?publisher=deepmind&q=bigbigan).",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "referenceCount": 41,
        "citationCount": 488,
        "influentialCitationCount": 40,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator, and demonstrates that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-07-04",
        "journal": {
            "pages": "10541-10551"
        },
        "citationStyles": {
            "bibtex": "@Article{Donahue2019LargeSA,\n author = {Jeff Donahue and K. Simonyan},\n booktitle = {Neural Information Processing Systems},\n pages = {10541-10551},\n title = {Large Scale Adversarial Representation Learning},\n year = {2019}\n}\n"
        }
    },
    "90_transformerxl-layerfusion-ca": {
        "paperId": "7b553b93682c25c8f03c97a7f8a05842c57ef2aa",
        "externalIds": {
            "MAG": "3046174616",
            "ArXiv": "2007.14917",
            "DBLP": "journals/corr/abs-2007-14917",
            "CorpusId": 220845722
        },
        "corpusId": 220845722,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/7b553b93682c25c8f03c97a7f8a05842c57ef2aa",
        "title": "Compressing Deep Neural Networks via Layer Fusion",
        "abstract": "This paper proposes \\textit{layer fusion} - a model compression technique that discovers which weights to combine and then fuses weights of similar fully-connected, convolutional and attention layers. Layer fusion can significantly reduce the number of layers of the original network with little additional computation overhead, while maintaining competitive performance. From experiments on CIFAR-10, we find that various deep convolution neural networks can remain within 2\\% accuracy points of the original networks up to a compression ratio of 3.33 when iteratively retrained with layer fusion. For experiments on the WikiText-2 language modelling dataset where pretrained transformer models are used, we achieve compression that leads to a network that is 20\\% of its original size while being within 5 perplexity points of the original network. We also find that other well-established compression techniques can achieve competitive performance when compared to their original networks given a sufficient number of retraining steps. Generally, we observe a clear inflection point in performance as the amount of compression increases, suggesting a bound on the amount of compression that can be achieved before an exponential degradation in performance.",
        "venue": "arXiv.org",
        "year": 2020,
        "referenceCount": 41,
        "citationCount": 3,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes layer fusion - a model compression technique that discovers which weights to combine and then fuses weights of similar fully-connected, convolutional and attention layers, suggesting a bound on the amount of compression that can be achieved before an exponential degradation in performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-07-29",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2007.14917"
        },
        "citationStyles": {
            "bibtex": "@Article{O'Neill2020CompressingDN,\n author = {James O'Neill and G. V. Steeg and A. Galstyan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Compressing Deep Neural Networks via Layer Fusion},\n volume = {abs/2007.14917},\n year = {2020}\n}\n"
        }
    },
    "91_wavenet": {
        "paperId": "df0402517a7338ae28bc54acaac400de6b456a46",
        "externalIds": {
            "MAG": "2519091744",
            "DBLP": "conf/ssw/OordDZSVGKSK16",
            "ArXiv": "1609.03499",
            "CorpusId": 6254678
        },
        "corpusId": 6254678,
        "publicationVenue": {
            "id": "3eb7999c-8390-439e-b358-8b9ce5edd9e3",
            "name": "Speech Synthesis Workshop",
            "type": "conference",
            "alternate_names": [
                "SSW",
                "Speech Synth Workshop"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/df0402517a7338ae28bc54acaac400de6b456a46",
        "title": "WaveNet: A Generative Model for Raw Audio",
        "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.",
        "venue": "Speech Synthesis Workshop",
        "year": 2016,
        "referenceCount": 64,
        "citationCount": 6362,
        "influentialCitationCount": 802,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "WaveNet, a deep neural network for generating raw audio waveforms, is introduced; it is shown that it can be efficiently trained on data with tens of thousands of samples per second of audio, and can be employed as a discriminative model, returning promising results for phoneme recognition."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-09-12",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1609.03499"
        },
        "citationStyles": {
            "bibtex": "@Article{Oord2016WaveNetAG,\n author = {A\u00e4ron van den Oord and S. Dieleman and H. Zen and K. Simonyan and O. Vinyals and Alex Graves and Nal Kalchbrenner and A. Senior and K. Kavukcuoglu},\n booktitle = {Speech Synthesis Workshop},\n journal = {ArXiv},\n title = {WaveNet: A Generative Model for Raw Audio},\n volume = {abs/1609.03499},\n year = {2016}\n}\n"
        }
    },
    "92_ct-mos_+_dynamiceval_(wt2)": {
        "paperId": "f617f7ba4040d6e85b384685da09fed35c841280",
        "externalIds": {
            "DBLP": "journals/corr/abs-2012-13575",
            "ArXiv": "2012.13575",
            "MAG": "2994888541",
            "CorpusId": 214250287
        },
        "corpusId": 214250287,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/f617f7ba4040d6e85b384685da09fed35c841280",
        "title": "Contextual Temperature for Language Modeling",
        "abstract": "Temperature scaling has been widely used as an effective approach to control the smoothness of a distribution, which helps the model performance in various tasks. Current practices to apply temperature scaling assume either a fixed, or a manually-crafted dynamically changing schedule. However, our studies indicate that the individual optimal trajectory for each class can change with the context. To this end, we propose contextual temperature, a generalized approach that learns an optimal temperature trajectory for each vocabulary over the context. Experimental results confirm that the proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2, respectively. In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties. These evidences further justify the need for the proposed method and its advantages over fixed temperature schedules.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 25,
        "citationCount": 7,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties, which justify the need for the proposed method and its advantages over fixed temperature schedules."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-09-25",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2012.13575"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2019ContextualTF,\n author = {Pei-Hsin Wang and Sheng-Iou Hsieh and Shih-Chieh Chang and Yu-Ting Chen and Jia-Yu Pan and Wei Wei and Da-Chang Juan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Contextual Temperature for Language Modeling},\n volume = {abs/2012.13575},\n year = {2019}\n}\n"
        }
    },
    "93_transformer_lm_+_minsen": {
        "paperId": "801a40cc1b18df679612f160b0b97e8e4e208e26",
        "externalIds": {
            "DBLP": "journals/corr/abs-2112-11540",
            "ArXiv": "2112.11540",
            "DOI": "10.1109/ICASSP39728.2021.9414076",
            "CorpusId": 235780511
        },
        "corpusId": 235780511,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/801a40cc1b18df679612f160b0b97e8e4e208e26",
        "title": "Mixed Precision Quantization of Transformer Language Models for Speech Recognition",
        "abstract": "State-of-the-art neural language models represented by Transformers are becoming increasingly complex and expensive for practical applications. Low-bit deep neural network quantization techniques provides a powerful solution to dramatically reduce their model size. Current low-bit quantization methods are based on uniform precision and fail to account for the varying performance sensitivity at different parts of the system to quantization errors. To this end, novel mixed precision DNN quantization methods are proposed in this paper. The optimal local precision settings are automatically learned using two techniques. The first is based on a quantization sensitivity metric in the form of Hessian trace weighted quantization perturbation. The second is based on mixed precision Transformer architecture search. Alternating direction methods of multipliers (ADMM) are used to efficiently train mixed precision quantized DNN systems. Experiments conducted on Penn Treebank (PTB) and a Switchboard corpus trained LF-MMI TDNN system suggest the proposed mixed precision Transformer quantization techniques achieved model size compression ratios of up to 16 times over the full precision baseline with no recognition performance degradation. When being used to compress a larger full precision Transformer LM with more layers, overall word error rate (WER) reductions up to 1.7% absolute (18% relative) were obtained.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2021,
        "referenceCount": 34,
        "citationCount": 10,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2112.11540",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Novel mixed precision DNN quantization methods based on Hessian trace weighted quantization perturbation and Alternating direction methods of multipliers are used to efficiently train mixed precision quantized DNN systems."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-06-06",
        "journal": {
            "name": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "pages": "7383-7387"
        },
        "citationStyles": {
            "bibtex": "@Article{Xu2021MixedPQ,\n author = {Junhao Xu and Shoukang Hu and Jianwei Yu and Xunying Liu and Helen M. Meng},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {7383-7387},\n title = {Mixed Precision Quantization of Transformer Language Models for Speech Recognition},\n year = {2021}\n}\n"
        }
    },
    "94_player_of_games": {
        "paperId": "cc9a562e7598ad52c14785fc8f03566e3ef3206c",
        "externalIds": {
            "DBLP": "journals/corr/abs-2112-03178",
            "CorpusId": 244908605
        },
        "corpusId": 244908605,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/cc9a562e7598ad52c14785fc8f03566e3ef3206c",
        "title": "Player of Games",
        "abstract": "Games have a long history of serving as a benchmark for progress in arti\ufb01cial intelligence. Recently, approaches using search and learning have shown strong performance across a set of perfect information games, and approaches using game-theoretic reasoning and learning have shown strong performance for speci\ufb01c imperfect information poker variants. We introduce Player of Games, a general-purpose algorithm that uni\ufb01es previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Player of Games is the \ufb01rst algorithm to achieve strong empirical performance in large perfect and imperfect information games \u2014 an important step towards truly general algorithms for arbitrary environments. We prove that Player of Games is sound, converging to perfect play as available computation time and approximation capacity increases. Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold\u2019em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 86,
        "citationCount": 0,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The introduction of Player of Games, a general-purpose algorithm that reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold\u2019em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2112.03178"
        },
        "citationStyles": {
            "bibtex": "@Article{Schmid2021PlayerOG,\n author = {Martin Schmid and Matej Moravc\u00edk and Neil Burch and Rudolf Kadlec and Joshua Davidson and K. Waugh and Nolan Bard and Finbarr Timbers and Marc Lanctot and Zach Holland and Elnaz Davoodi and Alden Christianson and Michael H. Bowling},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Player of Games},\n volume = {abs/2112.03178},\n year = {2021}\n}\n"
        }
    },
    "96_madaline_ii": {
        "paperId": "13b2cbeec852cb41502d702a74d00953351ae863",
        "externalIds": {
            "MAG": "2147208700",
            "DBLP": "journals/nn/WinterW88",
            "DOI": "10.1016/0893-6080(88)90187-6",
            "CorpusId": 61112014
        },
        "corpusId": 61112014,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/13b2cbeec852cb41502d702a74d00953351ae863",
        "title": "MADALINE RULE II: a training algorithm for neural networks",
        "abstract": null,
        "venue": "IEEE 1988 International Conference on Neural Networks",
        "year": 1988,
        "referenceCount": 4,
        "citationCount": 63,
        "influentialCitationCount": 4,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel algorithm for training multilayer fully connected feedforward networks of ADALINE neurons has been developed, called MRII for MADALINE RULE II, and Architectures that take advantage of MRII's quick learning to produce useful generalizations are presented."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "1988-07-24",
        "journal": {
            "name": "IEEE 1988 International Conference on Neural Networks",
            "pages": "401-408 vol.1"
        },
        "citationStyles": {
            "bibtex": "@Article{Winter1988MADALINERI,\n author = {R. Winter and B. Widrow},\n booktitle = {IEEE 1988 International Conference on Neural Networks},\n journal = {IEEE 1988 International Conference on Neural Networks},\n pages = {401-408 vol.1},\n title = {MADALINE RULE II: a training algorithm for neural networks},\n year = {1988}\n}\n"
        }
    },
    "97_deeply-supervised_nets": {
        "paperId": "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd",
        "externalIds": {
            "DBLP": "journals/corr/LeeXGZT14",
            "ArXiv": "1409.5185",
            "MAG": "2168894214",
            "CorpusId": 1289873
        },
        "corpusId": 1289873,
        "publicationVenue": {
            "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
            "name": "International Conference on Artificial Intelligence and Statistics",
            "type": "conference",
            "alternate_names": [
                "AISTATS",
                "Int Conf Artif Intell Stat"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd",
        "title": "Deeply-Supervised Nets",
        "abstract": "Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of learned features, especially in the early layers; (3) effectiveness in training due to the presence of the exploding and vanishing gradients. We introduce \"companion objective\" to the individual hidden layers, in addition to the overall objective at the output layer (a different strategy to layer-wise pre-training). We extend techniques from stochastic gradient methods to analyze our algorithm. The advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN).",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "year": 2014,
        "referenceCount": 39,
        "citationCount": 2027,
        "influentialCitationCount": 159,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent, and extends techniques from stochastic gradient methods to analyze the algorithm."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-09-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1409.5185"
        },
        "citationStyles": {
            "bibtex": "@Article{Lee2014DeeplySupervisedN,\n author = {Chen-Yu Lee and Saining Xie and Patrick W. Gallagher and Zhengyou Zhang and Z. Tu},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n journal = {ArXiv},\n title = {Deeply-Supervised Nets},\n volume = {abs/1409.5185},\n year = {2014}\n}\n"
        }
    },
    "98_gpt2-layerfusion-ws": {
        "paperId": "7b553b93682c25c8f03c97a7f8a05842c57ef2aa",
        "externalIds": {
            "MAG": "3046174616",
            "ArXiv": "2007.14917",
            "DBLP": "journals/corr/abs-2007-14917",
            "CorpusId": 220845722
        },
        "corpusId": 220845722,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/7b553b93682c25c8f03c97a7f8a05842c57ef2aa",
        "title": "Compressing Deep Neural Networks via Layer Fusion",
        "abstract": "This paper proposes \\textit{layer fusion} - a model compression technique that discovers which weights to combine and then fuses weights of similar fully-connected, convolutional and attention layers. Layer fusion can significantly reduce the number of layers of the original network with little additional computation overhead, while maintaining competitive performance. From experiments on CIFAR-10, we find that various deep convolution neural networks can remain within 2\\% accuracy points of the original networks up to a compression ratio of 3.33 when iteratively retrained with layer fusion. For experiments on the WikiText-2 language modelling dataset where pretrained transformer models are used, we achieve compression that leads to a network that is 20\\% of its original size while being within 5 perplexity points of the original network. We also find that other well-established compression techniques can achieve competitive performance when compared to their original networks given a sufficient number of retraining steps. Generally, we observe a clear inflection point in performance as the amount of compression increases, suggesting a bound on the amount of compression that can be achieved before an exponential degradation in performance.",
        "venue": "arXiv.org",
        "year": 2020,
        "referenceCount": 41,
        "citationCount": 3,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes layer fusion - a model compression technique that discovers which weights to combine and then fuses weights of similar fully-connected, convolutional and attention layers, suggesting a bound on the amount of compression that can be achieved before an exponential degradation in performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-07-29",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2007.14917"
        },
        "citationStyles": {
            "bibtex": "@Article{O'Neill2020CompressingDN,\n author = {James O'Neill and G. V. Steeg and A. Galstyan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Compressing Deep Neural Networks via Layer Fusion},\n volume = {abs/2007.14917},\n year = {2020}\n}\n"
        }
    },
    "99_data2vec_(vision)": {
        "paperId": "8f2bca9d684005675e294b33c26481e36f528cdb",
        "externalIds": {
            "ArXiv": "2202.03555",
            "DBLP": "conf/icml/BaevskiHXBGA22",
            "CorpusId": 246652264
        },
        "corpusId": 246652264,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/8f2bca9d684005675e294b33c26481e36f528cdb",
        "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
        "abstract": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "referenceCount": 86,
        "citationCount": 523,
        "influentialCitationCount": 93,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Data2vec is a framework that uses the same learning method for either speech, NLP or computer vision to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-02-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2202.03555"
        },
        "citationStyles": {
            "bibtex": "@Article{Baevski2022data2vecAG,\n author = {Alexei Baevski and Wei-Ning Hsu and Qiantong Xu and Arun Babu and Jiatao Gu and Michael Auli},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language},\n volume = {abs/2202.03555},\n year = {2022}\n}\n"
        }
    },
    "100_blenderbot_3": {
        "paperId": "a3076ecfed0571fbbb5217a5cc6b4b6f24f6f7dd",
        "externalIds": {
            "ArXiv": "2208.03188",
            "DBLP": "journals/corr/abs-2208-03188",
            "DOI": "10.48550/arXiv.2208.03188",
            "CorpusId": 251371589
        },
        "corpusId": 251371589,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/a3076ecfed0571fbbb5217a5cc6b4b6f24f6f7dd",
        "title": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage",
        "abstract": "We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 111,
        "citationCount": 171,
        "influentialCitationCount": 21,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2208.03188",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The goal of this research program is to enable the community to study ever-improving responsible agents that learn through interaction and release both the model weights and code, and the plan for continual learning using the data collected from deployment, which will also be publicly released."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-08-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2208.03188"
        },
        "citationStyles": {
            "bibtex": "@Article{Shuster2022BlenderBot3A,\n author = {Kurt Shuster and Jing Xu and M. Komeili and Da Ju and Eric Michael Smith and Stephen Roller and Megan Ung and Moya Chen and Kushal Arora and Joshua Lane and Morteza Behrooz and W.K.F. Ngan and Spencer Poff and Naman Goyal and Arthur Szlam and Y-Lan Boureau and Melanie Kambadur and J. Weston},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage},\n volume = {abs/2208.03188},\n year = {2022}\n}\n"
        }
    },
    "103_r-cnn_(t-net)": {
        "paperId": "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
        "externalIds": {
            "DBLP": "journals/corr/GirshickDDM13",
            "MAG": "2951638509",
            "ArXiv": "1311.2524",
            "DOI": "10.1109/CVPR.2014.81",
            "CorpusId": 215827080
        },
        "corpusId": 215827080,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/2f4df08d9072fc2ac181b7fced6a245315ce05c8",
        "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
        "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
        "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
        "year": 2013,
        "referenceCount": 55,
        "citationCount": 22985,
        "influentialCitationCount": 2762,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1311.2524",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2013-11-11",
        "journal": {
            "name": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
            "pages": "580-587"
        },
        "citationStyles": {
            "bibtex": "@Article{Girshick2013RichFH,\n author = {Ross B. Girshick and Jeff Donahue and Trevor Darrell and J. Malik},\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {580-587},\n title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},\n year = {2013}\n}\n"
        }
    },
    "104_fraternal_dropout_+_awd-lstm_3-layer_(wt2)": {
        "paperId": "415f18130edbe06e3e4806dfb0a1edcab6c241eb",
        "externalIds": {
            "ArXiv": "1711.00066",
            "MAG": "2785950808",
            "DBLP": "conf/iclr/ZolnaASB18",
            "CorpusId": 4410570
        },
        "corpusId": 4410570,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/415f18130edbe06e3e4806dfb0a1edcab6c241eb",
        "title": "Fraternal Dropout",
        "abstract": "Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks.",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "referenceCount": 31,
        "citationCount": 47,
        "influentialCitationCount": 5,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simple technique called fraternal dropout is proposed that takes advantage of dropout to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-10-31",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1711.00066"
        },
        "citationStyles": {
            "bibtex": "@Article{Zolna2017FraternalD,\n author = {Konrad Zolna and Devansh Arpit and Dendi Suhubdy and Yoshua Bengio},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Fraternal Dropout},\n volume = {abs/1711.00066},\n year = {2017}\n}\n"
        }
    },
    "106_deep_rectifier_networks": {
        "paperId": "67107f78a84bdb2411053cb54e94fa226eea6d8e",
        "externalIds": {
            "MAG": "2156387975",
            "DBLP": "journals/jmlr/GlorotBB11",
            "CorpusId": 2239473
        },
        "corpusId": 2239473,
        "publicationVenue": {
            "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
            "name": "International Conference on Artificial Intelligence and Statistics",
            "type": "conference",
            "alternate_names": [
                "AISTATS",
                "Int Conf Artif Intell Stat"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/67107f78a84bdb2411053cb54e94fa226eea6d8e",
        "title": "Deep Sparse Rectifier Neural Networks",
        "abstract": "While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-dierentiabil ity",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "year": 2011,
        "referenceCount": 37,
        "citationCount": 7496,
        "influentialCitationCount": 418,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-dierentiabil ity."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2011-06-14",
        "journal": {
            "pages": "315-323"
        },
        "citationStyles": {
            "bibtex": "@Article{Glorot2011DeepSR,\n author = {Xavier Glorot and Antoine Bordes and Yoshua Bengio},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n pages = {315-323},\n title = {Deep Sparse Rectifier Neural Networks},\n year = {2011}\n}\n"
        }
    },
    "107_hyperclova": {
        "paperId": "a6d8d04962f84ae6225e72723869a002b9fc8036",
        "externalIds": {
            "DBLP": "conf/emnlp/KimKLLKJ0KKSLJL21",
            "ArXiv": "2109.04650",
            "ACL": "2021.emnlp-main.274",
            "DOI": "10.18653/v1/2021.emnlp-main.274",
            "CorpusId": 237485423
        },
        "corpusId": 237485423,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/a6d8d04962f84ae6225e72723869a002b9fc8036",
        "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers",
        "abstract": "GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "referenceCount": 57,
        "citationCount": 85,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.274.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface is discussed and the performance benefits of prompt-based learning are shown and how it can be integrated into the prompt engineering pipeline."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-09-10",
        "journal": {
            "pages": "3405-3424"
        },
        "citationStyles": {
            "bibtex": "@Article{Kim2021WhatCC,\n author = {Boseop Kim and Hyoungseok Kim and Sang-Woo Lee and Gichang Lee and Donghyun Kwak and D. Jeon and Sunghyun Park and Sung-ju Kim and Seonhoon Kim and D. Seo and Heungsub Lee and Minyoung Jeong and Sungjae Lee and Minsub Kim and SukHyun Ko and Seokhun Kim and Taeyong Park and Jinuk Kim and Soyoung Kang and Nahyeon Ryu and Kang Min Yoo and Minsuk Chang and Soobin Suh and Sookyo In and Jinseong Park and Kyungduk Kim and Hiun Kim and Jisu Jeong and Yong Goo Yeo and Dong-hyun Ham and Dongju Park and Min Young Lee and Jaewook Kang and Inho Kang and Jung-Woo Ha and W. Park and Nako Sung},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {3405-3424},\n title = {What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers},\n year = {2021}\n}\n"
        }
    },
    "108_base_lm_+_knn_lm_+_continuous_cache": {
        "paperId": "7be8c119dbe065c52125ee7716601751f3116844",
        "externalIds": {
            "MAG": "2988841832",
            "ArXiv": "1911.00172",
            "DBLP": "journals/corr/abs-1911-00172",
            "CorpusId": 207870430
        },
        "corpusId": 207870430,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7be8c119dbe065c52125ee7716601751f3116844",
        "title": "Generalization through Memorization: Nearest Neighbor Language Models",
        "abstract": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 32,
        "citationCount": 518,
        "influentialCitationCount": 65,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is suggested that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-11-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1911.00172"
        },
        "citationStyles": {
            "bibtex": "@Article{Khandelwal2019GeneralizationTM,\n author = {Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and M. Lewis},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Generalization through Memorization: Nearest Neighbor Language Models},\n volume = {abs/1911.00172},\n year = {2019}\n}\n"
        }
    },
    "109_longnet": {
        "paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91",
        "externalIds": {
            "DBLP": "journals/corr/abs-2307-02486",
            "ArXiv": "2307.02486",
            "DOI": "10.48550/arXiv.2307.02486",
            "CorpusId": 259341682
        },
        "corpusId": 259341682,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/c12db2c60e8989f646a29ad4f4d24475e860ad91",
        "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens",
        "abstract": "Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. In this work, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between tokens; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 53,
        "citationCount": 48,
        "influentialCitationCount": 5,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.02486",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences, and proposes dilated attention, which expands the attentive field exponentially as the distance grows."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-07-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2307.02486"
        },
        "citationStyles": {
            "bibtex": "@Article{Ding2023LongNetST,\n author = {Jiayu Ding and Shuming Ma and Li Dong and Xingxing Zhang and Shaohan Huang and Wenhui Wang and Furu Wei},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens},\n volume = {abs/2307.02486},\n year = {2023}\n}\n"
        }
    },
    "110_cross-lingual_pos_tagger": {
        "paperId": "343733a063e491d234a36d3e1090a739318b3566",
        "externalIds": {
            "MAG": "2142523187",
            "DBLP": "conf/acl/DasP11",
            "ACL": "P11-1061",
            "CorpusId": 8396953
        },
        "corpusId": 8396953,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/343733a063e491d234a36d3e1090a739318b3566",
        "title": "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections",
        "abstract": "We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2011,
        "referenceCount": 28,
        "citationCount": 307,
        "influentialCitationCount": 21,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language, using graph-based label propagation for cross-lingual knowledge transfer."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2011-06-19",
        "journal": {
            "pages": "600-609"
        },
        "citationStyles": {
            "bibtex": "@Article{Das2011UnsupervisedPT,\n author = {Dipanjan Das and Slav Petrov},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {600-609},\n title = {Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections},\n year = {2011}\n}\n"
        }
    },
    "111_local_binary_patterns_for_facial_recognition": {
        "paperId": "e7c4665ce36a53484f8a7b7dfa821a9f6273eab4",
        "externalIds": {
            "MAG": "2163808566",
            "DBLP": "journals/pami/AhonenHP06",
            "DOI": "10.1109/TPAMI.2006.244",
            "CorpusId": 369876,
            "PubMed": "17108377"
        },
        "corpusId": 369876,
        "publicationVenue": {
            "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
            "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Pattern Anal Mach Intell"
            ],
            "issn": "0162-8828",
            "url": "http://www.computer.org/tpami/",
            "alternate_urls": [
                "http://www.computer.org/portal/web/tpami",
                "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e7c4665ce36a53484f8a7b7dfa821a9f6273eab4",
        "title": "Face Description with Local Binary Patterns: Application to Face Recognition",
        "abstract": "This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "year": 2006,
        "referenceCount": 31,
        "citationCount": 5711,
        "influentialCitationCount": 638,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features that is assessed in the face recognition problem under different challenges."
        },
        "publicationTypes": [
            "JournalArticle",
            "Study"
        ],
        "publicationDate": "2006-12-01",
        "journal": {
            "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "pages": "2037-2041",
            "volume": "28"
        },
        "citationStyles": {
            "bibtex": "@Article{Ahonen2006FaceDW,\n author = {T. Ahonen and A. Hadid and M. Pietik\u00e4inen},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {2037-2041},\n title = {Face Description with Local Binary Patterns: Application to Face Recognition},\n volume = {28},\n year = {2006}\n}\n"
        }
    },
    "112_constituency-tree_lstm": {
        "paperId": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
        "externalIds": {
            "MAG": "2963355447",
            "DBLP": "conf/acl/TaiSM15",
            "ArXiv": "1503.00075",
            "ACL": "P15-1150",
            "DOI": "10.3115/v1/P15-1150",
            "CorpusId": 3033526
        },
        "corpusId": 3033526,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/32de44f01a96d4473d21099d15e25bc2b9f08e2f",
        "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2015,
        "referenceCount": 41,
        "citationCount": 2940,
        "influentialCitationCount": 420,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/P15-1150.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Tree-LSTM is introduced, a generalization of LSTMs to tree-structured network topologies that outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences and sentiment classification."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-02-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1503.00075"
        },
        "citationStyles": {
            "bibtex": "@Article{Tai2015ImprovedSR,\n author = {Kai Sheng Tai and R. Socher and Christopher D. Manning},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},\n volume = {abs/1503.00075},\n year = {2015}\n}\n"
        }
    },
    "113_alvinn": {
        "paperId": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
        "externalIds": {
            "CorpusId": 18420840
        },
        "corpusId": 18420840,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
        "title": "ALVINN, an autonomous land vehicle in a neural network",
        "abstract": "The support-vector network is a new leaming machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very highdimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.",
        "venue": "",
        "year": 2015,
        "referenceCount": 6,
        "citationCount": 1736,
        "influentialCitationCount": 108,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": null,
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated and the performance of the support- vector network is compared to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Pomerleau2015ALVINNAA,\n author = {D. Pomerleau},\n title = {ALVINN, an autonomous land vehicle in a neural network},\n year = {2015}\n}\n"
        }
    },
    "115_qmoe:_practical_sub-1-bit_compression_of_trillion-parameter_models": {
        "paperId": "b136e8de996b95f2295b40bf367e5512c937b61e",
        "externalIds": {
            "DBLP": "journals/corr/abs-2310-16795",
            "ArXiv": "2310.16795",
            "DOI": "10.48550/arXiv.2310.16795",
            "CorpusId": 264451996
        },
        "corpusId": 264451996,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/b136e8de996b95f2295b40bf367e5512c937b61e",
        "title": "QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models",
        "abstract": "Mixture-of-Experts (MoE) architectures offer a general solution to the high inference costs of large language models (LLMs) via sparse routing, bringing faster and more accurate models, at the cost of massive parameter counts. For example, the SwitchTransformer-c2048 model has 1.6 trillion parameters, requiring 3.2TB of accelerator memory to run efficiently, which makes practical deployment challenging and expensive. In this paper, we present a solution to this memory problem, in form of a new compression and execution framework called QMoE. Specifically, QMoE consists of a scalable algorithm which accurately compresses trillion-parameter MoEs to less than 1 bit per parameter, in a custom format co-designed with bespoke GPU decoding kernels to facilitate efficient end-to-end compressed inference, with minor runtime overheads relative to uncompressed execution. Concretely, QMoE can compress the 1.6 trillion parameter SwitchTransformer-c2048 model to less than 160GB (20x compression, 0.8 bits per parameter) at only minor accuracy loss, in less than a day on a single GPU. This enables, for the first time, the execution of a trillion-parameter model on affordable commodity hardware, like a single server with 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs, at less than 5% runtime overhead relative to ideal uncompressed inference. The source code and compressed models are available at github.com/IST-DASLab/qmoe.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 51,
        "citationCount": 5,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "QMoE enables, for the first time, the execution of a trillion-parameter model on affordable commodity hardware, like a single server with 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs, at less than 5% runtime overhead relative to ideal uncompressed inference."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-10-25",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2310.16795"
        },
        "citationStyles": {
            "bibtex": "@Article{Frantar2023QMoEPS,\n author = {Elias Frantar and Dan Alistarh},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models},\n volume = {abs/2310.16795},\n year = {2023}\n}\n"
        }
    },
    "116_lstm+adam+lookahead": {
        "paperId": "600be3dde18d1059c6b56170bd04ee65ce79a848",
        "externalIds": {
            "DBLP": "journals/corr/abs-1907-08610",
            "MAG": "2970803838",
            "ArXiv": "1907.08610",
            "CorpusId": 197935378
        },
        "corpusId": 197935378,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/600be3dde18d1059c6b56170bd04ee65ce79a848",
        "title": "Lookahead Optimizer: k steps forward, 1 step back",
        "abstract": "The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "referenceCount": 51,
        "citationCount": 599,
        "influentialCitationCount": 76,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost, and can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-07-19",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1907.08610"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2019LookaheadOK,\n author = {Michael Ruogu Zhang and James Lucas and Geoffrey E. Hinton and Jimmy Ba},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Lookahead Optimizer: k steps forward, 1 step back},\n volume = {abs/1907.08610},\n year = {2019}\n}\n"
        }
    },
    "117_dinov2": {
        "paperId": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891",
        "externalIds": {
            "ArXiv": "2304.07193",
            "DBLP": "journals/corr/abs-2304-07193",
            "DOI": "10.48550/arXiv.2304.07193",
            "CorpusId": 258170077
        },
        "corpusId": 258170077,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891",
        "title": "DINOv2: Learning Robust Visual Features without Supervision",
        "abstract": "The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 140,
        "citationCount": 490,
        "influentialCitationCount": 94,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2304.07193",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work revisits existing approaches and combines different techniques to scale the pretraining in terms of data and model size, and proposes an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-04-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2304.07193"
        },
        "citationStyles": {
            "bibtex": "@Article{Oquab2023DINOv2LR,\n author = {M. Oquab and Timoth'ee Darcet and T. Moutakanni and Huy Q. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russ Howes and Po-Yao (Bernie) Huang and Shang-Wen Li and Ishan Misra and Michael G. Rabbat and Vasu Sharma and Gabriel Synnaeve and Huijiao Xu and H. J\u00e9gou and J. Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {DINOv2: Learning Robust Visual Features without Supervision},\n volume = {abs/2304.07193},\n year = {2023}\n}\n"
        }
    },
    "118_srgan": {
        "paperId": "df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3",
        "externalIds": {
            "DBLP": "conf/cvpr/LedigTHCCAATTWS17",
            "MAG": "2963470893",
            "ArXiv": "1609.04802",
            "DOI": "10.1109/CVPR.2017.19",
            "CorpusId": 211227
        },
        "corpusId": 211227,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3",
        "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
        "abstract": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 77,
        "citationCount": 9116,
        "influentialCitationCount": 1157,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1609.04802",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "SRGAN, a generative adversarial network (GAN) for image super-resolution (SR), is presented, to its knowledge, the first framework capable of inferring photo-realistic natural images for 4x upscaling factors and a perceptual loss function which consists of an adversarial loss and a content loss."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-09-15",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "105-114"
        },
        "citationStyles": {
            "bibtex": "@Article{Ledig2016PhotoRealisticSI,\n author = {C. Ledig and Lucas Theis and Ferenc Husz\u00e1r and Jose Caballero and Andrew P. Aitken and A. Tejani and J. Totz and Zehan Wang and Wenzhe Shi},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {105-114},\n title = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},\n year = {2016}\n}\n"
        }
    },
    "119_ulm-fit": {
        "paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a",
        "externalIds": {
            "MAG": "2798812533",
            "DBLP": "conf/acl/RuderH18",
            "ACL": "P18-1031",
            "DOI": "10.18653/v1/P18-1031",
            "CorpusId": 40100965
        },
        "corpusId": 40100965,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/1e077413b25c4d34945cc2707e17e46ed4fe784a",
        "title": "Universal Language Model Fine-tuning for Text Classification",
        "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "referenceCount": 57,
        "citationCount": 3129,
        "influentialCitationCount": 295,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-1031.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduces techniques that are key for fine- Tuning a language model."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-01-18",
        "journal": {
            "pages": "328-339"
        },
        "citationStyles": {
            "bibtex": "@Article{Howard2018UniversalLM,\n author = {Jeremy Howard and Sebastian Ruder},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {328-339},\n title = {Universal Language Model Fine-tuning for Text Classification},\n year = {2018}\n}\n"
        }
    },
    "120_mobilenet": {
        "paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e",
        "externalIds": {
            "DBLP": "journals/corr/HowardZCKWWAA17",
            "ArXiv": "1704.04861",
            "MAG": "2612445135",
            "CorpusId": 12670695
        },
        "corpusId": 12670695,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/3647d6d0f151dc05626449ee09cc7bce55be497e",
        "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
        "abstract": "We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.",
        "venue": "arXiv.org",
        "year": 2017,
        "referenceCount": 38,
        "citationCount": 16619,
        "influentialCitationCount": 2542,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces two simple global hyper-parameters that efficiently trade off between latency and accuracy and demonstrates the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-04-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1704.04861"
        },
        "citationStyles": {
            "bibtex": "@Article{Howard2017MobileNetsEC,\n author = {Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and M. Andreetto and Hartwig Adam},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},\n volume = {abs/1704.04861},\n year = {2017}\n}\n"
        }
    },
    "121_joint_probability_machine_translation": {
        "paperId": "32a9ba4a76d1e9948c1cb980800ad117531753f8",
        "externalIds": {
            "DBLP": "conf/emnlp/MarcuW02",
            "ACL": "W02-1018",
            "MAG": "2161792612",
            "DOI": "10.3115/1118693.1118711",
            "CorpusId": 1567400
        },
        "corpusId": 1567400,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/32a9ba4a76d1e9948c1cb980800ad117531753f8",
        "title": "A Phrase-Based,Joint Probability Model for Statistical Machine Translation",
        "abstract": "We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2002,
        "referenceCount": 16,
        "citationCount": 526,
        "influentialCitationCount": 33,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1118693.1118711",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A joint probability model for statistical machine translation is presented, which automatically learns word and phrase equivalents from bilingual corpora, which is more accurate than translations produced using IBM Model 4."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2002-07-06",
        "journal": {
            "pages": "133-139"
        },
        "citationStyles": {
            "bibtex": "@Article{Marcu2002APP,\n author = {D. Marcu and Daniel Wong},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {133-139},\n title = {A Phrase-Based,Joint Probability Model for Statistical Machine Translation},\n year = {2002}\n}\n"
        }
    },
    "122_alphago_master": {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "externalIds": {
            "MAG": "2766447205",
            "DBLP": "journals/nature/SilverSSAHGHBLB17",
            "DOI": "10.1038/nature24270",
            "CorpusId": 205261034,
            "PubMed": "29052630"
        },
        "corpusId": 205261034,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge",
        "abstract": null,
        "venue": "Nature",
        "year": 2017,
        "referenceCount": 68,
        "citationCount": 8044,
        "influentialCitationCount": 354,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://discovery.ucl.ac.uk/10045895/1/agz_unformatted_nature.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An algorithm based solely on reinforcement learning is introduced, without human data, guidance or domain knowledge beyond game rules, that achieves superhuman performance, winning 100\u20130 against the previously published, champion-defeating AlphaGo."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-10-19",
        "journal": {
            "name": "Nature",
            "pages": "354-359",
            "volume": "550"
        },
        "citationStyles": {
            "bibtex": "@Article{Silver2017MasteringTG,\n author = {David Silver and Julian Schrittwieser and K. Simonyan and Ioannis Antonoglou and Aja Huang and A. Guez and T. Hubert and Lucas baker and Matthew Lai and A. Bolton and Yutian Chen and T. Lillicrap and Fan Hui and L. Sifre and George van den Driessche and T. Graepel and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {354-359},\n title = {Mastering the game of Go without human knowledge},\n volume = {550},\n year = {2017}\n}\n"
        }
    },
    "123_xglm-7.5b": {
        "paperId": "1403e6b9adf7712c35ae56327d52fe54603b87e1",
        "externalIds": {
            "DBLP": "journals/corr/abs-2112-10668",
            "CorpusId": 260651613
        },
        "corpusId": 260651613,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/1403e6b9adf7712c35ae56327d52fe54603b87e1",
        "title": "Few-shot Learning with Multilingual Language Models",
        "abstract": "Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, and study their few-and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We present a detailed analysis of where the model suc-ceeds and fails, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models in social value tasks such as hate speech detection in 5 languages and find it has limitations similar to comparably sized GPT-3 models.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 85,
        "citationCount": 116,
        "influentialCitationCount": 16,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A detailed analysis of where the model succeeds and fails is presented, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2112.10668"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2021FewshotLW,\n author = {Xi Victoria Lin and Todor Mihaylov and Mikel Artetxe and Tianlu Wang and Shuohui Chen and Daniel Simig and Myle Ott and Naman Goyal and Shruti Bhosale and Jingfei Du and Ramakanth Pasunuru and Sam Shleifer and Punit Singh Koura and Vishrav Chaudhary and Brian O'Horo and Jeff Wang and Luke Zettlemoyer and Zornitsa Kozareva and Mona T. Diab and Ves Stoyanov and Xian Li},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Few-shot Learning with Multilingual Language Models},\n volume = {abs/2112.10668},\n year = {2021}\n}\n"
        }
    },
    "124_transformer_(adaptive_input_embeddings)": {
        "paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299",
        "externalIds": {
            "MAG": "2894175714",
            "DBLP": "conf/iclr/BaevskiA19",
            "ArXiv": "1809.10853",
            "CorpusId": 52892477
        },
        "corpusId": 52892477,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/d170bd486e4c0fe82601e322b0e9e0dde63ab299",
        "title": "Adaptive Input Representations for Neural Language Modeling",
        "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 35,
        "citationCount": 332,
        "influentialCitationCount": 45,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Adapt input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity are introduced and a systematic comparison of popular choices for a self-attentional architecture is performed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-09-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1809.10853"
        },
        "citationStyles": {
            "bibtex": "@Article{Baevski2018AdaptiveIR,\n author = {Alexei Baevski and Michael Auli},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Adaptive Input Representations for Neural Language Modeling},\n volume = {abs/1809.10853},\n year = {2018}\n}\n"
        }
    },
    "125_deep_multitask_nlp_network": {
        "paperId": "57458bc1cffe5caa45a885af986d70f723f406b4",
        "externalIds": {
            "MAG": "2117130368",
            "DBLP": "conf/icml/CollobertW08",
            "DOI": "10.1145/1390156.1390177",
            "CorpusId": 2617020
        },
        "corpusId": 2617020,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/57458bc1cffe5caa45a885af986d70f723f406b4",
        "title": "A unified architecture for natural language processing: deep neural networks with multitask learning",
        "abstract": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.",
        "venue": "International Conference on Machine Learning",
        "year": 2008,
        "referenceCount": 23,
        "citationCount": 5597,
        "influentialCitationCount": 297,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work describes a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense using a language model."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2008-07-05",
        "journal": {
            "pages": "160-167"
        },
        "citationStyles": {
            "bibtex": "@Article{Collobert2008AUA,\n author = {R. Collobert and J. Weston},\n booktitle = {International Conference on Machine Learning},\n pages = {160-167},\n title = {A unified architecture for natural language processing: deep neural networks with multitask learning},\n year = {2008}\n}\n"
        }
    },
    "127_proteinlm": {
        "paperId": "b42d20ec9580ebd76860890a1d7a7fdcc742677e",
        "externalIds": {
            "ArXiv": "2108.07435",
            "DBLP": "journals/corr/abs-2108-07435",
            "CorpusId": 237142472
        },
        "corpusId": 237142472,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/b42d20ec9580ebd76860890a1d7a7fdcc742677e",
        "title": "Modeling Protein Using Large-scale Pretrain Language Model",
        "abstract": "Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token-level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 28,
        "citationCount": 18,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Significant improvements are observed in both token-level and sequence-level tasks, demonstrating that the large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-08-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2108.07435"
        },
        "citationStyles": {
            "bibtex": "@Article{Xiao2021ModelingPU,\n author = {Yijia Xiao and J. Qiu and Ziang Li and Chang-Yu Hsieh and Jie Tang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Modeling Protein Using Large-scale Pretrain Language Model},\n volume = {abs/2108.07435},\n year = {2021}\n}\n"
        }
    },
    "128_reading_twice_for_nlu": {
        "paperId": "2f92b10acf7c405e55c74c1043dabd9ded1b1800",
        "externalIds": {
            "MAG": "2766508367",
            "ArXiv": "1706.02596",
            "CorpusId": 836118
        },
        "corpusId": 836118,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/2f92b10acf7c405e55c74c1043dabd9ded1b1800",
        "title": "Dynamic Integration of Background Knowledge in Neural NLU Systems",
        "abstract": "Common-sense or background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, the requisite background knowledge is indirectly acquired from static corpora. We develop a new reading architecture for the dynamic integration of explicit background knowledge in NLU models. A new task-agnostic reading module provides refined word representations to a task-specific NLU architecture by processing background knowledge in the form of free-text statements, together with the task-specific inputs. Strong performance on the tasks of document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of our approach. Analysis shows that our models learn to exploit knowledge selectively and in a semantically appropriate way.",
        "venue": "",
        "year": 2017,
        "referenceCount": 35,
        "citationCount": 60,
        "influentialCitationCount": 4,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new reading architecture for the dynamic integration of explicit background knowledge in NLU models is developed by developing a new task-agnostic reading module that provides refined word representations to a task-specific NLU architecture by processing background knowledge from static corpora in the form of free-text statements."
        },
        "publicationTypes": null,
        "publicationDate": "2017-06-08",
        "journal": {
            "name": "arXiv: Computation and Language",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Weissenborn2017DynamicIO,\n author = {Dirk Weissenborn and Tom'avs Kovcisk'y and Chris Dyer},\n journal = {arXiv: Computation and Language},\n title = {Dynamic Integration of Background Knowledge in Neural NLU Systems},\n year = {2017}\n}\n"
        }
    },
    "129_hide_and_seek": {
        "paperId": "33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b",
        "externalIds": {
            "ArXiv": "1909.07528",
            "DBLP": "journals/corr/abs-1909-07528",
            "MAG": "2973525135",
            "CorpusId": 202583612
        },
        "corpusId": 202583612,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b",
        "title": "Emergent Tool Use From Multi-Agent Autocurricula",
        "abstract": "Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 82,
        "citationCount": 533,
        "influentialCitationCount": 40,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work finds clear evidence of six emergent phases in agent strategy in the authors' environment, each of which creates a new pressure for the opposing team to adapt, and compares hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-09-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1909.07528"
        },
        "citationStyles": {
            "bibtex": "@Article{Baker2019EmergentTU,\n author = {Bowen Baker and I. Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Emergent Tool Use From Multi-Agent Autocurricula},\n volume = {abs/1909.07528},\n year = {2019}\n}\n"
        }
    },
    "130_genetic_algorithm": {
        "paperId": "a35f730be760c63da1d5ff011d45f1153c3737a8",
        "externalIds": {
            "MAG": "3011244588",
            "DOI": "10.1007/bf01556602",
            "CorpusId": 86717105
        },
        "corpusId": 86717105,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/a35f730be760c63da1d5ff011d45f1153c3737a8",
        "title": "Numerical testing of evolution theories",
        "abstract": null,
        "venue": "",
        "year": 1963,
        "referenceCount": 17,
        "citationCount": 150,
        "influentialCitationCount": 2,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Biology",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An interpretive system for the IBM 704 computer permitting interpretation of the genetic pattern of a numeric symbioorganism as a game strategy has been developed and a startling infection process caused by a parasite whose behaviour was influenced by the game competition is described."
        },
        "publicationTypes": null,
        "publicationDate": "1963-09-01",
        "journal": {
            "name": "Acta Biotheoretica",
            "pages": "99-126",
            "volume": "16"
        },
        "citationStyles": {
            "bibtex": "@Article{Barricelli1963NumericalTO,\n author = {N. A. Barricelli},\n journal = {Acta Biotheoretica},\n pages = {99-126},\n title = {Numerical testing of evolution theories},\n volume = {16},\n year = {1963}\n}\n"
        }
    },
    "131_xlm-roberta": {
        "paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6",
        "externalIds": {
            "MAG": "2983040767",
            "ArXiv": "1911.02116",
            "ACL": "2020.acl-main.747",
            "DBLP": "conf/acl/ConneauKGCWGGOZ20",
            "DOI": "10.18653/v1/2020.acl-main.747",
            "CorpusId": 207880568
        },
        "corpusId": 207880568,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6",
        "title": "Unsupervised Cross-lingual Representation Learning at Scale",
        "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "referenceCount": 42,
        "citationCount": 4457,
        "influentialCitationCount": 1134,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.747.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks, and the possibility of multilingual modeling without sacrificing per-language performance is shown for the first time."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-11-05",
        "journal": {
            "pages": "8440-8451"
        },
        "citationStyles": {
            "bibtex": "@Article{Conneau2019UnsupervisedCR,\n author = {Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzm\u00e1n and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {8440-8451},\n title = {Unsupervised Cross-lingual Representation Learning at Scale},\n year = {2019}\n}\n"
        }
    },
    "132_wizardlm-7b": {
        "paperId": "131f499e4d3503da93022d07fcf804a18483bea9",
        "externalIds": {
            "ArXiv": "2304.12244",
            "DBLP": "journals/corr/abs-2304-12244",
            "DOI": "10.48550/arXiv.2304.12244",
            "CorpusId": 258298159
        },
        "corpusId": 258298159,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/131f499e4d3503da93022d07fcf804a18483bea9",
        "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
        "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 43,
        "citationCount": 353,
        "influentialCitationCount": 64,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2304.12244",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs, and it is demonstrated that outputs from the authors' WizardLM are preferred to outputs from OpenAI ChatGPT."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-04-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2304.12244"
        },
        "citationStyles": {
            "bibtex": "@Article{Xu2023WizardLMEL,\n author = {Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {WizardLM: Empowering Large Language Models to Follow Complex Instructions},\n volume = {abs/2304.12244},\n year = {2023}\n}\n"
        }
    },
    "133_ddpm-ip_(celeba)": {
        "paperId": "e6a662cec0ad532371099a1397fbd4196ab8d8a1",
        "externalIds": {
            "ArXiv": "2301.11706",
            "DBLP": "conf/icml/NingSPCC23",
            "DOI": "10.48550/arXiv.2301.11706",
            "CorpusId": 256358422
        },
        "corpusId": 256358422,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/e6a662cec0ad532371099a1397fbd4196ab8d8a1",
        "title": "Input Perturbation Reduces Exposure Bias in Diffusion Models",
        "abstract": "Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64$\\times$64, we achieve a new state-of-the-art FID score of 1.27, while saving 37.5% of the training time. The code is publicly available at https://github.com/forever208/DDPM-IP",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "referenceCount": 58,
        "citationCount": 20,
        "influentialCitationCount": 5,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2301.11706",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper observes that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation, and proposes a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-01-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2301.11706"
        },
        "citationStyles": {
            "bibtex": "@Article{Ning2023InputPR,\n author = {Mang Ning and E. Sangineto and Angelo Porrello and S. Calderara and R. Cucchiara},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Input Perturbation Reduces Exposure Bias in Diffusion Models},\n volume = {abs/2301.11706},\n year = {2023}\n}\n"
        }
    },
    "134_glide": {
        "paperId": "7002ae048e4b8c9133a55428441e8066070995cb",
        "externalIds": {
            "ArXiv": "2112.10741",
            "DBLP": "journals/corr/abs-2112-10741",
            "CorpusId": 245335086
        },
        "corpusId": 245335086,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7002ae048e4b8c9133a55428441e8066070995cb",
        "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
        "abstract": "Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 51,
        "citationCount": 1777,
        "influentialCitationCount": 198,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work explores diffusion models for the problem of text-conditional image synthesis and compares two different guidance strategies: CLIP guidance and classifier-free guidance, finding that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-12-20",
        "journal": {
            "pages": "16784-16804"
        },
        "citationStyles": {
            "bibtex": "@Article{Nichol2021GLIDETP,\n author = {Alex Nichol and Prafulla Dhariwal and A. Ramesh and Pranav Shyam and Pamela Mishkin and Bob McGrew and I. Sutskever and Mark Chen},\n booktitle = {International Conference on Machine Learning},\n pages = {16784-16804},\n title = {GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},\n year = {2021}\n}\n"
        }
    },
    "135_stable_diffusion_xl": {
        "paperId": "d7890d1906d95c4ae4c430b350455156d6d8aed9",
        "externalIds": {
            "ArXiv": "2307.01952",
            "DBLP": "journals/corr/abs-2307-01952",
            "DOI": "10.48550/arXiv.2307.01952",
            "CorpusId": 259341735
        },
        "corpusId": 259341735,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/d7890d1906d95c4ae4c430b350455156d6d8aed9",
        "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
        "abstract": "We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 56,
        "citationCount": 282,
        "influentialCitationCount": 81,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.01952",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-07-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2307.01952"
        },
        "citationStyles": {
            "bibtex": "@Article{Podell2023SDXLIL,\n author = {Dustin Podell and Zion English and Kyle Lacey and A. Blattmann and Tim Dockhorn and Jonas Muller and Joe Penna and Robin Rombach},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis},\n volume = {abs/2307.01952},\n year = {2023}\n}\n"
        }
    },
    "136_naive_bayes": {
        "paperId": "b07ce649d6f6eb636872527104b0209d3edc8188",
        "externalIds": {
            "MAG": "167088100",
            "DBLP": "books/lib/DudaH73",
            "DOI": "10.2307/1573081",
            "CorpusId": 12946615
        },
        "corpusId": 12946615,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/b07ce649d6f6eb636872527104b0209d3edc8188",
        "title": "Pattern classification and scene analysis",
        "abstract": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis.",
        "venue": "A Wiley-Interscience publication",
        "year": 1974,
        "referenceCount": 0,
        "citationCount": 17335,
        "influentialCitationCount": 840,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
        },
        "publicationTypes": null,
        "publicationDate": "1974-01-23",
        "journal": {
            "pages": "I-XVII, 1-482"
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Duda1974PatternCA,\n author = {R. Duda and P. Hart},\n booktitle = {A Wiley-Interscience publication},\n pages = {I-XVII, 1-482},\n title = {Pattern classification and scene analysis},\n year = {1974}\n}\n"
        }
    },
    "137_routing_transformer": {
        "paperId": "657329c633709dd1ac34a30d57341b186b1a47c2",
        "externalIds": {
            "ACL": "2021.tacl-1.4",
            "MAG": "2997517014",
            "DBLP": "journals/tacl/RoySVG21",
            "ArXiv": "2003.05997",
            "DOI": "10.1162/tacl_a_00353",
            "CorpusId": 212718077
        },
        "corpusId": 212718077,
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/657329c633709dd1ac34a30d57341b186b1a47c2",
        "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
        "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1",
        "venue": "Transactions of the Association for Computational Linguistics",
        "year": 2020,
        "referenceCount": 61,
        "citationCount": 414,
        "influentialCitationCount": 41,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00353/1923932/tacl_a_00353.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-03-12",
        "journal": {
            "name": "Transactions of the Association for Computational Linguistics",
            "pages": "53-68",
            "volume": "9"
        },
        "citationStyles": {
            "bibtex": "@Article{Roy2020EfficientCS,\n author = {Aurko Roy and M. Saffar and Ashish Vaswani and David Grangier},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {53-68},\n title = {Efficient Content-Based Sparse Attention with Routing Transformers},\n volume = {9},\n year = {2020}\n}\n"
        }
    },
    "138_coca": {
        "paperId": "a26a7a74f1e5fd562be95c3611a0680759fbdf84",
        "externalIds": {
            "DBLP": "journals/corr/abs-2205-01917",
            "ArXiv": "2205.01917",
            "DOI": "10.48550/arXiv.2205.01917",
            "CorpusId": 248512473
        },
        "corpusId": 248512473,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/a26a7a74f1e5fd562be95c3611a0680759fbdf84",
        "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
        "abstract": "Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.",
        "venue": "Trans. Mach. Learn. Res.",
        "year": 2022,
        "referenceCount": 88,
        "citationCount": 744,
        "influentialCitationCount": 84,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2205.01917",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-05-04",
        "journal": {
            "name": "Trans. Mach. Learn. Res.",
            "volume": "2022"
        },
        "citationStyles": {
            "bibtex": "@Article{Yu2022CoCaCC,\n author = {Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},\n booktitle = {Trans. Mach. Learn. Res.},\n journal = {Trans. Mach. Learn. Res.},\n title = {CoCa: Contrastive Captioners are Image-Text Foundation Models},\n volume = {2022},\n year = {2022}\n}\n"
        }
    },
    "141_trajectory-pooled_conv_nets": {
        "paperId": "df658828fb4146877f1031ec9d07052f7eb31186",
        "externalIds": {
            "MAG": "3101453960",
            "DBLP": "conf/cvpr/Wang0T15",
            "ArXiv": "1505.04868",
            "DOI": "10.1109/CVPR.2015.7299059",
            "CorpusId": 4284367
        },
        "corpusId": 4284367,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/df658828fb4146877f1031ec9d07052f7eb31186",
        "title": "Action recognition with trajectory-pooled deep-convolutional descriptors",
        "abstract": "Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "referenceCount": 47,
        "citationCount": 1111,
        "influentialCitationCount": 130,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1505.04868",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features and deep-learned features, and achieves superior performance to the state of the art on these datasets."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-05-19",
        "journal": {
            "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "4305-4314"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2015ActionRW,\n author = {Limin Wang and Y. Qiao and Xiaoou Tang},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4305-4314},\n title = {Action recognition with trajectory-pooled deep-convolutional descriptors},\n year = {2015}\n}\n"
        }
    },
    "142_unsupervised_high-level_feature_learner": {
        "paperId": "72e93aa6767ee683de7f001fa72f1314e40a8f35",
        "externalIds": {
            "DBLP": "conf/icassp/Le13",
            "MAG": "2253807446",
            "ArXiv": "1112.6209",
            "DOI": "10.1109/ICASSP.2013.6639343",
            "CorpusId": 206741597
        },
        "corpusId": 206741597,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/72e93aa6767ee683de7f001fa72f1314e40a8f35",
        "title": "Building high-level features using large scale unsupervised learning",
        "abstract": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200\u00d7200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2011,
        "referenceCount": 56,
        "citationCount": 2217,
        "influentialCitationCount": 104,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.robotics.stanford.edu/%7Eang/papers/icml12-HighLevelFeaturesUsingUnsupervisedLearning.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Contrary to what appears to be a widely-held intuition, the experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2011-12-29",
        "journal": {
            "name": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "pages": "8595-8598"
        },
        "citationStyles": {
            "bibtex": "@Article{Le2011BuildingHF,\n author = {Quoc V. Le and Marc'Aurelio Ranzato and R. Monga and M. Devin and G. Corrado and Kai Chen and J. Dean and A. Ng},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},\n pages = {8595-8598},\n title = {Building high-level features using large scale unsupervised learning},\n year = {2011}\n}\n"
        }
    },
    "144_svm_for_face_detection": {
        "paperId": "9008cdacbdcff8a218a6928e94fe7c6dfc237b24",
        "externalIds": {
            "MAG": "2124351082",
            "DBLP": "conf/cvpr/OsunaFG97",
            "DOI": "10.1109/CVPR.1997.609310",
            "CorpusId": 2845602
        },
        "corpusId": 2845602,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/9008cdacbdcff8a218a6928e94fe7c6dfc237b24",
        "title": "Training support vector machines: an application to face detection",
        "abstract": "We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.",
        "venue": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
        "year": 1997,
        "referenceCount": 13,
        "citationCount": 2885,
        "influentialCitationCount": 104,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets is presented, and the feasibility of the approach on a face detection problem that involves a data set of 50,000 data points is demonstrated."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "1997-06-17",
        "journal": {
            "name": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "pages": "130-136"
        },
        "citationStyles": {
            "bibtex": "@Article{Osuna1997TrainingSV,\n author = {E. Osuna and R. Freund and F. Girosi},\n booktitle = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n journal = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n pages = {130-136},\n title = {Training support vector machines: an application to face detection},\n year = {1997}\n}\n"
        }
    },
    "145_compressive_transformers_for_long-range_sequence_modelling": {
        "paperId": "f51497f463566581874c941353dd9d80069c5b77",
        "externalIds": {
            "DBLP": "conf/iclr/RaePJHL20",
            "MAG": "2995575179",
            "ArXiv": "1911.05507",
            "CorpusId": 207930593
        },
        "corpusId": 207930593,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/f51497f463566581874c941353dd9d80069c5b77",
        "title": "Compressive Transformers for Long-Range Sequence Modelling",
        "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 49,
        "citationCount": 412,
        "influentialCitationCount": 55,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Compressive Transformer is presented, an attentive sequence model which compresses past memories for long-range sequence learning and can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-11-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1911.05507"
        },
        "citationStyles": {
            "bibtex": "@Article{Rae2019CompressiveTF,\n author = {Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and T. Lillicrap},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Compressive Transformers for Long-Range Sequence Modelling},\n volume = {abs/1911.05507},\n year = {2019}\n}\n"
        }
    },
    "146_dnabert": {
        "paperId": "c43d9cade31600400a0f62beb5bbcc1b548e009e",
        "externalIds": {
            "DBLP": "journals/bioinformatics/JiZLD21",
            "MAG": "3087291937",
            "DOI": "10.1101/2020.09.17.301879",
            "CorpusId": 221823863,
            "PubMed": "33538820"
        },
        "corpusId": 221823863,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/c43d9cade31600400a0f62beb5bbcc1b548e009e",
        "title": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
        "abstract": "Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios. To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, that forms global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on many sequence predictions tasks, after easy fine-tuning using small task-specific data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variants. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance.",
        "venue": "bioRxiv",
        "year": 2020,
        "referenceCount": 59,
        "citationCount": 305,
        "influentialCitationCount": 43,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2020/09/19/2020.09.17.301879.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Biology",
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel pre-trained bidirectional encoder representation that forms global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts, named DNABERT, and can be readily applied to other organisms with exceptional performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-09-19",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Ji2020DNABERTPB,\n author = {Yanrong Ji and Zhihan Zhou and Han Liu and R. Davuluri},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome},\n year = {2020}\n}\n"
        }
    },
    "148_trinet": {
        "paperId": "a42758b4943c7599925e8c8415ee9b8078ff57ad",
        "externalIds": {
            "DBLP": "journals/corr/HermansBL17",
            "ArXiv": "1703.07737",
            "MAG": "2598634450",
            "CorpusId": 1396647
        },
        "corpusId": 1396647,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/a42758b4943c7599925e8c8415ee9b8078ff57ad",
        "title": "In Defense of the Triplet Loss for Person Re-Identification",
        "abstract": "In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.",
        "venue": "arXiv.org",
        "year": 2017,
        "referenceCount": 56,
        "citationCount": 2797,
        "influentialCitationCount": 345,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-03-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1703.07737"
        },
        "citationStyles": {
            "bibtex": "@Article{Hermans2017InDO,\n author = {Alexander Hermans and Lucas Beyer and B. Leibe},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {In Defense of the Triplet Loss for Person Re-Identification},\n volume = {abs/1703.07737},\n year = {2017}\n}\n"
        }
    },
    "149_unsupervised_scale-invariant_learning": {
        "paperId": "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5",
        "externalIds": {
            "DBLP": "conf/cvpr/FergusPZ03",
            "MAG": "2154422044",
            "DOI": "10.1109/CVPR.2003.1211479",
            "CorpusId": 5745749
        },
        "corpusId": 5745749,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5",
        "title": "Object class recognition by unsupervised scale-invariant learning",
        "abstract": "We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals).",
        "venue": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.",
        "year": 2003,
        "referenceCount": 22,
        "citationCount": 2490,
        "influentialCitationCount": 228,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals)."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2003-06-18",
        "journal": {
            "name": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.",
            "pages": "II-II",
            "volume": "2"
        },
        "citationStyles": {
            "bibtex": "@Article{Fergus2003ObjectCR,\n author = {R. Fergus and P. Perona and Andrew Zisserman},\n booktitle = {2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.},\n journal = {2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.},\n pages = {II-II},\n title = {Object class recognition by unsupervised scale-invariant learning},\n volume = {2},\n year = {2003}\n}\n"
        }
    },
    "150_rl_for_helicopter_flight": {
        "paperId": "13e2d5c4d39bc58b42f9004e5b03905f847dfa0f",
        "externalIds": {
            "MAG": "2108734173",
            "DBLP": "conf/nips/NgKJS03",
            "CorpusId": 7142905
        },
        "corpusId": 7142905,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/13e2d5c4d39bc58b42f9004e5b03905f847dfa0f",
        "title": "Autonomous Helicopter Flight via Reinforcement Learning",
        "abstract": "Autonomous helicopter flight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter flight. We first fit a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to fly a number of maneuvers taken from an RC helicopter competition.",
        "venue": "Neural Information Processing Systems",
        "year": 2003,
        "referenceCount": 13,
        "citationCount": 355,
        "influentialCitationCount": 11,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper first fit a stochastic, nonlinear model of the helicopter dynamics, then uses the model to learn to hover in place, and to fly a number of maneuvers taken from an RC helicopter competition."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2003-12-09",
        "journal": {
            "pages": "799-806"
        },
        "citationStyles": {
            "bibtex": "@Article{Ng2003AutonomousHF,\n author = {A. Ng and H. Kim and Michael I. Jordan and S. Sastry},\n booktitle = {Neural Information Processing Systems},\n pages = {799-806},\n title = {Autonomous Helicopter Flight via Reinforcement Learning},\n year = {2003}\n}\n"
        }
    },
    "152_dit-xl_2_+_discriminator_guidance": {
        "paperId": "4ff52e24e02116b675abf085642b0de38f30b1eb",
        "externalIds": {
            "DBLP": "conf/icml/KimKKKM23",
            "ArXiv": "2211.17091",
            "DOI": "10.48550/arXiv.2211.17091",
            "CorpusId": 254096299
        },
        "corpusId": 254096299,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/4ff52e24e02116b675abf085642b0de38f30b1eb",
        "title": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
        "abstract": "The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "referenceCount": 77,
        "citationCount": 39,
        "influentialCitationCount": 5,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2211.17091",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-11-28",
        "journal": {
            "pages": "16567-16598"
        },
        "citationStyles": {
            "bibtex": "@Article{Kim2022RefiningGP,\n author = {Dongjun Kim and Yeongmin Kim and Wanmo Kang and Il-Chul Moon},\n booktitle = {International Conference on Machine Learning},\n pages = {16567-16598},\n title = {Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models},\n year = {2022}\n}\n"
        }
    },
    "153_wenet_(wt2)": {
        "paperId": "aed145657eb6074cb23e226f699b217527596369",
        "externalIds": {
            "MAG": "2928299337",
            "DBLP": "journals/corr/abs-1904-03819",
            "ArXiv": "1904.03819",
            "CorpusId": 102351005
        },
        "corpusId": 102351005,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/aed145657eb6074cb23e226f699b217527596369",
        "title": "WeNet: Weighted Networks for Recurrent Network Architecture Search",
        "abstract": "In recent years, there has been increasing demand for automatic architecture search in deep learning. Numerous approaches have been proposed and led to state-of-the-art results in various applications, including image classification and language modeling. In this paper, we propose a novel way of architecture search by means of weighted networks (WeNet), which consist of a number of networks, with each assigned a weight. These weights are updated with back-propagation to reflect the importance of different networks. Such weighted networks bear similarity to mixture of experts. We conduct experiments on Penn Treebank and WikiText-2. We show that the proposed WeNet can find recurrent architectures which result in state-of-the-art performance.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 38,
        "citationCount": 4,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel way of architecture search by means of weighted networks (WeNet), which consist of a number of networks, with each assigned a weight, which are updated with back-propagation to reflect the importance of different networks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-04-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1904.03819"
        },
        "citationStyles": {
            "bibtex": "@Article{Huang2019WeNetWN,\n author = {Zhiheng Huang and Bing Xiang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {WeNet: Weighted Networks for Recurrent Network Architecture Search},\n volume = {abs/1904.03819},\n year = {2019}\n}\n"
        }
    },
    "154_boxes": {
        "paperId": "2f027193fb703d0af58ec382bd1438daff9417d7",
        "externalIds": {
            "MAG": "46130386",
            "CorpusId": 18229198
        },
        "corpusId": 18229198,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/2f027193fb703d0af58ec382bd1438daff9417d7",
        "title": "BOXES: AN EXPERIMENT IN ADAPTIVE CONTROL",
        "abstract": "BOXES is the name of a computer program. We shall also use the word boxes to refer to a particular approach to decision-taking under uncertainty which has been used as the basis of a number of computer programs. Fig. 1 shows a photograph of an assemblage of actual boxes\u2014matchboxes to be exact. Although the construction of this Matchbox Educable Noughts and Crosses Engine (Michie 1961, 1963) was undertaken as a 'fun project', there was present a more serious intention to demonstrate the principle that it may be easier to learn to play many easy games than one difficult one. Consequently it may be advantageous to decompose a game into a number of mutually independent sub-games even if much relevant information is put out of reach in the process. The principle is related to the method of subgoals in problem-solving (see Newell et al. 1960) but differs in one fundamental respect: subgoals are linked in series, while sub-games are played in parallel, in a sense which will become apparent.",
        "venue": "",
        "year": 2013,
        "referenceCount": 10,
        "citationCount": 119,
        "influentialCitationCount": 5,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Although the construction of this Matchbox Educable Noughts and Crosses Engine was undertaken as a 'fun project', there was present a more serious intention to demonstrate the principle that it may be easier to learn to play many easy games than one difficult one."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Michie2013BOXESAE,\n author = {D. Michie and R. A. Chambers},\n title = {BOXES: AN EXPERIMENT IN ADAPTIVE CONTROL},\n year = {2013}\n}\n"
        }
    },
    "155_big-little_net_(speech)": {
        "paperId": "425032e4fdd531cd7a5d3e7effc12d16f9c076d9",
        "externalIds": {
            "MAG": "2949431202",
            "DBLP": "conf/iclr/ChenFMSF19",
            "ArXiv": "1807.03848",
            "CorpusId": 49671490
        },
        "corpusId": 49671490,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/425032e4fdd531cd7a5d3e7effc12d16f9c076d9",
        "title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
        "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at this https URL",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 62,
        "citationCount": 80,
        "influentialCitationCount": 8,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a novel Convolutional Neural Network architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy by using a multi-branch network, which has different computational complexity at different branches."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-07-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1807.03848"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2018BigLittleNA,\n author = {Chun-Fu Chen and Quanfu Fan and Neil Rohit Mallinar and Tom Sercu and R. Feris},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\n volume = {abs/1807.03848},\n year = {2018}\n}\n"
        }
    },
    "156_fisher-boost": {
        "paperId": "39f3b1804b8df5be645a1dcb4a876e128385d9be",
        "externalIds": {
            "MAG": "1606858007",
            "DBLP": "conf/eccv/PerronninSM10",
            "DOI": "10.1007/978-3-642-15561-1_11",
            "CorpusId": 10402702
        },
        "corpusId": 10402702,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/39f3b1804b8df5be645a1dcb4a876e128385d9be",
        "title": "Improving the Fisher Kernel for Large-Scale Image Classification",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2010,
        "referenceCount": 37,
        "citationCount": 2766,
        "influentialCitationCount": 379,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-15561-1_11.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "In an evaluation involving hundreds of thousands of training images, it is shown that classifiers learned on Flickr groups perform surprisingly well and that they can complement classifier learned on more carefully annotated datasets."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2010-09-05",
        "journal": {
            "pages": "143-156"
        },
        "citationStyles": {
            "bibtex": "@Article{Perronnin2010ImprovingTF,\n author = {Florent Perronnin and Jorge S\u00e1nchez and Thomas Mensink},\n booktitle = {European Conference on Computer Vision},\n pages = {143-156},\n title = {Improving the Fisher Kernel for Large-Scale Image Classification},\n year = {2010}\n}\n"
        }
    },
    "157_hubert": {
        "paperId": "4fffa5245d3972077c83614c2a08a47cb578631e",
        "externalIds": {
            "DBLP": "journals/corr/abs-2106-07447",
            "ArXiv": "2106.07447",
            "DOI": "10.1109/taslp.2021.3122291",
            "CorpusId": 235421619
        },
        "corpusId": 235421619,
        "publicationVenue": {
            "id": "309e00f7-4bbd-461f-ab37-a90cd14ef21d",
            "name": "IEEE/ACM Transactions on Audio Speech and Language Processing",
            "alternate_names": [
                "IEEE/ACM Trans Audio Speech Lang Process"
            ],
            "issn": "2329-9290",
            "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=6570655",
            "alternate_urls": [
                "https://signalprocessingsociety.org/publications-resources/ieeeacm-transactions-audio-speech-and-language-processing/ieeeacm"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/4fffa5245d3972077c83614c2a08a47cb578631e",
        "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
        "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960 h) and Libri-light (60,000 h) benchmarks with 10 min, 1 h, 10 h, 100 h, and 960 h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.12",
        "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "year": 2021,
        "referenceCount": 64,
        "citationCount": 1457,
        "influentialCitationCount": 446,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2106.07447",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-06-14",
        "journal": {
            "name": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
            "pages": "3451-3460",
            "volume": "29"
        },
        "citationStyles": {
            "bibtex": "@Article{Hsu2021HuBERTSS,\n author = {Wei-Ning Hsu and Benjamin Bolte and Yao-Hung Hubert Tsai and Kushal Lakhotia and R. Salakhutdinov and Abdel-rahman Mohamed},\n booktitle = {IEEE/ACM Transactions on Audio Speech and Language Processing},\n journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},\n pages = {3451-3460},\n title = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},\n volume = {29},\n year = {2021}\n}\n"
        }
    },
    "158_spatiotemporal_fusion_convnet": {
        "paperId": "9d9aced120e530484609164c836da64548693484",
        "externalIds": {
            "ArXiv": "1604.06573",
            "MAG": "2342662179",
            "DBLP": "conf/cvpr/FeichtenhoferPZ16",
            "DOI": "10.1109/CVPR.2016.213",
            "CorpusId": 12289712
        },
        "corpusId": 12289712,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/9d9aced120e530484609164c836da64548693484",
        "title": "Convolutional Two-Stream Network Fusion for Video Action Recognition",
        "abstract": "Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters, (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy, finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 38,
        "citationCount": 2418,
        "influentialCitationCount": 242,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1604.06573",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new ConvNet architecture for spatiotemporal fusion of video snippets is proposed, and its performance on standard benchmarks where this architecture achieves state-of-the-art results is evaluated."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-04-22",
        "journal": {
            "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "1933-1941"
        },
        "citationStyles": {
            "bibtex": "@Article{Feichtenhofer2016ConvolutionalTN,\n author = {Christoph Feichtenhofer and A. Pinz and Andrew Zisserman},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1933-1941},\n title = {Convolutional Two-Stream Network Fusion for Video Action Recognition},\n year = {2016}\n}\n"
        }
    },
    "159_support_vector_machines": {
        "paperId": "52b7bf3ba59b31f362aa07f957f1543a29a4279e",
        "externalIds": {
            "MAG": "2119821739",
            "DOI": "10.1023/A:1022627411411",
            "CorpusId": 52874011
        },
        "corpusId": 52874011,
        "publicationVenue": {
            "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
            "name": "Machine-mediated learning",
            "type": "journal",
            "alternate_names": [
                "Mach learn",
                "Machine Learning",
                "Mach Learn"
            ],
            "issn": "0732-6718",
            "alternate_issns": [
                "0885-6125"
            ],
            "url": "http://www.springer.com/computer/artificial/journal/10994",
            "alternate_urls": [
                "https://link.springer.com/journal/10994",
                "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/52b7bf3ba59b31f362aa07f957f1543a29a4279e",
        "title": "Support-Vector Networks",
        "abstract": null,
        "venue": "Machine-mediated learning",
        "year": 1995,
        "referenceCount": 26,
        "citationCount": 37932,
        "influentialCitationCount": 3310,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1023/A:1022627411411.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated and the performance of the support- vector network is compared to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
        },
        "publicationTypes": null,
        "publicationDate": "1995-09-15",
        "journal": {
            "name": "Machine Learning",
            "pages": "273-297",
            "volume": "20"
        },
        "citationStyles": {
            "bibtex": "@Article{Cortes1995SupportVectorN,\n author = {Corinna Cortes and V. Vapnik},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {273-297},\n title = {Support-Vector Networks},\n volume = {20},\n year = {1995}\n}\n"
        }
    },
    "160_rational_dqn_average": {
        "paperId": "69d0224968a23f45230640f18c58719ac420ce01",
        "externalIds": {
            "DBLP": "journals/corr/abs-2102-09407",
            "CorpusId": 231951309
        },
        "corpusId": 231951309,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/69d0224968a23f45230640f18c58719ac420ce01",
        "title": "Recurrent Rational Networks",
        "abstract": "Latest insights from biology show that intelligence does not only emerge from the connections between the neurons, but that individual neurons shoulder more computational responsibility. Current Neural Network architecture design and search are biased on \ufb01xed activation functions. Using more advanced learnable activation functions provide Neural Networks with higher learning capacity. However, general guidance for building such networks is still missing. In this work, we \ufb01rst explain why rationals offer an optimal choice for activation functions. We then show that they are closed under residual connections, and inspired by recurrence for residual networks we derive a self-regularized version of Rationals: Recurrent Rationals. We demonstrate that (Recurrent) Rational Networks lead to high performance improvements on Image Classi\ufb01cation and Deep Reinforcement Learning.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 38,
        "citationCount": 4,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work explains why rationals offer an optimal choice for activation functions, and shows that they are closed under residual connections, and inspired by recurrence for residual networks it is demonstrated that (Recurrent) Rational Networks lead to high performance improvements on Image Classi\ufb01cation and Deep Reinforcement Learning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2102.09407"
        },
        "citationStyles": {
            "bibtex": "@Article{Delfosse2021RecurrentRN,\n author = {Quentin Delfosse and P. Schramowski and Alejandro Molina and K. Kersting},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Recurrent Rational Networks},\n volume = {abs/2102.09407},\n year = {2021}\n}\n"
        }
    },
    "161_deeplabv3+": {
        "paperId": "9217e28b2273eb3b26e4e9b7b498b4661e6e09f5",
        "externalIds": {
            "ArXiv": "1802.02611",
            "MAG": "2787091153",
            "DBLP": "journals/corr/abs-1802-02611",
            "DOI": "10.1007/978-3-030-01234-2_49",
            "CorpusId": 3638670
        },
        "corpusId": 3638670,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/9217e28b2273eb3b26e4e9b7b498b4661e6e09f5",
        "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "referenceCount": 88,
        "citationCount": 9803,
        "influentialCitationCount": 1493,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1802.02611",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries and applies the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-02-07",
        "journal": {
            "pages": "833-851"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2018EncoderDecoderWA,\n author = {Liang-Chieh Chen and Yukun Zhu and G. Papandreou and Florian Schroff and Hartwig Adam},\n booktitle = {European Conference on Computer Vision},\n pages = {833-851},\n title = {Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},\n year = {2018}\n}\n"
        }
    },
    "162_projected_gan": {
        "paperId": "ffd43946c7fe947ef3213e7a668a36e9d41c8f4b",
        "externalIds": {
            "DBLP": "journals/corr/abs-2111-01007",
            "ArXiv": "2111.01007",
            "CorpusId": 240354401
        },
        "corpusId": 240354401,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/ffd43946c7fe947ef3213e7a668a36e9d41c8f4b",
        "title": "Projected GANs Converge Faster",
        "abstract": "Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train. They need careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space. Motivated by the finding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fr\\'echet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 96,
        "citationCount": 147,
        "influentialCitationCount": 28,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work projects generated and real samples into a fixed, pretrained feature space and proposes a more effective strategy that mixes features across channels and resolutions, which improves image quality, sample efficiency, and convergence speed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-11-01",
        "journal": {
            "pages": "17480-17492"
        },
        "citationStyles": {
            "bibtex": "@Article{Sauer2021ProjectedGC,\n author = {Axel Sauer and Kashyap Chitta and Jens Muller and Andreas Geiger},\n booktitle = {Neural Information Processing Systems},\n pages = {17480-17492},\n title = {Projected GANs Converge Faster},\n year = {2021}\n}\n"
        }
    },
    "164_agent57": {
        "paperId": "9651e1987a39d100dc0f6696a2077199e5075ea2",
        "externalIds": {
            "MAG": "3013618273",
            "ArXiv": "2003.13350",
            "DBLP": "conf/icml/BadiaPKSVGB20",
            "CorpusId": 214713757
        },
        "corpusId": 214713757,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9651e1987a39d100dc0f6696a2077199e5075ea2",
        "title": "Agent57: Outperforming the Atari Human Benchmark",
        "abstract": "Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "referenceCount": 51,
        "citationCount": 423,
        "influentialCitationCount": 35,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games and trains a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-03-30",
        "journal": {
            "pages": "507-517"
        },
        "citationStyles": {
            "bibtex": "@Article{Badia2020Agent57OT,\n author = {Adri\u00e0 Puigdom\u00e8nech Badia and Bilal Piot and Steven Kapturowski and P. Sprechmann and Alex Vitvitskyi and Daniel Guo and C. Blundell},\n booktitle = {International Conference on Machine Learning},\n pages = {507-517},\n title = {Agent57: Outperforming the Atari Human Benchmark},\n year = {2020}\n}\n"
        }
    },
    "166_specaugment": {
        "paperId": "b0fae9fbb4e580d92395eabafe73e317ae6510e3",
        "externalIds": {
            "ArXiv": "1904.08779",
            "MAG": "2936774411",
            "DBLP": "journals/corr/abs-1904-08779",
            "DOI": "10.21437/Interspeech.2019-2680",
            "CorpusId": 121321299
        },
        "corpusId": 121321299,
        "publicationVenue": {
            "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
            "name": "Interspeech",
            "type": "conference",
            "alternate_names": [
                "Conf Int Speech Commun Assoc",
                "INTERSPEECH",
                "Conference of the International Speech Communication Association"
            ],
            "issn": "2308-457X",
            "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
            "alternate_urls": [
                "http://www.isca-speech.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/b0fae9fbb4e580d92395eabafe73e317ae6510e3",
        "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
        "abstract": "We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.",
        "venue": "Interspeech",
        "year": 2019,
        "referenceCount": 50,
        "citationCount": 2845,
        "influentialCitationCount": 395,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.08779",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Engineering",
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents SpecAugment, a simple data augmentation method for speech recognition that is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients) and achieves state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-04-19",
        "journal": {
            "pages": "2613-2617"
        },
        "citationStyles": {
            "bibtex": "@Article{Park2019SpecAugmentAS,\n author = {Daniel S. Park and William Chan and Yu Zhang and Chung-Cheng Chiu and Barret Zoph and E. D. Cubuk and Quoc V. Le},\n booktitle = {Interspeech},\n pages = {2613-2617},\n title = {SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition},\n year = {2019}\n}\n"
        }
    },
    "168_xlm": {
        "paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "externalIds": {
            "DBLP": "journals/corr/abs-1901-07291",
            "ArXiv": "1901.07291",
            "MAG": "2970049541",
            "CorpusId": 58981712
        },
        "corpusId": 58981712,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "title": "Cross-lingual Language Model Pretraining",
        "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT\u201916 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT\u201916 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "referenceCount": 52,
        "citationCount": 2322,
        "influentialCitationCount": 489,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingsual language model objective."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-01-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1901.07291"
        },
        "citationStyles": {
            "bibtex": "@Article{Lample2019CrosslingualLM,\n author = {Guillaume Lample and Alexis Conneau},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Cross-lingual Language Model Pretraining},\n volume = {abs/1901.07291},\n year = {2019}\n}\n"
        }
    },
    "169_pdp_model_for_serial_order": {
        "paperId": "caa5eec3feba1e3f4c421f28daaa6d1906b573ec",
        "externalIds": {
            "MAG": "1485981043",
            "DOI": "10.1016/S0166-4115(97)80111-2",
            "CorpusId": 15375627
        },
        "corpusId": 15375627,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/caa5eec3feba1e3f4c421f28daaa6d1906b573ec",
        "title": "Serial Order: A Parallel Distributed Processing Approach",
        "abstract": null,
        "venue": "",
        "year": 1997,
        "referenceCount": 66,
        "citationCount": 1047,
        "influentialCitationCount": 83,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Psychology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Psychology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Psychology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A theory of learned sequential behavior is presented, with a focus on coarticulatory phenomena in speech, implemented as a recurrent parallel distributed processing network that is trained via a generalized error-correcting algorithm."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "Advances in psychology",
            "pages": "471-495",
            "volume": "121"
        },
        "citationStyles": {
            "bibtex": "@Article{Jordan1997SerialOA,\n author = {Michael I. Jordan},\n journal = {Advances in psychology},\n pages = {471-495},\n title = {Serial Order: A Parallel Distributed Processing Approach},\n volume = {121},\n year = {1997}\n}\n"
        }
    },
    "170_stacked-lstm+pruning": {
        "paperId": "7f122bdec26a4fe0badb1234b8036b1023de3405",
        "externalIds": {
            "DBLP": "journals/corr/abs-1906-06847",
            "ArXiv": "1906.06847",
            "MAG": "2952423590",
            "DOI": "10.1016/j.neunet.2019.11.018",
            "CorpusId": 189928324,
            "PubMed": "31855748"
        },
        "corpusId": 189928324,
        "publicationVenue": {
            "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
            "name": "Neural Networks",
            "type": "journal",
            "alternate_names": [
                "Neural Netw"
            ],
            "issn": "0893-6080",
            "url": "http://www.elsevier.com/locate/neunet",
            "alternate_urls": [
                "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                "http://www.sciencedirect.com/science/journal/08936080"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/7f122bdec26a4fe0badb1234b8036b1023de3405",
        "title": "Structured Pruning of Recurrent Neural Networks through Neuron Selection",
        "abstract": null,
        "venue": "Neural Networks",
        "year": 2019,
        "referenceCount": 50,
        "citationCount": 28,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1906.06847",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a structured pruning method through neuron selection which can remove the independent neuron of RNNs and introduces two sets of binary random variables, which can be interpreted as gates or switches to the input neurons and the hidden neurons, respectively."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-06-17",
        "journal": {
            "name": "Neural networks : the official journal of the International Neural Network Society",
            "pages": "\n          134-141\n        ",
            "volume": "123"
        },
        "citationStyles": {
            "bibtex": "@Article{Wen2019StructuredPO,\n author = {Liangjiang Wen and Xueyang Zhang and Haoli Bai and Zenglin Xu},\n booktitle = {Neural Networks},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          134-141\n        },\n title = {Structured Pruning of Recurrent Neural Networks through Neuron Selection},\n volume = {123},\n year = {2019}\n}\n"
        }
    },
    "172_robot_parkour": {
        "paperId": "24badbe57bbe956ea38afd4b02a214da1bc57a6c",
        "externalIds": {
            "DBLP": "journals/corr/abs-2309-05665",
            "ArXiv": "2309.05665",
            "DOI": "10.48550/arXiv.2309.05665",
            "CorpusId": 261696935
        },
        "corpusId": 261696935,
        "publicationVenue": {
            "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
            "name": "Conference on Robot Learning",
            "type": "conference",
            "alternate_names": [
                "CoRL",
                "Conf Robot Learn"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/24badbe57bbe956ea38afd4b02a214da1bc57a6c",
        "title": "Robot Parkour Learning",
        "abstract": "Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera. We demonstrate that our system can empower two different low-cost robots to autonomously select and execute appropriate parkour skills to traverse challenging real-world environments.",
        "venue": "Conference on Robot Learning",
        "year": 2023,
        "referenceCount": 125,
        "citationCount": 18,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.05665",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work develops a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running, and distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-09-11",
        "journal": {
            "pages": "73-92"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhuang2023RobotPL,\n author = {Ziwen Zhuang and Zipeng Fu and Jianren Wang and Christopher Atkeson and Soeren Schwertfeger and Chelsea Finn and Hang Zhao},\n booktitle = {Conference on Robot Learning},\n pages = {73-92},\n title = {Robot Parkour Learning},\n year = {2023}\n}\n"
        }
    },
    "173_transformer_large_+_hcp": {
        "paperId": "609e1c196fced582caf9113aa6a003b64d3cdcd6",
        "externalIds": {
            "DBLP": "conf/acl/BaiWS022",
            "ACL": "2022.acl-long.96",
            "ArXiv": "2203.10692",
            "DOI": "10.48550/arXiv.2203.10692",
            "CorpusId": 247593920
        },
        "corpusId": 247593920,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/609e1c196fced582caf9113aa6a003b64d3cdcd6",
        "title": "Better Language Model with Hypernym Class Prediction",
        "abstract": "Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2022,
        "referenceCount": 39,
        "citationCount": 6,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.10692",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This study hypothesizes that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words and train large neural LMs by gradually annealing from predicting the class to token prediction during training."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-03-21",
        "journal": {
            "pages": "1352-1362"
        },
        "citationStyles": {
            "bibtex": "@Article{Bai2022BetterLM,\n author = {Richard He Bai and Tong Wang and Alessandro Sordoni and Peng Shi},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {1352-1362},\n title = {Better Language Model with Hypernym Class Prediction},\n year = {2022}\n}\n"
        }
    },
    "175_retnet": {
        "paperId": "240103933ffe3dac2179cc160a2bd91299357a53",
        "externalIds": {
            "ArXiv": "2307.08621",
            "DBLP": "journals/corr/abs-2307-08621",
            "DOI": "10.48550/arXiv.2307.08621",
            "CorpusId": 259937453
        },
        "corpusId": 259937453,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/240103933ffe3dac2179cc160a2bd91299357a53",
        "title": "Retentive Network: A Successor to Transformer for Large Language Models",
        "abstract": "In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 40,
        "citationCount": 63,
        "influentialCitationCount": 10,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.08621",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance, and proposes the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-07-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2307.08621"
        },
        "citationStyles": {
            "bibtex": "@Article{Sun2023RetentiveNA,\n author = {Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Retentive Network: A Successor to Transformer for Large Language Models},\n volume = {abs/2307.08621},\n year = {2023}\n}\n"
        }
    },
    "176_denoising_diffusion_probabilistic_models_(lsun_bedroom)": {
        "paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a",
        "externalIds": {
            "MAG": "3100572490",
            "ArXiv": "2006.11239",
            "DBLP": "conf/nips/HoJA20",
            "CorpusId": 219955663
        },
        "corpusId": 219955663,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/5c126ae3421f05768d8edd97ecd44b1364e2c99a",
        "title": "Denoising Diffusion Probabilistic Models",
        "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "referenceCount": 72,
        "citationCount": 6335,
        "influentialCitationCount": 1644,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Physics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "High quality image synthesis results are presented using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics, which naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-06-19",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2006.11239"
        },
        "citationStyles": {
            "bibtex": "@Article{Ho2020DenoisingDP,\n author = {Jonathan Ho and Ajay Jain and P. Abbeel},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Denoising Diffusion Probabilistic Models},\n volume = {abs/2006.11239},\n year = {2020}\n}\n"
        }
    },
    "177_ibm-5": {
        "paperId": "ab7b5917515c460b90451e67852171a531671ab8",
        "externalIds": {
            "MAG": "2006969979",
            "ACL": "J93-2003",
            "DBLP": "journals/coling/BrownPPM94",
            "CorpusId": 13259913
        },
        "corpusId": 13259913,
        "publicationVenue": {
            "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
            "name": "International Conference on Computational Logic",
            "type": "conference",
            "alternate_names": [
                "CL",
                "Int Conf Comput Log"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/ab7b5917515c460b90451e67852171a531671ab8",
        "title": "The Mathematics of Statistical Machine Translation: Parameter Estimation",
        "abstract": "We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.",
        "venue": "International Conference on Computational Logic",
        "year": 1993,
        "referenceCount": 22,
        "citationCount": 4789,
        "influentialCitationCount": 618,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus, given a set of pairs of sentences that are translations of one another."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1993-06-01",
        "journal": {
            "name": "Comput. Linguistics",
            "pages": "263-311",
            "volume": "19"
        },
        "citationStyles": {
            "bibtex": "@Article{Brown1993TheMO,\n author = {P. Brown and S. D. Pietra and V. D. Pietra and R. Mercer},\n booktitle = {International Conference on Computational Logic},\n journal = {Comput. Linguistics},\n pages = {263-311},\n title = {The Mathematics of Statistical Machine Translation: Parameter Estimation},\n volume = {19},\n year = {1993}\n}\n"
        }
    },
    "178_florence": {
        "paperId": "21ec90872abd986c12afe39bebe807732ffa70c9",
        "externalIds": {
            "ArXiv": "2111.11432",
            "DBLP": "journals/corr/abs-2111-11432",
            "CorpusId": 244477674
        },
        "corpusId": 244477674,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/21ec90872abd986c12afe39bebe807732ffa70c9",
        "title": "Florence: A New Foundation Model for Computer Vision",
        "abstract": "Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 88,
        "citationCount": 591,
        "influentialCitationCount": 41,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine, from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth), by incorporating universal visual-language representations from Web-scale image-text data."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-11-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2111.11432"
        },
        "citationStyles": {
            "bibtex": "@Article{Yuan2021FlorenceAN,\n author = {Lu Yuan and Dongdong Chen and Yi-Ling Chen and N. Codella and Xiyang Dai and Jianfeng Gao and Houdong Hu and Xuedong Huang and Boxin Li and Chunyuan Li and Ce Liu and Mengchen Liu and Zicheng Liu and Yumao Lu and Yu Shi and Lijuan Wang and Jianfeng Wang and Bin Xiao and Zhen Xiao and Jianwei Yang and Michael Zeng and Luowei Zhou and Pengchuan Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Florence: A New Foundation Model for Computer Vision},\n volume = {abs/2111.11432},\n year = {2021}\n}\n"
        }
    },
    "179_adm": {
        "paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794",
        "externalIds": {
            "ArXiv": "2105.05233",
            "DBLP": "journals/corr/abs-2105-05233",
            "CorpusId": 234357997
        },
        "corpusId": 234357997,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/64ea8f180d0682e6c18d1eb688afdb2027c02794",
        "title": "Diffusion Models Beat GANs on Image Synthesis",
        "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 81,
        "citationCount": 3286,
        "influentialCitationCount": 490,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models, and classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on imageNet 512$\\ times$512."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-05-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2105.05233"
        },
        "citationStyles": {
            "bibtex": "@Article{Dhariwal2021DiffusionMB,\n author = {Prafulla Dhariwal and Alex Nichol},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Diffusion Models Beat GANs on Image Synthesis},\n volume = {abs/2105.05233},\n year = {2021}\n}\n"
        }
    },
    "180_gopher_(280b)": {
        "paperId": "68f141724814839d556a989646194be88641b143",
        "externalIds": {
            "ArXiv": "2112.11446",
            "DBLP": "journals/corr/abs-2112-11446",
            "CorpusId": 245353475
        },
        "corpusId": 245353475,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/68f141724814839d556a989646194be88641b143",
        "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
        "abstract": "Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 0,
        "citationCount": 852,
        "influentialCitationCount": 62,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-12-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2112.11446"
        },
        "citationStyles": {
            "bibtex": "@Article{Rae2021ScalingLM,\n author = {Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and M. Rauh and Po-Sen Huang and A. Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and J. Uesato and John F. J. Mellor and I. Higgins and Antonia Creswell and Nathan McAleese and Amy Wu and Erich Elsen and Siddhant M. Jayakumar and Elena Buchatskaya and D. Budden and Esme Sutherland and K. Simonyan and Michela Paganini and L. Sifre and Lena Martens and Xiang Lorraine Li and A. Kuncoro and Aida Nematzadeh and E. Gribovskaya and Domenic Donato and Angeliki Lazaridou and A. Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and N. Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Tobias Pohlen and Z. Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew G. Johnson and Blake A. Hechtman and Laura Weidinger and Iason Gabriel and William S. Isaac and Edward Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and O. Vinyals and Kareem W. Ayoub and Jeff Stanway and L. Bennett and D. Hassabis and K. Kavukcuoglu and G. Irving},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Scaling Language Models: Methods, Analysis & Insights from Training Gopher},\n volume = {abs/2112.11446},\n year = {2021}\n}\n"
        }
    },
    "181_retro-7b": {
        "paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641",
        "externalIds": {
            "DBLP": "journals/corr/abs-2112-04426",
            "ArXiv": "2112.04426",
            "CorpusId": 244954723
        },
        "corpusId": 244954723,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/002c256d30d6be4b23d365a8de8ae0e67e4c9641",
        "title": "Improving language models by retrieving from trillions of tokens",
        "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 65,
        "citationCount": 530,
        "influentialCitationCount": 47,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25% fewer parameters, and opens up new avenues for improving language models through explicit memory at unprecedented scale."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-12-08",
        "journal": {
            "pages": "2206-2240"
        },
        "citationStyles": {
            "bibtex": "@Article{Borgeaud2021ImprovingLM,\n author = {Sebastian Borgeaud and A. Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George van den Driessche and Jean-Baptiste Lespiau and Bogdan Damoc and Aidan Clark and Diego de Las Casas and Aurelia Guy and Jacob Menick and Roman Ring and T. Hennigan and Saffron Huang and Lorenzo Maggiore and Chris Jones and Albin Cassirer and Andy Brock and Michela Paganini and G. Irving and O. Vinyals and Simon Osindero and K. Simonyan and Jack W. Rae and Erich Elsen and L. Sifre},\n booktitle = {International Conference on Machine Learning},\n pages = {2206-2240},\n title = {Improving language models by retrieving from trillions of tokens},\n year = {2021}\n}\n"
        }
    },
    "182_gemini_ultra": {
        "paperId": "f79114e2572f095265179831d19de2eb54174415",
        "externalIds": {
            "DBLP": "journals/corr/abs-2312-11805",
            "ArXiv": "2312.11805",
            "DOI": "10.48550/arXiv.2312.11805",
            "CorpusId": 266361876
        },
        "corpusId": 266361876,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/f79114e2572f095265179831d19de2eb54174415",
        "title": "Gemini: A Family of Highly Capable Multimodal Models",
        "abstract": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 102,
        "citationCount": 284,
        "influentialCitationCount": 50,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Evaluation on a broad range of benchmarks shows that the most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks the authors examined."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-12-19",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2312.11805"
        },
        "citationStyles": {
            "bibtex": "@Article{Anil2023GeminiAF,\n author = {Gemini Team Google Rohan Anil and Sebastian Borgeaud and Yonghui Wu and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk and Andrew M. Dai and Anja Hauth and Katie Millican and David Silver and Slav Petrov and Melvin Johnson and Ioannis Antonoglou and Julian Schrittwieser and A. Glaese and Jilin Chen and Emily Pitler and T. Lillicrap and Angeliki Lazaridou and Orhan Firat and James Molloy and M. Isard and Paul Barham and Tom Hennigan and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and Ryan Doherty and Eli Collins and Clemens Meyer and Eliza Rutherford and Erica Moreira and Kareem W. Ayoub and Megha Goel and George Tucker and Enrique Piqueras and M. Krikun and Iain Barr and Nikolay Savinov and Ivo Danihelka and Becca Roelofs and Anais White and Anders Andreassen and Tamara von Glehn and Lakshman N. Yagati and Mehran Kazemi and Lucas Gonzalez and Misha Khalman and Jakub Sygnowski and Alexandre Frechette and Charlotte Smith and Laura Culp and Lev Proleev and Yi Luan and Xi Chen and James Lottes and Nathan Schucher and Federico Lebron and Alban Rrustemi and Natalie Clay and Phil Crone and Tom\u00e1s Kocisk\u00fd and Jeffrey Zhao and Bartek Perz and Dian Yu and Heidi Howard and Adam Bloniarz and Jack W. Rae and Han Lu and L. Sifre and Marcello Maggioni and Fred Alcober and Daniel H Garrette and Megan Barnes and S. Thakoor and Jacob Austin and Gabriel Barth-Maron and William Wong and Rishabh Joshi and Rahma Chaabouni and Deeni Fatiha and Arun Ahuja and Ruibo Liu and Yunxuan Li and Sarah Cogan and Jeremy Chen and Chao Jia and Chenjie Gu and Qiao Zhang and Jordan Grimstad and Ale Jakse Hartman and Martin Chadwick and Gaurav Singh Tomar and Xavier Garcia and Evan Senter and E. Taropa and Thanumalayan Sankaranarayana Pillai and J. Devlin and Michael Laskin and Diego de Las Casas and Dasha Valter and Connie Tao and Lorenzo Blanco and Adri\u00e0 Puigdom\u00e8nech Badia and David Reitter and Mianna Chen and Jenny Brennan and Clara Rivera and S. Brin and Shariq Iqbal and G. Surita and Jane Labanowski and Abhishek Rao and Stephanie Winkler and Emilio Parisotto and Yiming Gu and Kate Olszewska and Yujing Zhang and Ravichandra Addanki and Antoine Miech and Annie Louis and Laurent El Shafey and Denis Teplyashin and Geoff Brown and Elliot Catt and Nithya Attaluri and Jan Balaguer and Jackie Xiang and Pidong Wang and Zoe C. Ashwood and Anton Briukhov and Albert Webson and Sanjay Ganapathy and Smit Sanghavi and Ajay Kannan and Ming-Wei Chang and Axel Stjerngren and Josip Djolonga and Yuting Sun and Ankur Bapna and Matthew Aitchison and Pedram Pejman and H. Michalewski and Tianhe Yu and Cindy Wang and J Christopher Love and Junwhan Ahn and Dawn Bloxwich and Kehang Han and Peter Humphreys and Thibault Sellam and James Bradbury and Varun Godbole and Sina Samangooei and Bogdan Damoc and Alex Kaskasoli and S'ebastien M. R. Arnold and Vijay Vasudevan and Shubham Agrawal and Jason Riesa and Dmitry Lepikhin and Richard Tanburn and S. Srinivasan and Hyeontaek Lim and Sarah Hodkinson and Pranav Shyam and Johan Ferret and Steven Hand and Ankush Garg and T. Paine and Jian Li and Yujia Li and Minh Giang and Alexander Neitz and Zaheer Abbas and Sarah York and Machel Reid and Elizabeth Cole and Aakanksha Chowdhery and Dipanjan Das and Dominika Rogozi'nska and Vitaly Nikolaev and P. Sprechmann and Zachary Nado and Luk\u00e1s Zilka and Flavien Prost and Luheng He and Marianne Monteiro and Gaurav Mishra and Christoper A. Welty and Joshua Newlan and Dawei Jia and Miltiadis Allamanis and Clara Huiyi Hu and Raoul de Liedekerke and Justin Gilmer and Carl Saroufim and Shruti Rijhwani and Shaobo Hou and Disha Shrivastava and Anirudh Baddepudi and Alex Goldin and Adnan Ozturel and Albin Cassirer and Yunhan Xu and Daniel Sohn and Devendra Singh Sachan and Reinald Kim Amplayo and Craig Swanson and Dessie Petrova and Shashi Narayan and A. Guez and Siddhartha Brahma and Jessica Landon and Miteyan Patel and Ruizhe Zhao and Kevin Villela and Luyu Wang and Wenhao Jia and Matthew Rahtz and Mai Gim'enez and Legg Yeung and Hanzhao Lin and James Keeling and Petko Georgiev and Diana Mincu and Boxi Wu and Salem Haykal and Rachel Saputro and Kiran Vodrahalli and James Qin and Zeynep Cankara and Abhanshu Sharma and Nicholas Fernando and Will Hawkins and Behnam Neyshabur and Solomon Kim and Adrian Hutter and Priyanka Agrawal and Alex Castro-Ros and George van den Driessche and Tao Wang and Fan Yang and Shuo-yiin Chang and P. Komarek and Ross McIlroy and Mario Luvci'c and Guodong Zhang and Wael Farhan and Michael Sharman and Paul Natsev and Paul Michel and Yong Cheng and Yamini Bansal and Siyuan Qiao and Kris Cao and Siamak Shakeri and Christina Butterfield and Justin Chung and P. Rubenstein and Shivani Agrawal and A. Mensch and Kedar Soparkar and Karel Lenc and Timothy Chung and Aedan Pope and Lorenzo Maggiore and Jackie Kay and Priya Jhakra and Shibo Wang and Joshua Maynez and Mary Phuong and Taylor Tobin and A. Tacchetti and Maja Trebacz and Kevin Robinson and Yash Katariya and Sebastian Riedel and Paige Bailey and Kefan Xiao and Nimesh Ghelani and Lora Aroyo and Ambrose Slone and N. Houlsby and Xuehan Xiong and Zhen Yang and E. Gribovskaya and Jonas Adler and Mateo Wirth and Lisa Lee and Music Li and Thais Kagohara and Jay Pavagadhi and Sophie Bridgers and Anna Bortsova and Sanjay Ghemawat and Zafarali Ahmed and Tianqi Liu and Richard Powell and Vijay Bolina and Mariko Iinuma and Polina Zablotskaia and James Besley and Da-Woon Chung and Timothy Dozat and R. Comanescu and Xiance Si and Jeremy Greer and Guolong Su and M. Polacek and Raphael Lopez Kaufman and Simon Tokumine and Hexiang Hu and Elena Buchatskaya and Yingjie Miao and Mohamed Elhawaty and Aditya Siddhant and Nenad Toma\u0161ev and Jinwei Xing and Christina Greer and Helen Miller and Shereen Ashraf and Aurko Roy and Zizhao Zhang and Ada Ma and Angelos Filos and Milos Besta and Rory Blevins and Ted Klimenko and Chih-Kuan Yeh and Soravit Changpinyo and Jiaqi Mu and Oscar Chang and Mantas Pajarskas and Carrie Muir and Vered Cohen and Charline Le Lan and Krishna S Haridasan and Amit Marathe and Steven Hansen and Sholto Douglas and Rajkumar Samuel and Mingqiu Wang and Sophia Austin and Chang Lan and Jiepu Jiang and Justin Chiu and Jaime Alonso Lorenzo and Lars Lowe Sjosund and S'ebastien Cevey and Zach Gleicher and Thi Avrahami and Anudhyan Boral and Hansa Srinivasan and Vittorio Selo and Rhys May and Konstantinos Aisopos and L'eonard Hussenot and Livio Baldini Soares and Kate Baumli and Michael B. Chang and Adri\u00e0 Recasens and Benjamin Caine and A. Pritzel and Filip Pavetic and Fabio Pardo and Anita Gergely and Justin Frye and V. Ramasesh and Dan Horgan and Kartikeya Badola and Nora Kassner and Subhrajit Roy and Ethan Dyer and V'ictor Campos and Alex Tomala and Yunhao Tang and Dalia El Badawy and Elspeth White and Basil Mustafa and Oran Lang and Abhishek Jindal and S. Vikram and Zhitao Gong and S. Caelles and Ross Hemsley and Gregory Thornton and Fangxiaoyu Feng and Wojciech Stokowiec and Ce Zheng and Phoebe Thacker and cCauglar Unlu and Zhishuai Zhang and Mohammad Saleh and James Svensson and Maxwell L. Bileschi and Piyush Patil and Ankesh Anand and Roman Ring and Katerina Tsihlas and Arpi Vezer and Marco Selvi and Toby Shevlane and Mikel Rodriguez and T. Kwiatkowski and Samira Daruki and Keran Rong and Allan Dafoe and Nicholas FitzGerald and Keren Gu-Lemberg and Mina Khan and Lisa Anne Hendricks and Marie Pellat and Vladimir Feinberg and James Cobon-Kerr and Tara N. Sainath and M. Rauh and Sayed Hadi Hashemi and Richard Ives and Yana Hasson and YaGuang Li and Eric Noland and Yuan Cao and Nathan Byrd and Le Hou and Qingze Wang and Thibault Sottiaux and M. Paganini and Jean-Baptiste Lespiau and Alexandre Moufarek and Samer Hassan and K. Shivakumar and Joost R. van Amersfoort and Amol Mandhane and Pratik M. Joshi and Anirudh Goyal and Matthew Tung and Andy Brock and Hannah Sheahan and Vedant Misra and Cheng Li and Nemanja Raki'cevi'c and Mostafa Dehghani and Fangyu Liu and Sid Mittal and Junhyuk Oh and Seb Noury and Eren Sezener and Fantine Huot and Matthew Lamm and Nicola De Cao and Charlie Chen and Gamaleldin Elsayed and E. Chi and Mahdis Mahdieh and Ian Tenney and Nan Hua and Ivan Petrychenko and Patrick Kane and Dylan Scandinaro and Rishub Jain and J. Uesato and Romina Datta and Adam Sadovsky and Oskar Bunyan and Dominik Rabiej and Shimu Wu and John Zhang and Gautam Vasudevan and Edouard Leurent and Mahmoud Alnahlawi and Ionut-Razvan Georgescu and Nan Wei and Ivy Zheng and Betty Chan and Pam G Rabinovitch and P. Sta\u0144czyk and Ye Zhang and David Steiner and Subhajit Naskar and Michael Azzam and Matthew Johnson and Adam Paszke and Chung-Cheng Chiu and Jaume Sanchez Elias and Afroz Mohiuddin and Faizan Muhammad and Jin Miao and Andrew Lee and Nino Vieillard and Sahitya Potluri and Jane Park and Elnaz Davoodi and Jiageng Zhang and Jeff Stanway and D. Garmon and Abhijit Karmarkar and Zhe Dong and Jong Lee and Aviral Kumar and Luowei Zhou and Jonathan Evens and William Isaac and Zhe Chen and Johnson Jia and Anselm Levskaya and Zhenkai Zhu and C. Gorgolewski and Peter Grabowski and Yu Mao and Alberto Magni and Kaisheng Yao and Javier Snaider and Norman Casagrande and P. Suganthan and Evan Palmer and Geoffrey Irving and E. Loper and Manaal Faruqui and Isha Arkatkar and Nanxin Chen and Izhak Shafran and Michael Fink and Alfonso Castano and Irene Giannoumis and Wooyeol Kim and Mikolaj Rybi'nski and Ashwin Sreevatsa and J. Prendki and D. Soergel and Adrian Goedeckemeyer and W. Gierke and Mohsen Jafari and Meenu Gaba and Jeremy Wiesner and Diana Gage Wright and Yawen Wei and Harsha Vashisht and Yana Kulizhskaya and Jay Hoover and Maigo Le and Lu Li and Chimezie Iwuanyanwu and Lu Liu and Kevin Ramirez and A. Ya. Khorlin and Albert Cui and Tian Lin and Marin Georgiev and Marcus Wu and Ricardo Aguilar and Keith Pallo and A. Chakladar and Alena Repina and Xihui Wu and Tom van der Weide and Priya Ponnapalli and C. Kaplan and Ji\u0159\u00ed \u0160im\u0161a and Shuangfeng Li and Olivier Dousse and Jeff Piper and Nathan Ie and Minnie Lui and Rama Kumar Pasumarthi and Nathan Lintz and Anitha Vijayakumar and Lam Nguyen Thiet and D. Andor and Pedro Valenzuela and Cosmin Paduraru and Daiyi Peng and Katherine Lee and Shuyuan Zhang and Somer Greene and Duc Dung Nguyen and Paula Kurylowicz and S. Velury and Sebastian Krause and Cassidy Hardin and Lucas Dixon and Lili Janzer and Kiam Choo and Ziqiang Feng and Biao Zhang and Achintya Singhal and Tejasi Latkar and Mingyang Zhang and Quoc V. Le and Elena Allica Abellan and Dayou Du and Dan McKinnon and Natasha Antropova and Tolga Bolukbasi and Orgad Keller and David Reid and D. Finchelstein and Maria Abi Raad and Remi Crocker and Peter Hawkins and Robert Dadashi and Colin Gaffney and Sid Lall and Ken Franko and Egor Filonov and Anna Bulanova and R\u00e9mi Leblond and Vikas Yadav and Shirley Chung and Harry Askham and Luis C. Cobo and Kelvin Xu and Felix Fischer and Jun Xu and Christina Sorokin and Chris Alberti and Chu-Cheng Lin and Colin Evans and Hao Zhou and Alek Dimitriev and Hannah Forbes and D. Banarse and Zora Tung and Jeremiah Liu and Mark Omernick and Colton Bishop and Chintu Kumar and Rachel Sterneck and Ryan Foley and Rohan Jain and Swaroop Mishra and Jiawei Xia and Taylor Bos and Geoffrey Cideron and E. Amid and Francesco Piccinno and Xingyu Wang and Praseem Banzal and Petru Gurita and Hila Noga and Premal Shah and D. Mankowitz and Oleksandr Polozov and Nate Kushman and Victoria Krakovna and S. Brown and M. Bateni and Dennis Duan and Vlad Firoiu and Meghana Thotakuri and Tom Natan and Anhad Mohananey and Matthieu Geist and Sidharth Mudgal and Sertan Girgin and Hui Li and Jiayu Ye and Ofir Roval and Reiko Tojo and Michael Kwong and James Lee-Thorp and Christopher Yew and Quan Yuan and Sumit Bagri and Danila Sinopalnikov and Sabela Ramos and John F. J. Mellor and Abhishek Sharma and Aliaksei Severyn and Jonathan Lai and Kathy Wu and Heng-Tze Cheng and David Miller and Nicolas Sonnerat and Denis Vnukov and Rory Greig and Jennifer Beattie and Emily Caveness and Libin Bai and Julian Martin Eisenschlos and Alex Korchemniy and Tomy Tsai and Mimi Jasarevic and Weize Kong and Phuong Dao and Zeyu Zheng and Frederick Liu and Rui Zhu and Mark Geller and Tian Huey Teh and Jason Sanmiya and E. Gladchenko and Nejc Trdin and Andrei Sozanschi and Daniel Toyama and Evan Rosen and S. Tavakkol and Linting Xue and Chen Elkind and Oliver Woodman and John Carpenter and G. Papamakarios and Rupert Kemp and Sushant Kafle and Tanya Grunina and Rishika Sinha and Alice Talbert and Abhimanyu Goyal and Kalpesh Krishna and Diane Wu and Denese Owusu-Afriyie and Cosmo Du and Chloe Thornton and Jordi Pont-Tuset and P. Narayana and Jing Li and Sabaer Fatehi and J. Michael Wieting and Omar Ajmeri and Benigno Uria and Tao Zhu and Yeongil Ko and Laura Knight and Am'elie H'eliou and Ning Niu and Shane Gu and Chenxi Pang and Dustin Tran and Yeqing Li and Nir Levine and Ariel Stolovich and Norbert Kalb and Rebeca Santamaria-Fernandez and Sonam Goenka and Wenny Yustalim and Robin Strudel and Ali Elqursh and Balaji Lakshminarayanan and Charlie Deck and Shyam Upadhyay and Hyo Lee and Mike Dusenberry and Zonglin Li and Xuezhi Wang and Kyle Levin and Raphael Hoffmann and D. Holtmann-Rice and Olivier Bachem and Summer Yue and Sho Arora and Eric Malmi and Daniil Mirylenka and Qijun Tan and Christy Koh and S. Yeganeh and Siim Poder and Steven Zheng and F. Pongetti and Mukarram Tariq and Yanhua Sun and Lucian Ionita and Mojtaba Seyedhosseini and P. Tafti and Raghavendra Kotikalapudi and Zhiyu Liu and Anmol Gulati and Jasmine Liu and Xinyu Ye and Bart Chrzaszcz and Lily Wang and Nikhil Sethi and Tianrun Li and Ben Brown and Shreya Singh and Wei Fan and Aaron Parisi and Joe Stanton and Chenkai Kuang and Vinod Koverkathu and Christopher A. Choquette-Choo and Yunjie Li and TJ Lu and Abe Ittycheriah and Prakash Shroff and Pei Sun and Mani Varadarajan and Sanaz Bahargam and Rob Willoughby and D. Gaddy and Ishita Dasgupta and Guillaume Desjardins and M. Cornero and Brona Robenek and Bhavishya Mittal and Ben Albrecht and Ashish Shenoy and Fedor Moiseev and Henrik Jacobsson and Alireza Ghaffarkhah and Morgane Riviere and Alanna Walton and Cl'ement Crepy and Alicia Parrish and Yuan Liu and Zongwei Zhou and Clement Farabet and Carey Radebaugh and Praveen Srinivasan and Claudia van der Salm and A. Fidjeland and Salvatore Scellato and Eri Latorre-Chimoto and Hanna Klimczak-Pluci'nska and David Bridson and D. Cesare and Tom Hudson and Piermaria Mendolicchio and Lexi Walker and Alex Morris and Ivo Penchev and M. Mauger and Alexey Guseynov and Alison Reid and Seth Odoom and Lucia Loher and Victor Cotruta and Madhavi Yenugula and Dominik Grewe and Anastasia Petrushkina and Tom Duerig and Antonio Sanchez and Steve Yadlowsky and Amy Shen and Amir Globerson and Adam Kurzrok and Lynette Webb and Sahil Dua and Dong Li and Preethi Lahoti and Surya Bhupatiraju and Dan Hurt and Haroon Qureshi and Ananth Agarwal and Tomer Shani and Matan Eyal and Anuj Khare and Shreyas Belle and Lei Wang and Chetan Tekur and Mihir Kale and Jinliang Wei and Ruoxin Sang and Brennan Saeta and Tyler Liechty and Yi Sun and Yao Zhao and Stephan Lee and Pandu Nayak and Doug Fritz and Manish Reddy Vuyyuru and John Aslanides and Nidhi Vyas and M. Wicke and Xiao Ma and Taylan Bilal and Evgenii Eltyshev and Daniel Balle and Nina Martin and Hardie Cate and James Manyika and Keyvan Amiri and Yelin Kim and Xi Xiong and Kai Kang and Florian Luisier and Nilesh Tripuraneni and David Madras and Mandy Guo and Austin Waters and Oliver Wang and J. Ainslie and Jason Baldridge and Han Zhang and Garima Pruthi and Jakob Bauer and Feng Yang and Riham Mansour and Jason Gelman and Yang Xu and George Polovets and Ji Liu and Honglong Cai and Warren Chen and XiangHai Sheng and Emily Xue and Sherjil Ozair and Adams Wei Yu and Christof Angermueller and Xiaowei Li and Weiren Wang and Julia Wiesinger and Emmanouil Koukoumidis and Yuan Tian and Anand Iyer and Madhu Gurumurthy and Mark Goldenson and Parashar Shah and MK Blake and Hongkun Yu and Anthony Urbanowicz and J. Palomaki and Chrisantha Fernando and Kevin Brooks and Ken Durden and Harsh Mehta and Nikola Momchev and Elahe Rahimtoroghi and M. Georgaki and Amit Raul and Sebastian Ruder and Morgan Redshaw and Jinhyuk Lee and Komal Jalan and Dinghua Li and Ginger Perng and Blake A. Hechtman and Parker Schuh and Milad Nasr and Mianna Chen and Kieran Milan and Vladimir Mikulik and Trevor Strohman and Juliana Franco and Tim Green and D. Hassabis and K. Kavukcuoglu and Jeffrey Dean and O. Vinyals},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Gemini: A Family of Highly Capable Multimodal Models},\n volume = {abs/2312.11805},\n year = {2023}\n}\n"
        }
    },
    "183_transe": {
        "paperId": "2582ab7c70c9e7fcb84545944eba8f3a7f253248",
        "externalIds": {
            "DBLP": "conf/nips/BordesUGWY13",
            "MAG": "2127795553",
            "CorpusId": 14941970
        },
        "corpusId": 14941970,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
        "title": "Translating Embeddings for Modeling Multi-relational Data",
        "abstract": "We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "referenceCount": 18,
        "citationCount": 6108,
        "influentialCitationCount": 1872,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "TransE is proposed, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities, which proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2013-12-05",
        "journal": {
            "pages": "2787-2795"
        },
        "citationStyles": {
            "bibtex": "@Article{Bordes2013TranslatingEF,\n author = {Antoine Bordes and Nicolas Usunier and Alberto Garc\u00eda-Dur\u00e1n and J. Weston and Oksana Yakhnenko},\n booktitle = {Neural Information Processing Systems},\n pages = {2787-2795},\n title = {Translating Embeddings for Modeling Multi-relational Data},\n year = {2013}\n}\n"
        }
    },
    "184_all-attention_network_+_adaptive_span": {
        "paperId": "830995ef17cc291c13f42dfd9f462137de1d2179",
        "externalIds": {
            "DBLP": "journals/corr/abs-1907-01470",
            "ArXiv": "1907.01470",
            "MAG": "2955227499",
            "CorpusId": 195776151
        },
        "corpusId": 195776151,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/830995ef17cc291c13f42dfd9f462137de1d2179",
        "title": "Augmenting Self-attention with Persistent Memory",
        "abstract": "Transformer networks have lead to important progress in language modeling and machine translation. These models include two consecutive modules, a feed-forward layer and a self-attention layer. The latter allows the network to capture long term dependencies and are often regarded as the key ingredient in the success of Transformers. Building upon this intuition, we propose a new model that solely consists of attention layers. More precisely, we augment the self-attention layers with persistent memory vectors that play a similar role as the feed-forward layer. Thanks to these vectors, we can remove the feed-forward layer without degrading the performance of a transformer. Our evaluation shows the benefits brought by our model on standard character and word level language modeling benchmarks.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 45,
        "citationCount": 102,
        "influentialCitationCount": 7,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new model that solely consists of attention layers is proposed that augment the self-attention layers with persistent memory vectors that play a similar role as the feed-forward layer."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-07-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1907.01470"
        },
        "citationStyles": {
            "bibtex": "@Article{Sukhbaatar2019AugmentingSW,\n author = {Sainbayar Sukhbaatar and Edouard Grave and Guillaume Lample and H. J\u00e9gou and Armand Joulin},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Augmenting Self-attention with Persistent Memory},\n volume = {abs/1907.01470},\n year = {2019}\n}\n"
        }
    },
    "186_delta_rnn_(+_full_context)": {
        "paperId": "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61",
        "externalIds": {
            "DBLP": "journals/corr/abs-2106-06295",
            "ArXiv": "2106.06295",
            "CorpusId": 235417174
        },
        "corpusId": 235417174,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/86589b6286ef3c55b8b4fccfb41a3b30b7afdf61",
        "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers",
        "abstract": "Transformers with linearised attention (''linear Transformers'') have demonstrated the practical scalability and effectiveness of outer product-based Fast Weight Programmers (FWPs) from the '90s. However, the original FWP formulation is more general than the one of linear Transformers: a slow neural network (NN) continually reprograms the weights of a fast NN with arbitrary architecture. In existing linear Transformers, both NNs are feedforward and consist of a single layer. Here we explore new variations by adding recurrence to the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment. Our models exhibit properties of Transformers and RNNs. In the reinforcement learning setting, we report large improvements over LSTM in several Atari games. Our code is public.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 84,
        "citationCount": 39,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The novel recurrent FWPs (RFWPs) are evaluated on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment and report large improvements over LSTM in several Atari games."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-06-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2106.06295"
        },
        "citationStyles": {
            "bibtex": "@Article{Irie2021GoingBL,\n author = {Kazuki Irie and Imanol Schlag and R'obert Csord'as and J. Schmidhuber},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Going Beyond Linear Transformers with Recurrent Fast Weight Programmers},\n volume = {abs/2106.06295},\n year = {2021}\n}\n"
        }
    },
    "187_gato": {
        "paperId": "5922f437512158970c417f4413bface021df5f78",
        "externalIds": {
            "DBLP": "journals/corr/abs-2205-06175",
            "ArXiv": "2205.06175",
            "DOI": "10.48550/arXiv.2205.06175",
            "CorpusId": 248722148
        },
        "corpusId": 248722148,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/5922f437512158970c417f4413bface021df5f78",
        "title": "A Generalist Agent",
        "abstract": "Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.",
        "venue": "Trans. Mach. Learn. Res.",
        "year": 2022,
        "referenceCount": 105,
        "citationCount": 490,
        "influentialCitationCount": 43,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2205.06175",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This report describes the model and the data, and document the current capabilities of Gato, a single generalist agent that works as a multi-modal, multi-task,Multi-embodiment generalist policy beyond the realm of text outputs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-05-12",
        "journal": {
            "name": "Trans. Mach. Learn. Res.",
            "volume": "2022"
        },
        "citationStyles": {
            "bibtex": "@Article{Reed2022AGA,\n author = {S. Reed and Konrad Zolna and Emilio Parisotto and Sergio Gomez Colmenarejo and Alexander Novikov and Gabriel Barth-Maron and Mai Gimenez and Yury Sulsky and Jackie Kay and J. T. Springenberg and Tom Eccles and Jake Bruce and Ali Razavi and Ashley D. Edwards and N. Heess and Yutian Chen and R. Hadsell and O. Vinyals and Mahyar Bordbar and Nando de Freitas},\n booktitle = {Trans. Mach. Learn. Res.},\n journal = {Trans. Mach. Learn. Res.},\n title = {A Generalist Agent},\n volume = {2022},\n year = {2022}\n}\n"
        }
    },
    "188_nopos": {
        "paperId": "a2fc77f075f666b462d9350e7576f0ba9845c61b",
        "externalIds": {
            "DBLP": "journals/corr/abs-2203-16634",
            "ArXiv": "2203.16634",
            "DOI": "10.48550/arXiv.2203.16634",
            "CorpusId": 247839823
        },
        "corpusId": 247839823,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/a2fc77f075f666b462d9350e7576f0ba9845c61b",
        "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information",
        "abstract": "Causal transformer language models (LMs), such as GPT-3, typically require some form of positional encoding, such as positional embeddings. However, we show that LMs without any explicit positional encoding are still competitive with standard models, and that this phenomenon is robust across different datasets, model sizes, and sequence lengths. Probing experiments reveal that such models acquire an implicit notion of absolute positions throughout the network, effectively compensating for the missing information. We conjecture that causal attention enables the model to infer the number of predecessors that each token can attend to, thereby approximating its absolute position. Our findings indicate that causal LMs might derive positional awareness not only from the explicit positioning mechanism, but also from the effects of the causal mask.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2022,
        "referenceCount": 25,
        "citationCount": 48,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2203.16634",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work shows that LMs without any explicit positional encoding are still competitive with standard models, and that this phenomenon is robust across different datasets, model sizes, and sequence lengths."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-03-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2203.16634"
        },
        "citationStyles": {
            "bibtex": "@Article{Haviv2022TransformerLM,\n author = {Adi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Transformer Language Models without Positional Encodings Still Learn Positional Information},\n volume = {abs/2203.16634},\n year = {2022}\n}\n"
        }
    },
    "189_gpt3-6.7b_(rerun_of_original)": {
        "paperId": "0b0d7d87c58d41b92d907347b778032be5966f60",
        "externalIds": {
            "ArXiv": "2203.03466",
            "DBLP": "journals/corr/abs-2203-03466",
            "DOI": "10.48550/arXiv.2203.03466",
            "CorpusId": 247292726
        },
        "corpusId": 247292726,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0b0d7d87c58d41b92d907347b778032be5966f60",
        "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
        "abstract": "Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost. A Pytorch implementation of our technique can be found at github.com/microsoft/mup and installable via `pip install mup`.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 69,
        "citationCount": 77,
        "influentialCitationCount": 10,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.03466",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Physics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Physics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work shows that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes, which leads to a new HP tuning paradigm, muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-03-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2203.03466"
        },
        "citationStyles": {
            "bibtex": "@Article{Yang2022TensorPV,\n author = {Greg Yang and J. E. Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and J. Pachocki and Weizhu Chen and Jianfeng Gao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},\n volume = {abs/2203.03466},\n year = {2022}\n}\n"
        }
    },
    "191_resnet-1001": {
        "paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80",
        "externalIds": {
            "ArXiv": "1603.05027",
            "MAG": "2949427019",
            "DBLP": "journals/corr/HeZR016",
            "DOI": "10.1007/978-3-319-46493-0_38",
            "CorpusId": 6447277
        },
        "corpusId": 6447277,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/77f0a39b8e02686fd85b01971f8feb7f60971f80",
        "title": "Identity Mappings in Deep Residual Networks",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "referenceCount": 30,
        "citationCount": 8850,
        "influentialCitationCount": 1375,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1603.05027",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The propagation formulations behind the residual building blocks suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-03-16",
        "journal": {
            "pages": "630-645"
        },
        "citationStyles": {
            "bibtex": "@Article{He2016IdentityMI,\n author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},\n booktitle = {European Conference on Computer Vision},\n pages = {630-645},\n title = {Identity Mappings in Deep Residual Networks},\n year = {2016}\n}\n"
        }
    },
    "192_flan_t5-xxl_+_blip-2": {
        "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
        "externalIds": {
            "DBLP": "conf/icml/0008LSH23",
            "ArXiv": "2301.12597",
            "DOI": "10.48550/arXiv.2301.12597",
            "CorpusId": 256390509
        },
        "corpusId": 256390509,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
        "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
        "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "referenceCount": 48,
        "citationCount": 1221,
        "influentialCitationCount": 321,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2301.12597",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-01-30",
        "journal": {
            "pages": "19730-19742"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2023BLIP2BL,\n author = {Junnan Li and Dongxu Li and S. Savarese and Steven C. H. Hoi},\n booktitle = {International Conference on Machine Learning},\n pages = {19730-19742},\n title = {BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},\n year = {2023}\n}\n"
        }
    },
    "193_alphacode": {
        "paperId": "5cbe278b65a81602a864184bbca37de91448a5f5",
        "externalIds": {
            "ArXiv": "2203.07814",
            "DBLP": "journals/corr/abs-2203-07814",
            "DOI": "10.1126/science.abq1158",
            "CorpusId": 246527904,
            "PubMed": "36480631"
        },
        "corpusId": 246527904,
        "publicationVenue": {
            "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
            "name": "Science",
            "type": "journal",
            "issn": "0193-4511",
            "alternate_issns": [
                "0036-8075"
            ],
            "url": "https://www.jstor.org/journal/science",
            "alternate_urls": [
                "https://www.sciencemag.org/",
                "http://www.sciencemag.org/",
                "http://www.jstor.org/journals/00368075.html",
                "http://www.sciencemag.org/archive/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/5cbe278b65a81602a864184bbca37de91448a5f5",
        "title": "Competition-level code generation with AlphaCode",
        "abstract": "Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers\u2019 productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. \u2014YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.",
        "venue": "Science",
        "year": 2022,
        "referenceCount": 82,
        "citationCount": 654,
        "influentialCitationCount": 73,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2203.07814",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "AlphaCode is introduced, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform, marking the first time an artificial intelligence system has performed competitively in programming competitions."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-02-08",
        "journal": {
            "name": "Science",
            "pages": "1092 - 1097",
            "volume": "378"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2022CompetitionlevelCG,\n author = {Yujia Li and David Choi and Junyoung Chung and Nate Kushman and Julian Schrittwieser and R\u00e9mi Leblond and Tom and Eccles and James Keeling and Felix Gimeno and Agustin Dal Lago and T. Hubert and Peter Choy and Cyprien de and Masson d\u2019Autume and Igor Babuschkin and Xinyun Chen and Po-Sen Huang and Johannes Welbl and Sven Gowal and Alexey and Cherepanov and James Molloy and D. Mankowitz and Esme Sutherland Robson and Pushmeet Kohli and Nando de and Freitas and K. Kavukcuoglu and O. Vinyals},\n booktitle = {Science},\n journal = {Science},\n pages = {1092 - 1097},\n title = {Competition-level code generation with AlphaCode},\n volume = {378},\n year = {2022}\n}\n"
        }
    },
    "194_awd-fwm_(wt2)": {
        "paperId": "5e11e806d24dd80ecf0f91e7aacedbba8d9fd6fc",
        "externalIds": {
            "ArXiv": "2011.07831",
            "DBLP": "journals/corr/abs-2011-07831",
            "MAG": "3101023723",
            "CorpusId": 226965048
        },
        "corpusId": 226965048,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/5e11e806d24dd80ecf0f91e7aacedbba8d9fd6fc",
        "title": "Learning Associative Inference Using Fast Weight Memory",
        "abstract": "Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed Fast Weight Memory (FWM). Through differentiable operations at every step of a given input sequence, the LSTM updates and maintains compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling.",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "referenceCount": 80,
        "citationCount": 26,
        "influentialCitationCount": 2,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-11-16",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2011.07831"
        },
        "citationStyles": {
            "bibtex": "@Article{Schlag2020LearningAI,\n author = {Imanol Schlag and Tsendsuren Munkhdalai and J. Schmidhuber},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning Associative Inference Using Fast Weight Memory},\n volume = {abs/2011.07831},\n year = {2020}\n}\n"
        }
    },
    "195_nasnet-a": {
        "paperId": "d0611891b9e8a7c5731146097b6f201578f47b2f",
        "externalIds": {
            "MAG": "2964081807",
            "DBLP": "journals/corr/ZophVSL17",
            "ArXiv": "1707.07012",
            "DOI": "10.1109/CVPR.2018.00907",
            "CorpusId": 12227989
        },
        "corpusId": 12227989,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/d0611891b9e8a7c5731146097b6f201578f47b2f",
        "title": "Learning Transferable Architectures for Scalable Image Recognition",
        "abstract": "Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the \"NASNet search space\") which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a \"NASNet architecture\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.",
        "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
        "year": 2017,
        "referenceCount": 77,
        "citationCount": 4921,
        "influentialCitationCount": 796,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1707.07012",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes to search for an architectural building block on a small dataset and then transfer the block to a larger dataset and introduces a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-07-21",
        "journal": {
            "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "pages": "8697-8710"
        },
        "citationStyles": {
            "bibtex": "@Article{Zoph2017LearningTA,\n author = {Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {8697-8710},\n title = {Learning Transferable Architectures for Scalable Image Recognition},\n year = {2017}\n}\n"
        }
    },
    "196_goat-7b": {
        "paperId": "8c7846c9805834dbe2fb0c8f48253b8d65b79d6a",
        "externalIds": {
            "ArXiv": "2305.14201",
            "DBLP": "journals/corr/abs-2305-14201",
            "DOI": "10.48550/arXiv.2305.14201",
            "CorpusId": 258840942
        },
        "corpusId": 258840942,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/8c7846c9805834dbe2fb0c8f48253b8d65b79d6a",
        "title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
        "abstract": "We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 40,
        "citationCount": 34,
        "influentialCitationCount": 3,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14201",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks, is introduced and an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks into a series of learnable tasks by leveraging basic arithmetic principles is proposed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-05-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2305.14201"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2023GoatFL,\n author = {Tiedong Liu and K. H. Low},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks},\n volume = {abs/2305.14201},\n year = {2023}\n}\n"
        }
    },
    "197_transformer_+_simple_recurrent_unit": {
        "paperId": "7ba9b6266569bd7b6a3c2ec64348c5b969a5ceb7",
        "externalIds": {
            "ArXiv": "1709.02755",
            "MAG": "2951897977",
            "ACL": "D18-1477",
            "DBLP": "conf/emnlp/LeiZWDA18",
            "DOI": "10.18653/v1/D18-1477",
            "CorpusId": 52221821
        },
        "corpusId": 52221821,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/7ba9b6266569bd7b6a3c2ec64348c5b969a5ceb7",
        "title": "Simple Recurrent Units for Highly Parallelizable Recurrence",
        "abstract": "Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5\u20149x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model (Vaswani et al., 2017) on translation by incorporating SRU into the architecture.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2017,
        "referenceCount": 80,
        "citationCount": 246,
        "influentialCitationCount": 30,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D18-1477.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Simple Recurrent Unit is proposed, a light recurrent unit that balances model capacity and scalability, designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-09-08",
        "journal": {
            "pages": "4470-4481"
        },
        "citationStyles": {
            "bibtex": "@Article{Lei2017SimpleRU,\n author = {Tao Lei and Yu Zhang and Sida I. Wang and Huijing Dai and Yoav Artzi},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {4470-4481},\n title = {Simple Recurrent Units for Highly Parallelizable Recurrence},\n year = {2017}\n}\n"
        }
    },
    "198_contextnet_+_noisy_student": {
        "paperId": "4efda60a5db5010e5ba68dc5d8aeb7161df8191d",
        "externalIds": {
            "DBLP": "conf/interspeech/ParkZJHCLWL20",
            "ArXiv": "2005.09629",
            "MAG": "3026041220",
            "DOI": "10.21437/INTERSPEECH.2020-1470",
            "CorpusId": 218684988
        },
        "corpusId": 218684988,
        "publicationVenue": {
            "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
            "name": "Interspeech",
            "type": "conference",
            "alternate_names": [
                "Conf Int Speech Commun Assoc",
                "INTERSPEECH",
                "Conference of the International Speech Communication Association"
            ],
            "issn": "2308-457X",
            "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
            "alternate_urls": [
                "http://www.isca-speech.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/4efda60a5db5010e5ba68dc5d8aeb7161df8191d",
        "title": "Improved Noisy Student Training for Automatic Speech Recognition",
        "abstract": "Recently, a semi-supervised learning method known as \"noisy student training\" has been shown to improve image classification performance of deep networks significantly. Noisy student training is an iterative self-training method that leverages augmentation to improve network performance. In this work, we adapt and improve noisy student training for automatic speech recognition, employing (adaptive) SpecAugment as the augmentation method. We find effective methods to filter, balance and augment the data generated in between self-training iterations. By doing so, we are able to obtain word error rates (WERs) 4.2%/8.6% on the clean/noisy LibriSpeech test sets by only using the clean 100h subset of LibriSpeech as the supervised set and the rest (860h) as the unlabeled set. Furthermore, we are able to achieve WERs 1.7%/3.4% on the clean/noisy LibriSpeech test sets by using the unlab-60k subset of LibriLight as the unlabeled set for LibriSpeech 960h. We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%).",
        "venue": "Interspeech",
        "year": 2020,
        "referenceCount": 39,
        "citationCount": 201,
        "influentialCitationCount": 29,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2005.09629",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Engineering",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work adapt and improve noisy student training for automatic speech recognition, employing (adaptive) SpecAugment as the augmentation method and finding effective methods to filter, balance and augment the data generated in between self-training iterations."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-05-19",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2005.09629"
        },
        "citationStyles": {
            "bibtex": "@Article{Park2020ImprovedNS,\n author = {Daniel S. Park and Yu Zhang and Ye Jia and Wei Han and Chung-Cheng Chiu and Bo Li and Yonghui Wu and Quoc V. Le},\n booktitle = {Interspeech},\n journal = {ArXiv},\n title = {Improved Noisy Student Training for Automatic Speech Recognition},\n volume = {abs/2005.09629},\n year = {2020}\n}\n"
        }
    },
    "199_gcrn-m1,_dropout": {
        "paperId": "6b1793ece5993523855ce67c646de408318d1b12",
        "externalIds": {
            "MAG": "2565330852",
            "ArXiv": "1612.07659",
            "DBLP": "journals/corr/SeoDVB16",
            "DOI": "10.1007/978-3-030-04167-0_33",
            "CorpusId": 2687749
        },
        "corpusId": 2687749,
        "publicationVenue": {
            "id": "bc5a5118-8f5c-49c7-806e-fb8d44c10ae7",
            "name": "International Conference on Neural Information Processing",
            "type": "conference",
            "alternate_names": [
                "ICONIP",
                "Int Conf Neural Inf Process"
            ],
            "url": "https://link.springer.com/conference/iconip"
        },
        "url": "https://www.semanticscholar.org/paper/6b1793ece5993523855ce67c646de408318d1b12",
        "title": "Structured Sequence Modeling with Graph Convolutional Recurrent Networks",
        "abstract": null,
        "venue": "International Conference on Neural Information Processing",
        "year": 2016,
        "referenceCount": 28,
        "citationCount": 576,
        "influentialCitationCount": 73,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.07659",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The proposed model combines convolutional neural networks on graphs to identify spatial structures and RNN to find dynamic patterns in data structured by an arbitrary graph."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-12-22",
        "journal": {
            "pages": "362-373"
        },
        "citationStyles": {
            "bibtex": "@Article{Seo2016StructuredSM,\n author = {Youngjoo Seo and M. Defferrard and P. Vandergheynst and X. Bresson},\n booktitle = {International Conference on Neural Information Processing},\n pages = {362-373},\n title = {Structured Sequence Modeling with Graph Convolutional Recurrent Networks},\n year = {2016}\n}\n"
        }
    },
    "200_megatron-bert": {
        "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "externalIds": {
            "MAG": "2973727699",
            "ArXiv": "1909.08053",
            "DBLP": "journals/corr/abs-1909-08053",
            "CorpusId": 202660670
        },
        "corpusId": 202660670,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
        "abstract": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 62,
        "citationCount": 1112,
        "influentialCitationCount": 170,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters and shows that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-09-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1909.08053"
        },
        "citationStyles": {
            "bibtex": "@Article{Shoeybi2019MegatronLMTM,\n author = {M. Shoeybi and M. Patwary and Raul Puri and P. LeGresley and J. Casper and Bryan Catanzaro},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},\n volume = {abs/1909.08053},\n year = {2019}\n}\n"
        }
    },
    "201_once_for_all": {
        "paperId": "7823292e5c4b05c47af91ab6ddf671a0da709e82",
        "externalIds": {
            "MAG": "2969797940",
            "DBLP": "conf/iclr/CaiGWZH20",
            "ArXiv": "1908.09791",
            "CorpusId": 201666112
        },
        "corpusId": 201666112,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7823292e5c4b05c47af91ab6ddf671a0da709e82",
        "title": "Once for All: Train One Network and Specialize it for Efficient Deployment",
        "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at this https URL.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 50,
        "citationCount": 972,
        "influentialCitationCount": 208,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            },
            {
                "category": "Environmental Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost and propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-08-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1908.09791"
        },
        "citationStyles": {
            "bibtex": "@Article{Cai2019OnceFA,\n author = {Han Cai and Chuang Gan and Song Han},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Once for All: Train One Network and Specialize it for Efficient Deployment},\n volume = {abs/1908.09791},\n year = {2019}\n}\n"
        }
    },
    "202_conv-dbn": {
        "paperId": "1e80f755bcbf10479afd2338cec05211fdbd325c",
        "externalIds": {
            "MAG": "2130325614",
            "DBLP": "conf/icml/LeeGRN09",
            "DOI": "10.1145/1553374.1553453",
            "CorpusId": 12008458
        },
        "corpusId": 12008458,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/1e80f755bcbf10479afd2338cec05211fdbd325c",
        "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
        "abstract": "There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.",
        "venue": "International Conference on Machine Learning",
        "year": 2009,
        "referenceCount": 29,
        "citationCount": 2629,
        "influentialCitationCount": 209,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The convolutional deep belief network is presented, a hierarchical generative model which scales to realistic image sizes and is translation-invariant and supports efficient bottom-up and top-down probabilistic inference."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2009-06-14",
        "journal": {
            "pages": "609-616"
        },
        "citationStyles": {
            "bibtex": "@Article{Lee2009ConvolutionalDB,\n author = {Honglak Lee and R. Grosse and R. Ranganath and A. Ng},\n booktitle = {International Conference on Machine Learning},\n pages = {609-616},\n title = {Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations},\n year = {2009}\n}\n"
        }
    },
    "203_transformerxl_+_powersgd_+_l-greco": {
        "paperId": "3b5d743df8cbaaf0ce104d9a8efa77c86a16c85d",
        "externalIds": {
            "ArXiv": "2210.17357",
            "DBLP": "journals/corr/abs-2210-17357",
            "DOI": "10.48550/arXiv.2210.17357",
            "CorpusId": 253237782
        },
        "corpusId": 253237782,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/3b5d743df8cbaaf0ce104d9a8efa77c86a16c85d",
        "title": "L-GreCo: An Efficient and General Framework for Layerwise-Adaptive Gradient Compression",
        "abstract": "Data-parallel distributed training of deep neural networks (DNN) has gained very widespread adoption, but can still experience communication bottlenecks. To address this issue, entire families of compression mechanisms have been developed, including quantization, sparsification, and low-rank approximation, some of which are seeing significant practical adoption. Despite this progress, almost all known compression schemes apply compression uniformly across DNN layers, although layers are heterogeneous in terms of parameter count and their impact on model accuracy. In this work, we provide a general framework for adapting the degree of compression across the model's layers dynamically during training, improving the overall compression, while leading to substantial speedups, without sacrificing accuracy. Our framework, called L-GreCo, is based on an adaptive algorithm, which automatically picks the optimal compression parameters for model layers guaranteeing the best compression ratio while satisfying an error constraint. Extensive experiments over image classification and language modeling tasks shows that L-GreCo is effective across all existing families of compression methods, and achieves up to 2.5$\\times$ training speedup and up to 5$\\times$ compression improvement over efficient implementations of existing approaches, while recovering full accuracy. Moreover, L-GreCo is complementary to existing adaptive algorithms, improving their compression ratio by 50% and practical throughput by 66%.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 33,
        "citationCount": 4,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.17357",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work provides a general framework for adapting the degree of compression across the model's layers dynamically during training, improving the overall compression, while leading to substantial speedups, without sacrificing accuracy."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-31",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.17357"
        },
        "citationStyles": {
            "bibtex": "@Article{Alimohammadi2022LGreCoAE,\n author = {Mohammadreza Alimohammadi and I. Markov and Elias Frantar and Dan Alistarh},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {L-GreCo: An Efficient and General Framework for Layerwise-Adaptive Gradient Compression},\n volume = {abs/2210.17357},\n year = {2022}\n}\n"
        }
    },
    "204_medbert": {
        "paperId": "5d4de0fa45aeddc31142e6a24666d06ed7923f1e",
        "externalIds": {
            "PubMedCentral": "8137882",
            "DBLP": "journals/corr/abs-2005-12833",
            "MAG": "3030156796",
            "ArXiv": "2005.12833",
            "DOI": "10.1038/s41746-021-00455-y",
            "CorpusId": 218889776,
            "PubMed": "34017034"
        },
        "corpusId": 218889776,
        "publicationVenue": {
            "id": "ef485645-f75f-4344-8b9d-3c260e69503b",
            "name": "npj Digital Medicine",
            "alternate_names": [
                "npj Digit Med"
            ],
            "issn": "2398-6352",
            "url": "http://www.nature.com/npjdigitalmed/"
        },
        "url": "https://www.semanticscholar.org/paper/5d4de0fa45aeddc31142e6a24666d06ed7923f1e",
        "title": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction",
        "abstract": null,
        "venue": "npj Digital Medicine",
        "year": 2020,
        "referenceCount": 93,
        "citationCount": 361,
        "influentialCitationCount": 13,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.nature.com/articles/s41746-021-00455-y.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Inspired by BERT, Med-BERT is a contextualized embedding model pretrained on a structured EHR dataset of 28,490,650 patients that substantially improves the prediction accuracy and can boost the area under the receiver operating characteristics curve (AUC) by 1.21\u20136.14% in two disease prediction tasks from two clinical databases."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-05-22",
        "journal": {
            "name": "NPJ Digital Medicine",
            "volume": "4"
        },
        "citationStyles": {
            "bibtex": "@Article{Rasmy2020MedBERTPC,\n author = {L. Rasmy and Yang Xiang and Z. Xie and Cui Tao and Degui Zhi},\n booktitle = {npj Digital Medicine},\n journal = {NPJ Digital Medicine},\n title = {Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction},\n volume = {4},\n year = {2020}\n}\n"
        }
    },
    "206_mgk_4_heads_(medium)": {
        "paperId": "48af9b314181b04edcc0b7224ffe4689036b755f",
        "externalIds": {
            "ArXiv": "2110.08678",
            "DBLP": "conf/icml/NguyenNLNTBHO22",
            "CorpusId": 249626057
        },
        "corpusId": 249626057,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/48af9b314181b04edcc0b7224ffe4689036b755f",
        "title": "Improving Transformers with Probabilistic Attention Keys",
        "abstract": "Multi-head attention is a driving force behind state-of-the-art transformers, which achieve remarkable performance across a variety of natural language processing (NLP) and computer vision tasks. It has been observed that for many applications, those attention heads learn redundant embedding, and most of them can be removed without degrading the performance of the model. Inspired by this observation, we propose Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, Transformer-MGK accelerates training and inference, has fewer parameters, and requires fewer FLOPs to compute while achieving comparable or better accuracy across tasks. Transformer-MGK can also be easily extended to use with linear attention. We empirically demonstrate the advantage of Transformer-MGK in a range of practical applications, including language modeling and tasks that involve very long sequences. On the Wikitext-103 and Long Range Arena benchmark, Transformer-MGKs with 4 heads attain comparable or better performance to the baseline transformers with 8 heads.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 110,
        "citationCount": 19,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head that accelerates training and inference, has fewer parameters, and requires fewer FLOPs to compute while achieving comparable or better accuracy across tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-10-16",
        "journal": {
            "pages": "16595-16621"
        },
        "citationStyles": {
            "bibtex": "@Article{Nguyen2021ImprovingTW,\n author = {Tam Nguyen and T. Nguyen and Dung D. Le and Duy Khuong Nguyen and Viet-Anh Tran and Richard Baraniuk and Nhat Ho and S. Osher},\n booktitle = {International Conference on Machine Learning},\n pages = {16595-16621},\n title = {Improving Transformers with Probabilistic Attention Keys},\n year = {2021}\n}\n"
        }
    },
    "207_deepconpred2": {
        "paperId": "78cde359e88f625715bb32b00868c323b23f7c55",
        "externalIds": {
            "MAG": "2900324779",
            "PubMedCentral": "6247404",
            "DOI": "10.1016/j.csbj.2018.10.009",
            "CorpusId": 53982895,
            "PubMed": "30505403"
        },
        "corpusId": 53982895,
        "publicationVenue": {
            "id": "6fccd61b-6529-4c58-bb83-76a7ad759677",
            "name": "Computational and Structural Biotechnology Journal",
            "type": "journal",
            "alternate_names": [
                "Comput struct biotechnol j",
                "Comput Struct Biotechnol J",
                "Computational and structural biotechnology journal"
            ],
            "issn": "2001-0370",
            "url": "https://www.journals.elsevier.com/computational-and-structural-biotechnology-journal/",
            "alternate_urls": [
                "http://www.csbj-rncsb.org/#!"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/78cde359e88f625715bb32b00868c323b23f7c55",
        "title": "DeepConPred2: An Improved Method for the Prediction of Protein Residue Contacts",
        "abstract": null,
        "venue": "Computational and Structural Biotechnology Journal",
        "year": 2018,
        "referenceCount": 33,
        "citationCount": 23,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A second version of the authors' residue contact predictor, DeepConPred2, is introduced, which exhibits substantially improved performance and sufficiently reduced running time after model re-optimization and feature updates."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-11-10",
        "journal": {
            "name": "Computational and Structural Biotechnology Journal",
            "pages": "503 - 510",
            "volume": "16"
        },
        "citationStyles": {
            "bibtex": "@Article{Ding2018DeepConPred2AI,\n author = {Wenze Ding and Wenzhi Mao and Di Shao and Wenxuan Zhang and Haipeng Gong},\n booktitle = {Computational and Structural Biotechnology Journal},\n journal = {Computational and Structural Biotechnology Journal},\n pages = {503 - 510},\n title = {DeepConPred2: An Improved Method for the Prediction of Protein Residue Contacts},\n volume = {16},\n year = {2018}\n}\n"
        }
    },
    "208_instruct-gpt_+_mind's_eye": {
        "paperId": "2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8",
        "externalIds": {
            "DBLP": "journals/corr/abs-2210-05359",
            "ArXiv": "2210.05359",
            "DOI": "10.48550/arXiv.2210.05359",
            "CorpusId": 252815535
        },
        "corpusId": 252815535,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8",
        "title": "Mind's Eye: Grounded Language Model Reasoning through Simulation",
        "abstract": "Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 85,
        "citationCount": 52,
        "influentialCitationCount": 4,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.05359",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Mind's Eye is presented, a paradigm to ground language model reasoning in the physical world by using a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then using the simulation results as part of the input, which enables language models to perform reasoning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.05359"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2022MindsEG,\n author = {Ruibo Liu and Jason Wei and S. Gu and Te-Yen Wu and Soroush Vosoughi and Claire Cui and Denny Zhou and Andrew M. Dai},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Mind's Eye: Grounded Language Model Reasoning through Simulation},\n volume = {abs/2210.05359},\n year = {2022}\n}\n"
        }
    },
    "209_debertav3-large_+_kear_": {
        "paperId": "64c8350a66ada14ecc0baa3446c75884cc27e3dd",
        "externalIds": {
            "ArXiv": "2112.03254",
            "DBLP": "journals/corr/abs-2112-03254",
            "DOI": "10.24963/ijcai.2022/383",
            "CorpusId": 244909137
        },
        "corpusId": 244909137,
        "publicationVenue": {
            "id": "67f7f831-711a-43c8-8785-1e09005359b5",
            "name": "International Joint Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "Int Jt Conf Artif Intell",
                "IJCAI"
            ],
            "url": "http://www.ijcai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/64c8350a66ada14ecc0baa3446c75884cc27e3dd",
        "title": "Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention",
        "abstract": "Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear.\n\n By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems.\n\n We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. \n\n In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4% in comparison to the human accuracy of 88.9%.",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2021,
        "referenceCount": 67,
        "citationCount": 42,
        "influentialCitationCount": 6,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2022/0383.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Philosophy",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-12-06",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2112.03254"
        },
        "citationStyles": {
            "bibtex": "@Article{Xu2021HumanPO,\n author = {Yichong Xu and Chenguang Zhu and Shuohang Wang and Siqi Sun and Hao Cheng and Xiaodong Liu and Jianfeng Gao and Pengcheng He and Michael Zeng and Xuedong Huang},\n booktitle = {International Joint Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention},\n volume = {abs/2112.03254},\n year = {2021}\n}\n"
        }
    },
    "210_lstm+neuralcache": {
        "paperId": "f900e2fd4dad1cc300fe571dc26ee9422d2bf5fd",
        "externalIds": {
            "MAG": "2891358242",
            "DBLP": "journals/corr/abs-1809-08826",
            "ArXiv": "1809.08826",
            "DOI": "10.1109/SLT.2018.8639543",
            "CorpusId": 52812420
        },
        "corpusId": 52812420,
        "publicationVenue": {
            "id": "d8dfb5ba-9312-410c-a361-8ad05f945939",
            "name": "Spoken Language Technology Workshop",
            "type": "conference",
            "alternate_names": [
                "SLT",
                "Spok Lang Technol Workshop"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/f900e2fd4dad1cc300fe571dc26ee9422d2bf5fd",
        "title": "Information-Weighted Neural Cache Language Models for ASR",
        "abstract": "Neural cache language models (LMs) extend the idea of regular cache language models by making the cache probability dependent on the similarity between the current context and the context of the words in the cache. We make an extensive comparison of \u2018regular\u2019 cache models with neural cache models, both in terms of perplexity and WER after rescoring first-pass ASR results. Furthermore, we propose two extensions to this neural cache model that make use of the content value/information weight of the word: firstly, combining the cache probability and LM probability with an information-weighted interpolation and secondly, selectively adding only content words to the cache. We obtain a 29.9%/32.1% (validation/test set) relative improvement in perplexity with respect to a baseline LSTM LM on theWikiText-2 dataset, outperforming previous work on neural cache LMs. Additionally, we observe significant WER reductions with respect to the baseline model on the WSJ ASR task.",
        "venue": "Spoken Language Technology Workshop",
        "year": 2018,
        "referenceCount": 17,
        "citationCount": 2,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://lirias.kuleuven.be/bitstream/123456789/627494/2/4334_postprint.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An extensive comparison of \u2018regular\u2019 cache models with neural cache models, both in terms of perplexity and WER after rescoring first-pass ASR results, and proposes two extensions to this neural cache model that make use of the content value/information weight of the word."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-09-24",
        "journal": {
            "name": "2018 IEEE Spoken Language Technology Workshop (SLT)",
            "pages": "756-762"
        },
        "citationStyles": {
            "bibtex": "@Article{Verwimp2018InformationWeightedNC,\n author = {Lyan Verwimp and J. Pelemans and H. V. hamme and P. Wambacq},\n booktitle = {Spoken Language Technology Workshop},\n journal = {2018 IEEE Spoken Language Technology Workshop (SLT)},\n pages = {756-762},\n title = {Information-Weighted Neural Cache Language Models for ASR},\n year = {2018}\n}\n"
        }
    },
    "211_lenet-5": {
        "paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4",
        "externalIds": {
            "MAG": "2112796928",
            "DBLP": "journals/pieee/LeCunBBH98",
            "DOI": "10.1109/5.726791",
            "CorpusId": 14542261
        },
        "corpusId": 14542261,
        "publicationVenue": {
            "id": "6faaccca-1cc4-45a9-aeb6-96a4901d2606",
            "name": "Proceedings of the IEEE",
            "type": "journal",
            "alternate_names": [
                "Proc IEEE"
            ],
            "issn": "0018-9219",
            "alternate_issns": [
                "1558-2256"
            ],
            "url": "http://www.ieee.org/portal/pages/pubs/proceedings/",
            "alternate_urls": [
                "http://www.ieee.org/products/onlinepubs/pub/about_conference.html",
                "https://ieeexplore.ieee.org/servlet/opac?punumber=5",
                "http://proceedingsoftheieee.ieee.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/162d958ff885f1462aeda91cd72582323fd6a1f4",
        "title": "Gradient-based learning applied to document recognition",
        "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.",
        "venue": "Proceedings of the IEEE",
        "year": 1998,
        "referenceCount": 150,
        "citationCount": 46091,
        "influentialCitationCount": 5810,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://hal.science/hal-03926082/document",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
        },
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "publicationDate": null,
        "journal": {
            "name": "Proc. IEEE",
            "pages": "2278-2324",
            "volume": "86"
        },
        "citationStyles": {
            "bibtex": "@Article{LeCun1998GradientbasedLA,\n author = {Yann LeCun and L. Bottou and Yoshua Bengio and P. Haffner},\n booktitle = {Proceedings of the IEEE},\n journal = {Proc. IEEE},\n pages = {2278-2324},\n title = {Gradient-based learning applied to document recognition},\n volume = {86},\n year = {1998}\n}\n"
        }
    },
    "212_max-margin_markov_networks": {
        "paperId": "0c450531e1121cfb657be5195e310217a4675397",
        "externalIds": {
            "MAG": "2105644991",
            "DBLP": "conf/nips/TaskarGK03",
            "CorpusId": 201720
        },
        "corpusId": 201720,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/0c450531e1121cfb657be5195e310217a4675397",
        "title": "Max-Margin Markov Networks",
        "abstract": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.",
        "venue": "Neural Information Processing Systems",
        "year": 2003,
        "referenceCount": 20,
        "citationCount": 1490,
        "influentialCitationCount": 203,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2003-12-09",
        "journal": {
            "pages": "25-32"
        },
        "citationStyles": {
            "bibtex": "@Article{Taskar2003MaxMarginMN,\n author = {B. Taskar and Carlos Guestrin and D. Koller},\n booktitle = {Neural Information Processing Systems},\n pages = {25-32},\n title = {Max-Margin Markov Networks},\n year = {2003}\n}\n"
        }
    },
    "213_rgc+asq_(wt2)": {
        "paperId": "44f137984b5c56798684dd0dbfc7ab0c13f47a93",
        "externalIds": {
            "DBLP": "journals/corr/abs-1808-04357",
            "ArXiv": "1808.04357",
            "MAG": "2952144909",
            "DOI": "10.1016/j.jpdc.2019.05.016",
            "CorpusId": 52008288
        },
        "corpusId": 52008288,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/44f137984b5c56798684dd0dbfc7ab0c13f47a93",
        "title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning",
        "abstract": null,
        "venue": "J. Parallel Distributed Comput.",
        "year": 2018,
        "referenceCount": 33,
        "citationCount": 21,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1808.04357",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper develops an RGC method that is able to reduce the end-to-end training time on real-world multi-GPU systems, and introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-08-13",
        "journal": {
            "name": "J. Parallel Distributed Comput.",
            "pages": "30-39",
            "volume": "133"
        },
        "citationStyles": {
            "bibtex": "@Article{Fang2018RedSyncR,\n author = {Jiarui Fang and H. Fu and Guangwen Yang and Cho-Jui Hsieh},\n booktitle = {J. Parallel Distributed Comput.},\n journal = {J. Parallel Distributed Comput.},\n pages = {30-39},\n title = {RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\n volume = {133},\n year = {2018}\n}\n"
        }
    },
    "214_vit-base_32": {
        "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
        "externalIds": {
            "MAG": "3094502228",
            "DBLP": "journals/corr/abs-2010-11929",
            "ArXiv": "2010.11929",
            "CorpusId": 225039882
        },
        "corpusId": 225039882,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "referenceCount": 65,
        "citationCount": 16918,
        "influentialCitationCount": 2876,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-10-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2010.11929"
        },
        "citationStyles": {
            "bibtex": "@Article{Dosovitskiy2020AnII,\n author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and G. Heigold and S. Gelly and Jakob Uszkoreit and N. Houlsby},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n volume = {abs/2010.11929},\n year = {2020}\n}\n"
        }
    },
    "215_xglm": {
        "paperId": "1403e6b9adf7712c35ae56327d52fe54603b87e1",
        "externalIds": {
            "DBLP": "journals/corr/abs-2112-10668",
            "CorpusId": 260651613
        },
        "corpusId": 260651613,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/1403e6b9adf7712c35ae56327d52fe54603b87e1",
        "title": "Few-shot Learning with Multilingual Language Models",
        "abstract": "Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, and study their few-and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We present a detailed analysis of where the model suc-ceeds and fails, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models in social value tasks such as hate speech detection in 5 languages and find it has limitations similar to comparably sized GPT-3 models.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 85,
        "citationCount": 116,
        "influentialCitationCount": 16,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A detailed analysis of where the model succeeds and fails is presented, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2112.10668"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2021FewshotLW,\n author = {Xi Victoria Lin and Todor Mihaylov and Mikel Artetxe and Tianlu Wang and Shuohui Chen and Daniel Simig and Myle Ott and Naman Goyal and Shruti Bhosale and Jingfei Du and Ramakanth Pasunuru and Sam Shleifer and Punit Singh Koura and Vishrav Chaudhary and Brian O'Horo and Jeff Wang and Luke Zettlemoyer and Zornitsa Kozareva and Mona T. Diab and Ves Stoyanov and Xian Li},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Few-shot Learning with Multilingual Language Models},\n volume = {abs/2112.10668},\n year = {2021}\n}\n"
        }
    },
    "216_vd-rhn": {
        "paperId": "7dba53e72c182e25e98e8f73a99d75ff69dda0c2",
        "externalIds": {
            "MAG": "2952158118",
            "ArXiv": "1607.03474",
            "DBLP": "journals/corr/ZillySKS16",
            "CorpusId": 1101453
        },
        "corpusId": 1101453,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7dba53e72c182e25e98e8f73a99d75ff69dda0c2",
        "title": "Recurrent Highway Networks",
        "abstract": "Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "referenceCount": 60,
        "citationCount": 396,
        "influentialCitationCount": 62,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem is introduced that illuminates several modeling and optimization issues and improves the understanding of the LSTM cell."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-07-12",
        "journal": {
            "pages": "4189-4198"
        },
        "citationStyles": {
            "bibtex": "@Article{Zilly2016RecurrentHN,\n author = {J. Zilly and R. Srivastava and J. Koutn\u00edk and J. Schmidhuber},\n booktitle = {International Conference on Machine Learning},\n pages = {4189-4198},\n title = {Recurrent Highway Networks},\n year = {2016}\n}\n"
        }
    },
    "217_large_regularized_lstm": {
        "paperId": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97",
        "externalIds": {
            "MAG": "1591801644",
            "ArXiv": "1409.2329",
            "DBLP": "journals/corr/ZarembaSV14",
            "CorpusId": 17719760
        },
        "corpusId": 17719760,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97",
        "title": "Recurrent Neural Network Regularization",
        "abstract": "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.",
        "venue": "arXiv.org",
        "year": 2014,
        "referenceCount": 37,
        "citationCount": 2499,
        "influentialCitationCount": 335,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper shows how to correctly apply dropout to LSTMs, and shows that it substantially reduces overfitting on a variety of tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-09-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1409.2329"
        },
        "citationStyles": {
            "bibtex": "@Article{Zaremba2014RecurrentNN,\n author = {Wojciech Zaremba and I. Sutskever and O. Vinyals},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Recurrent Neural Network Regularization},\n volume = {abs/1409.2329},\n year = {2014}\n}\n"
        }
    },
    "218_shufflenet_v2": {
        "paperId": "c02b909a514af6b9255315e2d50112845ca5ed0e",
        "externalIds": {
            "MAG": "2883780447",
            "DBLP": "conf/eccv/MaZZS18",
            "ArXiv": "1807.11164",
            "DOI": "10.1007/978-3-030-01264-9_8",
            "CorpusId": 51880435
        },
        "corpusId": 51880435,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/c02b909a514af6b9255315e2d50112845ca5ed0e",
        "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "referenceCount": 44,
        "citationCount": 3693,
        "influentialCitationCount": 468,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1807.11164",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs, and derives several practical guidelines for efficient network design, called ShuffleNet V2."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-07-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1807.11164"
        },
        "citationStyles": {
            "bibtex": "@Article{Ma2018ShuffleNetVP,\n author = {Ningning Ma and Xiangyu Zhang and Haitao Zheng and Jian Sun},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design},\n volume = {abs/1807.11164},\n year = {2018}\n}\n"
        }
    },
    "219_cross-lingual_alignment": {
        "paperId": "f1d791b9dd32577609ddd48e6001e46f1780062c",
        "externalIds": {
            "MAG": "2949188476",
            "DBLP": "journals/corr/abs-1902-09492",
            "ACL": "N19-1162",
            "ArXiv": "1902.09492",
            "DOI": "10.18653/v1/N19-1162",
            "CorpusId": 67856005
        },
        "corpusId": 67856005,
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "url": "https://www.semanticscholar.org/paper/f1d791b9dd32577609ddd48e6001e46f1780062c",
        "title": "Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing",
        "abstract": "We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts, aligning them poses a challenge due to their dynamic nature. To this end, we construct context-independent variants of the original monolingual spaces and utilize their mapping to derive an alignment for the context-dependent spaces. This mapping readily supports processing of a target language, improving transfer by context-aware embeddings. Our experimental results demonstrate the effectiveness of this approach for zero-shot and few-shot learning of dependency parsing. Specifically, our method consistently outperforms the previous state-of-the-art on 6 tested languages, yielding an improvement of 6.8 LAS points on average.",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "referenceCount": 65,
        "citationCount": 190,
        "influentialCitationCount": 22,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1902.09492",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion, that consistently outperforms the previous state-of-the-art on 6 tested languages, yielding an improvement of 6.8 LAS points on average."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-02-25",
        "journal": {
            "pages": "1599-1613"
        },
        "citationStyles": {
            "bibtex": "@Article{Schuster2019CrossLingualAO,\n author = {Tal Schuster and Ori Ram and R. Barzilay and A. Globerson},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {1599-1613},\n title = {Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing},\n year = {2019}\n}\n"
        }
    },
    "220_xuanyuan_2.0": {
        "paperId": "6783b17fe4328f48403f57009a73f784de09f645",
        "externalIds": {
            "DBLP": "journals/corr/abs-2305-12002",
            "ArXiv": "2305.12002",
            "DOI": "10.1145/3583780.3615285",
            "CorpusId": 258833440
        },
        "corpusId": 258833440,
        "publicationVenue": {
            "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
            "name": "International Conference on Information and Knowledge Management",
            "type": "conference",
            "alternate_names": [
                "Conference on Information and Knowledge Management",
                "Conf Inf Knowl Manag",
                "Int Conf Inf Knowl Manag",
                "CIKM"
            ],
            "url": "http://www.cikm.org/"
        },
        "url": "https://www.semanticscholar.org/paper/6783b17fe4328f48403f57009a73f784de09f645",
        "title": "XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters",
        "abstract": "Recently, with the popularity of ChatGPT, large-scale language models have experienced rapid development. However, there is a scarcity of open-sourced chat models specifically designed for the Chinese language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese chat model to date, built upon the BLOOM-176B architecture. Additionally, we propose a novel training method called hybrid-tuning to mitigate catastrophic forgetting. By integrating general and domain-specific knowledge, as well as combining the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing accurate and contextually appropriate responses in the Chinese financial domain.",
        "venue": "International Conference on Information and Knowledge Management",
        "year": 2023,
        "referenceCount": 31,
        "citationCount": 24,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.12002",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Business",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces XuanYuan 2.0, the largest Chinese chat model to date, built upon the BLOOM-176B architecture, and proposes a novel training method called hybrid-tuning to mitigate catastrophic forgetting."
        },
        "publicationTypes": [
            "JournalArticle",
            "Book",
            "Conference"
        ],
        "publicationDate": "2023-05-19",
        "journal": {
            "name": "Proceedings of the 32nd ACM International Conference on Information and Knowledge Management"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2023XuanYuan2A,\n author = {Xuanyu Zhang and Qing Yang and Dongliang Xu},\n booktitle = {International Conference on Information and Knowledge Management},\n journal = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},\n title = {XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters},\n year = {2023}\n}\n"
        }
    },
    "221_xgen-7b": {
        "paperId": "d9d84f8b9e6bf4839f43ab865158d79a3b9bb1e9",
        "externalIds": {
            "DBLP": "journals/corr/abs-2309-03450",
            "ArXiv": "2309.03450",
            "DOI": "10.48550/arXiv.2309.03450",
            "CorpusId": 261582398
        },
        "corpusId": 261582398,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/d9d84f8b9e6bf4839f43ab865158d79a3b9bb1e9",
        "title": "XGen-7B Technical Report",
        "abstract": "Large Language Models (LLMs) have become ubiquitous across various domains, transforming the way we interact with information and conduct research. However, most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context. To address this, we have trained XGen, a series of 7B parameter models on up to 8K sequence length for up to 1.5T tokens. We have also finetuned the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-Inst). We open-source our models for both research advancements and commercial applications. Our evaluation on standard benchmarks shows that XGen models achieve comparable or better results when compared with state-of-the-art open-source LLMs. Our targeted evaluation on long sequence modeling tasks shows the benefits of our 8K-sequence models over 2K-sequence open-source LLMs.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 44,
        "citationCount": 5,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.03450",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work has trained XGen, a series of 7B parameter models on up to 8K sequence length for up to 1.5T tokens, and finetuned the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-Inst)."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-09-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2309.03450"
        },
        "citationStyles": {
            "bibtex": "@Article{Nijkamp2023XGen7BTR,\n author = {Erik Nijkamp and Tian Xie and Hiroaki Hayashi and Bo Pang and Congying Xia and Chen Xing and Jesse Vig and Semih Yavuz and Philippe Laban and Ben Krause and Senthil Purushwalkam and Tong Niu and Wojciech Kry'sci'nski and Lidiya Murakhovs'ka and Prafulla Kumar Choubey and A. R. Fabbri and Ye Liu and Rui Meng and Lifu Tu and Meghana Moorthy Bhat and Chien-Sheng Wu and Silvio Savarese and Yingbo Zhou and Shafiq R. Joty and Caiming Xiong},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {XGen-7B Technical Report},\n volume = {abs/2309.03450},\n year = {2023}\n}\n"
        }
    },
    "223_youtube_recommendation_model": {
        "paperId": "5e383584ccbc8b920eaf3cfce3869da646ff5550",
        "externalIds": {
            "MAG": "2512971201",
            "DBLP": "conf/recsys/CovingtonAS16",
            "DOI": "10.1145/2959100.2959190",
            "CorpusId": 207240067
        },
        "corpusId": 207240067,
        "publicationVenue": {
            "id": "61275a16-1e0d-479f-ac4e-f295310761f0",
            "name": "ACM Conference on Recommender Systems",
            "type": "conference",
            "alternate_names": [
                "Conf Recomm Syst",
                "RecSys",
                "ACM Conf Recomm Syst",
                "Conference on Recommender Systems"
            ],
            "url": "http://recsys.acm.org/"
        },
        "url": "https://www.semanticscholar.org/paper/5e383584ccbc8b920eaf3cfce3869da646ff5550",
        "title": "Deep Neural Networks for YouTube Recommendations",
        "abstract": "YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.",
        "venue": "ACM Conference on Recommender Systems",
        "year": 2016,
        "referenceCount": 27,
        "citationCount": 2569,
        "influentialCitationCount": 220,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://dl.acm.org/ft_gateway.cfm?id=2959190&type=pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper details a deep candidate generation model and then describes a separate deep ranking model and provides practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact."
        },
        "publicationTypes": [
            "Book",
            "JournalArticle"
        ],
        "publicationDate": "2016-09-07",
        "journal": {
            "name": "Proceedings of the 10th ACM Conference on Recommender Systems"
        },
        "citationStyles": {
            "bibtex": "@Book{Covington2016DeepNN,\n author = {Paul Covington and Jay K. Adams and Emre Sargin},\n booktitle = {ACM Conference on Recommender Systems},\n journal = {Proceedings of the 10th ACM Conference on Recommender Systems},\n title = {Deep Neural Networks for YouTube Recommendations},\n year = {2016}\n}\n"
        }
    },
    "226_simcse": {
        "paperId": "c26759e6c701201af2f62f7ee4eb68742b5bf085",
        "externalIds": {
            "ArXiv": "2104.08821",
            "DBLP": "conf/emnlp/GaoYC21",
            "ACL": "2021.emnlp-main.552",
            "DOI": "10.18653/v1/2021.emnlp-main.552",
            "CorpusId": 233296292
        },
        "corpusId": 233296292,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/c26759e6c701201af2f62f7ee4eb68742b5bf085",
        "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "abstract": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \u201centailment\u201d pairs as positives and \u201ccontradiction\u201d pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman\u2019s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show\u2014both theoretically and empirically\u2014that contrastive learning objective regularizes pre-trained embeddings\u2019 anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "referenceCount": 68,
        "citationCount": 1973,
        "influentialCitationCount": 459,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.552.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "SimCSE is presented, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings and regularizes pre-trainedembeddings\u2019 anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-04-18",
        "journal": {
            "pages": "6894-6910"
        },
        "citationStyles": {
            "bibtex": "@Article{Gao2021SimCSESC,\n author = {Tianyu Gao and Xingcheng Yao and Danqi Chen},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {6894-6910},\n title = {SimCSE: Simple Contrastive Learning of Sentence Embeddings},\n year = {2021}\n}\n"
        }
    },
    "229_gpt2-large+lhopt": {
        "paperId": "c0f77574d63a1d608009ad096e8f0fc791eb1297",
        "externalIds": {
            "ArXiv": "2106.00958",
            "DBLP": "journals/corr/abs-2106-00958",
            "CorpusId": 235293742
        },
        "corpusId": 235293742,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/c0f77574d63a1d608009ad096e8f0fc791eb1297",
        "title": "A Generalizable Approach to Learning Optimizers",
        "abstract": "A core issue with learning to optimize neural networks has been the lack of generalization to real world problems. To address this, we describe a system designed from a generalization-first perspective, learning to update optimizer hyperparameters instead of model parameters directly using novel features, actions, and a reward function. This system outperforms Adam at all neural network tasks including on modalities not seen during training. We achieve 2x speedups on ImageNet, and a 2.5x speedup on a language modeling task using over 5 orders of magnitude more compute than the training tasks.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 56,
        "citationCount": 21,
        "influentialCitationCount": 5,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work describes a system designed from a generalization-first perspective, learning to update optimizer hyperparameters instead of model parameters directly using novel features, actions, and a reward function that outperforms Adam at all neural network tasks including on modalities not seen during training."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-06-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2106.00958"
        },
        "citationStyles": {
            "bibtex": "@Article{Almeida2021AGA,\n author = {Diogo Almeida and Clemens Winter and Jie Tang and Wojciech Zaremba},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Generalizable Approach to Learning Optimizers},\n volume = {abs/2106.00958},\n year = {2021}\n}\n"
        }
    },
    "232_char-cnn-bilstm": {
        "paperId": "c43009717ca0ae53f2cf1766237fd58f7c5b52ab",
        "externalIds": {
            "DBLP": "journals/corr/abs-1906-05678",
            "ArXiv": "1906.05678",
            "MAG": "2950658695",
            "CorpusId": 189762462
        },
        "corpusId": 189762462,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/c43009717ca0ae53f2cf1766237fd58f7c5b52ab",
        "title": "Telephonetic: Making Neural Language Models Robust to ASR and Semantic Noise",
        "abstract": "Speech processing systems rely on robust feature extraction to handle phonetic and semantic variations found in natural language. While techniques exist for desensitizing features to common noise patterns produced by Speech-to-Text (STT) and Text-to-Speech (TTS) systems, the question remains how to best leverage state-of-the-art language models (which capture rich semantic features, but are trained on only written text) on inputs with ASR errors. In this paper, we present Telephonetic, a data augmentation framework that helps robustify language model features to ASR corrupted inputs. To capture phonetic alterations, we employ a character-level language model trained using probabilistic masking. Phonetic augmentations are generated in two stages: a TTS encoder (Tacotron 2, WaveGlow) and a STT decoder (DeepSpeech). Similarly, semantic perturbations are produced by sampling from nearby words in an embedding space, which is computed using the BERT language model. Words are selected for augmentation according to a hierarchical grammar sampling strategy. Telephonetic is evaluated on the Penn Treebank (PTB) corpus, and demonstrates its effectiveness as a bootstrapping technique for transferring neural language models to the speech domain. Notably, our language model achieves a test perplexity of 37.49 on PTB, which to our knowledge is state-of-the-art among models trained only on PTB.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 11,
        "citationCount": 2,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Telephonetic, a data augmentation framework that helps robustify language model features to ASR corrupted inputs, is presented and demonstrates its effectiveness as a bootstrapping technique for transferring neural language models to the speech domain."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-06-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1906.05678"
        },
        "citationStyles": {
            "bibtex": "@Article{Larson2019TelephoneticMN,\n author = {Christopher Larson and Tarek Lahlou and Diana Mingels and Zachary Kulis and Erik T. Mueller},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Telephonetic: Making Neural Language Models Robust to ASR and Semantic Noise},\n volume = {abs/1906.05678},\n year = {2019}\n}\n"
        }
    },
    "233_bellkor_2009": {
        "paperId": "8b52e5e421a531d8c6f272ce1063610f52f11411",
        "externalIds": {
            "MAG": "40442397",
            "CorpusId": 6114578
        },
        "corpusId": 6114578,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/8b52e5e421a531d8c6f272ce1063610f52f11411",
        "title": "The BellKor Solution to the Netflix Grand Prize",
        "abstract": "This article describes part of our contribution to the \u201cBellKor\u2019s Pragmatic Chaos\u201d final solution, which won the Netflix Grand Prize. The other portion of the contribution was created while working at AT&T with Robert Bell and Chris Volinsky, as reported in our 2008 Progress Prize report [3]. The final solution includes all the predictors described there. In this article we describe only the newer predictors. So what is new over last year\u2019s solution? First we further improved the baseline predictors (Sec. III). This in turn improves our other models, which incorporate those predictors, like the matrix factorization model (Sec. IV). In addition, an extension of the neighborhood model that addresses temporal dynamics was introduced (Sec. V). On the Restricted Boltzmann Machines (RBM) front, we use a new RBM model with superior accuracy by conditioning the visible units (Sec. VI). The final addition is the introduction of a new blending algorithm, which is based on gradient boosted decision trees (GBDT) (Sec. VII).",
        "venue": "",
        "year": 2009,
        "referenceCount": 16,
        "citationCount": 509,
        "influentialCitationCount": 37,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Part of the contribution to the \u201cBellKor\u2019s Pragmatic Chaos\u201d final solution, which won the Netflix Grand Prize, is described, which improved the baseline predictors and introduced a new blending algorithm based on gradient boosted decision trees."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Koren2009TheBS,\n author = {Y. Koren},\n title = {The BellKor Solution to the Netflix Grand Prize},\n year = {2009}\n}\n"
        }
    },
    "234_proberta": {
        "paperId": "603f52d670aff69e0cad2d4465f007dea1a494a0",
        "externalIds": {
            "DBLP": "conf/bcb/NambiarHLMHR20",
            "MAG": "3101509328",
            "DOI": "10.1145/3388440.3412467",
            "CorpusId": 219946712
        },
        "corpusId": 219946712,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/603f52d670aff69e0cad2d4465f007dea1a494a0",
        "title": "Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks",
        "abstract": "The scientific community is rapidly generating protein sequence information, but only a fraction of these proteins can be experimentally characterized. While promising deep learning approaches for protein prediction tasks have emerged, they have computational limitations or are designed to solve a specific task. We present a Transformer neural network that pre-trains task-agnostic sequence representations. This model is fine-tuned to solve two different protein prediction tasks: protein family classification and protein interaction prediction. Our method is comparable to existing state-of-the art approaches for protein family classification, while being much more general than other architectures. Further, our method outperforms all other approaches for protein interaction prediction. These results offer a promising framework for fine-tuning the pre-trained sequence representations for other protein prediction tasks.",
        "venue": "bioRxiv",
        "year": 2020,
        "referenceCount": 65,
        "citationCount": 96,
        "influentialCitationCount": 4,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.1145/3388440.3412467",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A Transformer neural network that pre-trains task-agnostic sequence representations for protein prediction tasks, comparable to existing state-of-the art approaches for protein family classification and much more general than other architectures."
        },
        "publicationTypes": [
            "JournalArticle",
            "Book"
        ],
        "publicationDate": "2020-06-16",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Nambiar2020TransformingTL,\n author = {Ananthan Nambiar and Simon Liu and Mark Hopkins and Maeve Heflin and S. Maslov and Anna M. Ritz},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks},\n year = {2020}\n}\n"
        }
    },
    "235_refined_part_pooling": {
        "paperId": "753d2a35c9edf5dfcac4ef3a6adc993b657b01f0",
        "externalIds": {
            "DBLP": "conf/eccv/SunZYTW18",
            "MAG": "2783855081",
            "ArXiv": "1711.09349",
            "DOI": "10.1007/978-3-030-01225-0_30",
            "CorpusId": 10013306
        },
        "corpusId": 10013306,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/753d2a35c9edf5dfcac4ef3a6adc993b657b01f0",
        "title": "Beyond Part Models: Person Retrieval with Refined Part Pooling",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2017,
        "referenceCount": 51,
        "citationCount": 1869,
        "influentialCitationCount": 425,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1711.09349",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper targets at learning discriminative part-informed features for person retrieval and lays emphasis on the content consistency within each part, resulting in refined parts with enhanced within-part consistency."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-11-26",
        "journal": {
            "pages": "501-518"
        },
        "citationStyles": {
            "bibtex": "@Article{Sun2017BeyondPM,\n author = {Yifan Sun and Liang Zheng and Yi Yang and Q. Tian and Shengjin Wang},\n booktitle = {European Conference on Computer Vision},\n pages = {501-518},\n title = {Beyond Part Models: Person Retrieval with Refined Part Pooling},\n year = {2017}\n}\n"
        }
    },
    "237_resnet-50_billion-scale": {
        "paperId": "88ee291cf1f57fd0f4914a80b986a08a90d887f1",
        "externalIds": {
            "ArXiv": "1905.00546",
            "DBLP": "journals/corr/abs-1905-00546",
            "MAG": "2943152387",
            "CorpusId": 143422950
        },
        "corpusId": 143422950,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/88ee291cf1f57fd0f4914a80b986a08a90d887f1",
        "title": "Billion-scale semi-supervised learning for image classification",
        "abstract": "This paper presents a study of semi-supervised learning with large convolutional networks. We propose a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images (up to 1 billion). Our main goal is to improve the performance for a given target architecture, like ResNet-50 or ResNext. We provide an extensive analysis of the success factors of our approach, which leads us to formulate some recommendations to produce high-accuracy models for image classification with semi-supervised learning. As a result, our approach brings important gains to standard architectures for image, video and fine-grained classification. For instance, by leveraging one billion unlabelled images, our learned vanilla ResNet-50 achieves 81.2% top-1 accuracy on the ImageNet benchmark.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 44,
        "citationCount": 403,
        "influentialCitationCount": 23,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images to improve the performance for a given target architecture, like ResNet-50 or ResNext."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-05-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1905.00546"
        },
        "citationStyles": {
            "bibtex": "@Article{Yalniz2019BillionscaleSL,\n author = {I. Z. Yalniz and H. J\u00e9gou and Kan Chen and Manohar Paluri and D. Mahajan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Billion-scale semi-supervised learning for image classification},\n volume = {abs/1905.00546},\n year = {2019}\n}\n"
        }
    },
    "238_awd-lstm-mos_+_dynamic_evaluation_(wt2,_2017)": {
        "paperId": "ef9ddbc35676ce8ffc2a8067044473727839dbac",
        "externalIds": {
            "MAG": "2949570458",
            "ArXiv": "1711.03953",
            "DBLP": "conf/iclr/YangDSC18",
            "CorpusId": 26238954
        },
        "corpusId": 26238954,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/ef9ddbc35676ce8ffc2a8067044473727839dbac",
        "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
        "abstract": "We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "referenceCount": 44,
        "citationCount": 322,
        "influentialCitationCount": 61,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck, and a simple and effective method is proposed to address this issue."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-11-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1711.03953"
        },
        "citationStyles": {
            "bibtex": "@Article{Yang2017BreakingTS,\n author = {Zhilin Yang and Zihang Dai and R. Salakhutdinov and William W. Cohen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n volume = {abs/1711.03953},\n year = {2017}\n}\n"
        }
    },
    "239_whisper": {
        "paperId": "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec",
        "externalIds": {
            "DBLP": "journals/corr/abs-2212-04356",
            "ArXiv": "2212.04356",
            "DOI": "10.48550/arXiv.2212.04356",
            "CorpusId": 252923993
        },
        "corpusId": 252923993,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/a02fbaf22237a1aedacb1320b6007cd70c1fe6ec",
        "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
        "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "referenceCount": 100,
        "citationCount": 1025,
        "influentialCitationCount": 183,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2212.04356",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Engineering",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-12-06",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2212.04356"
        },
        "citationStyles": {
            "bibtex": "@Article{Radford2022RobustSR,\n author = {Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and C. McLeavey and I. Sutskever},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n volume = {abs/2212.04356},\n year = {2022}\n}\n"
        }
    },
    "240_bluumi": {
        "paperId": "1a849f298dcd49fe99ebca6749c0564585aa3018",
        "externalIds": {
            "DBLP": "conf/emnlp/LuukkonenKLEKKG23",
            "ArXiv": "2311.05640",
            "DOI": "10.48550/arXiv.2311.05640",
            "CorpusId": 265128939
        },
        "corpusId": 265128939,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/1a849f298dcd49fe99ebca6749c0564585aa3018",
        "title": "FinGPT: Large Generative Models for a Small Language",
        "abstract": "Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at https://turkunlp.org/gpt3-finnish.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2023,
        "referenceCount": 45,
        "citationCount": 3,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population, and trains seven monolingual models from scratch, resulting in a 176 billion parameter model the authors call BLUUMI."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-11-03",
        "journal": {
            "pages": "2710-2726"
        },
        "citationStyles": {
            "bibtex": "@Article{Luukkonen2023FinGPTLG,\n author = {Risto Luukkonen and Ville Komulainen and Jouni Luoma and Anni Eskelinen and Jenna Kanerva and Hanna-Mari Kupari and Filip Ginter and Veronika Laippala and Niklas Muennighoff and Aleksandra Piktus and Thomas Wang and Nouamane Tazi and Teven Le Scao and Thomas Wolf and Osma Suominen and Samuli Sairanen and Mikko Merioksa and Jyrki Heinonen and Aija Vahtola and Samuel Antao and Sampo Pyysalo},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {2710-2726},\n title = {FinGPT: Large Generative Models for a Small Language},\n year = {2023}\n}\n"
        }
    },
    "241_libratus": {
        "paperId": "0dcc5e613f2adff3f1eade232cd0488a4cde50ca",
        "externalIds": {
            "DBLP": "conf/ijcai/BrownS17",
            "MAG": "2741375793",
            "DOI": "10.24963/ijcai.2017/772",
            "CorpusId": 30765578
        },
        "corpusId": 30765578,
        "publicationVenue": {
            "id": "67f7f831-711a-43c8-8785-1e09005359b5",
            "name": "International Joint Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "Int Jt Conf Artif Intell",
                "IJCAI"
            ],
            "url": "http://www.ijcai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/0dcc5e613f2adff3f1eade232cd0488a4cde50ca",
        "title": "Libratus: The Superhuman AI for No-Limit Poker",
        "abstract": "No-limit Texas Hold'em is the most popular variant of poker in the world. Heads-up no-limit Texas Hold'em is the main benchmark challenge for AI in imperfect-information games. We present Libratus, the first--and so far only--AI to defeat top human professionals in that game. Libratus's architecture features three main modules, each of which has new algorithms: pre-computing a solution to an abstraction of the game which provides a high-level blueprint for the strategy of the AI, a new nested subgame-solving algorithm which repeatedly calculates a more detailed strategy as play progresses, and a self-improving module which augments the pre-computed blueprint over time.",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2017,
        "referenceCount": 28,
        "citationCount": 89,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2017/0772.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents Libratus, the first--and so far only--AI to defeat top human professionals in Heads-up no-limit Texas Hold'em."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-08-19",
        "journal": {
            "pages": "5226-5228"
        },
        "citationStyles": {
            "bibtex": "@Article{Brown2017LibratusTS,\n author = {Noam Brown and T. Sandholm},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {5226-5228},\n title = {Libratus: The Superhuman AI for No-Limit Poker},\n year = {2017}\n}\n"
        }
    },
    "243_low-cost_collaborative_network": {
        "paperId": "e3196bc12bcbf2eb51658f78e844517fb9a47b5d",
        "externalIds": {
            "MAG": "2952838799",
            "DBLP": "journals/corr/DongHYY17",
            "ArXiv": "1703.08651",
            "DOI": "10.1109/cvpr.2017.205",
            "CorpusId": 15541908
        },
        "corpusId": 15541908,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e3196bc12bcbf2eb51658f78e844517fb9a47b5d",
        "title": "More is Less: A More Complicated Network with Less Inference Complexity",
        "abstract": "In this paper, we present a novel and general network structure towards accelerating the inference process of convolutional neural networks, which is more complicated in network structure yet with less inference complexity. The core idea is to equip each original convolutional layer with another low-cost collaborative layer (LCCL), and the element-wise multiplication of the ReLU outputs of these two parallel layers produces the layer-wise output. The combined layer is potentially more discriminative than the original convolutional layer, and its inference is faster for two reasons: 1) the zero cells of the LCCL feature maps will remain zero after element-wise multiplication, and thus it is safe to skip the calculation of the corresponding high-cost convolution in the original convolutional layer, 2) LCCL is very fast if it is implemented as a 1*1 convolution or only a single filter shared by all channels. Extensive experiments on the CIFAR-10, CIFAR-100 and ILSCRC-2012 benchmarks show that our proposed network structure can accelerate the inference process by 32% on average with negligible performance drop.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "referenceCount": 41,
        "citationCount": 264,
        "influentialCitationCount": 43,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1703.08651",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel and general network structure towards accelerating the inference process of convolutional neural networks, which is more complicated in network structure yet with less inference complexity."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-03-25",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "1895-1903"
        },
        "citationStyles": {
            "bibtex": "@Article{Dong2017MoreIL,\n author = {Xuanyi Dong and Junshi Huang and Yi Yang and Shuicheng Yan},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1895-1903},\n title = {More is Less: A More Complicated Network with Less Inference Complexity},\n year = {2017}\n}\n"
        }
    },
    "244_fuzzy_nn": {
        "paperId": "092a6d908918703b94ed240557d67d6495f03195",
        "externalIds": {
            "MAG": "2095727900",
            "DBLP": "journals/tnn/PalM92",
            "DOI": "10.1109/72.159058",
            "CorpusId": 20199390,
            "PubMed": "18276468"
        },
        "corpusId": 20199390,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/092a6d908918703b94ed240557d67d6495f03195",
        "title": "Multilayer perceptron, fuzzy sets, and classification",
        "abstract": "A fuzzy neural network model based on the multilayer perceptron, using the backpropagation algorithm, and capable of fuzzy classification of patterns is described. The input vector consists of membership values to linguistic properties while the output vector is defined in terms of fuzzy class membership values. This allows efficient modeling of fuzzy uncertain patterns with appropriate weights being assigned to the backpropagated errors depending upon the membership values at the corresponding outputs. During training, the learning rate is gradually decreased in discrete steps until the network converges to a minimum error solution. The effectiveness of the algorithm is demonstrated on a speech recognition problem. The results are compared with those of the conventional MLP, the Bayes classifier, and other related models.",
        "venue": "IEEE Trans. Neural Networks",
        "year": 1992,
        "referenceCount": 30,
        "citationCount": 1076,
        "influentialCitationCount": 78,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A fuzzy neural network model based on the multilayer perceptron, using the backpropagation algorithm, and capable of fuzzy classification of patterns is described, and the results are compared with those of the conventional MLP, the Bayes classifier, and other related models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1992-09-01",
        "journal": {
            "name": "IEEE transactions on neural networks",
            "pages": "\n          683-97\n        ",
            "volume": "3 5"
        },
        "citationStyles": {
            "bibtex": "@Article{Pal1992MultilayerPF,\n author = {S. Pal and S. Mitra},\n booktitle = {IEEE Trans. Neural Networks},\n journal = {IEEE transactions on neural networks},\n pages = {\n          683-97\n        },\n title = {Multilayer perceptron, fuzzy sets, and classification},\n volume = {3 5},\n year = {1992}\n}\n"
        }
    },
    "246_micronet_(adaptive,_cache)": {
        "paperId": "66ceae37af0e7ace4755a5e8f41162d0f6cf7677",
        "externalIds": {
            "DBLP": "journals/corr/abs-2005-07877",
            "MAG": "3025572566",
            "ArXiv": "2005.07877",
            "CorpusId": 218674536
        },
        "corpusId": 218674536,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/66ceae37af0e7ace4755a5e8f41162d0f6cf7677",
        "title": "MicroNet for Efficient Language Modeling",
        "abstract": "It is important to design compact language models for efficient deployment. We improve upon recent advances in both the language modeling domain and the model-compression domain to construct parameter and computation efficient language models. We use an efficient transformer-based architecture with adaptive embedding and softmax, differentiable non-parametric cache, Hebbian softmax, knowledge distillation, network pruning, and low-bit quantization. In this paper, we provide the winning solution to the NeurIPS 2019 MicroNet Challenge in the language modeling track. Compared to the baseline language model provided by the MicroNet Challenge, our model is 90 times more parameter-efficient and 36 times more computation-efficient while achieving the required test perplexity of 35 on the Wikitext-103 dataset. We hope that this work will aid future research into efficient language models, and we have released our full source code at this https URL.",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "referenceCount": 36,
        "citationCount": 8,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work improves upon recent advances in both the language modeling domain and the model-compression domain to construct parameter and computation efficient language models that are 90 times more parameter-efficient and 36 times more computation-efficient while achieving the required test perplexity of 35 on the Wikitext-103 dataset."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-05-16",
        "journal": {
            "pages": "215-231"
        },
        "citationStyles": {
            "bibtex": "@Article{Yan2020MicroNetFE,\n author = {Zhongxia Yan and Hanrui Wang and Demi Guo and Song Han},\n booktitle = {Neural Information Processing Systems},\n pages = {215-231},\n title = {MicroNet for Efficient Language Modeling},\n year = {2020}\n}\n"
        }
    },
    "247_wgan-gp": {
        "paperId": "edf73ab12595c6709f646f542a0d2b33eb20a3f4",
        "externalIds": {
            "ArXiv": "1704.00028",
            "MAG": "2605135824",
            "DBLP": "conf/nips/GulrajaniAADC17",
            "CorpusId": 10894094
        },
        "corpusId": 10894094,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/edf73ab12595c6709f646f542a0d2b33eb20a3f4",
        "title": "Improved Training of Wasserstein GANs",
        "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "referenceCount": 37,
        "citationCount": 8114,
        "influentialCitationCount": 1398,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input, which performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-03-31",
        "journal": {
            "pages": "5767-5777"
        },
        "citationStyles": {
            "bibtex": "@Article{Gulrajani2017ImprovedTO,\n author = {Ishaan Gulrajani and Faruk Ahmed and Mart\u00edn Arjovsky and Vincent Dumoulin and Aaron C. Courville},\n booktitle = {Neural Information Processing Systems},\n pages = {5767-5777},\n title = {Improved Training of Wasserstein GANs},\n year = {2017}\n}\n"
        }
    },
    "250_awd-lstm-mos+pdr_+_dynamic_evaluation_(wt2)": {
        "paperId": "733a791abdcdf521eede813599c32059e82e3f3e",
        "externalIds": {
            "DBLP": "journals/corr/abs-1808-05908",
            "MAG": "2886076728",
            "ACL": "P19-1142",
            "ArXiv": "1808.05908",
            "DOI": "10.18653/v1/P19-1142",
            "CorpusId": 52039219
        },
        "corpusId": 52039219,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/733a791abdcdf521eede813599c32059e82e3f3e",
        "title": "Improved Language Modeling by Decoding the Past",
        "abstract": "Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method improves perplexity on the Penn Treebank dataset by up to 1.8 points and by up to 2.3 points on the WikiText-2 dataset, over strong regularized baselines using a single softmax. With a mixture-of-softmax model, we show gains of up to 1.0 perplexity points on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "referenceCount": 36,
        "citationCount": 6,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1142.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a new regularization method based on decoding the last token in the context using the predicted distribution of the next token, which biases the model towards retaining more contextual information, in turn improving its ability to predict the nexttoken."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-08-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1808.05908"
        },
        "citationStyles": {
            "bibtex": "@Article{Brahma2018ImprovedLM,\n author = {Siddhartha Brahma},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Improved Language Modeling by Decoding the Past},\n volume = {abs/1808.05908},\n year = {2018}\n}\n"
        }
    },
    "251_characterizing_verbatim_short-term_memory_in_neural_language_models_(182m)": {
        "paperId": "4583738cfaf1fd90f33de6cfb0f4bb2693f54fa6",
        "externalIds": {
            "ArXiv": "2210.13569",
            "DBLP": "conf/conll/ArmeniHL22",
            "ACL": "2022.conll-1.28",
            "DOI": "10.48550/arXiv.2210.13569",
            "CorpusId": 253107369
        },
        "corpusId": 253107369,
        "publicationVenue": {
            "id": "3779a5a7-9119-4f69-84fe-f7eef193eb49",
            "name": "Conference on Computational Natural Language Learning",
            "type": "conference",
            "alternate_names": [
                "CoNLL",
                "Conf Comput Nat Lang Learn"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/4583738cfaf1fd90f33de6cfb0f4bb2693f54fa6",
        "title": "Characterizing Verbatim Short-Term Memory in Neural Language Models",
        "abstract": "When a language model is trained to predict natural language sequences, its prediction at each moment depends on a representation of prior context. What kind of information about the prior context can language models retrieve? We tested whether language models could retrieve the exact words that occurred previously in a text. In our paradigm, language models (transformers and an LSTM) processed English text in which a list of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the first to the second list. We found that the transformers retrieved both the identity and ordering of nouns from the first list. Further, the transformers\u2019 retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth. Lastly, their ability to index prior tokens was dependent on learned attention patterns. In contrast, the LSTM exhibited less precise retrieval, which was limited to list-initial tokens and to short intervening texts. The LSTM\u2019s retrieval was not sensitive to the order of nouns and it improved when the list was semantically coherent. We conclude that transformers implemented something akin to a working memory system that could flexibly retrieve individual token representations across arbitrary delays; conversely, the LSTM maintained a coarser and more rapidly-decaying semantic gist of prior tokens, weighted toward the earliest items.",
        "venue": "Conference on Computational Natural Language Learning",
        "year": 2022,
        "referenceCount": 42,
        "citationCount": 3,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.13569",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "transformers implemented something akin to a working memory system that could flexibly retrieve individual token representations across arbitrary delays; conversely, the LSTM maintained a coarser and more rapidly-decaying semantic gist of prior tokens, weighted toward the earliest items."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-24",
        "journal": {
            "pages": "405-424"
        },
        "citationStyles": {
            "bibtex": "@Article{Armeni2022CharacterizingVS,\n author = {K. Armeni and C. Honey and Tal Linzen},\n booktitle = {Conference on Computational Natural Language Learning},\n pages = {405-424},\n title = {Characterizing Verbatim Short-Term Memory in Neural Language Models},\n year = {2022}\n}\n"
        }
    },
    "252_tcn_(p-mnist)": {
        "paperId": "3bb63fdb4670745f8c97d8cad1a8a9603b1c16f5",
        "externalIds": {
            "DBLP": "conf/iclr/BaiKK18",
            "MAG": "2786228682",
            "CorpusId": 3291855
        },
        "corpusId": 3291855,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/3bb63fdb4670745f8c97d8cad1a8a9603b1c16f5",
        "title": "Convolutional Sequence Modeling Revisited",
        "abstract": "Although both convolutional and recurrent architectures have a long history in sequence prediction, the current \u201cdefault\u201d mindset in much of the deep learning community is that generic sequence modeling is best handled using recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should a practitioner use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. In particular, the models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We further show that the potential \u201cinfinite memory\u201d advantage that RNNs have over TCNs is largely absent in practice: TCNs indeed exhibit longer effective history sizes than their recurrent counterparts. As a whole, we argue that it may be time to (re)consider ConvNets as the default \u201cgo to\u201d architecture for sequence modeling.",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 11,
        "citationCount": 55,
        "influentialCitationCount": 11,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is argued that it may be time to (re)consider ConvNets as the default \u201cgo to\u201d architecture for sequence modeling, and the potential \u201cinfinite memory\u201d advantage that RNNs have over TCNs is largely absent in practice."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-02-12",
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Bai2018ConvolutionalSM,\n author = {Shaojie Bai and J. Z. Kolter and V. Koltun},\n booktitle = {International Conference on Learning Representations},\n title = {Convolutional Sequence Modeling Revisited},\n year = {2018}\n}\n"
        }
    },
    "253_6-act_tether": {
        "paperId": "da200182f1e0660689bfda884736058dad1db16f",
        "externalIds": {
            "ArXiv": "2104.04112",
            "DBLP": "conf/iccv/YeBDW21",
            "DOI": "10.1109/ICCV48922.2021.01581",
            "CorpusId": 237362445
        },
        "corpusId": 237362445,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/da200182f1e0660689bfda884736058dad1db16f",
        "title": "Auxiliary Tasks and Exploration Enable ObjectGoal Navigation",
        "abstract": "ObjectGoal Navigation (ObjectNav) is an embodied task wherein agents are to navigate to an object instance in an unseen environment. Prior works have shown that end-to-end ObjectNav agents that use vanilla visual and recurrent modules, e.g. a CNN+RNN, perform poorly due to overfitting and sample inefficiency. This has motivated current state-of-the-art methods to mix analytic and learned components and operate on explicit spatial maps of the environment. We instead re-enable a generic learned agent by adding auxiliary learning tasks and an exploration reward. Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8% relative improvement over prior state-of-the-art, respectively, on the Habitat ObjectNav Challenge [35]. From our analysis, we propose that agents will act to simplify their visual inputs so as to smooth their RNN dynamics, and that auxiliary tasks reduce overfitting by minimizing effective RNN dimensionality; i.e. a performant ObjectNav agent that must maintain coherent plans over long horizons does so by learning smooth, low-dimensional recurrent dynamics. Site: joel99.github.io/objectnav/",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "referenceCount": 42,
        "citationCount": 52,
        "influentialCitationCount": 7,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes that agents will act to simplify their visual inputs so as to smooth their RNN dynamics, and that auxiliary tasks reduce overfitting by minimizing effective RNN dimensionality; i.e. a performant ObjectNav agent that must maintain coherent plans over long horizons does so by learning smooth, low-dimensional recurrent dynamics."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-04-08",
        "journal": {
            "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "pages": "16097-16106"
        },
        "citationStyles": {
            "bibtex": "@Article{Ye2021AuxiliaryTA,\n author = {Joel Ye and Dhruv Batra and Abhishek Das and Erik Wijmans},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {16097-16106},\n title = {Auxiliary Tasks and Exploration Enable ObjectGoal Navigation},\n year = {2021}\n}\n"
        }
    },
    "255_stack_rnn": {
        "paperId": "d38e8631bba0720becdaf7b89f79d9f9dca45d82",
        "externalIds": {
            "DBLP": "journals/corr/JoulinM15",
            "MAG": "1732222442",
            "ArXiv": "1503.01007",
            "CorpusId": 172783
        },
        "corpusId": 172783,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/d38e8631bba0720becdaf7b89f79d9f9dca45d82",
        "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets",
        "abstract": "Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "referenceCount": 42,
        "citationCount": 388,
        "influentialCitationCount": 34,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The limitations of standard deep learning approaches are discussed and it is shown that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-03-03",
        "journal": {
            "pages": "190-198"
        },
        "citationStyles": {
            "bibtex": "@Article{Joulin2015InferringAP,\n author = {Armand Joulin and Tomas Mikolov},\n booktitle = {Neural Information Processing Systems},\n pages = {190-198},\n title = {Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},\n year = {2015}\n}\n"
        }
    },
    "256_image_classification_with_the_fisher_vector:_theory_and_practice": {
        "paperId": "b90d880025b6655016851d93e80e80aaaa885ca0",
        "externalIds": {
            "DBLP": "journals/ijcv/SanchezPMV13",
            "MAG": "1966385142",
            "DOI": "10.1007/s11263-013-0636-x",
            "CorpusId": 207252215
        },
        "corpusId": 207252215,
        "publicationVenue": {
            "id": "939ee07c-6009-43f8-b884-69238b40659e",
            "name": "International Journal of Computer Vision",
            "type": "journal",
            "alternate_names": [
                "Int J Comput Vis"
            ],
            "issn": "0920-5691",
            "url": "https://www.springer.com/computer/image+processing/journal/11263",
            "alternate_urls": [
                "https://link.springer.com/journal/11263",
                "http://link.springer.com/journal/11263"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/b90d880025b6655016851d93e80e80aaaa885ca0",
        "title": "Image Classification with the Fisher Vector: Theory and Practice",
        "abstract": null,
        "venue": "International Journal of Computer Vision",
        "year": 2013,
        "referenceCount": 94,
        "citationCount": 1248,
        "influentialCitationCount": 167,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ri.conicet.gov.ar/bitstream/11336/12271/1/CONICET_Digital_Nro.12107.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes to use the Fisher Kernel framework as an alternative patch encoding strategy: it describes patches by their deviation from an \u201cuniversal\u201d generative Gaussian mixture model, and reports experimental results showing that the FV framework is a state-of-the-art patch encoding technique."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2013-06-12",
        "journal": {
            "name": "International Journal of Computer Vision",
            "pages": "222 - 245",
            "volume": "105"
        },
        "citationStyles": {
            "bibtex": "@Article{S\u00e1nchez2013ImageCW,\n author = {Jorge S\u00e1nchez and Florent Perronnin and Thomas Mensink and Jakob Verbeek},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {222 - 245},\n title = {Image Classification with the Fisher Vector: Theory and Practice},\n volume = {105},\n year = {2013}\n}\n"
        }
    },
    "257_boss_(darpa_urban_challenge)": {
        "paperId": "c0b125fb68761ed48f1c3a784e0142cbdf743bbd",
        "externalIds": {
            "MAG": "2406067508",
            "DBLP": "conf/darpa/UrmsonABBBCDDGGGHHHKKLMMPPRRSSSSSWWZBBDLNSZSTDF09",
            "DOI": "10.1007/978-3-642-03991-1_1",
            "CorpusId": 11849332
        },
        "corpusId": 11849332,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/c0b125fb68761ed48f1c3a784e0142cbdf743bbd",
        "title": "Autonomous driving in urban environments: Boss and the Urban Challenge",
        "abstract": null,
        "venue": "J. Field Robotics",
        "year": 2008,
        "referenceCount": 24,
        "citationCount": 1757,
        "influentialCitationCount": 70,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/rob.20255",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            },
            {
                "category": "Environmental Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Boss is an autonomous vehicle that uses on\u2010board sensors to track other vehicles, detect static obstacles, and localize itself relative to a road model using a spiral system development process with a heavy emphasis on regular, regressive system testing."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2008-08-01",
        "journal": {
            "name": "Journal of Field Robotics",
            "volume": "25"
        },
        "citationStyles": {
            "bibtex": "@Article{Urmson2008AutonomousDI,\n author = {C. Urmson and Joshua Anhalt and J. Bagnell and Christopher R. Baker and R. Bittner and M. N. Clark and J. Dolan and D. Duggins and Tugrul Galatali and Christopher Geyer and Michele Gittleman and Sam Harbaugh and M. Hebert and T. Howard and Sascha Kolski and A. Kelly and M. Likhachev and M. McNaughton and Nick Miller and K. Peterson and Brian Pilnick and R. Rajkumar and P. Rybski and B. Salesky and Young-Woo Seo and Sanjiv Singh and Jarrod M. Snider and A. Stentz and W. Whittaker and Ziv Wolkowicki and Jason Ziglar and Hong Bae and Thomas Brown and Daniel Demitrish and B. Litkouhi and J. Nickolaou and Varsha Sadekar and Wende Zhang and Joshua Struble and Michael Taylor and M. Darms and D. Ferguson},\n booktitle = {J. Field Robotics},\n journal = {Journal of Field Robotics},\n title = {Autonomous driving in urban environments: Boss and the Urban Challenge},\n volume = {25},\n year = {2008}\n}\n"
        }
    },
    "258_dd-ppo": {
        "paperId": "b0e95b881add810353b12e78615613a0132be754",
        "externalIds": {
            "MAG": "3004691725",
            "DBLP": "conf/iclr/WijmansKMLEPSB20",
            "ArXiv": "1911.00357",
            "CorpusId": 210839350
        },
        "corpusId": 210839350,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/b0e95b881add810353b12e78615613a0132be754",
        "title": "DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames",
        "abstract": "We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever \"stale\"), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. \n\nThis massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially \"solves\" the task -- near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of \"ImageNet pre-training + task-specific fine-tuning\" for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 49,
        "citationCount": 346,
        "influentialCitationCount": 66,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of \"ImageNet pre-training + task-specific fine-tuning\" for embodied AI."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-11-01",
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Wijmans2019DDPPOLN,\n author = {Erik Wijmans and Abhishek Kadian and Ari S. Morcos and Stefan Lee and Irfan Essa and Devi Parikh and M. Savva and Dhruv Batra},\n booktitle = {International Conference on Learning Representations},\n title = {DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames},\n year = {2019}\n}\n"
        }
    },
    "259_codet5-large": {
        "paperId": "6d994b4f5a46cd14e8f09f1e9e49120546b15e31",
        "externalIds": {
            "DBLP": "journals/corr/abs-2207-01780",
            "ArXiv": "2207.01780",
            "DOI": "10.48550/arXiv.2207.01780",
            "CorpusId": 250280117
        },
        "corpusId": 250280117,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/6d994b4f5a46cd14e8f09f1e9e49120546b15e31",
        "title": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning",
        "abstract": "Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose\"CodeRL\", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "referenceCount": 91,
        "citationCount": 114,
        "influentialCitationCount": 19,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2207.01780",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes \"CodeRL\", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL), which treats the code-generating LM as an actor network, and introduces a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-07-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2207.01780"
        },
        "citationStyles": {
            "bibtex": "@Article{Le2022CodeRLMC,\n author = {Hung Le and Yue Wang and Akhilesh Deepak Gotmare and S. Savarese and S. Hoi},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning},\n volume = {abs/2207.01780},\n year = {2022}\n}\n"
        }
    },
    "260_walking_minotaur_robot": {
        "paperId": "2ed619fbc7902155d54f6f21da16ad6c120eac63",
        "externalIds": {
            "MAG": "2963339188",
            "DBLP": "journals/corr/abs-1812-11103",
            "ArXiv": "1812.11103",
            "DOI": "10.15607/RSS.2019.XV.011",
            "CorpusId": 57189150
        },
        "corpusId": 57189150,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/2ed619fbc7902155d54f6f21da16ad6c120eac63",
        "title": "Learning to Walk via Deep Reinforcement Learning",
        "abstract": "Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.",
        "venue": "Robotics: Science and Systems",
        "year": 2018,
        "referenceCount": 62,
        "citationCount": 367,
        "influentialCitationCount": 22,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://doi.org/10.15607/rss.2019.xv.011",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies is proposed and achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-12-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1812.11103"
        },
        "citationStyles": {
            "bibtex": "@Article{Haarnoja2018LearningTW,\n author = {Tuomas Haarnoja and Aurick Zhou and Sehoon Ha and Jie Tan and G. Tucker and S. Levine},\n booktitle = {Robotics: Science and Systems},\n journal = {ArXiv},\n title = {Learning to Walk via Deep Reinforcement Learning},\n volume = {abs/1812.11103},\n year = {2018}\n}\n"
        }
    },
    "261_statement_curriculum_learning": {
        "paperId": "916a06a6d51aa93de27aac2f3e14faed08dd6706",
        "externalIds": {
            "ArXiv": "2202.01344",
            "DBLP": "journals/corr/abs-2202-01344",
            "CorpusId": 246485695
        },
        "corpusId": 246485695,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/916a06a6d51aa93de27aac2f3e14faed08dd6706",
        "title": "Formal Mathematics Statement Curriculum Learning",
        "abstract": "We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 46,
        "citationCount": 57,
        "influentialCitationCount": 13,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that at same compute budget, expert iteration, by which the authors mean proof search interleaved with learning, dramatically outperforms proof search only and is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-02-03",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2202.01344"
        },
        "citationStyles": {
            "bibtex": "@Article{Polu2022FormalMS,\n author = {Stanislas Polu and Jesse Michael Han and Kunhao Zheng and Mantas Baksys and Igor Babuschkin and I. Sutskever},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Formal Mathematics Statement Curriculum Learning},\n volume = {abs/2202.01344},\n year = {2022}\n}\n"
        }
    },
    "262_rtop-k(distributed_setting)": {
        "paperId": "f3d8e75ffef919b72e0e3d79e6618f4d73945a4d",
        "externalIds": {
            "MAG": "3027827208",
            "DBLP": "journals/jsait/BarnesIIO20",
            "ArXiv": "2005.10761",
            "DOI": "10.1109/JSAIT.2020.3042094",
            "CorpusId": 218763352
        },
        "corpusId": 218763352,
        "publicationVenue": {
            "id": "b571f31a-c68a-44b2-9c95-906830982180",
            "name": "IEEE Journal on Selected Areas in Information Theory",
            "type": "journal",
            "alternate_names": [
                "IEEE J Sel Area Inf Theory"
            ],
            "issn": "2641-8770"
        },
        "url": "https://www.semanticscholar.org/paper/f3d8e75ffef919b72e0e3d79e6618f4d73945a4d",
        "title": "rTop-k: A Statistical Estimation Approach to Distributed SGD",
        "abstract": "The large communication cost for exchanging gradients between different nodes significantly limits the scalability of distributed training for large-scale learning models. Motivated by this observation, there has been significant recent interest in techniques that reduce the communication cost of distributed Stochastic Gradient Descent (SGD), with gradient sparsification techniques such as top-<inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula> and random-<inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula> shown to be particularly effective. The same observation has also motivated a separate line of work in distributed statistical estimation theory focusing on the impact of communication constraints on the estimation efficiency of different statistical models. The primary goal of this paper is to connect these two research lines and demonstrate how statistical estimation models and their analysis can lead to new insights in the design of communication-efficient training techniques. We propose a simple statistical estimation model for the stochastic gradients which captures the sparsity and skewness of their distribution. The statistically optimal communication scheme arising from the analysis of this model leads to a new sparsification technique for SGD, which concatenates random-<inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula> and top-<inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula>, considered separately in the prior literature. We show through extensive experiments on both image and language domains with CIFAR-10, ImageNet, and Penn Treebank datasets that the concatenated application of these two sparsification methods consistently and significantly outperforms either method applied alone.",
        "venue": "IEEE Journal on Selected Areas in Information Theory",
        "year": 2020,
        "referenceCount": 54,
        "citationCount": 45,
        "influentialCitationCount": 4,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simple statistical estimation model is proposed for the stochastic gradients which captures the sparsity and skewness of their distribution and leads to a new sparsification technique for SGD, which concatenates random-<inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex- Math></inline-Formula> and top-<tex-Math notation=\" LaTeX) and significantly outperforms either method applied alone."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-05-21",
        "journal": {
            "name": "IEEE Journal on Selected Areas in Information Theory",
            "pages": "897-907",
            "volume": "1"
        },
        "citationStyles": {
            "bibtex": "@Article{Barnes2020rTopkAS,\n author = {L. P. Barnes and Huseyin A. Inan and Berivan Isik and Ayfer \u00d6zg\u00fcr},\n booktitle = {IEEE Journal on Selected Areas in Information Theory},\n journal = {IEEE Journal on Selected Areas in Information Theory},\n pages = {897-907},\n title = {rTop-k: A Statistical Estimation Approach to Distributed SGD},\n volume = {1},\n year = {2020}\n}\n"
        }
    },
    "264_qwen-vl": {
        "paperId": "fc6a2f7478f68adefd69e2071f27e38aa1647f2f",
        "externalIds": {
            "ArXiv": "2308.12966",
            "CorpusId": 261101015
        },
        "corpusId": 261101015,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/fc6a2f7478f68adefd69e2071f27e38aa1647f2f",
        "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
        "abstract": "In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",
        "venue": "",
        "year": 2023,
        "referenceCount": 85,
        "citationCount": 62,
        "influentialCitationCount": 15,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks."
        },
        "publicationTypes": null,
        "publicationDate": "2023-08-24",
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Bai2023QwenVLAV,\n author = {Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},\n title = {Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n year = {2023}\n}\n"
        }
    },
    "265_shufflenet_v1": {
        "paperId": "9da734397acd7ff7c557960c62fb1b400b27bd89",
        "externalIds": {
            "DBLP": "journals/corr/ZhangZLS17",
            "MAG": "2724359148",
            "ArXiv": "1707.01083",
            "DOI": "10.1109/CVPR.2018.00716",
            "CorpusId": 24982157
        },
        "corpusId": 24982157,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/9da734397acd7ff7c557960c62fb1b400b27bd89",
        "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
        "abstract": "We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13\u00c3\u2014 actual speedup over AlexNet while maintaining comparable accuracy.",
        "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
        "year": 2017,
        "referenceCount": 50,
        "citationCount": 5400,
        "influentialCitationCount": 577,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1707.01083",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An extremely computation-efficient CNN architecture named ShuffleNet is introduced, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs), to greatly reduce computation cost while maintaining accuracy."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-07-04",
        "journal": {
            "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "pages": "6848-6856"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2017ShuffleNetAE,\n author = {Xiangyu Zhang and Xinyu Zhou and Mengxiao Lin and Jian Sun},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {6848-6856},\n title = {ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices},\n year = {2017}\n}\n"
        }
    },
    "266_retinanet-r50": {
        "paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20",
        "externalIds": {
            "MAG": "2950100464",
            "DBLP": "journals/corr/abs-1708-02002",
            "DOI": "10.1109/ICCV.2017.324",
            "CorpusId": 47252984
        },
        "corpusId": 47252984,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/79cfb51a51fc093f66aac8e858afe2e14d4a1f20",
        "title": "Focal Loss for Dense Object Detection",
        "abstract": "The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "referenceCount": 42,
        "citationCount": 18288,
        "influentialCitationCount": 2622,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1708.02002",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes to address the extreme foreground-background class imbalance encountered during training of dense detectors by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples, and develops a novel Focal Loss, which focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-08-07",
        "journal": {
            "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
            "pages": "2999-3007"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2017FocalLF,\n author = {Tsung-Yi Lin and Priya Goyal and Ross B. Girshick and Kaiming He and Piotr Doll\u00e1r},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {2999-3007},\n title = {Focal Loss for Dense Object Detection},\n year = {2017}\n}\n"
        }
    },
    "267_orca_2-13b": {
        "paperId": "d2ea161e6b2114529d875d16cfaad4c824e17a8c",
        "externalIds": {
            "DBLP": "journals/corr/abs-2311-11045",
            "ArXiv": "2311.11045",
            "DOI": "10.48550/arXiv.2311.11045",
            "CorpusId": 265295592
        },
        "corpusId": 265295592,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/d2ea161e6b2114529d875d16cfaad4c824e17a8c",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "abstract": "Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 0,
        "citationCount": 26,
        "influentialCitationCount": 3,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-11-18",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2311.11045"
        },
        "citationStyles": {
            "bibtex": "@Article{Mitra2023Orca2T,\n author = {Arindam Mitra and Luciano Del Corro and Shweti Mahajan and Andres Codas and Clarisse Simoes and Sahaj Agrawal and Xuxi Chen and Anastasia Razdaibiedina and Erik Jones and Kriti Aggarwal and Hamid Palangi and Guoqing Zheng and Corby Rosset and Hamed Khanpour and Ahmed Awadallah},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Orca 2: Teaching Small Language Models How to Reason},\n volume = {abs/2311.11045},\n year = {2023}\n}\n"
        }
    },
    "268_transformer-xl_+_autodropout_(wt2)": {
        "paperId": "ef8854a62e05c8e741894166689a9cd8352a1df0",
        "externalIds": {
            "ArXiv": "2101.01761",
            "DBLP": "conf/aaai/PhamL21",
            "DOI": "10.1609/aaai.v35i11.17127",
            "CorpusId": 230770203
        },
        "corpusId": 230770203,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/ef8854a62e05c8e741894166689a9cd8352a1df0",
        "title": "AutoDropout: Learning Dropout Patterns to Regularize Deep Networks",
        "abstract": "Neural networks are often over-parameterized and hence benefit from aggressive regularization. Conventional regularization methods, such as dropout or weight decay, do not leverage the structures of the network's inputs and hidden states. As a result, these conventional methods are less effective than methods that leverage the structures, such as SpatialDropout and DropBlock, which randomly drop the values at certain contiguous areas in the hidden states and setting them to zero. Although the locations of dropping areas random, the patterns of SpatialDropout and DropBlock are manually designed and fixed. Here we propose to learn the dropping patterns. In our method, a controller learns to generate a dropping pattern at every channel and layer of a target network, such as a ConvNet or a Transformer. The target network is then trained with the dropping pattern, and its resulting validation performance is used as a signal for the controller to learn from. We show that this method works well for both image recognition on CIFAR-10 and ImageNet, as well as language modeling on Penn Treebank and WikiText-2. The learned dropping patterns also transfers to different tasks and datasets, such as from language model on Penn Treebank to Engligh-French translation on WMT 2014. Our code will be available at: https://github.com/googleresearch/google-research/tree/master/auto_dropout.",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2021,
        "referenceCount": 72,
        "citationCount": 42,
        "influentialCitationCount": 5,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/17127/16934",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes to learn the dropping patterns of a target network, and shows that this method works well for both image recognition on CIFAR-10 and ImageNet, as well as language modeling on Penn Treebank and WikiText-2."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-01-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2101.01761"
        },
        "citationStyles": {
            "bibtex": "@Article{Pham2021AutoDropoutLD,\n author = {Hieu Pham and Quoc V. Le},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {AutoDropout: Learning Dropout Patterns to Regularize Deep Networks},\n volume = {abs/2101.01761},\n year = {2021}\n}\n"
        }
    },
    "269_genslm": {
        "paperId": "62a45ab7b676f3877d41f66f6c9ddf1ec44a1c5f",
        "externalIds": {
            "DBLP": "journals/ijhpca/ZvyaginBHDZBCKPMMIOVPWHEFXLSNRDV23",
            "PubMedCentral": "9709791",
            "DOI": "10.1101/2022.10.10.511571",
            "CorpusId": 252899108,
            "PubMed": "36451881"
        },
        "corpusId": 252899108,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/62a45ab7b676f3877d41f66f6c9ddf1ec44a1c5f",
        "title": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
        "abstract": "We seek to transform how new and emergent variants of pandemic-causing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pre-training on over 110 million prokaryotic gene sequences and fine-tuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole-genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.",
        "venue": "bioRxiv",
        "year": 2022,
        "referenceCount": 80,
        "citationCount": 27,
        "influentialCitationCount": 3,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2022/11/23/2022.10.10.511571.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Biology",
            "Medicine",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "GenSLMs represents one of the first whole-genome scale foundation models which can generalize to other prediction tasks and is presented as an initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-11-23",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Zvyagin2022GenSLMsGL,\n author = {Max Zvyagin and Alexander Brace and Kyle Hippe and Yuntian Deng and Bin Zhang and Cindy Orozco Bohorquez and Austin R. Clyde and B. Kale and Danilo Perez-Rivera and Heng Ma and Carla M. Mann and Michael W. Irvin and J. G. Pauloski and Logan T. Ward and Valerie Hayot and M. Emani and Sam Foreman and Zhen Xie and Diangen Lin and Maulik Shukla and Weili Nie and Josh Romero and Christian Dallago and Arash Vahdat and Chaowei Xiao and Tom Gibbs and Ian T. Foster and James J. Davis and M. Papka and T. Brettin and Rick L. Stevens and Anima Anandkumar and V. Vishwanath and Arvind Ramanathan},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics},\n year = {2022}\n}\n"
        }
    },
    "270_multiresolution_cnn": {
        "paperId": "6d4c9c923e9f145d1c01a2de2afc38ec23c44253",
        "externalIds": {
            "MAG": "2308045930",
            "DBLP": "conf/cvpr/KarpathyTSLSF14",
            "DOI": "10.1109/CVPR.2014.223",
            "CorpusId": 206592218
        },
        "corpusId": 206592218,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/6d4c9c923e9f145d1c01a2de2afc38ec23c44253",
        "title": "Large-Scale Video Classification with Convolutional Neural Networks",
        "abstract": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).",
        "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
        "year": 2014,
        "referenceCount": 30,
        "citationCount": 6027,
        "influentialCitationCount": 446,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.cs.cmu.edu/~rahuls/pub/cvpr2014-deepvideo-rahuls.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work studies multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggests a multiresolution, foveated architecture as a promising way of speeding up the training."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-06-23",
        "journal": {
            "name": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
            "pages": "1725-1732"
        },
        "citationStyles": {
            "bibtex": "@Article{Karpathy2014LargeScaleVC,\n author = {A. Karpathy and G. Toderici and Sanketh Shetty and Thomas Leung and R. Sukthankar and Li Fei-Fei},\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1725-1732},\n title = {Large-Scale Video Classification with Convolutional Neural Networks},\n year = {2014}\n}\n"
        }
    },
    "272_neumf_(pinterest)": {
        "paperId": "ad42c33c299ef1c53dfd4697e3f7f98ed0ca31dd",
        "externalIds": {
            "DBLP": "conf/www/HeLZNHC17",
            "MAG": "2605350416",
            "ArXiv": "1708.05031",
            "DOI": "10.1145/3038912.3052569",
            "CorpusId": 13907106
        },
        "corpusId": 13907106,
        "publicationVenue": {
            "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
            "name": "The Web Conference",
            "type": "conference",
            "alternate_names": [
                "Web Conf",
                "WWW"
            ],
            "url": "http://www.iw3c2.org/"
        },
        "url": "https://www.semanticscholar.org/paper/ad42c33c299ef1c53dfd4697e3f7f98ed0ca31dd",
        "title": "Neural Collaborative Filtering",
        "abstract": "In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation --- collaborative filtering --- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering --- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
        "venue": "The Web Conference",
        "year": 2017,
        "referenceCount": 51,
        "citationCount": 4688,
        "influentialCitationCount": 862,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work strives to develop techniques based on neural networks to tackle the key problem in recommendation --- collaborative filtering --- on the basis of implicit feedback, and presents a general framework named NCF, short for Neural network-based Collaborative Filtering."
        },
        "publicationTypes": [
            "JournalArticle",
            "Book",
            "Conference"
        ],
        "publicationDate": "2017-04-03",
        "journal": {
            "name": "Proceedings of the 26th International Conference on World Wide Web"
        },
        "citationStyles": {
            "bibtex": "@Article{He2017NeuralCF,\n author = {Xiangnan He and Lizi Liao and Hanwang Zhang and Liqiang Nie and Xia Hu and Tat-Seng Chua},\n booktitle = {The Web Conference},\n journal = {Proceedings of the 26th International Conference on World Wide Web},\n title = {Neural Collaborative Filtering},\n year = {2017}\n}\n"
        }
    },
    "273_sppnet": {
        "paperId": "cbb19236820a96038d000dc629225d36e0b6294a",
        "externalIds": {
            "MAG": "2179352600",
            "DBLP": "journals/pami/HeZR015",
            "ArXiv": "1406.4729",
            "DOI": "10.1007/978-3-319-10578-9_23",
            "CorpusId": 436933,
            "PubMed": "26353135"
        },
        "corpusId": 436933,
        "publicationVenue": {
            "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
            "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Pattern Anal Mach Intell"
            ],
            "issn": "0162-8828",
            "url": "http://www.computer.org/tpami/",
            "alternate_urls": [
                "http://www.computer.org/portal/web/tpami",
                "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/cbb19236820a96038d000dc629225d36e0b6294a",
        "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",
        "abstract": null,
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "year": 2014,
        "referenceCount": 46,
        "citationCount": 9337,
        "influentialCitationCount": 671,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10578-9_23.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work equips the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement, and develops a new network structure, called SPP-net, which can generate a fixed-length representation regardless of image size/scale."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-06-18",
        "journal": {
            "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "pages": "1904-1916",
            "volume": "37"
        },
        "citationStyles": {
            "bibtex": "@Article{He2014SpatialPP,\n author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1904-1916},\n title = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},\n volume = {37},\n year = {2014}\n}\n"
        }
    },
    "274_tslm+mos_(wt2)": {
        "paperId": "fa744ef316f58139506f36bb3504ba5b27301918",
        "externalIds": {
            "MAG": "2952180917",
            "DBLP": "journals/corr/abs-1901-11167",
            "ArXiv": "1901.11167",
            "DOI": "10.1609/AAAI.V33I01.33017450",
            "CorpusId": 59523745
        },
        "corpusId": 59523745,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/fa744ef316f58139506f36bb3504ba5b27301918",
        "title": "A Generalized Language Model in Tensor Space",
        "abstract": "In the literature, tensors have been effectively used for capturing the context information in language models. However, the existing methods usually adopt relatively-low order tensors, which have limited expressive power in modeling language. Developing a higher-order tensor representation is challenging, in terms of deriving an effective solution and showing its generality. In this paper, we propose a language model named Tensor Space Language Model (TSLM), by utilizing tensor networks and tensor decomposition. In TSLM, we build a high-dimensional semantic space constructed by the tensor product of word vectors. Theoretically, we prove that such tensor representation is a generalization of the n-gram language model. We further show that this high-order tensor representation can be decomposed to a recursive calculation of conditional probability for language modeling. The experimental results on Penn Tree Bank (PTB) dataset and WikiText benchmark demonstrate the effectiveness of TSLM.",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "referenceCount": 31,
        "citationCount": 12,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4735/4613",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "In TSLM, a language model named Tensor Space Language Model is proposed, by utilizing tensor networks and tensor decomposition, which builds a high-dimensional semantic space constructed by the tensor product of word vectors and proves that such tensor representation is a generalization of the n-gram language model."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-01-31",
        "journal": {
            "pages": "7450-7458"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2019AGL,\n author = {Lipeng Zhang and P. Zhang and Xindian Ma and Shuqin Gu and Zhan Su and D. Song},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {7450-7458},\n title = {A Generalized Language Model in Tensor Space},\n year = {2019}\n}\n"
        }
    },
    "275_b2t_connection_(16l)": {
        "paperId": "4312e267af00c76231df7d740298e6b4499441ec",
        "externalIds": {
            "DBLP": "journals/corr/abs-2206-00330",
            "DOI": "10.48550/arXiv.2206.00330",
            "CorpusId": 249240533
        },
        "corpusId": 249240533,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/4312e267af00c76231df7d740298e6b4499441ec",
        "title": "On Layer Normalizations and Residual Connections in Transformers",
        "abstract": "In the perspective of a layer normalization (LN) position, the architecture of Transformers can be categorized into two types: Post-LN and Pre-LN. Recent Transformers prefer to select Pre-LN because the training in Post-LN with deep Transformers, e.g., ten or more layers, often becomes unstable, resulting in useless models. However, in contrast, Post-LN has also consistently achieved better performance than Pre-LN in relatively shallow Transformers, e.g., six or fewer layers. This study \ufb01rst investigates the reason for these discrepant observations empirically and theoretically and discovers 1, the LN in Post-LN is the source of the vanishing gradient problem that mainly leads the unstable training whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger gradient norms in higher layers during the back-propagation that may lead an effective training. Exploiting the new \ufb01ndings, we propose a method that can equip both higher stability and effective training by a simple modi\ufb01cation from Post-LN. We conduct experiments on a wide range of text generation tasks and demonstrate that our method outperforms Pre-LN, and stable training regardless of the shallow or deep layer settings.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 35,
        "citationCount": 6,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2206.00330",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This study investigates the reason for these discrepant observations empirically and theoretically and proposes a method that can equip both higher stability and effective training by a simple modi\ufb01cation from Post-LN, and demonstrates that the method outperforms Pre-Ln, and stable training regardless of the shallow or deep layer settings."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2206.00330"
        },
        "citationStyles": {
            "bibtex": "@Article{Takase2022OnLN,\n author = {Sho Takase and Shun Kiyono and Sosuke Kobayashi and Jun Suzuki},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {On Layer Normalizations and Residual Connections in Transformers},\n volume = {abs/2206.00330},\n year = {2022}\n}\n"
        }
    },
    "276_resnet-200": {
        "paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80",
        "externalIds": {
            "ArXiv": "1603.05027",
            "MAG": "2949427019",
            "DBLP": "journals/corr/HeZR016",
            "DOI": "10.1007/978-3-319-46493-0_38",
            "CorpusId": 6447277
        },
        "corpusId": 6447277,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/77f0a39b8e02686fd85b01971f8feb7f60971f80",
        "title": "Identity Mappings in Deep Residual Networks",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "referenceCount": 30,
        "citationCount": 8850,
        "influentialCitationCount": 1375,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1603.05027",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The propagation formulations behind the residual building blocks suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-03-16",
        "journal": {
            "pages": "630-645"
        },
        "citationStyles": {
            "bibtex": "@Article{He2016IdentityMI,\n author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},\n booktitle = {European Conference on Computer Vision},\n pages = {630-645},\n title = {Identity Mappings in Deep Residual Networks},\n year = {2016}\n}\n"
        }
    },
    "278_image_generation": {
        "paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
        "externalIds": {
            "CorpusId": 211146177
        },
        "corpusId": 211146177,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
        "title": "AUTO-ENCODING VARIATIONAL BAYES",
        "abstract": "To make decisions based on a model fit by Auto-Encoding Variational Bayes (AEVB), practitioners typically use importance sampling to estimate a functional of the posterior distribution. The variational distribution found by AEVB serves as the proposal distribution for importance sampling. However, this proposal distribution may give unreliable (high variance) importance sampling estimates, thus leading to poor decisions. We explore how changing the objective function for learning the variational distribution, while continuing to learn the generative model based on the ELBO, affects the quality of downstream decisions. For a particular model, we characterize the error of importance sampling as a function of posterior variance and show that proposal distributions learned with evidence upper bounds are better. Motivated by these theoretical results, we propose a novel variant of the VAE. In addition to experimenting with MNIST, we present a full-fledged application of the proposed method to single-cell RNA sequencing. In this challenging instance of multiple hypothesis testing, the proposed method surpasses the current state of the art.",
        "venue": "",
        "year": 2020,
        "referenceCount": 53,
        "citationCount": 8498,
        "influentialCitationCount": 1337,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": null,
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work describes the error of importance sampling as a function of posterior variance and shows that proposal distributions learned with evidence upper bounds are better than the current state of the art."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Lopez2020AUTOENCODINGVB,\n author = {Romain Lopez and Pierre Boyeau and N. Yosef and Michael I. Jordan and J. Regier},\n title = {AUTO-ENCODING VARIATIONAL BAYES},\n year = {2020}\n}\n"
        }
    },
    "279_sandwich_transformer": {
        "paperId": "3ff8d265f4351e4b1fdac5b586466bee0b5d6fff",
        "externalIds": {
            "DBLP": "conf/acl/PressSL20",
            "ArXiv": "1911.03864",
            "MAG": "2988945824",
            "ACL": "2020.acl-main.270",
            "DOI": "10.18653/v1/2020.acl-main.270",
            "CorpusId": 207853045
        },
        "corpusId": 207853045,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/3ff8d265f4351e4b1fdac5b586466bee0b5d6fff",
        "title": "Improving Transformer Models by Reordering their Sublayers",
        "abstract": "Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "referenceCount": 32,
        "citationCount": 69,
        "influentialCitationCount": 5,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.270.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a new transformer pattern that adheres to this property, the sandwich transformer, and shows that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-11-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1911.03864"
        },
        "citationStyles": {
            "bibtex": "@Article{Press2019ImprovingTM,\n author = {Ofir Press and Noah A. Smith and Omer Levy},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Improving Transformer Models by Reordering their Sublayers},\n volume = {abs/1911.03864},\n year = {2019}\n}\n"
        }
    },
    "280_matrixfac_for_recommenders": {
        "paperId": "d4bbcc842f22547eaf5884251eaa68251895dccb",
        "externalIds": {
            "MAG": "2054141820",
            "DBLP": "journals/computer/KorenBV09",
            "DOI": "10.1109/MC.2009.263",
            "CorpusId": 58370896
        },
        "corpusId": 58370896,
        "publicationVenue": {
            "id": "f6572f66-2623-4a5e-b0d9-4a5028dea98f",
            "name": "Computer",
            "type": "journal",
            "alternate_names": [
                "IEEE Computer",
                "IEEE Comput"
            ],
            "issn": "0018-9162",
            "url": "http://www.computer.org/computer",
            "alternate_urls": [
                "https://ieeexplore.ieee.org/servlet/opac?punumber=2",
                "http://www.computer.org/portal/site/ieeecs/index.jsp",
                "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/d4bbcc842f22547eaf5884251eaa68251895dccb",
        "title": "Matrix Factorization Techniques for Recommender Systems",
        "abstract": "As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.",
        "venue": "Computer",
        "year": 2009,
        "referenceCount": 12,
        "citationCount": 8667,
        "influentialCitationCount": 846,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2009-08-01",
        "journal": {
            "name": "Computer",
            "volume": "42"
        },
        "citationStyles": {
            "bibtex": "@Article{Koren2009MatrixFT,\n author = {Y. Koren and Robert M. Bell and C. Volinsky},\n booktitle = {Computer},\n journal = {Computer},\n title = {Matrix Factorization Techniques for Recommender Systems},\n volume = {42},\n year = {2009}\n}\n"
        }
    },
    "281_seamlessm4t": {
        "paperId": "036b07c91b23a152a1b764aa492a7503dd3474b8",
        "externalIds": {
            "ArXiv": "2312.05187",
            "DBLP": "journals/corr/abs-2312-05187",
            "DOI": "10.48550/arXiv.2312.05187",
            "CorpusId": 266149504
        },
        "corpusId": 266149504,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/036b07c91b23a152a1b764aa492a7503dd3474b8",
        "title": "Seamless: Multilingual Expressive and Streaming Speech Translation",
        "abstract": "Large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model-SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. SeamlessM4T v2 provides the foundation on which our next two models are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. The contributions to this work are publicly released and accessible at https://github.com/facebookresearch/seamless_communication",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 0,
        "citationCount": 7,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A family of models that enable end-to-end expressive and multilingual translations in a streaming fashion, including the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes are introduced."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-12-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2312.05187"
        },
        "citationStyles": {
            "bibtex": "@Article{Communication2023SeamlessME,\n author = {Seamless Communication and Lo\u00efc Barrault and Yu-An Chung and Mariano Coria Meglioli and David Dale and Ning Dong and M. Duppenthaler and Paul-Ambroise Duquenne and Brian Ellis and Hady ElSahar and Justin Haaheim and John Hoffman and Min-Jae Hwang and H. Inaguma and Christopher Klaiber and Ilia Kulikov and Pengwei Li and Daniel Licht and Jean Maillard and Ruslan Mavlyutov and Alice Rakotoarison and Kaushik Ram Sadagopan and Abinesh Ramakrishnan and Tuan Tran and Guillaume Wenzek and Yilin Yang and Ethan Ye and Ivan Evtimov and Pierre Fernandez and Cynthia Gao and Prangthip Hansanti and Elahe Kalbassi and A. Kallet and Artyom Kozhevnikov and Gabriel Mejia Gonzalez and Robin San Roman and Christophe Touret and Corinne Wong and Carleigh Wood and Bokai Yu and Pierre Andrews and Can Balioglu and Peng-Jen Chen and M. Costa-juss\u00e0 and Maha Elbayad and Hongyu Gong and Francisco Guzm'an and Kevin Heffernan and Somya Jain and Justine T. Kao and Ann Lee and Xutai Ma and Alexandre Mourachko and Benjamin Peloquin and Juan Pino and Sravya Popuri and C. Ropers and Safiyyah Saleem and H. Schwenk and Anna Y. Sun and Paden Tomasello and Changhan Wang and Jeff Wang and Skyler Wang and Mary Williamson},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Seamless: Multilingual Expressive and Streaming Speech Translation},\n volume = {abs/2312.05187},\n year = {2023}\n}\n"
        }
    },
    "283_dqn": {
        "paperId": "2319a491378867c7049b3da055c5df60e1671158",
        "externalIds": {
            "DBLP": "journals/corr/MnihKSGAWR13",
            "MAG": "1757796397",
            "ArXiv": "1312.5602",
            "CorpusId": 15238391
        },
        "corpusId": 15238391,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158",
        "title": "Playing Atari with Deep Reinforcement Learning",
        "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",
        "venue": "arXiv.org",
        "year": 2013,
        "referenceCount": 30,
        "citationCount": 10121,
        "influentialCitationCount": 1309,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning, which outperforms all previous approaches on six of the games and surpasses a human expert on three of them."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2013-12-19",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1312.5602"
        },
        "citationStyles": {
            "bibtex": "@Article{Mnih2013PlayingAW,\n author = {Volodymyr Mnih and K. Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin A. Riedmiller},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Playing Atari with Deep Reinforcement Learning},\n volume = {abs/1312.5602},\n year = {2013}\n}\n"
        }
    },
    "284_hmm_word_alignment": {
        "paperId": "59c442932e9fcfcac6df5566c2bcd1ec331548c9",
        "externalIds": {
            "MAG": "2038698865",
            "ACL": "C96-2141",
            "DBLP": "conf/coling/VogelNT96",
            "DOI": "10.3115/993268.993313",
            "CorpusId": 11644259
        },
        "corpusId": 11644259,
        "publicationVenue": {
            "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
            "name": "International Conference on Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Int Conf Comput Linguistics",
                "COLING"
            ],
            "url": "https://www.aclweb.org/anthology/venues/coling/"
        },
        "url": "https://www.semanticscholar.org/paper/59c442932e9fcfcac6df5566c2bcd1ec331548c9",
        "title": "HMM-Based Word Alignment in Statistical Translation",
        "abstract": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.",
        "venue": "International Conference on Computational Linguistics",
        "year": 1996,
        "referenceCount": 14,
        "citationCount": 976,
        "influentialCitationCount": 85,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/993268.993313",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new model for word alignment in statistical translation using a first-order Hidden Markov model for the word alignment problem as they are used successfully in speech recognition for the time alignment problem."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "1996-08-05",
        "journal": {
            "pages": "836-841"
        },
        "citationStyles": {
            "bibtex": "@Article{Vogel1996HMMBasedWA,\n author = {S. Vogel and H. Ney and Christoph Tillmann},\n booktitle = {International Conference on Computational Linguistics},\n pages = {836-841},\n title = {HMM-Based Word Alignment in Statistical Translation},\n year = {1996}\n}\n"
        }
    },
    "285_maximum_entropy_models_for_machine_translation": {
        "paperId": "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0",
        "externalIds": {
            "MAG": "2154124206",
            "ACL": "P02-1038",
            "DBLP": "conf/acl/OchN02",
            "DOI": "10.3115/1073083.1073133",
            "CorpusId": 284436
        },
        "corpusId": 284436,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/37fadfb6d60e83e24c72d8a90da5644b39d6e8f0",
        "title": "Discriminative Training and Maximum Entropy Models for Statistical Machine Translation",
        "abstract": "We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2002,
        "referenceCount": 17,
        "citationCount": 1216,
        "influentialCitationCount": 62,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1073083.1073133",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case and shows that a baseline statistical machinetranslation system is significantly improved using this approach."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2002-07-06",
        "journal": {
            "pages": "295-302"
        },
        "citationStyles": {
            "bibtex": "@Article{Och2002DiscriminativeTA,\n author = {F. Och and H. Ney},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {295-302},\n title = {Discriminative Training and Maximum Entropy Models for Statistical Machine Translation},\n year = {2002}\n}\n"
        }
    },
    "286_diffq_transformer_(16l)": {
        "paperId": "138d7ee8cc618e7b365bda6d8a8f4e6cb85a0c37",
        "externalIds": {
            "DBLP": "journals/corr/abs-2104-09987",
            "ArXiv": "2104.09987",
            "CorpusId": 233307280
        },
        "corpusId": 233307280,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/138d7ee8cc618e7b365bda6d8a8f4e6cb85a0c37",
        "title": "Differentiable Model Compression via Pseudo Quantization Noise",
        "abstract": "We propose DiffQ a differentiable method for model compression for quantizing model parameters without gradient approximations (e.g., Straight Through Estimator). We suggest adding independent pseudo quantization noise to model parameters during training to approximate the effect of a quantization operator. DiffQ is differentiable both with respect to the unquantized weights and the number of bits used. Given a single hyper-parameter balancing between the quantized model size and accuracy, DiffQ optimizes the number of bits used per individual weight or groups of weights, in end-to-end training. We experimentally verify that our method is competitive with STE based quantization techniques on several benchmarks and architectures for image classification, language modeling, and audio source separation. For instance, on the ImageNet dataset, DiffQ compresses a 12 layers transformer-based model by more than a factor of 8, (lower than 4 bits precision per weight on average), with a loss of 0.3% in model accuracy. Code is available at github.com/facebookresearch/diffq.",
        "venue": "Trans. Mach. Learn. Res.",
        "year": 2021,
        "referenceCount": 72,
        "citationCount": 27,
        "influentialCitationCount": 9,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "DiffQ is a differentiable method for model compression for quantizing model parameters without gradient approximations (e.g., Straight Through Estimator) and is competitive with STE based quantization techniques on several benchmarks and architectures for image classification, language modeling, and audio source separation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-04-20",
        "journal": {
            "name": "Trans. Mach. Learn. Res.",
            "volume": "2022"
        },
        "citationStyles": {
            "bibtex": "@Article{D'efossez2021DifferentiableMC,\n author = {Alexandre D'efossez and Yossi Adi and Gabriel Synnaeve},\n booktitle = {Trans. Mach. Learn. Res.},\n journal = {Trans. Mach. Learn. Res.},\n title = {Differentiable Model Compression via Pseudo Quantization Noise},\n volume = {2022},\n year = {2021}\n}\n"
        }
    },
    "287_densephrases": {
        "paperId": "24c26cb896e79391b6fbbc99db37345081b9da28",
        "externalIds": {
            "DBLP": "conf/acl/LeeSKC20",
            "ArXiv": "2012.12624",
            "ACL": "2021.acl-long.518",
            "DOI": "10.18653/v1/2021.acl-long.518",
            "CorpusId": 229363636
        },
        "corpusId": 229363636,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/24c26cb896e79391b6fbbc99db37345081b9da28",
        "title": "Learning Dense Representations of Phrases at Scale",
        "abstract": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "referenceCount": 52,
        "citationCount": 92,
        "influentialCitationCount": 15,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.acl-long.518.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work shows for the first time that it can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA and proposes a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-12-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2012.12624"
        },
        "citationStyles": {
            "bibtex": "@Article{Lee2020LearningDR,\n author = {Jinhyuk Lee and Mujeen Sung and Jaewoo Kang and Danqi Chen},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Learning Dense Representations of Phrases at Scale},\n volume = {abs/2012.12624},\n year = {2020}\n}\n"
        }
    },
    "288_polynet": {
        "paperId": "aad34665649953fa4bbacdc6eff4edb5408df6b3",
        "externalIds": {
            "MAG": "2552294571",
            "DBLP": "journals/corr/ZhangLLL16",
            "ArXiv": "1611.05725",
            "DOI": "10.1109/CVPR.2017.415",
            "CorpusId": 17265670
        },
        "corpusId": 17265670,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/aad34665649953fa4bbacdc6eff4edb5408df6b3",
        "title": "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks",
        "abstract": "A number of studies have shown that increasing the depth or width of convolutional networks is a rewarding approach to improve the performance of image recognition. In our study, however, we observed difficulties along both directions. On one hand, the pursuit for very deep networks is met with a diminishing return and increased training difficulty, on the other hand, widening a network would result in a quadratic growth in both computational cost and memory demand. These difficulties motivate us to explore structural diversity in designing deep networks, a new dimension beyond just depth and width. Specifically, we present a new family of modules, namely the PolyInception, which can be flexibly inserted in isolation or in a composition as replacements of different parts of a network. Choosing PolyInception modules with the guidance of architectural efficiency can improve the expressive power while preserving comparable computational cost. The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 38,
        "citationCount": 243,
        "influentialCitationCount": 30,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.05725",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents a new family of modules, namely the PolyInception, which can be flexibly inserted in isolation or in a composition as replacements of different parts of a network, and demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-11-17",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "3900-3908"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2016PolyNetAP,\n author = {Xingcheng Zhang and Zhizhong Li and Chen Change Loy and Dahua Lin},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {3900-3908},\n title = {PolyNet: A Pursuit of Structural Diversity in Very Deep Networks},\n year = {2016}\n}\n"
        }
    },
    "289_lstm-char-large": {
        "paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
        "externalIds": {
            "DBLP": "journals/corr/KimJSR15",
            "ArXiv": "1508.06615",
            "MAG": "2951559648",
            "DOI": "10.1609/aaai.v30i1.10362",
            "CorpusId": 686481
        },
        "corpusId": 686481,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
        "title": "Character-Aware Neural Language Models",
        "abstract": "\n \n We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway net work over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.\n \n",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "referenceCount": 54,
        "citationCount": 1597,
        "influentialCitationCount": 159,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10362/10221",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simple neural language model that relies only on character-level inputs that is able to encode, from characters only, both semantic and orthographic information and suggests that on many languages, character inputs are sufficient for language modeling."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-08-26",
        "journal": {
            "pages": "2741-2749"
        },
        "citationStyles": {
            "bibtex": "@Article{Kim2015CharacterAwareNL,\n author = {Yoon Kim and Yacine Jernite and D. Sontag and Alexander M. Rush},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {2741-2749},\n title = {Character-Aware Neural Language Models},\n year = {2015}\n}\n"
        }
    },
    "290_alphageometry": {
        "paperId": "9b093787f9be3d480cd11f8ec6ca5b0e44050d6d",
        "externalIds": {
            "PubMedCentral": "10794143",
            "DBLP": "journals/nature/TrinhWLHL24",
            "DOI": "10.1038/s41586-023-06747-5",
            "CorpusId": 267032902,
            "PubMed": "38233616"
        },
        "corpusId": 267032902,
        "publicationVenue": {
            "id": "fb2c0204-6ec9-490c-8221-3cc3698d0e13",
            "name": "The Naturalist",
            "type": "journal",
            "alternate_names": [
                "Nat",
                "nat",
                "Le Naturaliste",
                "The naturalist"
            ],
            "issn": "0028-0771",
            "alternate_issns": [
                "1280-4509"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/9b093787f9be3d480cd11f8ec6ca5b0e44050d6d",
        "title": "Solving olympiad geometry without human demonstrations",
        "abstract": null,
        "venue": "The Naturalist",
        "year": 2024,
        "referenceCount": 5,
        "citationCount": 29,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.nature.com/articles/s41586-023-06747-5.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "AlphaGeometry is a neuro-symbolic system that uses a neural language model that uses a neural language model, trained from scratch on large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2024-01-01",
        "journal": {
            "name": "Nature",
            "pages": "476 - 482",
            "volume": "625"
        },
        "citationStyles": {
            "bibtex": "@Article{Trinh2024SolvingOG,\n author = {Trieu H. Trinh and Yuhuai Wu and Quoc V. Le and He He and Thang Luong},\n booktitle = {The Naturalist},\n journal = {Nature},\n pages = {476 - 482},\n title = {Solving olympiad geometry without human demonstrations},\n volume = {625},\n year = {2024}\n}\n"
        }
    },
    "291_resnet-152_(imagenet)": {
        "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "externalIds": {
            "DBLP": "conf/cvpr/HeZRS16",
            "MAG": "2949650786",
            "ArXiv": "1512.03385",
            "DOI": "10.1109/cvpr.2016.90",
            "CorpusId": 206594692
        },
        "corpusId": 206594692,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "title": "Deep Residual Learning for Image Recognition",
        "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "referenceCount": 53,
        "citationCount": 154626,
        "influentialCitationCount": 27115,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://repositorio.unal.edu.co/bitstream/unal/81443/1/98670607.2022.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-12-10",
        "journal": {
            "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "770-778"
        },
        "citationStyles": {
            "bibtex": "@Article{He2015DeepRL,\n author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {770-778},\n title = {Deep Residual Learning for Image Recognition},\n year = {2015}\n}\n"
        }
    },
    "292_domain_adaptation": {
        "paperId": "d3edbfee56884d2b6d9aa51a6c525f9a05248802",
        "externalIds": {
            "DBLP": "conf/iccv/GopalanLC11",
            "MAG": "2128053425",
            "DOI": "10.1109/ICCV.2011.6126344",
            "CorpusId": 10337178
        },
        "corpusId": 10337178,
        "publicationVenue": {
            "id": "4144b5fb-0a80-4663-8ebf-80ca0c47231a",
            "name": "Vision",
            "type": "conference",
            "alternate_names": [
                "International Conference on Computer Vision",
                "Int Conf Comput Vis",
                "VISION"
            ],
            "issn": "0917-1142",
            "alternate_issns": [
                "2411-5150",
                "2086-4213"
            ],
            "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1000285",
            "alternate_urls": [
                "https://www.mdpi.com/journal/vision",
                "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-1000285"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/d3edbfee56884d2b6d9aa51a6c525f9a05248802",
        "title": "Domain adaptation for object recognition: An unsupervised approach",
        "abstract": "Adapting the classifier trained on a source domain to recognize instances from a new target domain is an important problem that is receiving recent attention. In this paper, we present one of the first studies on unsupervised domain adaptation in the context of object recognition, where we have labeled data only from the source domain (and therefore do not have correspondences between object categories across domains). Motivated by incremental learning, we create intermediate representations of data between the two domains by viewing the generative subspaces (of same dimension) created from these domains as points on the Grassmann manifold, and sampling points along the geodesic between them to obtain subspaces that provide a meaningful description of the underlying domain shift. We then obtain the projections of labeled source domain data onto these subspaces, from which a discriminative classifier is learnt to classify projected data from the target domain. We discuss extensions of our approach for semi-supervised adaptation, and for cases with multiple source and target domains, and report competitive results on standard datasets.",
        "venue": "Vision",
        "year": 2011,
        "referenceCount": 46,
        "citationCount": 1046,
        "influentialCitationCount": 129,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents one of the first studies on unsupervised domain adaptation in the context of object recognition, where data has been labeled only from the source domain (and therefore do not have correspondences between object categories across domains)."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2011-11-06",
        "journal": {
            "name": "2011 International Conference on Computer Vision",
            "pages": "999-1006"
        },
        "citationStyles": {
            "bibtex": "@Article{Gopalan2011DomainAF,\n author = {Raghuraman Gopalan and Ruonan Li and R. Chellappa},\n booktitle = {Vision},\n journal = {2011 International Conference on Computer Vision},\n pages = {999-1006},\n title = {Domain adaptation for object recognition: An unsupervised approach},\n year = {2011}\n}\n"
        }
    },
    "293_xlmr-xxl": {
        "paperId": "c553280c1fc1d0bc7b94683bb75910e309b0d579",
        "externalIds": {
            "DBLP": "conf/rep4nlp/GoyalDOAC21",
            "ACL": "2021.repl4nlp-1.4",
            "ArXiv": "2105.00572",
            "DOI": "10.18653/v1/2021.repl4nlp-1.4",
            "CorpusId": 233481097
        },
        "corpusId": 233481097,
        "publicationVenue": {
            "id": "8b169440-4c13-4cf4-b3f9-1dc7c39dc888",
            "name": "Workshop on Representation Learning for NLP",
            "type": "conference",
            "alternate_names": [
                "RepL4NLP",
                "Workshop Represent Learn NLP"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/c553280c1fc1d0bc7b94683bb75910e309b0d579",
        "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
        "abstract": "Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed and outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests larger capacity models for language understanding may obtain strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.",
        "venue": "Workshop on Representation Learning for NLP",
        "year": 2021,
        "referenceCount": 21,
        "citationCount": 72,
        "influentialCitationCount": 9,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.repl4nlp-1.4.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This study presents the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters, which suggest larger capacity models for language understanding may obtain strong performance on high-resource languages while greatly improving low- resource languages."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-05-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2105.00572"
        },
        "citationStyles": {
            "bibtex": "@Article{Goyal2021LargerScaleTF,\n author = {Naman Goyal and Jingfei Du and Myle Ott and Giridhar Anantharaman and Alexis Conneau},\n booktitle = {Workshop on Representation Learning for NLP},\n journal = {ArXiv},\n title = {Larger-Scale Transformers for Multilingual Masked Language Modeling},\n volume = {abs/2105.00572},\n year = {2021}\n}\n"
        }
    },
    "294_sachs": {
        "paperId": "2521c3d76bc439c961b7003080f4a7a661949547",
        "externalIds": {
            "MAG": "2073307618",
            "DOI": "10.1126/SCIENCE.1105809",
            "CorpusId": 8160280,
            "PubMed": "15845847"
        },
        "corpusId": 8160280,
        "publicationVenue": {
            "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
            "name": "Science",
            "type": "journal",
            "issn": "0193-4511",
            "alternate_issns": [
                "0036-8075"
            ],
            "url": "https://www.jstor.org/journal/science",
            "alternate_urls": [
                "https://www.sciencemag.org/",
                "http://www.sciencemag.org/",
                "http://www.jstor.org/journals/00368075.html",
                "http://www.sciencemag.org/archive/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/2521c3d76bc439c961b7003080f4a7a661949547",
        "title": "Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data",
        "abstract": "Machine learning was applied for the automated derivation of causal influences in cellular signaling networks. This derivation relied on the simultaneous measurement of multiple phosphorylated protein and phospholipid components in thousands of individual primary human immune system cells. Perturbing these cells with molecular interventions drove the ordering of connections between pathway components, wherein Bayesian network computational methods automatically elucidated most of the traditionally reported signaling relationships and predicted novel interpathway network causalities, which we verified experimentally. Reconstruction of network models from physiologically relevant primary single cells might be applied to understanding native-state tissue signaling biology, complex drug actions, and dysfunctional signaling in diseased cells.",
        "venue": "Science",
        "year": 2005,
        "referenceCount": 28,
        "citationCount": 1739,
        "influentialCitationCount": 205,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Medicine",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Reconstruction of network models from physiologically relevant primary single cells might be applied to understanding native-state tissue signaling biology, complex drug actions, and dysfunctional signaling in diseased cells."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2005-04-22",
        "journal": {
            "name": "Science",
            "pages": "523 - 529",
            "volume": "308"
        },
        "citationStyles": {
            "bibtex": "@Article{Sachs2005CausalPN,\n author = {K. Sachs and O. Perez and D. Pe\u2019er and D. Lauffenburger and G. Nolan},\n booktitle = {Science},\n journal = {Science},\n pages = {523 - 529},\n title = {Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data},\n volume = {308},\n year = {2005}\n}\n"
        }
    },
    "296_kosmos-2": {
        "paperId": "3b6179c293df29e31d31cea46476f104ab6950f2",
        "externalIds": {
            "ArXiv": "2306.14824",
            "DBLP": "journals/corr/abs-2306-14824",
            "DOI": "10.48550/arXiv.2306.14824",
            "CorpusId": 259262263
        },
        "corpusId": 259262263,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/3b6179c293df29e31d31cea46476f104ab6950f2",
        "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
        "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 42,
        "citationCount": 190,
        "influentialCitationCount": 30,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2306.14824",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Kosmos-2, a Multimodal Large Language Model (MLLM), is introduced, enabling new capabilities of perceiving object descriptions and grounding text to the visual world and sheds light on the big convergence of language, multimodal perception, action, and world modeling."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-06-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2306.14824"
        },
        "citationStyles": {
            "bibtex": "@Article{Peng2023Kosmos2GM,\n author = {Zhiliang Peng and Wenhui Wang and Li Dong and Y. Hao and Shaohan Huang and Shuming Ma and Furu Wei},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Kosmos-2: Grounding Multimodal Large Language Models to the World},\n volume = {abs/2306.14824},\n year = {2023}\n}\n"
        }
    },
    "297_palm-saycan": {
        "paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc",
        "externalIds": {
            "DBLP": "conf/corl/IchterBCFHHHIIJ22",
            "ArXiv": "2204.01691",
            "CorpusId": 247939706
        },
        "corpusId": 247939706,
        "publicationVenue": {
            "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
            "name": "Conference on Robot Learning",
            "type": "conference",
            "alternate_names": [
                "CoRL",
                "Conf Robot Learn"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/cb5e3f085caefd1f3d5e08637ab55d39e61234fc",
        "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
        "abstract": "Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's\"hands and eyes,\"while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.",
        "venue": "Conference on Robot Learning",
        "year": 2022,
        "referenceCount": 113,
        "citationCount": 845,
        "influentialCitationCount": 82,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate, and shows how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-04-04",
        "journal": {
            "pages": "287-318"
        },
        "citationStyles": {
            "bibtex": "@Article{Ahn2022DoAI,\n author = {Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and K. Gopalakrishnan and Karol Hausman and Alexander Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and A. Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and N. Joshi and Ryan C. Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and S. Levine and Yao Lu and Linda Luu and Carolina Parada and P. Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and D. Reyes and P. Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and F. Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan},\n booktitle = {Conference on Robot Learning},\n pages = {287-318},\n title = {Do As I Can, Not As I Say: Grounding Language in Robotic Affordances},\n year = {2022}\n}\n"
        }
    },
    "299_sru++_large": {
        "paperId": "2fd10e095b146f99da8cdc6ff58720e2e8fca36d",
        "externalIds": {
            "DBLP": "journals/corr/abs-2102-12459",
            "ACL": "2021.emnlp-main.602",
            "ArXiv": "2102.12459",
            "DOI": "10.18653/v1/2021.emnlp-main.602",
            "CorpusId": 232035542
        },
        "corpusId": 232035542,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/2fd10e095b146f99da8cdc6ff58720e2e8fca36d",
        "title": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
        "abstract": "Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "referenceCount": 61,
        "citationCount": 38,
        "influentialCitationCount": 3,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.602.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling that exhibits strong modeling capacity and training efficiency and suggests jointly leveragingFast recurrence with little attention as a promising direction for accelerating model training and inference."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-02-24",
        "journal": {
            "pages": "7633-7648"
        },
        "citationStyles": {
            "bibtex": "@Article{Lei2021WhenAM,\n author = {Tao Lei},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {7633-7648},\n title = {When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute},\n year = {2021}\n}\n"
        }
    },
    "300_simclr": {
        "paperId": "34733eaf66007516347a40ad5d9bbe1cc9dacb6b",
        "externalIds": {
            "DBLP": "journals/corr/abs-2002-05709",
            "ArXiv": "2002.05709",
            "MAG": "3005680577",
            "CorpusId": 211096730
        },
        "corpusId": 211096730,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/34733eaf66007516347a40ad5d9bbe1cc9dacb6b",
        "title": "A Simple Framework for Contrastive Learning of Visual Representations",
        "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "referenceCount": 67,
        "citationCount": 12697,
        "influentialCitationCount": 2854,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that composition of data augmentations plays a critical role in defining effective predictive tasks, and introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-02-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2002.05709"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2020ASF,\n author = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey E. Hinton},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {A Simple Framework for Contrastive Learning of Visual Representations},\n volume = {abs/2002.05709},\n year = {2020}\n}\n"
        }
    },
    "301_polarity_classifier": {
        "paperId": "8a1653cc1d86585c055ef680fedad6fdd565f027",
        "externalIds": {
            "DBLP": "journals/coling/WilsonWH09",
            "ACL": "J09-3003",
            "MAG": "2146111747",
            "DOI": "10.1162/coli.08-012-R1-06-90",
            "CorpusId": 9423000
        },
        "corpusId": 9423000,
        "publicationVenue": {
            "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
            "name": "International Conference on Computational Logic",
            "type": "conference",
            "alternate_names": [
                "CL",
                "Int Conf Comput Log"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/8a1653cc1d86585c055ef680fedad6fdd565f027",
        "title": "Articles: Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis",
        "abstract": "Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation). However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word's prior polarity. Positive words are used in phrases expressing negative sentiments, or vice versa. Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment. The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task. Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity. The evaluation includes assessing the performance of features across multiple machine learning algorithms. For all learning algorithms except one, the combination of all features together gives the best performance. Another facet of the evaluation considers how the presence of neutral instances affects the performance of features for distinguishing between positive and negative polarity. These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system's ability to identify when an instance is neutral.",
        "venue": "International Conference on Computational Logic",
        "year": 2009,
        "referenceCount": 66,
        "citationCount": 354,
        "influentialCitationCount": 14,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://direct.mit.edu/coli/article-pdf/35/3/399/1798638/coli.08-012-r1-06-90.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task, and it is shown that the presence of neutral instances greatly degrades the performance of features for distinguishing between positive and negative polarity."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2009-09-01",
        "journal": {
            "name": "Computational Linguistics",
            "pages": "399-433",
            "volume": "35"
        },
        "citationStyles": {
            "bibtex": "@Article{Wilson2009ArticlesRC,\n author = {Theresa Wilson and Janyce Wiebe and Paul Hoffmann},\n booktitle = {International Conference on Computational Logic},\n journal = {Computational Linguistics},\n pages = {399-433},\n title = {Articles: Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis},\n volume = {35},\n year = {2009}\n}\n"
        }
    },
    "302_decoupled_weight_decay_regularization": {
        "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
        "externalIds": {
            "MAG": "2950541952",
            "DBLP": "conf/iclr/LoshchilovH19",
            "CorpusId": 53592270
        },
        "corpusId": 53592270,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/d07284a6811f1b2745d91bdb06b040b57f226882",
        "title": "Decoupled Weight Decay Regularization",
        "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "referenceCount": 35,
        "citationCount": 12745,
        "influentialCitationCount": 2269,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function, and provides empirical evidence that this modification substantially improves Adam's generalization performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-11-14",
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Loshchilov2017DecoupledWD,\n author = {I. Loshchilov and F. Hutter},\n booktitle = {International Conference on Learning Representations},\n title = {Decoupled Weight Decay Regularization},\n year = {2017}\n}\n"
        }
    },
    "303_pg-swgan": {
        "paperId": "2d20253a2c87c8c6a30441051a373d6ce269fb83",
        "externalIds": {
            "MAG": "2795520001",
            "DBLP": "journals/corr/abs-1904-05408",
            "ArXiv": "1904.05408",
            "DOI": "10.1109/CVPR.2019.00383",
            "CorpusId": 3694842
        },
        "corpusId": 3694842,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/2d20253a2c87c8c6a30441051a373d6ce269fb83",
        "title": "Sliced Wasserstein Generative Models",
        "abstract": "In generative modeling, the Wasserstein distance (WD) has emerged as a useful metric to measure the discrepancy between generated and real data distributions. Unfortunately, it is challenging to approximate the WD of high-dimensional distributions. In contrast, the sliced Wasserstein distance (SWD) factorizes high-dimensional distributions into their multiple one-dimensional marginal distributions and is thus easier to approximate. In this paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by conventional SWD approximation methods, we propose to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion. As concrete applications of our SWD approximations, we design two types of differentiable SWD blocks to equip modern generative frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In the experiments, we not only show the superiority of the proposed generative models on standard image synthesis benchmarks, but also demonstrate the state-of-the-art performance on challenging high resolution image and video generation in an unsupervised manner.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "referenceCount": 48,
        "citationCount": 104,
        "influentialCitationCount": 12,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion and designs two types of differentiable SWD blocks to equip modern generative frameworks---Auto-Encoders and Generative Adversarial Networks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-04-10",
        "journal": {
            "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "3708-3717"
        },
        "citationStyles": {
            "bibtex": "@Article{Wu2019SlicedWG,\n author = {Jiqing Wu and Zhiwu Huang and Dinesh Acharya and Wen Li and Janine Thoma and D. Paudel and L. Gool},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {3708-3717},\n title = {Sliced Wasserstein Generative Models},\n year = {2019}\n}\n"
        }
    },
    "304_pandemonium_(morse)": {
        "paperId": "beab0985636bf54d52385e5009613e029cdb86dc",
        "externalIds": {
            "MAG": "2215343611",
            "CorpusId": 61337194
        },
        "corpusId": 61337194,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/beab0985636bf54d52385e5009613e029cdb86dc",
        "title": "Pandemonium: a paradigm for learning",
        "abstract": null,
        "venue": "",
        "year": 1988,
        "referenceCount": 1,
        "citationCount": 596,
        "influentialCitationCount": 21,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Education",
                "source": "s2-fos-model"
            },
            {
                "category": "Philosophy",
                "source": "s2-fos-model"
            }
        ],
        "tldr": null,
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "pages": "115-122",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Selfridge1988PandemoniumAP,\n author = {O. G. Selfridge},\n pages = {115-122},\n title = {Pandemonium: a paradigm for learning},\n year = {1988}\n}\n"
        }
    },
    "305_theseus_6_768": {
        "paperId": "2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7",
        "externalIds": {
            "DBLP": "conf/emnlp/XuZGWZ20",
            "MAG": "3101248447",
            "ArXiv": "2002.02925",
            "ACL": "2020.emnlp-main.633",
            "DOI": "10.18653/v1/2020.emnlp-main.633",
            "CorpusId": 211066200
        },
        "corpusId": 211066200,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7",
        "title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing",
        "abstract": "In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2020,
        "referenceCount": 49,
        "citationCount": 172,
        "influentialCitationCount": 14,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.emnlp-main.633.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a novel model compression approach to effectively compress BERT by progressive module replacing, which outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-02-07",
        "journal": {
            "pages": "7859-7869"
        },
        "citationStyles": {
            "bibtex": "@Article{Xu2020BERTofTheseusCB,\n author = {Canwen Xu and Wangchunshu Zhou and Tao Ge and Furu Wei and Ming Zhou},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {7859-7869},\n title = {BERT-of-Theseus: Compressing BERT by Progressive Module Replacing},\n year = {2020}\n}\n"
        }
    },
    "306_transformer-c": {
        "paperId": "981dbdf6f87f13f3f3047a925c519fc39a35202b",
        "externalIds": {
            "ArXiv": "2104.03474",
            "MAG": "3172041450",
            "DBLP": "conf/naacl/SunI21",
            "ACL": "2021.naacl-main.407",
            "DOI": "10.18653/V1/2021.NAACL-MAIN.407",
            "CorpusId": 233181647
        },
        "corpusId": 233181647,
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "url": "https://www.semanticscholar.org/paper/981dbdf6f87f13f3f3047a925c519fc39a35202b",
        "title": "Revisiting Simple Neural Probabilistic Language Models",
        "abstract": "Recent progress in language modeling has been driven not only by advances in neural architectures, but also through hardware and optimization improvements. In this paper, we revisit the neural probabilistic language model (NPLM) of Bengio et al. (2003), which simply concatenates word embeddings within a fixed window and passes the result through a feed-forward network to predict the next word. When scaled up to modern hardware, this model (despite its many limitations) performs much better than expected on word-level language model benchmarks. Our analysis reveals that the NPLM achieves lower perplexity than a baseline Transformer with short input contexts but struggles to handle long-term dependencies. Inspired by this result, we modify the Transformer by replacing its first self-attention layer with the NPLM\u2019s local concatenation layer, which results in small but consistent perplexity decreases across three word-level language modeling datasets.",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 36,
        "citationCount": 8,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.naacl-main.407.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper revisits the neural probabilistic language model of Bengio et al. (2003), which simply concatenates word embeddings within a fixed window and passes the result through a feed-forward network to predict the next word, and results in small but consistent perplexity decreases across three word-level language modeling datasets."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-04-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2104.03474"
        },
        "citationStyles": {
            "bibtex": "@Article{Sun2021RevisitingSN,\n author = {Simeng Sun and Mohit Iyyer},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Revisiting Simple Neural Probabilistic Language Models},\n volume = {abs/2104.03474},\n year = {2021}\n}\n"
        }
    },
    "307_deepnet": {
        "paperId": "2db1885750482e14df470b80badf8135c59c78d3",
        "externalIds": {
            "ArXiv": "2203.00555",
            "DBLP": "journals/corr/abs-2203-00555",
            "DOI": "10.48550/arXiv.2203.00555",
            "CorpusId": 247187905
        },
        "corpusId": 247187905,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/2db1885750482e14df470b80badf8135c59c78d3",
        "title": "DeepNet: Scaling Transformers to 1, 000 Layers",
        "abstract": "In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making DeepNorm a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 45,
        "citationCount": 100,
        "influentialCitationCount": 11,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.00555",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new normalization function (DeepNorm) is introduced to modify the residual connection in Transformer, accompanying with theoretically derived initialization, which combines the best of two worlds, i.e., good performance of Post- LN and stable training of Pre-LN, making DeepNorm a preferred alternative."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-03-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2203.00555"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2022DeepNetST,\n author = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {DeepNet: Scaling Transformers to 1, 000 Layers},\n volume = {abs/2203.00555},\n year = {2022}\n}\n"
        }
    },
    "308_gated_hornn_(3rd_order)": {
        "paperId": "ceffa9494aa7c23c242e6f2295e299a1c06e2314",
        "externalIds": {
            "MAG": "2345668077",
            "DBLP": "journals/corr/SoltaniJ16",
            "ArXiv": "1605.00064",
            "CorpusId": 15389911
        },
        "corpusId": 15389911,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/ceffa9494aa7c23c242e6f2295e299a1c06e2314",
        "title": "Higher Order Recurrent Neural Networks",
        "abstract": "In this paper, we study novel neural network structures to better model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8 data sets. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs.",
        "venue": "arXiv.org",
        "year": 2016,
        "referenceCount": 36,
        "citationCount": 57,
        "influentialCitationCount": 11,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-04-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1605.00064"
        },
        "citationStyles": {
            "bibtex": "@Article{Soltani2016HigherOR,\n author = {Rohollah Soltani and Hui Jiang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Higher Order Recurrent Neural Networks},\n volume = {abs/1605.00064},\n year = {2016}\n}\n"
        }
    },
    "309_helpful_harmless_preference_model": {
        "paperId": "0286b2736a114198b25fb5553c671c33aed5d477",
        "externalIds": {
            "ArXiv": "2204.05862",
            "DBLP": "journals/corr/abs-2204-05862",
            "DOI": "10.48550/arXiv.2204.05862",
            "CorpusId": 248118878
        },
        "corpusId": 248118878,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0286b2736a114198b25fb5553c671c33aed5d477",
        "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
        "abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) to \ufb01netune language models to act as helpful and harmless assistants. We \ufb01nd this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, ef\ufb01ciently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. Figure These plots show that PM accuracy decreases as we focus exclusively on comparisons between pairs of samples with high score. We have normalized all preference models to have the same mean score on a held-out dataset so that they\u2019re directly comparable, and then plotted accuracy for the comparisons where both samples have scores above a speci\ufb01c threshold.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 72,
        "citationCount": 781,
        "influentialCitationCount": 121,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2204.05862",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, and a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization is identified."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-04-12",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2204.05862"
        },
        "citationStyles": {
            "bibtex": "@Article{Bai2022TrainingAH,\n author = {Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and T. Henighan and Nicholas Joseph and Saurav Kadavath and John Kernion and Tom Conerly and S. El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and S. Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom B. Brown and Jack Clark and Sam McCandlish and C. Olah and Benjamin Mann and Jared Kaplan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},\n volume = {abs/2204.05862},\n year = {2022}\n}\n"
        }
    },
    "310_spatially-sparse_cnn": {
        "paperId": "c8cb691eae3a2e79adf07548d348ab58e90ee2ba",
        "externalIds": {
            "MAG": "189277179",
            "DBLP": "journals/corr/Graham14",
            "ArXiv": "1409.6070",
            "CorpusId": 14731791
        },
        "corpusId": 14731791,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/c8cb691eae3a2e79adf07548d348ab58e90ee2ba",
        "title": "Spatially-sparse convolutional neural networks",
        "abstract": "Convolutional neural networks (CNNs) perform well on problems such as handwriting recognition and image classification. However, the performance of the networks is often limited by budget and time constraints, particularly when trying to train deep networks. \nMotivated by the problem of online handwriting recognition, we developed a CNN for processing spatially-sparse inputs; a character drawn with a one-pixel wide pen on a high resolution grid looks like a sparse matrix. Taking advantage of the sparsity allowed us more efficiently to train and test large, deep CNNs. On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test error of 3.82%. \nAlthough pictures are not sparse, they can be thought of as sparse by adding padding. Applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the CIFAR small picture datasets: 6.28% on CIFAR-10 and 24.30% for CIFAR-100.",
        "venue": "arXiv.org",
        "year": 2014,
        "referenceCount": 14,
        "citationCount": 219,
        "influentialCitationCount": 19,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A CNN for processing spatially-sparse inputs, motivated by the problem of online handwriting recognition, and applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the CIFAR small picture datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-09-21",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1409.6070"
        },
        "citationStyles": {
            "bibtex": "@Article{Graham2014SpatiallysparseCN,\n author = {Benjamin Graham},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Spatially-sparse convolutional neural networks},\n volume = {abs/1409.6070},\n year = {2014}\n}\n"
        }
    },
    "311_tfe_svm": {
        "paperId": "0fb238dfbe14943c1b6672c20d417c85381f1ae2",
        "externalIds": {
            "DBLP": "journals/pr/LauerSB07",
            "MAG": "2170393096",
            "DOI": "10.1016/j.patcog.2006.10.011",
            "CorpusId": 3870480
        },
        "corpusId": 3870480,
        "publicationVenue": {
            "id": "266f640f-003e-453b-ab76-57e4053252f8",
            "name": "Pattern Recognition",
            "type": "journal",
            "alternate_names": [
                "Pattern Recognit"
            ],
            "issn": "0031-3203",
            "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
            "alternate_urls": [
                "https://www.journals.elsevier.com/pattern-recognition",
                "http://www.sciencedirect.com/science/journal/00313203"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/0fb238dfbe14943c1b6672c20d417c85381f1ae2",
        "title": "A trainable feature extractor for handwritten digit recognition",
        "abstract": null,
        "venue": "Pattern Recognition",
        "year": 2007,
        "referenceCount": 23,
        "citationCount": 317,
        "influentialCitationCount": 25,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://hal.archives-ouvertes.fr/docs/00/05/75/61/PDF/LauerSuenBlochPR.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A trainable feature extractor based on the LeNet5 convolutional neural network architecture is introduced to solve the first problem in a black box scheme without prior knowledge on the data and the results show that the system can outperform both SVMs and Le net5 while providing performances comparable to the best performance on this database."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2007-06-01",
        "journal": {
            "name": "Pattern Recognit.",
            "pages": "1816-1824",
            "volume": "40"
        },
        "citationStyles": {
            "bibtex": "@Article{Lauer2007ATF,\n author = {Fabien Lauer and C. Suen and G. Bloch},\n booktitle = {Pattern Recognition},\n journal = {Pattern Recognit.},\n pages = {1816-1824},\n title = {A trainable feature extractor for handwritten digit recognition},\n volume = {40},\n year = {2007}\n}\n"
        }
    },
    "312_llama_2-70b": {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "externalIds": {
            "ArXiv": "2307.09288",
            "DBLP": "journals/corr/abs-2307-09288",
            "CorpusId": 259950998
        },
        "corpusId": 259950998,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 131,
        "citationCount": 2639,
        "influentialCitationCount": 430,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work develops and releases Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters, which may be a suitable substitute for closed-source models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-07-18",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2307.09288"
        },
        "citationStyles": {
            "bibtex": "@Article{Touvron2023Llama2O,\n author = {Hugo Touvron and Louis Martin and Kevin R. Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and D. Bikel and Lukas Blecher and Cristian Cant\u00f3n Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and A. Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel M. Kloumann and A. Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and R. Subramanian and Xia Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zhengxu Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},\n volume = {abs/2307.09288},\n year = {2023}\n}\n"
        }
    },
    "313_fine-tuned-awd-lstm-doc(fin)": {
        "paperId": "2ae123c62051e6283df4dc184094b801fea2a445",
        "externalIds": {
            "MAG": "2899852118",
            "DBLP": "journals/corr/abs-1811-04623",
            "ArXiv": "1811.04623",
            "CorpusId": 53279456
        },
        "corpusId": 53279456,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/2ae123c62051e6283df4dc184094b801fea2a445",
        "title": "Fine-tuning of Language Models with Discriminator",
        "abstract": "Cross-entropy loss is a common choice when it comes to multiclass classification tasks and language modeling in particular. Minimizing this loss results in language models of very good quality. We show that it is possible to fine-tune these models and make them perform even better if they are fine-tuned with sum of cross-entropy loss and reverse Kullback-Leibler divergence. The latter is estimated using discriminator network that we train in advance. During fine-tuning we can use this discriminator to figure out if probabilities of some words are overestimated and reduce them in this case. The novel approach that we propose allows us to reach state-of-the-art quality on Penn TreeBank: perplexity of the fine-tuned model drops down by more than 0.5 and is now below 54.0 in standard evaluation setting; however, in dynamic evaluation framework the improvement is much less perceptible. Our fine-tuning algorithm is rather fast and requires almost no hyperparameter tuning. We test it on different datasets including WikiText-2 and large-scale dataset. In the former case we also reach state-of-the-art results.",
        "venue": "arXiv.org",
        "year": 2018,
        "referenceCount": 18,
        "citationCount": 2,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that it is possible to fine-tune language models and make them perform even better if they are fine-tuned with sum of cross-entropy loss and reverse Kullback-Leibler divergence and this algorithm is fast and requires almost no hyperparameter tuning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-11-12",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1811.04623"
        },
        "citationStyles": {
            "bibtex": "@Article{Popov2018FinetuningOL,\n author = {Vadim Popov and M. Kudinov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Fine-tuning of Language Models with Discriminator},\n volume = {abs/1811.04623},\n year = {2018}\n}\n"
        }
    },
    "315_context-dependent_rnn": {
        "paperId": "d1275b2a2ab53013310e759e5c6878b96df643d4",
        "externalIds": {
            "DBLP": "conf/slt/MikolovZ12",
            "MAG": "1999965501",
            "DOI": "10.1109/SLT.2012.6424228",
            "CorpusId": 11383176
        },
        "corpusId": 11383176,
        "publicationVenue": {
            "id": "d8dfb5ba-9312-410c-a361-8ad05f945939",
            "name": "Spoken Language Technology Workshop",
            "type": "conference",
            "alternate_names": [
                "SLT",
                "Spok Lang Technol Workshop"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/d1275b2a2ab53013310e759e5c6878b96df643d4",
        "title": "Context dependent recurrent neural network language model",
        "abstract": "Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe improvements in word-error-rate.",
        "venue": "Spoken Language Technology Workshop",
        "year": 2012,
        "referenceCount": 40,
        "citationCount": 587,
        "influentialCitationCount": 61,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper improves recurrent neural network language models performance by providing a contextual real-valued input vector in association with each word to convey contextual information about the sentence being modeled by performing Latent Dirichlet Allocation using a block of preceding text."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2012-12-01",
        "journal": {
            "name": "2012 IEEE Spoken Language Technology Workshop (SLT)",
            "pages": "234-239"
        },
        "citationStyles": {
            "bibtex": "@Article{Mikolov2012ContextDR,\n author = {Tomas Mikolov and G. Zweig},\n booktitle = {Spoken Language Technology Workshop},\n journal = {2012 IEEE Spoken Language Technology Workshop (SLT)},\n pages = {234-239},\n title = {Context dependent recurrent neural network language model},\n year = {2012}\n}\n"
        }
    },
    "317_transformer-xl_+_sis": {
        "paperId": "ddac4cda451abf2377c5b536903bf31e38f5a3cc",
        "externalIds": {
            "DBLP": "conf/icml/VermaP21",
            "CorpusId": 235825404
        },
        "corpusId": 235825404,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/ddac4cda451abf2377c5b536903bf31e38f5a3cc",
        "title": "Sparsifying Networks via Subdifferential Inclusion",
        "abstract": "Sparsifying deep neural networks is of paramount interest in many areas, especially when those networks have to be implemented on low-memory devices. In this article, we propose a new formulation of the problem of generating sparse weights for a pre-trained neural network. By leveraging the properties of standard nonlinear activation functions, we show that the problem is equivalent to an approximate subdifferential inclusion problem. The accuracy of the approximation controls the sparsity. We show that the proposed approach is valid for a broad class of activation functions (ReLU, sigmoid, softmax). We propose an iterative optimization algorithm to induce sparsity whose convergence is guaranteed. Because of the algorithm \ufb02exibility, the sparsity can be ensured from partial training data in a minibatch manner. To demonstrate the effectiveness of our method, we perform experiments on various networks in different applicative contexts:",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 81,
        "citationCount": 11,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This article proposes an iterative optimization algorithm to induce sparsity whose convergence is guaranteed, and shows that the proposed approach is valid for a broad class of activation functions (ReLU, sigmoid, softmax)."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "10542-10552"
        },
        "citationStyles": {
            "bibtex": "@Article{Verma2021SparsifyingNV,\n author = {S. Verma and J. Pesquet},\n booktitle = {International Conference on Machine Learning},\n pages = {10542-10552},\n title = {Sparsifying Networks via Subdifferential Inclusion},\n year = {2021}\n}\n"
        }
    },
    "318_tensor-transformer(1core)+pn_(wt103)": {
        "paperId": "4076e421d1758fdb68411242044cd45747b7e35b",
        "externalIds": {
            "MAG": "3037973456",
            "DBLP": "conf/icml/ShenYGMK20",
            "CorpusId": 220250313
        },
        "corpusId": 220250313,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/4076e421d1758fdb68411242044cd45747b7e35b",
        "title": "PowerNorm: Rethinking Batch Normalization in Transformers",
        "abstract": "The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at \\url{this https URL}.",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "referenceCount": 55,
        "citationCount": 58,
        "influentialCitationCount": 5,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN, and proposes Power Normalization (PN), a novel normalization scheme that significantly outperforms both LN and BN."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-03-17",
        "journal": {
            "pages": "8741-8751"
        },
        "citationStyles": {
            "bibtex": "@Article{Shen2020PowerNormRB,\n author = {Sheng Shen and Z. Yao and A. Gholami and Michael W. Mahoney and K. Keutzer},\n booktitle = {International Conference on Machine Learning},\n pages = {8741-8751},\n title = {PowerNorm: Rethinking Batch Normalization in Transformers},\n year = {2020}\n}\n"
        }
    },
    "319_codefuse-13b": {
        "paperId": "57eddf6eff27193984314141d04c333c11532fc6",
        "externalIds": {
            "DBLP": "journals/corr/abs-2310-06266",
            "ArXiv": "2310.06266",
            "DOI": "10.1145/3639477.3639719",
            "CorpusId": 263830089
        },
        "corpusId": 263830089,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/57eddf6eff27193984314141d04c333c11532fc6",
        "title": "CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model",
        "abstract": "Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 55,
        "citationCount": 1,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-10-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2310.06266"
        },
        "citationStyles": {
            "bibtex": "@Article{Di2023CodeFuse13BAP,\n author = {Peng Di and Jianguo Li and Hang Yu and Wei Jiang and Wenting Cai and Yang Cao and Chaoyu Chen and Dajun Chen and Hongwei Chen and Liang Chen and Gang Fan and Jie Gong and Zi Gong and Wen Hu and Tingting Guo and Zhichao Lei and Ting Li and Zheng Li and Ming Liang and Cong Liao and Bingchang Liu and Jiachen Liu and Zhiwei Liu and Shaojun Lu and Mingquan Shen and Guangpei Wang and Huan Wang and Z. Wang and Zhaogui Xu and Jiawei Yang and Qing Ye and Gehao Zhang and Yu Zhang and Zelin Zhao and Xunjin Zheng and Hailian Zhou and Lifu Zhu and Xianying Zhu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model},\n volume = {abs/2310.06266},\n year = {2023}\n}\n"
        }
    },
    "320_glee": {
        "paperId": "2f027193fb703d0af58ec382bd1438daff9417d7",
        "externalIds": {
            "MAG": "46130386",
            "CorpusId": 18229198
        },
        "corpusId": 18229198,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/2f027193fb703d0af58ec382bd1438daff9417d7",
        "title": "BOXES: AN EXPERIMENT IN ADAPTIVE CONTROL",
        "abstract": "BOXES is the name of a computer program. We shall also use the word boxes to refer to a particular approach to decision-taking under uncertainty which has been used as the basis of a number of computer programs. Fig. 1 shows a photograph of an assemblage of actual boxes\u2014matchboxes to be exact. Although the construction of this Matchbox Educable Noughts and Crosses Engine (Michie 1961, 1963) was undertaken as a 'fun project', there was present a more serious intention to demonstrate the principle that it may be easier to learn to play many easy games than one difficult one. Consequently it may be advantageous to decompose a game into a number of mutually independent sub-games even if much relevant information is put out of reach in the process. The principle is related to the method of subgoals in problem-solving (see Newell et al. 1960) but differs in one fundamental respect: subgoals are linked in series, while sub-games are played in parallel, in a sense which will become apparent.",
        "venue": "",
        "year": 2013,
        "referenceCount": 10,
        "citationCount": 119,
        "influentialCitationCount": 5,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Although the construction of this Matchbox Educable Noughts and Crosses Engine was undertaken as a 'fun project', there was present a more serious intention to demonstrate the principle that it may be easier to learn to play many easy games than one difficult one."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Michie2013BOXESAE,\n author = {D. Michie and R. A. Chambers},\n title = {BOXES: AN EXPERIMENT IN ADAPTIVE CONTROL},\n year = {2013}\n}\n"
        }
    },
    "321_bayesian_automated_hyperparameter_tuning": {
        "paperId": "2e2089ae76fe914706e6fa90081a79c8fe01611e",
        "externalIds": {
            "ArXiv": "1206.2944",
            "MAG": "2950182411",
            "DBLP": "conf/nips/SnoekLA12",
            "CorpusId": 632197
        },
        "corpusId": 632197,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/2e2089ae76fe914706e6fa90081a79c8fe01611e",
        "title": "Practical Bayesian Optimization of Machine Learning Algorithms",
        "abstract": "The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.",
        "venue": "Neural Information Processing Systems",
        "year": 2012,
        "referenceCount": 28,
        "citationCount": 6671,
        "influentialCitationCount": 519,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work describes new algorithms that take into account the variable cost of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation and shows that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2012-06-13",
        "journal": {
            "pages": "2960-2968"
        },
        "citationStyles": {
            "bibtex": "@Article{Snoek2012PracticalBO,\n author = {Jasper Snoek and H. Larochelle and Ryan P. Adams},\n booktitle = {Neural Information Processing Systems},\n pages = {2960-2968},\n title = {Practical Bayesian Optimization of Machine Learning Algorithms},\n year = {2012}\n}\n"
        }
    },
    "322_densenet-264": {
        "paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c",
        "externalIds": {
            "DBLP": "conf/cvpr/HuangLMW17",
            "MAG": "2963446712",
            "ArXiv": "1608.06993",
            "DOI": "10.1109/CVPR.2017.243",
            "CorpusId": 9433631
        },
        "corpusId": 9433631,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/5694e46284460a648fe29117cbc55f6c9be3fa3c",
        "title": "Densely Connected Convolutional Networks",
        "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections&#x2014;one between each layer and its subsequent layer&#x2014;our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 51,
        "citationCount": 30250,
        "influentialCitationCount": 4202,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1608.06993",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion, and has several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-08-25",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "2261-2269"
        },
        "citationStyles": {
            "bibtex": "@Article{Huang2016DenselyCC,\n author = {Gao Huang and Zhuang Liu and Kilian Q. Weinberger},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2261-2269},\n title = {Densely Connected Convolutional Networks},\n year = {2016}\n}\n"
        }
    },
    "323_4_layer_qrnn_(h=2500)": {
        "paperId": "680aafd3d51e666b297e27b93d9554cc2caf1c4d",
        "externalIds": {
            "DBLP": "journals/corr/abs-1803-08240",
            "ArXiv": "1803.08240",
            "MAG": "2792376130",
            "CorpusId": 4111774
        },
        "corpusId": 4111774,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/680aafd3d51e666b297e27b93d9554cc2caf1c4d",
        "title": "An Analysis of Neural Language Modeling at Multiple Scales",
        "abstract": "Many of the leading approaches in language modeling introduce novel, complex and specialized architectures. We take existing state-of-the-art word level language models based on LSTMs and QRNNs and extend them to both larger vocabularies as well as character-level granularity. When properly tuned, LSTMs and QRNNs achieve state-of-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103) datasets, respectively. Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU.",
        "venue": "arXiv.org",
        "year": 2018,
        "referenceCount": 35,
        "citationCount": 165,
        "influentialCitationCount": 21,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work takes existing state-of-the-art word level language models based on LSTMs and QRNNs and extend them to both larger vocabularies as well as character-level granularity, achieving state- of- the-art results on character- level and word-level datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-03-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1803.08240"
        },
        "citationStyles": {
            "bibtex": "@Article{Merity2018AnAO,\n author = {Stephen Merity and N. Keskar and R. Socher},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An Analysis of Neural Language Modeling at Multiple Scales},\n volume = {abs/1803.08240},\n year = {2018}\n}\n"
        }
    },
    "324_detic": {
        "paperId": "86b42cac364985919987789795be7c3a577ee3de",
        "externalIds": {
            "DBLP": "journals/corr/abs-2201-02605",
            "ArXiv": "2201.02605",
            "DOI": "10.1007/978-3-031-20077-9_21",
            "CorpusId": 245827815
        },
        "corpusId": 245827815,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/86b42cac364985919987789795be7c3a577ee3de",
        "title": "Detecting Twenty-thousand Classes using Image-level Supervision",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2022,
        "referenceCount": 81,
        "citationCount": 319,
        "influentialCitationCount": 72,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2201.02605",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts, making it much easier to implement and compatible with a range of detection architectures and backbones."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-01-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2201.02605"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhou2022DetectingTC,\n author = {Xingyi Zhou and Rohit Girdhar and Armand Joulin and Phillip Krahenbuhl and Ishan Misra},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {Detecting Twenty-thousand Classes using Image-level Supervision},\n volume = {abs/2201.02605},\n year = {2022}\n}\n"
        }
    },
    "325_transformer-xl+adamgapaware(ga)": {
        "paperId": "4b25d3b729875a9efc193c92dc4ca34f6dbb96e2",
        "externalIds": {
            "MAG": "2994911616",
            "ArXiv": "1909.10802",
            "DBLP": "conf/iclr/BarkaiHS20",
            "CorpusId": 202734152
        },
        "corpusId": 202734152,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/4b25d3b729875a9efc193c92dc4ca34f6dbb96e2",
        "title": "Gap Aware Mitigation of Gradient Staleness",
        "abstract": "Cloud computing is becoming increasingly popular as a platform for distributed training of deep neural networks. Synchronous stochastic gradient descent (SSGD) suffers from substantial slowdowns due to stragglers if the environment is non-dedicated, as is common in cloud computing. Asynchronous SGD (ASGD) methods are immune to these slowdowns but are scarcely used due to gradient staleness, which encumbers the convergence process. Recent techniques have had limited success mitigating the gradient staleness when scaling up to many workers (computing nodes). In this paper we define the Gap as a measure of gradient staleness and propose Gap-Aware (GA), a novel asynchronous-distributed method that penalizes stale gradients linearly to the Gap and performs well even when scaling to large numbers of workers. Our evaluation on the CIFAR, ImageNet, and WikiText-103 datasets shows that GA outperforms the currently acceptable gradient penalization method, in final test accuracy. We also provide convergence rate proof for GA. Despite prior beliefs, we show that if GA is applied, momentum becomes beneficial in asynchronous environments, even when the number of workers scales up.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 39,
        "citationCount": 15,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper defines the Gap as a measure of gradient staleness and proposes Gap-Aware (GA), a novel asynchronous-distributed method that penalizes stale gradients linearly to the Gap and performs well even when scaling to large numbers of workers."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-09-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1909.10802"
        },
        "citationStyles": {
            "bibtex": "@Article{Barkai2019GapAM,\n author = {Saar Barkai and Ido Hakimi and A. Schuster},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Gap Aware Mitigation of Gradient Staleness},\n volume = {abs/1909.10802},\n year = {2019}\n}\n"
        }
    },
    "326_simclrv2": {
        "paperId": "3e7f5f4382ac6f9c4fef6197dd21abf74456acd1",
        "externalIds": {
            "DBLP": "conf/nips/ChenKSNH20",
            "MAG": "3100859887",
            "ArXiv": "2006.10029",
            "CorpusId": 219721239
        },
        "corpusId": 219721239,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/3e7f5f4382ac6f9c4fef6197dd21abf74456acd1",
        "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners",
        "abstract": "One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\\% ImageNet top-1 accuracy with just 1\\% of the labels ($\\le$13 labeled images per class) using ResNet-50, a $10\\times$ improvement in label efficiency over the previous state-of-the-art. With 10\\% of labels, ResNet-50 trained with our method achieves 77.5\\% top-1 accuracy, outperforming standard supervised training with all of the labels.",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "referenceCount": 75,
        "citationCount": 1762,
        "influentialCitationCount": 306,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLRs), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-06-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2006.10029"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2020BigSM,\n author = {Ting Chen and Simon Kornblith and Kevin Swersky and Mohammad Norouzi and Geoffrey E. Hinton},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Big Self-Supervised Models are Strong Semi-Supervised Learners},\n volume = {abs/2006.10029},\n year = {2020}\n}\n"
        }
    },
    "327_mbart-50": {
        "paperId": "1f58c42f44113f1c3c8a97c538e78f37f839f4b8",
        "externalIds": {
            "MAG": "3046368065",
            "ArXiv": "2008.00401",
            "DBLP": "journals/corr/abs-2008-00401",
            "CorpusId": 220936592
        },
        "corpusId": 220936592,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/1f58c42f44113f1c3c8a97c538e78f37f839f4b8",
        "title": "Multilingual Translation with Extensible Multilingual Pretraining and Finetuning",
        "abstract": "Recent work demonstrates the potential of multilingual pretraining of creating one model that can be used for various tasks in different languages. Previous work in multilingual pretraining has demonstrated that machine translation systems can be created by finetuning on bitext. In this work, we show that multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one direction, a pretrained model is finetuned on many directions at the same time. Compared to multilingual models trained from scratch, starting from pretrained models incorporates the benefits of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is not available. We demonstrate that pretrained models can be extended to incorporate additional languages without loss of performance. We double the number of languages in mBART to support multilingual machine translation models of 50 languages. Finally, we create the ML50 benchmark, covering low, mid, and high resource languages, to facilitate reproducible research by standardizing training and evaluation data. On ML50, we demonstrate that multilingual finetuning improves on average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while improving 9.3 BLEU on average over bilingual baselines from scratch.",
        "venue": "arXiv.org",
        "year": 2020,
        "referenceCount": 27,
        "citationCount": 318,
        "influentialCitationCount": 57,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work shows that multilingual translation models can be created through multilingual finetuning, and demonstrates that pretrained models can been extended to incorporate additional languages without loss of performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-08-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2008.00401"
        },
        "citationStyles": {
            "bibtex": "@Article{Tang2020MultilingualTW,\n author = {Y. Tang and C. Tran and Xian Li and Peng-Jen Chen and Naman Goyal and Vishrav Chaudhary and Jiatao Gu and Angela Fan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Multilingual Translation with Extensible Multilingual Pretraining and Finetuning},\n volume = {abs/2008.00401},\n year = {2020}\n}\n"
        }
    },
    "328_attend-infer-repeat": {
        "paperId": "2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1",
        "externalIds": {
            "MAG": "2951070067",
            "DBLP": "conf/nips/EslamiHWTSKH16",
            "ArXiv": "1603.08575",
            "CorpusId": 8122361
        },
        "corpusId": 8122361,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1",
        "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
        "abstract": "We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "referenceCount": 45,
        "citationCount": 500,
        "influentialCitationCount": 55,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents a framework for efficient inference in structured image models that explicitly reason about objects by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-03-28",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1603.08575"
        },
        "citationStyles": {
            "bibtex": "@Article{Eslami2016AttendIR,\n author = {S. Eslami and N. Heess and T. Weber and Yuval Tassa and David Szepesvari and K. Kavukcuoglu and Geoffrey E. Hinton},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Attend, Infer, Repeat: Fast Scene Understanding with Generative Models},\n volume = {abs/1603.08575},\n year = {2016}\n}\n"
        }
    },
    "329_transformerxl_+_spectrum_control": {
        "paperId": "7fed15cc79332f83b7bfe920c02a9c954322ddcc",
        "externalIds": {
            "DBLP": "conf/iclr/Wang0HHWG20",
            "MAG": "2996657533",
            "CorpusId": 211145667
        },
        "corpusId": 211145667,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7fed15cc79332f83b7bfe920c02a9c954322ddcc",
        "title": "Improving Neural Language Generation with Spectrum Control",
        "abstract": "Recent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an anisotropic cone in the vector space, which is called the representation degeneration problem. In this paper, we propose a novel spectrum control approach to address this degeneration problem. The core idea of our method is to directly guide the spectra training of the output embedding matrix with a slow-decaying singular value prior distribution through a reparameterization framework. We show that our proposed method encourages isotropy of the learned word representations while maintains the modeling power of these contextual neural models. We further provide a theoretical analysis and insight on the benefit of modeling singular value distribution. We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model, and various Transformer-based models for machine translation, on common benchmark datasets for these tasks.",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "referenceCount": 46,
        "citationCount": 61,
        "influentialCitationCount": 12,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a novel spectrum control approach to directly guide the spectra training of the output embedding matrix with a slow-decaying singular value prior distribution through a reparameterization framework and demonstrates that this method outperforms the state-of-the-art Trans transformer-XL modeling for language model, and various Transformer-based models for machine translation, on common benchmark datasets for these tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-04-30",
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2020ImprovingNL,\n author = {Lingxiao Wang and Jing Huang and Kevin Huang and Ziniu Hu and Guangtao Wang and Quanquan Gu},\n booktitle = {International Conference on Learning Representations},\n title = {Improving Neural Language Generation with Spectrum Control},\n year = {2020}\n}\n"
        }
    },
    "330_gsm": {
        "paperId": "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f",
        "externalIds": {
            "ACL": "P17-1018",
            "MAG": "2740747242",
            "DBLP": "conf/acl/WangYWCZ17",
            "DOI": "10.18653/v1/P17-1018",
            "CorpusId": 12501880
        },
        "corpusId": 12501880,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f",
        "title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering",
        "abstract": "In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2017,
        "referenceCount": 38,
        "citationCount": 646,
        "influentialCitationCount": 106,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P17-1018.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage, are presented and holds the first place on the SQuAD leaderboard for both single and ensemble model."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-07-01",
        "journal": {
            "pages": "189-198"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2017GatedSN,\n author = {Wenhui Wang and Nan Yang and Furu Wei and Baobao Chang and M. Zhou},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {189-198},\n title = {Gated Self-Matching Networks for Reading Comprehension and Question Answering},\n year = {2017}\n}\n"
        }
    },
    "331_encodec": {
        "paperId": "cdcfeb447fa8554c131c0a13a7ffcba30c0381e1",
        "externalIds": {
            "DBLP": "journals/corr/abs-2210-13438",
            "ArXiv": "2210.13438",
            "DOI": "10.48550/arXiv.2210.13438",
            "CorpusId": 253097788
        },
        "corpusId": 253097788,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/cdcfeb447fa8554c131c0a13a7ffcba30c0381e1",
        "title": "High Fidelity Neural Audio Compression",
        "abstract": "We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 76,
        "citationCount": 169,
        "influentialCitationCount": 41,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.13438",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Engineering",
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel loss balancer mechanism to stabilize training is introduced: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.13438"
        },
        "citationStyles": {
            "bibtex": "@Article{D'efossez2022HighFN,\n author = {Alexandre D'efossez and Jade Copet and Gabriel Synnaeve and Yossi Adi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {High Fidelity Neural Audio Compression},\n volume = {abs/2210.13438},\n year = {2022}\n}\n"
        }
    },
    "332_multi-cause_binary_clustering": {
        "paperId": "fd7d7767129ed180db39d38be28c1ae389481d2f",
        "externalIds": {
            "MAG": "2042912927",
            "DBLP": "journals/neco/Saund95",
            "DOI": "10.1162/neco.1995.7.1.51",
            "CorpusId": 18231498
        },
        "corpusId": 18231498,
        "publicationVenue": {
            "id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
            "name": "Neural Computation",
            "type": "journal",
            "alternate_names": [
                "Neural Comput"
            ],
            "issn": "0899-7667",
            "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667",
            "alternate_urls": [
                "http://ieeexplore.ieee.org/servlet/opac?punumber=6720226",
                "http://www.mitpressjournals.org/loi/neco",
                "https://www.mitpressjournals.org/loi/neco"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/fd7d7767129ed180db39d38be28c1ae389481d2f",
        "title": "A Multiple Cause Mixture Model for Unsupervised Learning",
        "abstract": "This paper presents a formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data. Unlike the hard k-means clustering algorithm and the soft mixture model, each of which assumes that a single hidden event generates each data point, a multiple cause model accounts for observed data by combining assertions from many hidden causes, each of which can pertain to varying degree to any subset of the observable dimensions. We employ an objective function and iterative gradient descent learning algorithm resembling the conventional mixture model. A crucial issue is the mixing function for combining beliefs from different cluster centers in order to generate data predictions whose errors are minimized both during recognition and learning. The mixing function constitutes a prior assumption about underlying structural regularities of the data domain; we demonstrate a weakness inherent to the popular weighted sum followed by sigmoid squashing, and offer alternative forms of the nonlinearity for two types of data domain. Results are presented demonstrating the algorithm's ability successfully to discover coherent multiple causal representations in several experimental data sets.",
        "venue": "Neural Computation",
        "year": 1995,
        "referenceCount": 17,
        "citationCount": 143,
        "influentialCitationCount": 14,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data, which employs an objective function and iterative gradient descent learning algorithm resembling the conventional mixture model and demonstrates its ability to discover coherent multiple causal representations in several experimental data sets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1995-01-02",
        "journal": {
            "name": "Neural Computation",
            "pages": "51-71",
            "volume": "7"
        },
        "citationStyles": {
            "bibtex": "@Article{Saund1995AMC,\n author = {E. Saund},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {51-71},\n title = {A Multiple Cause Mixture Model for Unsupervised Learning},\n volume = {7},\n year = {1995}\n}\n"
        }
    },
    "333_dit-xl_2": {
        "paperId": "736973165f98105fec3729b7db414ae4d80fcbeb",
        "externalIds": {
            "DBLP": "journals/corr/abs-2212-09748",
            "ArXiv": "2212.09748",
            "DOI": "10.1109/ICCV51070.2023.00387",
            "CorpusId": 254854389
        },
        "corpusId": 254854389,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/736973165f98105fec3729b7db414ae4d80fcbeb",
        "title": "Scalable Diffusion Models with Transformers",
        "abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops\u2014through increased transformer depth/width or increased number of input tokens\u2014consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512\u00d7512 and 256\u00d7256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2022,
        "referenceCount": 69,
        "citationCount": 224,
        "influentialCitationCount": 51,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2212.09748",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new class of diffusion models based on the transformer architecture is explored, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches that outperform all prior diffusion models on the class-conditional ImageNet 512\u00d7512 and 256\u00d7256 benchmarks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-12-19",
        "journal": {
            "name": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "pages": "4172-4182"
        },
        "citationStyles": {
            "bibtex": "@Article{Peebles2022ScalableDM,\n author = {William S. Peebles and Saining Xie},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {4172-4182},\n title = {Scalable Diffusion Models with Transformers},\n year = {2022}\n}\n"
        }
    },
    "334_distilprotbert": {
        "paperId": "af87344d3f3cc58147f5a71f13efa9aab0b2efcc",
        "externalIds": {
            "DOI": "10.1101/2022.05.09.491157",
            "CorpusId": 248754159,
            "PubMed": "36124789"
        },
        "corpusId": 248754159,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/af87344d3f3cc58147f5a71f13efa9aab0b2efcc",
        "title": "DistilProtBert: A distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts",
        "abstract": "Summary Recently, Deep Learning models, initially developed in the field of Natural Language Processing (NLP), were applied successfully to analyze protein sequences. A major drawback of these models is their size in terms of the number of parameters needed to be fitted and the amount of computational resources they require. Recently, \u201cdistilled\u201d models using the concept of student and teacher networks have been widely used in NLP. Here, we adapted this concept to the problem of protein sequence analysis, by developing DistilProtBert, a distilled version of the successful ProtBert model. Implementing this approach, we reduced the size of the network and the running time by 50%, and the computational resources needed for pretraining by 98% relative to ProtBert model. Using two published tasks, we showed that the performance of the distilled model approaches that of the full model. We next tested the ability of DistilProtBert to distinguish between real and random protein sequences. The task is highly challenging if the composition is maintained on the level of singlet, doublet and triplet amino acids. Indeed, traditional machine learning algorithms have difficulties with this task. Here, we show that DistilProtBert preforms very well on singlet, doublet, and even triplet-shuffled versions of the human proteome, with AUC of 0.92, 0.91, and 0.87 respectively. Finally, we suggest that by examining the small number of false-positive classifications (i.e., shuffled sequences classified as proteins by DistilProtBert) we may be able to identify de-novo potential natural-like proteins based on random shuffling of amino acid sequences. Availability https://github.com/yarongef/DistilProtBert Contact yaron.geffen@biu.ac.il",
        "venue": "bioRxiv",
        "year": 2022,
        "referenceCount": 28,
        "citationCount": 15,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://academic.oup.com/bioinformatics/article-pdf/38/Supplement_2/ii95/50485222/btac474.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Medicine",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that DistilProtBert preforms very well on singlet, doublet, and even triplet-shuffled versions of the human proteome, with AUC of 0.92, 0.91, and 0.87, and it is suggested that by examining the small number of false-positive classifications the authors may be able to identify de-novo potential natural-like proteins based on random shuffling of amino acid sequences."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-05-10",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Geffen2022DistilProtBertAD,\n author = {Yaron Geffen and Yanay Ofran and R. Unger},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {DistilProtBert: A distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts},\n year = {2022}\n}\n"
        }
    },
    "335_regularized_svd_for_collaborative_filtering": {
        "paperId": "f732d0f69fe4e84a95c32706b28b9e4ef1753c61",
        "externalIds": {
            "MAG": "2908831574",
            "CorpusId": 4181849
        },
        "corpusId": 4181849,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/f732d0f69fe4e84a95c32706b28b9e4ef1753c61",
        "title": "Improving regularized singular value decomposition for collaborative filtering",
        "abstract": "A key part of a recommender system is a collaborative filtering algorithm predicting users\u2019 preferences for items. In this paper we describe different efficient collaborative filtering techniques and a framework for combining them to obtain a good prediction. The methods described in this paper are the most important parts of a solution predicting users\u2019 preferences for movies with error rate 7.04% better on the Netflix Prize dataset than the reference algorithm Netflix Cinematch. The set of predictors used includes algorithms suggested by Netflix Prize contestants: regularized singular value decomposition of data with missing values, K-means, postprocessing SVD with KNN. We propose extending the set of predictors with the following methods: addition of biases to the regularized SVD, postprocessing SVD with kernel ridge regression, using a separate linear model for each movie, and using methods similar to the regularized SVD, but with fewer parameters. All predictors and selected 2-way interactions between them are combined using linear regression on a holdout set.",
        "venue": "",
        "year": 2007,
        "referenceCount": 9,
        "citationCount": 848,
        "influentialCitationCount": 103,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Different efficient collaborative filtering techniques and a framework for combining them to obtain a good prediction are described, predicting users\u2019 preferences for movies with error rate 7.04% better on the Netflix Prize dataset than the reference algorithm Netflix Cinematch."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Paterek2007ImprovingRS,\n author = {Arkadiusz Paterek},\n title = {Improving regularized singular value decomposition for collaborative filtering},\n year = {2007}\n}\n"
        }
    },
    "336_contriever": {
        "paperId": "4f4a409f701f7552d45c46a5b0fea69dca6f8e84",
        "externalIds": {
            "ArXiv": "2112.09118",
            "DBLP": "journals/tmlr/IzacardCHRBJG22",
            "CorpusId": 249097975
        },
        "corpusId": 249097975,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/4f4a409f701f7552d45c46a5b0fea69dca6f8e84",
        "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
        "abstract": "Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.",
        "venue": "Trans. Mach. Learn. Res.",
        "year": 2021,
        "referenceCount": 68,
        "citationCount": 267,
        "influentialCitationCount": 63,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work explores the limits of contrastive learning as a way to train unsupervised dense retrievers and shows that it leads to strong performance in various retrieval settings and can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-12-16",
        "journal": {
            "name": "Trans. Mach. Learn. Res.",
            "volume": "2022"
        },
        "citationStyles": {
            "bibtex": "@Article{Izacard2021UnsupervisedDI,\n author = {Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},\n booktitle = {Trans. Mach. Learn. Res.},\n journal = {Trans. Mach. Learn. Res.},\n title = {Unsupervised Dense Information Retrieval with Contrastive Learning},\n volume = {2022},\n year = {2021}\n}\n"
        }
    },
    "338_perceptron_(1960)": {
        "paperId": "ae76ce1ba27ac29addce4aab93b927e9bc7f7c67",
        "externalIds": {
            "MAG": "2136166259",
            "DOI": "10.1109/JRPROC.1960.287598",
            "CorpusId": 51656509
        },
        "corpusId": 51656509,
        "publicationVenue": {
            "id": "82b56675-f0f3-487f-b83e-155a762e2855",
            "name": "Proceedings of the IRE",
            "alternate_names": [
                "Proc IRE"
            ],
            "issn": "0096-8390",
            "alternate_issns": [
                "2162-6634"
            ],
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=10933"
        },
        "url": "https://www.semanticscholar.org/paper/ae76ce1ba27ac29addce4aab93b927e9bc7f7c67",
        "title": "Perceptron Simulation Experiments",
        "abstract": "An experimental simulation program, which has been in progress at the Cornell Aeronautical Laboratory since 1957, is described. This program uses the IBM 704 computer to simulate perceptual learning, recognition, and spontaneous classification of visual stimuli in the perceptron, a theoretical brain model which has been described elsewhere. The paper includes a brief review of the organization of simple perceptrons, and theoretically predicted performance curves are compared with those obtained from the simulation programs, in several types of experiments, designed to study \"forced\" and \"spontaneous\" learning of pattern discriminations.",
        "venue": "Proceedings of the IRE",
        "year": 1960,
        "referenceCount": 4,
        "citationCount": 152,
        "influentialCitationCount": 4,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The paper includes a brief review of the organization of simple perceptrons, and theoretically predicted performance curves are compared with those obtained from the simulation programs, in several types of experiments, designed to study \"forced\" and \"spontaneous\" learning of pattern discriminations."
        },
        "publicationTypes": [
            "Review"
        ],
        "publicationDate": "1960-03-01",
        "journal": {
            "name": "Proceedings of the IRE",
            "pages": "301-309",
            "volume": "48"
        },
        "citationStyles": {
            "bibtex": "@Article{Rosenblatt1960PerceptronSE,\n author = {F. Rosenblatt},\n booktitle = {Proceedings of the IRE},\n journal = {Proceedings of the IRE},\n pages = {301-309},\n title = {Perceptron Simulation Experiments},\n volume = {48},\n year = {1960}\n}\n"
        }
    },
    "341_word_representations": {
        "paperId": "8492070dc4031ed825e95e4803781752bb5e909f",
        "externalIds": {
            "ACL": "P10-1040",
            "MAG": "2158139315",
            "DBLP": "conf/acl/TurianRB10",
            "CorpusId": 629094
        },
        "corpusId": 629094,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/8492070dc4031ed825e95e4803781752bb5e909f",
        "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning",
        "abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2010,
        "referenceCount": 52,
        "citationCount": 2269,
        "influentialCitationCount": 156,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work evaluates Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeds of words on both NER and chunking, and finds that each of the three word representations improves the accuracy of these baselines."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2010-07-11",
        "journal": {
            "pages": "384-394"
        },
        "citationStyles": {
            "bibtex": "@Article{Turian2010WordRA,\n author = {Joseph P. Turian and Lev-Arie Ratinov and Yoshua Bengio},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {384-394},\n title = {Word Representations: A Simple and General Method for Semi-Supervised Learning},\n year = {2010}\n}\n"
        }
    },
    "342_transformer_-_librivox_+_decoding_rescoring": {
        "paperId": "24472a31618bbc260e2bf45bd72427097875142b",
        "externalIds": {
            "ArXiv": "1911.08460",
            "MAG": "2991213871",
            "DBLP": "journals/corr/abs-1911-08460",
            "CorpusId": 208158190
        },
        "corpusId": 208158190,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/24472a31618bbc260e2bf45bd72427097875142b",
        "title": "End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures",
        "abstract": "We study pseudo-labeling for the semi-supervised training of ResNet, Time-Depth Separable ConvNets, and Transformers for speech recognition, with either CTC or Seq2Seq loss functions. We perform experiments on the standard LibriSpeech dataset, and leverage additional unlabeled data from LibriVox through pseudo-labeling. We show that while Transformer-based acoustic models have superior performance with the supervised dataset alone, semi-supervision improves all models across architectures and loss functions and bridges much of the performance gaps between them. In doing so, we reach a new state-of-the-art for end-to-end acoustic models decoded with an external language model in the standard supervised learning setting, and a new absolute state-of-the-art with semi-supervised training. Finally, we study the effect of leveraging different amounts of unlabeled audio, propose several ways of evaluating the characteristics of unlabeled audio which improve acoustic modeling, and show that acoustic models trained with more audio rely less on external language models.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 55,
        "citationCount": 216,
        "influentialCitationCount": 13,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work studies pseudo-labeling for the semi-supervised training of ResNet, Time-Depth Separable ConvNets, and Transformers for speech recognition, with either CTC or Seq2Seq loss functions, and reaches a new state-of-the-art for end-to-end acoustic models decoded with an external language model in the standard supervised learning setting."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-11-19",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1911.08460"
        },
        "citationStyles": {
            "bibtex": "@Article{Synnaeve2019EndtoendAF,\n author = {Gabriel Synnaeve and Qiantong Xu and Jacob Kahn and Edouard Grave and Tatiana Likhomanenko and Vineel Pratap and Anuroop Sriram and Vitaliy Liptchinsky and R. Collobert},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures},\n volume = {abs/1911.08460},\n year = {2019}\n}\n"
        }
    },
    "343_dall-e": {
        "paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "externalIds": {
            "DBLP": "journals/corr/abs-2102-12092",
            "MAG": "3170016573",
            "ArXiv": "2102.12092",
            "CorpusId": 232035663
        },
        "corpusId": 232035663,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "title": "Zero-Shot Text-to-Image Generation",
        "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 64,
        "citationCount": 2815,
        "influentialCitationCount": 291,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work describes a simple approach based on a transformer that autoregressively models the text and image tokens as a single stream of data that is competitive with previous domain-specific models when evaluated in a zero-shot fashion."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-02-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2102.12092"
        },
        "citationStyles": {
            "bibtex": "@Article{Ramesh2021ZeroShotTG,\n author = {A. Ramesh and Mikhail Pavlov and Gabriel Goh and S. Gray and Chelsea Voss and Alec Radford and Mark Chen and I. Sutskever},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Zero-Shot Text-to-Image Generation},\n volume = {abs/2102.12092},\n year = {2021}\n}\n"
        }
    },
    "344_altclip": {
        "paperId": "4cee5b151e0f309b070525a4252a25bbb10bc0a7",
        "externalIds": {
            "DBLP": "journals/corr/abs-2211-06679",
            "ArXiv": "2211.06679",
            "DOI": "10.48550/arXiv.2211.06679",
            "CorpusId": 253511222
        },
        "corpusId": 253511222,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/4cee5b151e0f309b070525a4252a25bbb10bc0a7",
        "title": "AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities",
        "abstract": "In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model. Starting from the pre-trained multimodal representation model CLIP released by OpenAI, we altered its text encoder with a pre-trained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at https://github.com/FlagAI-Open/FlagAI.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2022,
        "referenceCount": 60,
        "citationCount": 37,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2211.06679",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "very close performances with CLIP on almost all tasks are obtained, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-11-12",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2211.06679"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2022AltCLIPAT,\n author = {Zhongzhi Chen and Guangyi Liu and Bo Zhang and Fulong Ye and Qinghong Yang and Ledell Yu Wu},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities},\n volume = {abs/2211.06679},\n year = {2022}\n}\n"
        }
    },
    "345_basic-l_+_lion": {
        "paperId": "a7f59b2162ae0ea2520753b1b9b17277490a9458",
        "externalIds": {
            "DBLP": "journals/corr/abs-2302-06675",
            "ArXiv": "2302.06675",
            "DOI": "10.48550/arXiv.2302.06675",
            "CorpusId": 256846990
        },
        "corpusId": 256846990,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/a7f59b2162ae0ea2520753b1b9b17277490a9458",
        "title": "Symbolic Discovery of Optimization Algorithms",
        "abstract": "We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\\textbf{Lion}$ ($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and 91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "referenceCount": 109,
        "citationCount": 135,
        "influentialCitationCount": 28,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.06675",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Lion is a simple and effective optimization algorithm that requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function and is more memory-efficient than Adam as it only keeps track of the momentum."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-02-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2302.06675"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2023SymbolicDO,\n author = {Xiangning Chen and Chen Liang and Da Huang and Esteban Real and Kaiyuan Wang and Yao Liu and Hieu Pham and Xuanyi Dong and Thang Luong and Cho-Jui Hsieh and Yifeng Lu and Quoc V. Le},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Symbolic Discovery of Optimization Algorithms},\n volume = {abs/2302.06675},\n year = {2023}\n}\n"
        }
    },
    "347_alphastar": {
        "paperId": "361c00b22e29d0816ca896513d2c165e26399821",
        "externalIds": {
            "DBLP": "journals/nature/VinyalsBCMDCCPE19",
            "MAG": "2982316857",
            "DOI": "10.1038/s41586-019-1724-z",
            "CorpusId": 204972004,
            "PubMed": "31666705"
        },
        "corpusId": 204972004,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/361c00b22e29d0816ca896513d2c165e26399821",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
        "abstract": null,
        "venue": "Nature",
        "year": 2019,
        "referenceCount": 59,
        "citationCount": 2835,
        "influentialCitationCount": 135,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The agent, AlphaStar, is evaluated, which uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ranking among the top 0.2% of human players for the real-time strategy game StarCraft II."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-10-30",
        "journal": {
            "name": "Nature",
            "pages": "350 - 354",
            "volume": "575"
        },
        "citationStyles": {
            "bibtex": "@Article{Vinyals2019GrandmasterLI,\n author = {O. Vinyals and Igor Babuschkin and Wojciech M. Czarnecki and Micha\u00ebl Mathieu and A. Dudzik and Junyoung Chung and David Choi and Richard Powell and Timo Ewalds and Petko Georgiev and Junhyuk Oh and Dan Horgan and M. Kroiss and Ivo Danihelka and Aja Huang and L. Sifre and Trevor Cai and J. Agapiou and Max Jaderberg and A. Vezhnevets and R\u00e9mi Leblond and Tobias Pohlen and Valentin Dalibard and D. Budden and Yury Sulsky and James Molloy and T. Paine and Caglar Gulcehre and Ziyun Wang and T. Pfaff and Yuhuai Wu and Roman Ring and Dani Yogatama and Dario W\u00fcnsch and Katrina McKinney and Oliver Smith and T. Schaul and T. Lillicrap and K. Kavukcuoglu and D. Hassabis and C. Apps and David Silver},\n booktitle = {Nature},\n journal = {Nature},\n pages = {350 - 354},\n title = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},\n volume = {575},\n year = {2019}\n}\n"
        }
    },
    "348_transformer-xl_+_rms_dynamic_eval": {
        "paperId": "cd63025532a62fa245a02ec05e32ac4d23089631",
        "externalIds": {
            "MAG": "2936652946",
            "ArXiv": "1904.08378",
            "DBLP": "journals/corr/abs-1904-08378",
            "CorpusId": 119296728
        },
        "corpusId": 119296728,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/cd63025532a62fa245a02ec05e32ac4d23089631",
        "title": "Dynamic Evaluation of Transformer Language Models",
        "abstract": "This research note combines two methods that have recently improved the state of the art in language modeling: Transformers and dynamic evaluation. Transformers use stacked layers of self-attention that allow them to capture long range dependencies in sequential data. Dynamic evaluation fits models to the recent sequence history, allowing them to assign higher probabilities to re-occurring sequential patterns. By applying dynamic evaluation to Transformer-XL models, we improve the state of the art on enwik8 from 0.99 to 0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3 to 16.4 perplexity points.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 25,
        "citationCount": 36,
        "influentialCitationCount": 2,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This research note combines two methods that have recently improved the state of the art in language modeling: Transformers and dynamic evaluation, and applies dynamic evaluation to Transformer-XL models to improve the state on enwik8, text8, and WikiText-103."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-04-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1904.08378"
        },
        "citationStyles": {
            "bibtex": "@Article{Krause2019DynamicEO,\n author = {Ben Krause and Emmanuel Kahembwe and Iain Murray and S. Renals},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dynamic Evaluation of Transformer Language Models},\n volume = {abs/1904.08378},\n year = {2019}\n}\n"
        }
    },
    "349_bidirectional_rnn": {
        "paperId": "e23c34414e66118ecd9b08cf0cd4d016f59b0b85",
        "externalIds": {
            "DBLP": "journals/tsp/SchusterP97",
            "MAG": "2131774270",
            "DOI": "10.1109/78.650093",
            "CorpusId": 18375389
        },
        "corpusId": 18375389,
        "publicationVenue": {
            "id": "1f6f3f05-6a23-42f0-8d31-98ab8089c1f2",
            "name": "IEEE Transactions on Signal Processing",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Signal Process"
            ],
            "issn": "1053-587X",
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=78",
            "alternate_urls": [
                "http://www.signalprocessingsociety.org/publications/periodicals/tsp/",
                "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=78"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e23c34414e66118ecd9b08cf0cd4d016f59b0b85",
        "title": "Bidirectional recurrent neural networks",
        "abstract": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.",
        "venue": "IEEE Transactions on Signal Processing",
        "year": 1997,
        "referenceCount": 18,
        "citationCount": 7239,
        "influentialCitationCount": 695,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1997-11-01",
        "journal": {
            "name": "IEEE Trans. Signal Process.",
            "pages": "2673-2681",
            "volume": "45"
        },
        "citationStyles": {
            "bibtex": "@Article{Schuster1997BidirectionalRN,\n author = {M. Schuster and K. Paliwal},\n booktitle = {IEEE Transactions on Signal Processing},\n journal = {IEEE Trans. Signal Process.},\n pages = {2673-2681},\n title = {Bidirectional recurrent neural networks},\n volume = {45},\n year = {1997}\n}\n"
        }
    },
    "350_openai_five": {
        "paperId": "b19729b27a1b4c24b52f87308c907653300afa7f",
        "externalIds": {
            "MAG": "2996037775",
            "DBLP": "journals/corr/abs-1912-06680",
            "ArXiv": "1912.06680",
            "CorpusId": 209376771
        },
        "corpusId": 209376771,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/b19729b27a1b4c24b52f87308c907653300afa7f",
        "title": "Dota 2 with Large Scale Deep Reinforcement Learning",
        "abstract": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 50,
        "citationCount": 1400,
        "influentialCitationCount": 76,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-12-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1912.06680"
        },
        "citationStyles": {
            "bibtex": "@Article{Berner2019Dota2W,\n author = {Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemyslaw Debiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Christopher Hesse and R. J\u00f3zefowicz and S. Gray and Catherine Olsson and J. Pachocki and Michael Petrov and Henrique Pond\u00e9 de Oliveira Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and I. Sutskever and Jie Tang and Filip Wolski and Susan Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dota 2 with Large Scale Deep Reinforcement Learning},\n volume = {abs/1912.06680},\n year = {2019}\n}\n"
        }
    },
    "352_transformer_local-attention_(nest-b)": {
        "paperId": "f80775a79d42a1ddfc0df808ea760c57af4949d0",
        "externalIds": {
            "DBLP": "conf/aaai/ZhangZ0CAP22",
            "ArXiv": "2105.12723",
            "DOI": "10.1609/aaai.v36i3.20252",
            "CorpusId": 245353951
        },
        "corpusId": 245353951,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/f80775a79d42a1ddfc0df808ea760c57af4949d0",
        "title": "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",
        "abstract": "Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8 times faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer.",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2021,
        "referenceCount": 71,
        "citationCount": 103,
        "influentialCitationCount": 10,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/20252/20011",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper explores the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way and finds that the block aggregation function plays a critical role in enabling cross-block non-local information communication."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-05-26",
        "journal": {
            "pages": "3417-3425"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2021NestedHT,\n author = {Zizhao Zhang and Han Zhang and Long Zhao and Ting Chen and Sercan \u00d6. Arik and Tomas Pfister},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {3417-3425},\n title = {Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding},\n year = {2021}\n}\n"
        }
    },
    "353_continuous_speech_recognition_by_statistical_methods": {
        "paperId": "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9",
        "externalIds": {
            "MAG": "2160721289",
            "DOI": "10.1109/PROC.1976.10159",
            "CorpusId": 31408841
        },
        "corpusId": 31408841,
        "publicationVenue": {
            "id": "6faaccca-1cc4-45a9-aeb6-96a4901d2606",
            "name": "Proceedings of the IEEE",
            "type": "journal",
            "alternate_names": [
                "Proc IEEE"
            ],
            "issn": "0018-9219",
            "alternate_issns": [
                "1558-2256"
            ],
            "url": "http://www.ieee.org/portal/pages/pubs/proceedings/",
            "alternate_urls": [
                "http://www.ieee.org/products/onlinepubs/pub/about_conference.html",
                "https://ieeexplore.ieee.org/servlet/opac?punumber=5",
                "http://proceedingsoftheieee.ieee.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/32a175b36ec7f2f08cb3dfac30ce141e144ec9e9",
        "title": "Continuous speech recognition by statistical methods",
        "abstract": "Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.",
        "venue": "Proceedings of the IEEE",
        "year": 1976,
        "referenceCount": 30,
        "citationCount": 1039,
        "influentialCitationCount": 43,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experimental results are presented that indicate the power of the methods and concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding."
        },
        "publicationTypes": null,
        "publicationDate": "1976-04-01",
        "journal": {
            "name": "Proceedings of the IEEE",
            "pages": "532-556",
            "volume": "64"
        },
        "citationStyles": {
            "bibtex": "@Article{Jelinek1976ContinuousSR,\n author = {F. Jelinek},\n booktitle = {Proceedings of the IEEE},\n journal = {Proceedings of the IEEE},\n pages = {532-556},\n title = {Continuous speech recognition by statistical methods},\n volume = {64},\n year = {1976}\n}\n"
        }
    },
    "354_quantized_admm": {
        "paperId": "7d0a35d9668142d9365de7666c7201321341c0dc",
        "externalIds": {
            "DBLP": "journals/corr/abs-2111-14836",
            "MAG": "3016230677",
            "ArXiv": "2111.14836",
            "DOI": "10.1109/ICASSP40776.2020.9053483",
            "CorpusId": 216526299
        },
        "corpusId": 216526299,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/7d0a35d9668142d9365de7666c7201321341c0dc",
        "title": "Low-bit Quantization of Recurrent Neural Network Language Models Using Alternating Direction Methods of Multipliers",
        "abstract": "The high memory consumption and computational costs of Recurrent neural network language models (RNNLMs) limit their wider application on resource constrained devices. In recent years, neural network quantization techniques that are capable of producing extremely low-bit compression, for example, binarized RNNLMs, are gaining increasing research interests. Directly training of quantized neural networks is difficult. By formulating quantized RNNLMs training as an optimization problem, this paper presents a novel method to train quantized RNNLMs from scratch using alternating direction methods of multipliers (ADMM). This method can also flexibly adjust the trade-off between the compression rate and model performance using tied low-bit quantization tables. Experiments on two tasks: Penn Treebank (PTB), and Switchboard (SWBD) suggest the proposed ADMM quantization achieved a model size compression factor of up to 31 times over the full precision baseline RNNLMs. Faster convergence of 5 times in model training over the baseline binarized RNNLM quantization was also obtained.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2020,
        "referenceCount": 36,
        "citationCount": 7,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2111.14836",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a novel method to train quantized RNNLMs from scratch using alternating direction methods of multipliers (ADMM) and can also flexibly adjust the trade-off between the compression rate and model performance using tied low-bit quantization tables."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-05-01",
        "journal": {
            "name": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "pages": "7939-7943"
        },
        "citationStyles": {
            "bibtex": "@Article{Xu2020LowbitQO,\n author = {Junhao Xu and Xie Chen and Shoukang Hu and Jianwei Yu and Xunying Liu and H. Meng},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {7939-7943},\n title = {Low-bit Quantization of Recurrent Neural Network Language Models Using Alternating Direction Methods of Multipliers},\n year = {2020}\n}\n"
        }
    },
    "355_part-of-sentence_tagging_model": {
        "paperId": "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4",
        "externalIds": {
            "DBLP": "conf/acl/MaH16",
            "ArXiv": "1603.01354",
            "MAG": "2295030615",
            "ACL": "P16-1101",
            "DOI": "10.18653/v1/P16-1101",
            "CorpusId": 10489017
        },
        "corpusId": 10489017,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4",
        "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2016,
        "referenceCount": 64,
        "citationCount": 2448,
        "influentialCitationCount": 299,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P16-1101.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel neutral network architecture is introduced that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF, thus making it applicable to a wide range of sequence labeling tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-03-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1603.01354"
        },
        "citationStyles": {
            "bibtex": "@Article{Ma2016EndtoendSL,\n author = {Xuezhe Ma and E. Hovy},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF},\n volume = {abs/1603.01354},\n year = {2016}\n}\n"
        }
    },
    "356_sest": {
        "paperId": "d2ad6ae5f57844c4474cd3f318c3cdc469994fae",
        "externalIds": {
            "ACL": "D17-1085",
            "ArXiv": "1703.00572",
            "DBLP": "journals/corr/LiuHWYN17",
            "MAG": "2951529757",
            "DOI": "10.18653/v1/D17-1085",
            "CorpusId": 2265207
        },
        "corpusId": 2265207,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/d2ad6ae5f57844c4474cd3f318c3cdc469994fae",
        "title": "Structural Embedding of Syntactic Trees for Machine Comprehension",
        "abstract": "Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2017,
        "referenceCount": 27,
        "citationCount": 48,
        "influentialCitationCount": 6,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D17-1085.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experimental results demonstrate that the proposed structural embedding of syntactic trees (SEST) can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-03-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1703.00572"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2017StructuralEO,\n author = {R. Liu and Junjie Hu and Wei Wei and Zi Yang and Eric Nyberg},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Structural Embedding of Syntactic Trees for Machine Comprehension},\n volume = {abs/1703.00572},\n year = {2017}\n}\n"
        }
    },
    "357_ei-rehn-1000d": {
        "paperId": "ca969c5ab22d8587d9fe1d0c8a1780aa26d5da3d",
        "externalIds": {
            "ArXiv": "1708.04116",
            "DBLP": "journals/corr/abs-1708-04116",
            "MAG": "2746975868",
            "CorpusId": 33653047
        },
        "corpusId": 33653047,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/ca969c5ab22d8587d9fe1d0c8a1780aa26d5da3d",
        "title": "Early Improving Recurrent Elastic Highway Network",
        "abstract": "To model time-varying nonlinear temporal dynamics in sequential data, a recurrent network capable of varying and adjusting the recurrence depth between input intervals is examined. The recurrence depth is extended by several intermediate hidden state units, and the weight parameters involved in determining these units are dynamically calculated. The motivation behind the paper lies on overcoming a deficiency in Recurrent Highway Networks and improving their performances which are currently at the forefront of RNNs: 1) Determining the appropriate number of recurrent depth in RHN for different tasks is a huge burden and just setting it to a large number is computationally wasteful with possible repercussion in terms of performance degradation and high latency. Expanding on the idea of adaptive computation time (ACT), with the use of an elastic gate in the form of a rectified exponentially decreasing function taking on as arguments as previous hidden state and input, the proposed model is able to evaluate the appropriate recurrent depth for each input. The rectified gating function enables the most significant intermediate hidden state updates to come early such that significant performance gain is achieved early. 2) Updating the weights from that of previous intermediate layer offers a richer representation than the use of shared weights across all intermediate recurrence layers. The weight update procedure is just an expansion of the idea underlying hypernetworks. To substantiate the effectiveness of the proposed network, we conducted three experiments: regression on synthetic data, human activity recognition, and language modeling on the Penn Treebank dataset. The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments.",
        "venue": "arXiv.org",
        "year": 2017,
        "referenceCount": 21,
        "citationCount": 5,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "To model time-varying nonlinear temporal dynamics in sequential data, a recurrent network capable of varying and adjusting the recurrence depth between input intervals is examined and shows better performance than other state-of-the-art recurrent networks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-08-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1708.04116"
        },
        "citationStyles": {
            "bibtex": "@Article{Park2017EarlyIR,\n author = {Hyunsin Park and C. Yoo},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Early Improving Recurrent Elastic Highway Network},\n volume = {abs/1708.04116},\n year = {2017}\n}\n"
        }
    },
    "358_wide_residual_network": {
        "paperId": "1c4e9156ca07705531e45960b7a919dc473abb51",
        "externalIds": {
            "MAG": "2964137095",
            "DBLP": "conf/bmvc/ZagoruykoK16",
            "ArXiv": "1605.07146",
            "DOI": "10.5244/C.30.87",
            "CorpusId": 15276198
        },
        "corpusId": 15276198,
        "publicationVenue": {
            "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
            "name": "British Machine Vision Conference",
            "type": "conference",
            "alternate_names": [
                "Br Mach Vis Conf",
                "BMVC"
            ],
            "url": "http://www.bmva.org/bmvc/"
        },
        "url": "https://www.semanticscholar.org/paper/1c4e9156ca07705531e45960b7a919dc473abb51",
        "title": "Wide Residual Networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL",
        "venue": "British Machine Vision Conference",
        "year": 2016,
        "referenceCount": 32,
        "citationCount": 6693,
        "influentialCitationCount": 1147,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.bmva.org/bmvc/2016/papers/paper087/abstract087.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper conducts a detailed experimental study on the architecture of ResNet blocks and proposes a novel architecture where the depth and width of residual networks are decreased and the resulting network structures are called wide residual networks (WRNs), which are far superior over their commonly used thin and very deep counterparts."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-05-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1605.07146"
        },
        "citationStyles": {
            "bibtex": "@Article{Zagoruyko2016WideRN,\n author = {Sergey Zagoruyko and N. Komodakis},\n booktitle = {British Machine Vision Conference},\n journal = {ArXiv},\n title = {Wide Residual Networks},\n volume = {abs/1605.07146},\n year = {2016}\n}\n"
        }
    },
    "359_compress-lstm_(66m)": {
        "paperId": "4f7a8039781bffaa897da1a3a6f139196cc2a86d",
        "externalIds": {
            "ArXiv": "1902.02380",
            "DBLP": "journals/asc/GrachevIS19",
            "MAG": "2914294010",
            "DOI": "10.1016/j.asoc.2019.03.057",
            "CorpusId": 59608632
        },
        "corpusId": 59608632,
        "publicationVenue": {
            "id": "b1994124-f1e8-4f96-a165-b6f19a04fe7e",
            "name": "Applied Soft Computing",
            "type": "journal",
            "alternate_names": [
                "Appl Soft Comput"
            ],
            "issn": "1568-4946",
            "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/621920/description#description",
            "alternate_urls": [
                "https://www.journals.elsevier.com/applied-soft-computing",
                "http://www.sciencedirect.com/science/journal/15684946"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/4f7a8039781bffaa897da1a3a6f139196cc2a86d",
        "title": "Compression of Recurrent Neural Networks for Efficient Language Modeling",
        "abstract": null,
        "venue": "Applied Soft Computing",
        "year": 2019,
        "referenceCount": 47,
        "citationCount": 35,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1902.02380",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A general pipeline for applying the most suitable methods to compress recurrent neural networks for language modeling is proposed, showing that the most efficient results in terms of speed and compression\u2013perplexity balance are obtained by matrix decomposition techniques."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-02-06",
        "journal": {
            "name": "Appl. Soft Comput.",
            "pages": "354-362",
            "volume": "79"
        },
        "citationStyles": {
            "bibtex": "@Article{Grachev2019CompressionOR,\n author = {Artem M. Grachev and D. Ignatov and A. Savchenko},\n booktitle = {Applied Soft Computing},\n journal = {Appl. Soft Comput.},\n pages = {354-362},\n title = {Compression of Recurrent Neural Networks for Efficient Language Modeling},\n volume = {79},\n year = {2019}\n}\n"
        }
    },
    "360_amended-darts": {
        "paperId": "7ffac30cd47fe173bec897d2b8b81c93e2771b85",
        "externalIds": {
            "ArXiv": "1910.11831",
            "DBLP": "journals/corr/abs-1910-11831",
            "MAG": "2981711199",
            "CorpusId": 204915951
        },
        "corpusId": 204915951,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/7ffac30cd47fe173bec897d2b8b81c93e2771b85",
        "title": "Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters",
        "abstract": "DARTS is a popular algorithm for neural architecture search (NAS). Despite its great advantage in search efficiency, DARTS often suffers weak stability, which reflects in the large variation among individual trials as well as the sensitivity to the hyper-parameters of the search process. This paper owes such instability to an optimization gap between the super-network and its sub-networks, namely, improving the validation accuracy of the super-network does not necessarily lead to a higher expectation on the performance of the sampled sub-networks. Then, we point out that the gap is due to the inaccurate estimation of the architectural gradients, based on which we propose an amended estimation method. Mathematically, our method guarantees a bounded error from the true gradients while the original estimation does not. Our approach bridges the gap from two aspects, namely, amending the estimation on the architectural gradients, and unifying the hyper-parameter settings in the search and re-training stages. Experiments on CIFAR10 and ImageNet demonstrate that our approach largely improves search stability and, more importantly, enables DARTS-based approaches to explore much larger search spaces that have not been investigated before.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 47,
        "citationCount": 45,
        "influentialCitationCount": 9,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The proposed amended estimation method bridges the gap from two aspects, namely, amending the estimation on the architectural gradients, and unifying the hyper-parameter settings in the search and re-training stages, and enables DARTS-based approaches to explore much larger search spaces that have not been investigated before."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-10-25",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1910.11831"
        },
        "citationStyles": {
            "bibtex": "@Article{Bi2019StabilizingDW,\n author = {Kaifeng Bi and Changping Hu and Lingxi Xie and Xin Chen and Longhui Wei and Qi Tian},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters},\n volume = {abs/1910.11831},\n year = {2019}\n}\n"
        }
    },
    "362_multipop_adaptive_continuous_stack_(wt2)": {
        "paperId": "27981998aaef92952eabef2c1490b926f9150c4f",
        "externalIds": {
            "MAG": "2786167576",
            "DBLP": "conf/iclr/YogatamaMMLKDB18",
            "CorpusId": 51766417
        },
        "corpusId": 51766417,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/27981998aaef92952eabef2c1490b926f9150c4f",
        "title": "Memory Architectures in Recurrent Neural Network Language Models",
        "abstract": "We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin & Mikolov,2015; Grefenstette et al., 2015) to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 19,
        "citationCount": 61,
        "influentialCitationCount": 2,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-02-15",
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Yogatama2018MemoryAI,\n author = {Dani Yogatama and Yishu Miao and G\u00e1bor Melis and Wang Ling and A. Kuncoro and Chris Dyer and Phil Blunsom},\n booktitle = {International Conference on Learning Representations},\n title = {Memory Architectures in Recurrent Neural Network Language Models},\n year = {2018}\n}\n"
        }
    },
    "363_sparse_coding_model_for_v1_receptive_fields": {
        "paperId": "2805537bec87a6177037b18f9a3a9d3f1038867b",
        "externalIds": {
            "MAG": "2105464873",
            "DOI": "10.1016/S0042-6989(97)00169-7",
            "CorpusId": 14208692,
            "PubMed": "9425546"
        },
        "corpusId": 14208692,
        "publicationVenue": {
            "id": "ae3fa0a2-21b1-46fb-b171-8bfd54e77fb1",
            "name": "Vision Research",
            "type": "journal",
            "alternate_names": [
                "Vis Res"
            ],
            "issn": "0042-6989",
            "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/263/description#description",
            "alternate_urls": [
                "https://www.journals.elsevier.com/vision-research/",
                "http://www.sciencedirect.com/science/journal/00426989"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/2805537bec87a6177037b18f9a3a9d3f1038867b",
        "title": "Sparse coding with an overcomplete basis set: A strategy employed by V1?",
        "abstract": null,
        "venue": "Vision Research",
        "year": 1997,
        "referenceCount": 48,
        "citationCount": 3739,
        "influentialCitationCount": 229,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1997-12-01",
        "journal": {
            "name": "Vision Research",
            "pages": "3311-3325",
            "volume": "37"
        },
        "citationStyles": {
            "bibtex": "@Article{Olshausen1997SparseCW,\n author = {B. Olshausen and D. Field},\n booktitle = {Vision Research},\n journal = {Vision Research},\n pages = {3311-3325},\n title = {Sparse coding with an overcomplete basis set: A strategy employed by V1?},\n volume = {37},\n year = {1997}\n}\n"
        }
    },
    "364_linear_transformer_(large)": {
        "paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "externalIds": {
            "ArXiv": "2102.11174",
            "DBLP": "conf/icml/SchlagIS21",
            "CorpusId": 235377069
        },
        "corpusId": 235377069,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "title": "Linear Transformers Are Secretly Fast Weight Programmers",
        "abstract": "We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early '90s, where a ``slow\"neural net learns by gradient descent to program the ``fast weights\"of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values. The FWP also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 71,
        "citationCount": 124,
        "influentialCitationCount": 15,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work infer a memory capacity limitation of recent linearised softmax attention variants, and replaces the purely additive outer products of self-invented activation patterns by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-02-22",
        "journal": {
            "pages": "9355-9366"
        },
        "citationStyles": {
            "bibtex": "@Article{Schlag2021LinearTA,\n author = {Imanol Schlag and Kazuki Irie and J. Schmidhuber},\n booktitle = {International Conference on Machine Learning},\n pages = {9355-9366},\n title = {Linear Transformers Are Secretly Fast Weight Programmers},\n year = {2021}\n}\n"
        }
    },
    "365_t2r_+_random_init": {
        "paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "externalIds": {
            "DBLP": "journals/corr/abs-2103-13076",
            "ACL": "2021.emnlp-main.830",
            "ArXiv": "2103.13076",
            "DOI": "10.18653/v1/2021.emnlp-main.830",
            "CorpusId": 232335426
        },
        "corpusId": 232335426,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/054e307c1edf4b28137ffcbce980fe81f0647d20",
        "title": "Finetuning Pretrained Transformers into RNNs",
        "abstract": "Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a signifi- cant computational cost, as the attention mechanism\u2019s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "referenceCount": 91,
        "citationCount": 41,
        "influentialCitationCount": 6,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.830.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a swap-then-finetune procedure, which in an off-the-shelf pretrained transformer, replaces the softmax attention with its linear-complexity recurrent alternative and then finetune, and provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-03-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2103.13076"
        },
        "citationStyles": {
            "bibtex": "@Article{Kasai2021FinetuningPT,\n author = {Jungo Kasai and Hao Peng and Yizhe Zhang and Dani Yogatama and Gabriel Ilharco and Nikolaos Pappas and Yi Mao and Weizhu Chen and Noah A. Smith},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Finetuning Pretrained Transformers into RNNs},\n volume = {abs/2103.13076},\n year = {2021}\n}\n"
        }
    },
    "366_segatron_xl_large,_m=384": {
        "paperId": "320efa53dea3e8f836790682fbd4196132c49749",
        "externalIds": {
            "DBLP": "conf/aaai/BaiSLXTX0021",
            "MAG": "3112776819",
            "DOI": "10.1609/aaai.v35i14.17485",
            "CorpusId": 229285330
        },
        "corpusId": 229285330,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/320efa53dea3e8f836790682fbd4196132c49749",
        "title": "Segatron: Segment-Aware Transformer for Language Modeling and Understanding",
        "abstract": "Transformers are powerful for sequence modeling. Nearly all state-of-the-art language models and pre-trained language models are based on the Transformer architecture. However, it distinguishes sequential tokens only with the token position index. We hypothesize that better contextual representations can be generated from the Transformer with richer positional information. To verify this, we propose a segment-aware Transformer (Segatron), by replacing the original token position encoding with a combined position encoding of paragraph, sentence, and token. We first introduce the segment-aware mechanism to Transformer-XL, which is a popular Transformer-based language model with memory extension and relative position encoding. We find that our method can further improve the Transformer-XL base model and large model, achieving 17.1 perplexity on the WikiText-103 dataset. We further investigate the pre-training masked language modeling task with Segatron. Experimental results show that BERT pre-trained with Segatron (SegaBERT) can outperform BERT with vanilla Transformer on various NLP tasks, and outperforms RoBERTa on zero-shot sentence representation learning. Our code is available on GitHub.",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2020,
        "referenceCount": 39,
        "citationCount": 17,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/17485/17292",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A segment-aware Transformer (Segatron) is proposed, by replacing the original token position encoding with a combined position encoding of paragraph, sentence, and token, and it is hypothesized that better contextual representations can be generated from the Transformer with richer positional information."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-12-16",
        "journal": {
            "pages": "12526-12534"
        },
        "citationStyles": {
            "bibtex": "@Article{Bai2020SegatronST,\n author = {Richard He Bai and Peng Shi and Jimmy J. Lin and Yuqing Xie and Luchen Tan and Kun Xiong and Wen Gao and Ming Li},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {12526-12534},\n title = {Segatron: Segment-Aware Transformer for Language Modeling and Understanding},\n year = {2020}\n}\n"
        }
    },
    "367_dlrm-2022": {
        "paperId": "88bb275161782df0d0873de01797964e6ca7b9d4",
        "externalIds": {
            "ArXiv": "2104.05158",
            "DBLP": "conf/isca/MudigereHHJT0LO22",
            "DOI": "10.1145/3470496.3533727",
            "CorpusId": 237414938
        },
        "corpusId": 237414938,
        "publicationVenue": {
            "id": "deedf64a-dd5c-4b33-b345-ff83bfb93d71",
            "name": "International Symposium on Computer Architecture",
            "type": "conference",
            "alternate_names": [
                "Int Symp Comput Archit",
                "ISCA"
            ],
            "url": "http://www.cs.wisc.edu/~arch/www/"
        },
        "url": "https://www.semanticscholar.org/paper/88bb275161782df0d0873de01797964e6ca7b9d4",
        "title": "Software-hardware co-design for fast and scalable training of deep learning recommendation models",
        "abstract": "Deep learning recommendation models (DLRMs) have been used across many business-critical services at Meta and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper, we present Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs. Neo employs a novel 4D parallelism strategy that combines table-wise, row-wise, column-wise, and data parallelism for training massive embedding operators in DLRMs. In addition, Neo enables extremely high-performance and memory-efficient embedding computations using a variety of critical systems optimizations, including hybrid kernel fusion, software-managed caching, and quality-preserving compression. Finally, Neo is paired with ZionEX, a new hardware platform co-designed with Neo's 4D parallelism for optimizing communications for large-scale DLRM training. Our evaluation on 128 GPUs using 16 ZionEX nodes shows that Neo outperforms existing systems by up to 40\u00d7 for training 12-trillion-parameter DLRM models deployed in production.",
        "venue": "International Symposium on Computer Architecture",
        "year": 2021,
        "referenceCount": 75,
        "citationCount": 87,
        "influentialCitationCount": 12,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2104.05158",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs that employs a novel 4D parallelism strategy that combines table-wise, row- Wise, column- wise, and data parallelism for training massive embedding operators inDLRMs."
        },
        "publicationTypes": [
            "JournalArticle",
            "Book",
            "Conference"
        ],
        "publicationDate": "2021-04-12",
        "journal": {
            "name": "Proceedings of the 49th Annual International Symposium on Computer Architecture"
        },
        "citationStyles": {
            "bibtex": "@Article{Mudigere2021SoftwarehardwareCF,\n author = {Dheevatsa Mudigere and Y. Hao and Jianyu Huang and Zhihao Jia and Andrew Tulloch and Srinivas Sridharan and Xing Liu and Mustafa Ozdal and Jade Nie and Jongsoo Park and Liangchen Luo and J. Yang and Leon Gao and Dmytro Ivchenko and Aarti Basant and Yuxi Hu and Jiyan Yang and E. K. Ardestani and Xiaodong Wang and Rakesh Komuravelli and Ching-Hsiang Chu and Serhat Yilmaz and Huayu Li and Jiyuan Qian and Zhuobo Feng and Yi-An Ma and Junjie Yang and Ellie Wen and Hong Li and Lin Yang and Chonglin Sun and Whitney Zhao and Dimitry Melts and Krishnaveni Dhulipala and Kranthi G. Kishore and Tyler N. Graf and Assaf Eisenman and Kiran Kumar Matam and Adi Gangidi and Guoqiang Jerry Chen and M. Krishnan and A. Nayak and Krishnakumar Nair and Bharath Muthiah and Mahmoud khorashadi and P. Bhattacharya and Petr Lapukhov and M. Naumov and A. Mathews and Lin Qiao and M. Smelyanskiy and Bill Jia and Vijay Rao},\n booktitle = {International Symposium on Computer Architecture},\n journal = {Proceedings of the 49th Annual International Symposium on Computer Architecture},\n title = {Software-hardware co-design for fast and scalable training of deep learning recommendation models},\n year = {2021}\n}\n"
        }
    },
    "368_robocat": {
        "paperId": "2562fe379554d201aad312f786903f4c60b68acf",
        "externalIds": {
            "ArXiv": "2306.11706",
            "CorpusId": 259203978
        },
        "corpusId": 259203978,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/2562fe379554d201aad312f786903f4c60b68acf",
        "title": "RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation",
        "abstract": "The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100-1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.",
        "venue": "",
        "year": 2023,
        "referenceCount": 0,
        "citationCount": 18,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The proposed multi-embodiment, multi-task generalist agent for robotic manipulation, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience that spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions."
        },
        "publicationTypes": null,
        "publicationDate": "2023-06-20",
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Bousmalis2023RoboCatAS,\n author = {Konstantinos Bousmalis and G. Vezzani and Dushyant Rao and Coline Devin and Alex X. Lee and Maria Bauz\u00e1 and Todor Davchev and Yuxiang Zhou and Agrim Gupta and A. Raju and Antoine Laurens and C. Fantacci and Valentin Dalibard and Martina Zambelli and M. Martins and Rugile Pevceviciute and M. Blokzijl and Misha Denil and Nathan Batchelor and Thomas Lampe and Emilio Parisotto and Konrad Zolna and Scott E. Reed and Sergio Gomez Colmenarejo and Jonathan Scholz and A. Abdolmaleki and O. Groth and Jean-Baptiste Regli and Oleg O. Sushkov and Tom Rothorl and Jos\u00e9 Enrique Chen and Y. Aytar and David Barker and Joy Ortiz and Martin A. Riedmiller and J. T. Springenberg and R. Hadsell and F. Nori and N. Heess},\n title = {RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation},\n year = {2023}\n}\n"
        }
    },
    "369_plato-xl": {
        "paperId": "992129aa96c97fa3b2ced0ddbc4c7d07bfaaf821",
        "externalIds": {
            "DBLP": "journals/corr/abs-2109-09519",
            "ArXiv": "2109.09519",
            "CorpusId": 237572122
        },
        "corpusId": 237572122,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/992129aa96c97fa3b2ced0ddbc4c7d07bfaaf821",
        "title": "PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation",
        "abstract": "To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.",
        "venue": "AACL/IJCNLP",
        "year": 2021,
        "referenceCount": 54,
        "citationCount": 48,
        "influentialCitationCount": 5,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-09-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2109.09519"
        },
        "citationStyles": {
            "bibtex": "@Article{Bao2021PLATOXLET,\n author = {Siqi Bao and H. He and Fan Wang and Hua Wu and Haifeng Wang and Wenquan Wu and Zhihua Wu and Zhen Guo and Hua Lu and Xinxian Huang and Xin Tian and Xinchao Xu and Yingzhan Lin and Zhengyu Niu},\n booktitle = {AACL/IJCNLP},\n journal = {ArXiv},\n title = {PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation},\n volume = {abs/2109.09519},\n year = {2021}\n}\n"
        }
    },
    "370_nplm": {
        "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
        "externalIds": {
            "DBLP": "conf/nips/BengioDV00",
            "MAG": "2140679639",
            "CorpusId": 221275765
        },
        "corpusId": 221275765,
        "publicationVenue": {
            "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
            "name": "Journal of machine learning research",
            "type": "journal",
            "alternate_names": [
                "Journal of Machine Learning Research",
                "J mach learn res",
                "J Mach Learn Res"
            ],
            "issn": "1532-4435",
            "alternate_issns": [
                "1533-7928"
            ],
            "url": "http://www.ai.mit.edu/projects/jmlr/",
            "alternate_urls": [
                "http://jmlr.csail.mit.edu/",
                "http://www.jmlr.org/",
                "http://portal.acm.org/affiliated/jmlr"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/6c2b28f9354f667cd5bd07afc0471d8334430da7",
        "title": "A Neural Probabilistic Language Model",
        "abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.",
        "venue": "Journal of machine learning research",
        "year": 2003,
        "referenceCount": 40,
        "citationCount": 6791,
        "influentialCitationCount": 461,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work reports on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2003-03-01",
        "journal": {
            "name": "J. Mach. Learn. Res.",
            "pages": "1137-1155",
            "volume": "3"
        },
        "citationStyles": {
            "bibtex": "@Article{Bengio2003ANP,\n author = {Yoshua Bengio and R\u00e9jean Ducharme and Pascal Vincent and Christian Janvin},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1137-1155},\n title = {A Neural Probabilistic Language Model},\n volume = {3},\n year = {2003}\n}\n"
        }
    },
    "371_noisy_student_(l2)": {
        "paperId": "20ba55ee3229db5cb190a00e788c59f08d2a767d",
        "externalIds": {
            "DBLP": "journals/corr/abs-1911-04252",
            "MAG": "3035160371",
            "ArXiv": "1911.04252",
            "DOI": "10.1109/cvpr42600.2020.01070",
            "CorpusId": 207853355
        },
        "corpusId": 207853355,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/20ba55ee3229db5cb190a00e788c59f08d2a767d",
        "title": "Self-Training With Noisy Student Improves ImageNet Classification",
        "abstract": "We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "referenceCount": 110,
        "citationCount": 1938,
        "influentialCitationCount": 234,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1911.04252",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-11-11",
        "journal": {
            "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "10684-10695"
        },
        "citationStyles": {
            "bibtex": "@Article{Xie2019SelfTrainingWN,\n author = {Qizhe Xie and E. Hovy and Minh-Thang Luong and Quoc V. Le},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {10684-10695},\n title = {Self-Training With Noisy Student Improves ImageNet Classification},\n year = {2019}\n}\n"
        }
    },
    "372_palm-e": {
        "paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3",
        "externalIds": {
            "DBLP": "journals/corr/abs-2303-03378",
            "ArXiv": "2303.03378",
            "DOI": "10.48550/arXiv.2303.03378",
            "CorpusId": 257364842
        },
        "corpusId": 257364842,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/38fe8f324d2162e63a967a9ac6648974fc4c66f3",
        "title": "PaLM-E: An Embodied Multimodal Language Model",
        "abstract": "Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "referenceCount": 71,
        "citationCount": 614,
        "influentialCitationCount": 53,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.03378",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts to enable general inference in the real world."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-03-06",
        "journal": {
            "pages": "8469-8488"
        },
        "citationStyles": {
            "bibtex": "@Article{Driess2023PaLMEAE,\n author = {Danny Driess and F. Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Q. Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and P. Sermanet and Daniel Duckworth and S. Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Peter R. Florence},\n booktitle = {International Conference on Machine Learning},\n pages = {8469-8488},\n title = {PaLM-E: An Embodied Multimodal Language Model},\n year = {2023}\n}\n"
        }
    },
    "375_onlstm-syd": {
        "paperId": "a48abad56acb085fe180c76a40d361aacd0dc049",
        "externalIds": {
            "DBLP": "conf/acl/DuLSOBZ20",
            "MAG": "3025873536",
            "ArXiv": "2005.05864",
            "ACL": "2020.acl-main.591",
            "DOI": "10.18653/v1/2020.acl-main.591",
            "CorpusId": 218595902
        },
        "corpusId": 218595902,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/a48abad56acb085fe180c76a40d361aacd0dc049",
        "title": "Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach",
        "abstract": "It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called \u201csyntactic distances\u201d, where information between these two separate objectives shares the same intermediate representation. Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "referenceCount": 54,
        "citationCount": 11,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.591.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-05-12",
        "journal": {
            "pages": "6611-6628"
        },
        "citationStyles": {
            "bibtex": "@Article{Du2020ExploitingSS,\n author = {Wenyu Du and Zhouhan Lin and Yikang Shen and T. O\u2019Donnell and Yoshua Bengio and Yue Zhang},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {6611-6628},\n title = {Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach},\n year = {2020}\n}\n"
        }
    },
    "377_efficientnet-l2": {
        "paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9",
        "externalIds": {
            "DBLP": "conf/icml/TanL19",
            "MAG": "2946948417",
            "ArXiv": "1905.11946",
            "CorpusId": 167217261
        },
        "corpusId": 167217261,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9",
        "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "abstract": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "referenceCount": 54,
        "citationCount": 12531,
        "influentialCitationCount": 1790,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new scaling method is proposed that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient and is demonstrated the effectiveness of this method on scaling up MobileNets and ResNet."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-05-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1905.11946"
        },
        "citationStyles": {
            "bibtex": "@Article{Tan2019EfficientNetRM,\n author = {Mingxing Tan and Quoc V. Le},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},\n volume = {abs/1905.11946},\n year = {2019}\n}\n"
        }
    },
    "379_a_bayesian_approach_to_unsupervised_one-shot_learning_of_object_categories": {
        "paperId": "d044d7d92dd1fb80275d04d035aed71bcd3374e5",
        "externalIds": {
            "DBLP": "conf/iccv/Fei-FeiFP03",
            "DOI": "10.1109/ICCV.2003.1238476",
            "CorpusId": 267912746
        },
        "corpusId": 267912746,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/d044d7d92dd1fb80275d04d035aed71bcd3374e5",
        "title": "A Bayesian approach to unsupervised one-shot learning of object categories",
        "abstract": "Learning visual models of object categories notoriously requires thousands of training examples; this is due to the diversity and richness of object appearance which requires models containing hundreds of parameters. We present a method for learning object categories from just a few images (1 /spl sim/ 5). It is based on incorporating \"generic\" knowledge which may be obtained from previously learnt models of unrelated categories. We operate in a variational Bayesian framework: object categories are represented by probabilistic models, and \"prior\" knowledge is represented as a probability density function on the parameters of these models. The \"posterior\" model for an object category is obtained by updating the prior in the light of one or more observations. Our ideas are demonstrated on four diverse categories (human faces, airplanes, motorcycles, spotted cats). Initially three categories are learnt from hundreds of training examples, and a \"prior\" is estimated from these. Then the model of the fourth category is learnt from 1 to 5 training examples, and is used for detecting new exemplars a set of test images.",
        "venue": "Proceedings Ninth IEEE International Conference on Computer Vision",
        "year": 2003,
        "referenceCount": 19,
        "citationCount": 108,
        "influentialCitationCount": 6,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://lear.inrialpes.fr/people/triggs/events/iccv03/cdrom/iccv03/1134_fei-fei.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents a method for learning object categories from just a few images, based on incorporating \"generic\" knowledge which may be obtained from previously learnt models of unrelated categories, in a variational Bayesian framework."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": null,
        "journal": {
            "name": "Proceedings Ninth IEEE International Conference on Computer Vision",
            "pages": "1134-1141 vol.2"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2003ABA,\n author = {F. Li and Rob Fergus and Pietro Perona},\n booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},\n journal = {Proceedings Ninth IEEE International Conference on Computer Vision},\n pages = {1134-1141 vol.2},\n title = {A Bayesian approach to unsupervised one-shot learning of object categories},\n year = {2003}\n}\n"
        }
    },
    "380_prott5-xxl": {
        "paperId": "1c53d27c742fb4658fa03085c7c2ca014a122385",
        "externalIds": {
            "CorpusId": 252617747
        },
        "corpusId": 252617747,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/1c53d27c742fb4658fa03085c7c2ca014a122385",
        "title": "ProtTrans: Towards Cracking the Language of Life\u2019s Code Through Self-Supervised Learning",
        "abstract": "\u2014Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The \ufb01rst was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the \ufb01rst time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life . To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",
        "venue": "",
        "year": 2021,
        "referenceCount": 100,
        "citationCount": 75,
        "influentialCitationCount": 7,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": null,
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Results implied that protein LMs learned some of the grammar of the language of life by outperforming the state-of-the-art without using evolutionary information thereby bypassing expensive database searches."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Elnaggar2021ProtTransTC,\n author = {Ahmed Elnaggar and M. Heinzinger and Christian Dallago and Ghalia Rehawi and Yu Wang and Llion Jones and Tom Gibbs and Tamas B. Feh\u00e9r and Christoph Angerer and Martin Steinegger and D. Bhowmik and B. Rost},\n title = {ProtTrans: Towards Cracking the Language of Life\u2019s Code Through Self-Supervised Learning},\n year = {2021}\n}\n"
        }
    },
    "382_denoising_autoencoders": {
        "paperId": "843959ffdccf31c6694d135fad07425924f785b1",
        "externalIds": {
            "MAG": "2025768430",
            "DBLP": "conf/icml/VincentLBM08",
            "DOI": "10.1145/1390156.1390294",
            "CorpusId": 207168299
        },
        "corpusId": 207168299,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/843959ffdccf31c6694d135fad07425924f785b1",
        "title": "Extracting and composing robust features with denoising autoencoders",
        "abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.",
        "venue": "International Conference on Machine Learning",
        "year": 2008,
        "referenceCount": 30,
        "citationCount": 6753,
        "influentialCitationCount": 470,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2008-07-05",
        "journal": {
            "pages": "1096-1103"
        },
        "citationStyles": {
            "bibtex": "@Article{Vincent2008ExtractingAC,\n author = {Pascal Vincent and H. Larochelle and Yoshua Bengio and Pierre-Antoine Manzagol},\n booktitle = {International Conference on Machine Learning},\n pages = {1096-1103},\n title = {Extracting and composing robust features with denoising autoencoders},\n year = {2008}\n}\n"
        }
    },
    "383_elastic_weight_consolidation": {
        "paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
        "externalIds": {
            "ArXiv": "1612.00796",
            "MAG": "2560647685",
            "DBLP": "journals/corr/KirkpatrickPRVD16",
            "DOI": "10.1073/pnas.1611835114",
            "CorpusId": 4704285,
            "PubMed": "28292907"
        },
        "corpusId": 4704285,
        "publicationVenue": {
            "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
            "name": "Proceedings of the National Academy of Sciences of the United States of America",
            "type": "journal",
            "alternate_names": [
                "PNAS",
                "PNAS online",
                "Proceedings of the National Academy of Sciences of the United States of America.",
                "Proc National Acad Sci",
                "Proceedings of the National Academy of Sciences",
                "Proc National Acad Sci u s Am"
            ],
            "issn": "0027-8424",
            "alternate_issns": [
                "1091-6490"
            ],
            "url": "https://www.jstor.org/journal/procnatiacadscie",
            "alternate_urls": [
                "http://www.pnas.org/",
                "https://www.pnas.org/",
                "http://www.jstor.org/journals/00278424.html",
                "www.pnas.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
        "title": "Overcoming catastrophic forgetting in neural networks",
        "abstract": "Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.",
        "venue": "Proceedings of the National Academy of Sciences of the United States of America",
        "year": 2016,
        "referenceCount": 48,
        "citationCount": 5099,
        "influentialCitationCount": 918,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.pnas.org/content/pnas/114/13/3521.full.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that it is possible to overcome the limitation of connectionist models and train networks that can maintain expertise on tasks that they have not experienced for a long time and selectively slowing down learning on the weights important for previous tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-12-02",
        "journal": {
            "name": "Proceedings of the National Academy of Sciences",
            "pages": "3521 - 3526",
            "volume": "114"
        },
        "citationStyles": {
            "bibtex": "@Article{Kirkpatrick2016OvercomingCF,\n author = {J. Kirkpatrick and Razvan Pascanu and Neil C. Rabinowitz and J. Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and A. Grabska-Barwinska and D. Hassabis and C. Clopath and D. Kumaran and R. Hadsell},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {3521 - 3526},\n title = {Overcoming catastrophic forgetting in neural networks},\n volume = {114},\n year = {2016}\n}\n"
        }
    },
    "385_alexatm_20b": {
        "paperId": "914254fac74a2da051cccf6ca16afcaad416a079",
        "externalIds": {
            "DBLP": "journals/corr/abs-2208-01448",
            "ArXiv": "2208.01448",
            "DOI": "10.48550/arXiv.2208.01448",
            "CorpusId": 251253416
        },
        "corpusId": 251253416,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/914254fac74a2da051cccf6ca16afcaad416a079",
        "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
        "abstract": "In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 91,
        "citationCount": 60,
        "influentialCitationCount": 12,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2208.01448",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling tasks, are more efficient few-shot learners than decoder-only models on various tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-08-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2208.01448"
        },
        "citationStyles": {
            "bibtex": "@Article{Soltan2022AlexaTM2F,\n author = {Saleh Soltan and Shankar Ananthakrishnan and Jack G. M. FitzGerald and Rahul Gupta and Wael Hamza and Haidar Khan and Charith S. Peris and Stephen Rawls and Andrew Rosenbaum and Anna Rumshisky and Chandan Prakash and Mukund Sridhar and Fabian Triefenbach and Apurv Verma and G. Tur and Premkumar Natarajan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model},\n volume = {abs/2208.01448},\n year = {2022}\n}\n"
        }
    },
    "387_tensorreasoner": {
        "paperId": "50d53cc562225549457cbc782546bfbe1ac6f0cf",
        "externalIds": {
            "MAG": "2127426251",
            "DBLP": "conf/nips/SocherCMN13",
            "CorpusId": 8429835
        },
        "corpusId": 8429835,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/50d53cc562225549457cbc782546bfbe1ac6f0cf",
        "title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion",
        "abstract": "Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the \"Sumatran tiger\" and \"Bengal tiger.\" Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "referenceCount": 25,
        "citationCount": 1842,
        "influentialCitationCount": 282,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An expressive neural tensor network suitable for reasoning over relationships between two entities given a subset of the knowledge base is introduced and performance can be improved when entities are represented as an average of their constituting word vectors."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2013-12-05",
        "journal": {
            "pages": "926-934"
        },
        "citationStyles": {
            "bibtex": "@Article{Socher2013ReasoningWN,\n author = {R. Socher and Danqi Chen and Christopher D. Manning and A. Ng},\n booktitle = {Neural Information Processing Systems},\n pages = {926-934},\n title = {Reasoning With Neural Tensor Networks for Knowledge Base Completion},\n year = {2013}\n}\n"
        }
    },
    "388_internal_functionality_of_visual_invariants": {
        "paperId": "22e69b6c0129af46dda3335ced3ef88cdb63bca0",
        "externalIds": {
            "MAG": "2001127164",
            "DOI": "10.1007/BF00337644",
            "CorpusId": 27652096,
            "PubMed": "454703"
        },
        "corpusId": 27652096,
        "publicationVenue": {
            "id": "57cada26-a03e-494e-929e-a71ac35f2ad0",
            "name": "Biological cybernetics",
            "type": "journal",
            "alternate_names": [
                "Biological cybern",
                "Biological Cybern",
                "Biological Cybernetics"
            ],
            "issn": "0340-1200",
            "url": "http://link.springer.com/journal/422",
            "alternate_urls": [
                "https://link.springer.com/journal/volumesAndIssues/422"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/22e69b6c0129af46dda3335ced3ef88cdb63bca0",
        "title": "The internal representation of solid shape with respect to vision",
        "abstract": null,
        "venue": "Biological cybernetics",
        "year": 1979,
        "referenceCount": 21,
        "citationCount": 306,
        "influentialCitationCount": 7,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Physics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is argued that the internal model of any object must take the form of a function, such that for any intended action the resulting reafference is predictable and relations with Gestalt theories of perception are discussed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1979-12-01",
        "journal": {
            "name": "Biological Cybernetics",
            "pages": "211-216",
            "volume": "32"
        },
        "citationStyles": {
            "bibtex": "@Article{Koenderink1979TheIR,\n author = {J. Koenderink and A. Doorn},\n booktitle = {Biological cybernetics},\n journal = {Biological Cybernetics},\n pages = {211-216},\n title = {The internal representation of solid shape with respect to vision},\n volume = {32},\n year = {1979}\n}\n"
        }
    },
    "389_r-transformer": {
        "paperId": "449892c8e095a97b4c9e058ae5be1e9177d805b7",
        "externalIds": {
            "MAG": "2956480774",
            "DBLP": "journals/corr/abs-1907-05572",
            "ArXiv": "1907.05572",
            "CorpusId": 196471339
        },
        "corpusId": 196471339,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/449892c8e095a97b4c9e058ae5be1e9177d805b7",
        "title": "R-Transformer: Recurrent Neural Network Enhanced Transformer",
        "abstract": "Recurrent Neural Networks have long been the dominating choice for sequence modeling. However, it severely suffers from two issues: impotent in capturing very long-term dependencies and unable to parallelize the sequential computation procedure. Therefore, many non-recurrent sequence models that are built on convolution and attention operations have been proposed recently. Notably, models with multi-head attention such as Transformer have demonstrated extreme effectiveness in capturing long-term dependencies in a variety of sequence modeling tasks. Despite their success, however, these models lack necessary components to model local structures in sequences and heavily rely on position embeddings that have limited effects and require a considerable amount of design efforts. In this paper, we propose the R-Transformer which enjoys the advantages of both RNNs and the multi-head attention mechanism while avoids their respective drawbacks. The proposed model can effectively capture both local structures and global long-term dependencies in sequences without any use of position embeddings. We evaluate R-Transformer through extensive experiments with data from a wide range of domains and the empirical results show that R-Transformer outperforms the state-of-the-art methods by a large margin in most of the tasks. We have made the code publicly available at \\url{this https URL}.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 35,
        "citationCount": 83,
        "influentialCitationCount": 4,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The R-Transformer is proposed which enjoys the advantages of both RNNs and the multi-head attention mechanism while avoids their respective drawbacks and can effectively capture both local structures and global long-term dependencies in sequences without any use of position embeddings."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-07-12",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1907.05572"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2019RTransformerRN,\n author = {Z. Wang and Yao Ma and Zitao Liu and Jiliang Tang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {R-Transformer: Recurrent Neural Network Enhanced Transformer},\n volume = {abs/1907.05572},\n year = {2019}\n}\n"
        }
    },
    "391_pointer_sentinel-lstm_(medium)": {
        "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
        "externalIds": {
            "ArXiv": "1609.07843",
            "MAG": "2525332836",
            "DBLP": "journals/corr/MerityXBS16",
            "CorpusId": 16299141
        },
        "corpusId": 16299141,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/efbd381493bb9636f489b965a2034d529cd56bcd",
        "title": "Pointer Sentinel Mixture Models",
        "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "referenceCount": 29,
        "citationCount": 1785,
        "influentialCitationCount": 333,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank while using far fewer parameters than a standard softmax LSTM and the freely available WikiText corpus is introduced."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-09-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1609.07843"
        },
        "citationStyles": {
            "bibtex": "@Article{Merity2016PointerSM,\n author = {Stephen Merity and Caiming Xiong and James Bradbury and R. Socher},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Pointer Sentinel Mixture Models},\n volume = {abs/1609.07843},\n year = {2016}\n}\n"
        }
    },
    "393_cryptogru": {
        "paperId": "6a0310351409ada91e209b43f310b471ada458ca",
        "externalIds": {
            "DBLP": "conf/emnlp/FengL0F21",
            "ACL": "2021.emnlp-main.156",
            "MAG": "3094488859",
            "ArXiv": "2010.11796",
            "DOI": "10.18653/v1/2021.emnlp-main.156",
            "CorpusId": 225040312
        },
        "corpusId": 225040312,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/6a0310351409ada91e209b43f310b471ada458ca",
        "title": "CRYPTOGRU: Low Latency Privacy-Preserving Text Analysis With GRU",
        "abstract": "Homomorphic encryption (HE) and garbled circuit (GC) provide the protection for users\u2019 privacy. However, simply mixing the HE and GC in RNN models suffer from long inference latency due to slow activation functions. In this paper, we present a novel hybrid structure of HE and GC gated recurrent unit (GRU) network, , for low-latency secure inferences. replaces computationally expensive GC-based tanh with fast GC-based ReLU, and then quantizes sigmoid and ReLU to smaller bit-length to accelerate activations in a GRU. We evaluate with multiple GRU models trained on 4 public datasets. Experimental results show achieves top-notch accuracy and improves the secure inference latency by up to 138\\times over one of the state-of-the-art secure networks on the Penn Treebank dataset.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2020,
        "referenceCount": 25,
        "citationCount": 11,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.156.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel hybrid structure of HE and GC gated recurrent unit (GRU) network, which replaces computationally expensive GC-based tanh with fast GC- based ReLU, and then quantizes sigmoid and ReLU to smaller bit-length to accelerate activations in a GRU for low-latency secure inferences."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-10-22",
        "journal": {
            "pages": "2052-2057"
        },
        "citationStyles": {
            "bibtex": "@Article{Feng2020CRYPTOGRULL,\n author = {Bo Feng and Qian Lou and Lei Jiang and G. Fox},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {2052-2057},\n title = {CRYPTOGRU: Low Latency Privacy-Preserving Text Analysis With GRU},\n year = {2020}\n}\n"
        }
    },
    "394_ernie_3.0": {
        "paperId": "319b84be7a843250bc81d7086f79a4126d550277",
        "externalIds": {
            "DBLP": "journals/corr/abs-2107-02137",
            "ArXiv": "2107.02137",
            "CorpusId": 235731579
        },
        "corpusId": 235731579,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/319b84be7a843250bc81d7086f79a4126d550277",
        "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
        "abstract": "Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 102,
        "citationCount": 241,
        "influentialCitationCount": 39,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A unified framework named ERNIE 3.0 is proposed for pre-training large-scale knowledge enhanced models that fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few- shot learning or fine-tuning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-07-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2107.02137"
        },
        "citationStyles": {
            "bibtex": "@Article{Sun2021ERNIE3L,\n author = {Yu Sun and Shuohuan Wang and Shikun Feng and Siyu Ding and Chao Pang and Junyuan Shang and Jiaxiang Liu and Xuyi Chen and Yanbin Zhao and Yuxiang Lu and Weixin Liu and Zhihua Wu and Weibao Gong and Jianzhong Liang and Zhizhou Shang and Peng Sun and Wei Liu and Ouyang Xuan and Dianhai Yu and Hao Tian and Hua Wu and Haifeng Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation},\n volume = {abs/2107.02137},\n year = {2021}\n}\n"
        }
    },
    "395_gnmt": {
        "paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
        "externalIds": {
            "ArXiv": "1609.08144",
            "MAG": "2525778437",
            "DBLP": "journals/corr/WuSCLNMKCGMKSJL16",
            "CorpusId": 3603249
        },
        "corpusId": 3603249,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/c6850869aa5e78a107c378d2e8bfa39633158c0c",
        "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
        "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",
        "venue": "arXiv.org",
        "year": 2016,
        "referenceCount": 54,
        "citationCount": 6154,
        "influentialCitationCount": 414,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "GNMT, Google's Neural Machine Translation system, is presented, which attempts to address many of the weaknesses of conventional phrase-based translation systems and provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delicited models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-09-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1609.08144"
        },
        "citationStyles": {
            "bibtex": "@Article{Wu2016GooglesNM,\n author = {Yonghui Wu and M. Schuster and Z. Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and M. Krikun and Yuan Cao and Qin Gao and Klaus Macherey and J. Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Lukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and H. Kazawa and K. Stevens and George Kurian and Nishant Patil and Wei Wang and C. Young and Jason R. Smith and Jason Riesa and Alex Rudnick and O. Vinyals and G. Corrado and Macduff Hughes and J. Dean},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},\n volume = {abs/1609.08144},\n year = {2016}\n}\n"
        }
    },
    "396_amoebanet-a_(f=448)": {
        "paperId": "50bdda28de3dcf82a0e10f9ec13eea248b19edb5",
        "externalIds": {
            "MAG": "2785430118",
            "DBLP": "journals/corr/abs-1802-01548",
            "ArXiv": "1802.01548",
            "DOI": "10.1609/aaai.v33i01.33014780",
            "CorpusId": 3640974
        },
        "corpusId": 3640974,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/50bdda28de3dcf82a0e10f9ec13eea248b19edb5",
        "title": "Regularized Evolution for Image Classifier Architecture Search",
        "abstract": "The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier\u2014 AmoebaNet-A\u2014that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "referenceCount": 90,
        "citationCount": 2575,
        "influentialCitationCount": 381,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4405/4283",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work evolves an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time and gives evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-02-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1802.01548"
        },
        "citationStyles": {
            "bibtex": "@Article{Real2018RegularizedEF,\n author = {Esteban Real and A. Aggarwal and Yanping Huang and Quoc V. Le},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Regularized Evolution for Image Classifier Architecture Search},\n volume = {abs/1802.01548},\n year = {2018}\n}\n"
        }
    },
    "397_searchfusion": {
        "paperId": "9b223c8a31e0ea1d1f2c9787ffd8416dfc90c912",
        "externalIds": {
            "MAG": "2088049833",
            "DBLP": "journals/ijcv/UijlingsSGS13",
            "DOI": "10.1007/s11263-013-0620-5",
            "CorpusId": 216077384
        },
        "corpusId": 216077384,
        "publicationVenue": {
            "id": "939ee07c-6009-43f8-b884-69238b40659e",
            "name": "International Journal of Computer Vision",
            "type": "journal",
            "alternate_names": [
                "Int J Comput Vis"
            ],
            "issn": "0920-5691",
            "url": "https://www.springer.com/computer/image+processing/journal/11263",
            "alternate_urls": [
                "https://link.springer.com/journal/11263",
                "http://link.springer.com/journal/11263"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/9b223c8a31e0ea1d1f2c9787ffd8416dfc90c912",
        "title": "Selective Search for Object Recognition",
        "abstract": null,
        "venue": "International Journal of Computer Vision",
        "year": 2013,
        "referenceCount": 49,
        "citationCount": 5131,
        "influentialCitationCount": 452,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.huppelen.nl/publications/selectiveSearchDraft.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper introduces selective search which combines the strength of both an exhaustive search and segmentation, and shows that its selective search enables the use of the powerful Bag-of-Words model for recognition."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2013-04-02",
        "journal": {
            "name": "International Journal of Computer Vision",
            "pages": "154 - 171",
            "volume": "104"
        },
        "citationStyles": {
            "bibtex": "@Article{Uijlings2013SelectiveSF,\n author = {Jasper R. R. Uijlings and K. V. D. Sande and Theo Gevers and A. Smeulders},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {154 - 171},\n title = {Selective Search for Object Recognition},\n volume = {104},\n year = {2013}\n}\n"
        }
    },
    "398_ditto": {
        "paperId": "6151ee4af6a3fe78f2df7c605598cd9e02b23c5b",
        "externalIds": {
            "DBLP": "journals/corr/abs-2206-02369",
            "ArXiv": "2206.02369",
            "DOI": "10.48550/arXiv.2206.02369",
            "CorpusId": 249395390
        },
        "corpusId": 249395390,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/6151ee4af6a3fe78f2df7c605598cd9e02b23c5b",
        "title": "Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation",
        "abstract": "While large-scale neural language models, such as GPT2 and BART, have achieved impressive results on various text generation tasks, they tend to get stuck in undesirable sentence-level loops with maximization-based decoding algorithms (\\textit{e.g.}, greedy search). This phenomenon is counter-intuitive since there are few consecutive sentence-level repetitions in human corpora (e.g., 0.02\\% in Wikitext-103). To investigate the underlying reasons for generating consecutive sentence-level repetitions, we study the relationship between the probabilities of the repetitive tokens and their previous repetitions in the context. Through our quantitative experiments, we find that 1) Language models have a preference to repeat the previous sentence; 2) The sentence-level repetitions have a \\textit{self-reinforcement effect}: the more times a sentence is repeated in the context, the higher the probability of continuing to generate that sentence; 3) The sentences with higher initial probabilities usually have a stronger self-reinforcement effect. Motivated by our findings, we propose a simple and effective training method \\textbf{DITTO} (Pseu\\underline{D}o-Repet\\underline{IT}ion Penaliza\\underline{T}i\\underline{O}n), where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data. Although our method is motivated by mitigating repetitions, experiments show that DITTO not only mitigates the repetition issue without sacrificing perplexity, but also achieves better generation quality. Extensive experiments on open-ended text generation (Wikitext-103) and text summarization (CNN/DailyMail) demonstrate the generality and effectiveness of our method.",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "referenceCount": 44,
        "citationCount": 37,
        "influentialCitationCount": 4,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2206.02369",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simple and effective training method, DITTO, where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data, which mitigates the repetition issue without sacrificing perplexity, and achieves better generation quality."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-06-06",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2206.02369"
        },
        "citationStyles": {
            "bibtex": "@Article{Xu2022LearningTB,\n author = {Jin Xu and Xiaojiang Liu and Jianhao Yan and Deng Cai and Huayang Li and Jian Li},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation},\n volume = {abs/2206.02369},\n year = {2022}\n}\n"
        }
    },
    "399_calm": {
        "paperId": "094be5e085ce8a2419b62df3808fafb33ed8d6fb",
        "externalIds": {
            "DOI": "10.1101/2022.12.15.519894",
            "CorpusId": 254928263
        },
        "corpusId": 254928263,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/094be5e085ce8a2419b62df3808fafb33ed8d6fb",
        "title": "Codon language embeddings provide strong signals for protein engineering",
        "abstract": "Protein representations from deep language models have yielded state-of-the-art performance across many tasks in computational protein engineering. In recent years, progress has primarily focused on parameter count, with recent models\u2019 capacities surpassing the size of the very datasets they were trained on. Here, we propose an alternative direction. We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks. In some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters. These results suggest that, in addition to commonly studied scale and model complexity, the information content of biological data provides an orthogonal direction to improve the power of machine learning in biology.",
        "venue": "bioRxiv",
        "year": 2022,
        "referenceCount": 63,
        "citationCount": 5,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2022/12/19/2022.12.15.519894.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-12-19",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Outeiral2022CodonLE,\n author = {C. Outeiral and C. Deane},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {Codon language embeddings provide strong signals for protein engineering},\n year = {2022}\n}\n"
        }
    },
    "400_proteinbert": {
        "paperId": "c07651110d3b98b63607557b57808d15d99013dd",
        "externalIds": {
            "PubMedCentral": "9386727",
            "DBLP": "journals/bioinformatics/BrandesOPRL22",
            "MAG": "3191004466",
            "DOI": "10.1093/bioinformatics/btac020",
            "CorpusId": 235219208,
            "PubMed": "35020807"
        },
        "corpusId": 235219208,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/c07651110d3b98b63607557b57808d15d99013dd",
        "title": "ProteinBERT: a universal deep-learning model of protein sequence and function",
        "abstract": "Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme consists of masked language modeling combined with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to very large sequence lengths. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains state-of-the-art performance on multiple benchmarks covering diverse protein properties (including protein structure, post translational modifications and biophysical attributes), despite using a far smaller model than competing deep-learning methods. Overall, ProteinBERT provides an efficient framework for rapidly training protein predictors, even with limited labeled data. Code and pretrained model weights are available at https://github.com/nadavbra/protein_bert.",
        "venue": "bioRxiv",
        "year": 2021,
        "referenceCount": 53,
        "citationCount": 240,
        "influentialCitationCount": 12,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://academic.oup.com/bioinformatics/article-pdf/38/8/2102/49009610/btac020.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Biology",
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "ProteinBERT is introduced, a deep language model specifically designed for proteins that obtains state-of-the-art performance on multiple benchmarks covering diverse protein properties, despite using a far smaller model than competing deep-learning methods."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-05-25",
        "journal": {
            "name": "Bioinformatics",
            "pages": "2102 - 2110",
            "volume": "38"
        },
        "citationStyles": {
            "bibtex": "@Article{Brandes2021ProteinBERTAU,\n author = {Nadav Brandes and Dan Ofer and Yam Peleg and Nadav Rappoport and M. Linial},\n booktitle = {bioRxiv},\n journal = {Bioinformatics},\n pages = {2102 - 2110},\n title = {ProteinBERT: a universal deep-learning model of protein sequence and function},\n volume = {38},\n year = {2021}\n}\n"
        }
    },
    "401_alphafold": {
        "paperId": "3a083d843f891b3574494c385699c21766ce8b7a",
        "externalIds": {
            "MAG": "2999044305",
            "DBLP": "journals/nature/Senior0JKSGQZNB20",
            "DOI": "10.1038/s41586-019-1923-7",
            "CorpusId": 210221987,
            "PubMed": "31942072"
        },
        "corpusId": 210221987,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/3a083d843f891b3574494c385699c21766ce8b7a",
        "title": "Improved protein structure prediction using potentials from deep learning",
        "abstract": null,
        "venue": "Nature",
        "year": 2020,
        "referenceCount": 57,
        "citationCount": 2059,
        "influentialCitationCount": 70,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://discovery.ucl.ac.uk/10089234/1/343019_3_art_0_py4t4l_convrt.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that a neural network can be trained to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions, and the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-01-01",
        "journal": {
            "name": "Nature",
            "pages": "706 - 710",
            "volume": "577"
        },
        "citationStyles": {
            "bibtex": "@Article{Senior2020ImprovedPS,\n author = {A. Senior and Richard Evans and J. Jumper and J. Kirkpatrick and L. Sifre and Tim Green and Chongli Qin and Augustin Z\u00eddek and A. W. R. Nelson and Alex Bridgland and Hugo Penedones and Stig Petersen and K. Simonyan and Steve Crossan and Pushmeet Kohli and David T. Jones and David Silver and K. Kavukcuoglu and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {706 - 710},\n title = {Improved protein structure prediction using potentials from deep learning},\n volume = {577},\n year = {2020}\n}\n"
        }
    },
    "402_platypus-70b": {
        "paperId": "96f6ad72733599db609332987ec6b65e30f11d07",
        "externalIds": {
            "ArXiv": "2308.07317",
            "DBLP": "journals/corr/abs-2308-07317",
            "DOI": "10.48550/arXiv.2308.07317",
            "CorpusId": 260886870
        },
        "corpusId": 260886870,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/96f6ad72733599db609332987ec6b65e30f11d07",
        "title": "Platypus: Quick, Cheap, and Powerful Refinement of LLMs",
        "abstract": "We present $\\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on $\\textit{a single}$ A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field. Project page: https://platypus-llm.github.io",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 51,
        "citationCount": 50,
        "influentialCitationCount": 4,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2308.07317",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-artfine-tuned LLMs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-08-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2308.07317"
        },
        "citationStyles": {
            "bibtex": "@Article{Lee2023PlatypusQC,\n author = {Ariel N. Lee and Cole J. Hunter and Nataniel Ruiz},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Platypus: Quick, Cheap, and Powerful Refinement of LLMs},\n volume = {abs/2308.07317},\n year = {2023}\n}\n"
        }
    },
    "403_rt-2": {
        "paperId": "38939304bb760473141c2aca0305e44fbe04e6e8",
        "externalIds": {
            "DBLP": "journals/corr/abs-2307-15818",
            "ArXiv": "2307.15818",
            "DOI": "10.48550/arXiv.2307.15818",
            "CorpusId": 260293142
        },
        "corpusId": 260293142,
        "publicationVenue": {
            "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
            "name": "Conference on Robot Learning",
            "type": "conference",
            "alternate_names": [
                "CoRL",
                "Conf Robot Learn"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/38939304bb760473141c2aca0305e44fbe04e6e8",
        "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
        "abstract": "We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).",
        "venue": "Conference on Robot Learning",
        "year": 2023,
        "referenceCount": 92,
        "citationCount": 237,
        "influentialCitationCount": 17,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.15818",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a simple, general recipe to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-07-28",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2307.15818"
        },
        "citationStyles": {
            "bibtex": "@Article{Brohan2023RT2VM,\n author = {Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and K. Choromanski and Tianli Ding and Danny Driess and Chelsea Finn and Peter R. Florence and Chuyuan Fu and Montse Gonzalez Arenas and K. Gopalakrishnan and Kehang Han and Karol Hausman and Alexander Herzog and Jasmine Hsu and Brian Ichter and A. Irpan and Nikhil Joshi and Ryan C. Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal and S. Levine and H. Michalewski and Igor Mordatch and Karl Pertsch and Kanishka Rao and Krista Reymann and M. Ryoo and Grecia Salazar and Pannag R. Sanketi and P. Sermanet and Jaspiar Singh and Anika Singh and Radu Soricut and Huong Tran and Vincent Vanhoucke and Q. Vuong and Ayzaan Wahid and Stefan Welker and Paul Wohlhart and Ted Xiao and Tianhe Yu and Brianna Zitkovich},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},\n volume = {abs/2307.15818},\n year = {2023}\n}\n"
        }
    },
    "404_jft": {
        "paperId": "8760bc7631c0cb04e7138254e9fd6451b7def8ca",
        "externalIds": {
            "DBLP": "journals/corr/SunSSG17",
            "MAG": "2734663976",
            "ArXiv": "1707.02968",
            "DOI": "10.1109/ICCV.2017.97",
            "CorpusId": 6842201
        },
        "corpusId": 6842201,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/8760bc7631c0cb04e7138254e9fd6451b7def8ca",
        "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
        "abstract": "The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10 \u00d7 or 100 \u00d7 ? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between \u2018enormous data\u2019 and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "referenceCount": 43,
        "citationCount": 2003,
        "influentialCitationCount": 103,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1707.02968",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that the performance on vision tasks increases logarithmically based on volume of training data size, and it is shown that representation learning (or pre-training) still holds a lot of promise."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-07-10",
        "journal": {
            "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
            "pages": "843-852"
        },
        "citationStyles": {
            "bibtex": "@Article{Sun2017RevisitingUE,\n author = {Chen Sun and Abhinav Shrivastava and Saurabh Singh and A. Gupta},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {843-852},\n title = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},\n year = {2017}\n}\n"
        }
    },
    "406_rbm-tuning": {
        "paperId": "e95d3934e51107da7610acd0b1bcb6551671f9f1",
        "externalIds": {
            "DBLP": "series/lncs/Hinton12",
            "MAG": "44815768",
            "DOI": "10.1007/978-3-642-35289-8_32",
            "CorpusId": 21145246
        },
        "corpusId": 21145246,
        "publicationVenue": {
            "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
            "name": "Neural Networks",
            "type": "journal",
            "alternate_names": [
                "Neural Netw"
            ],
            "issn": "0893-6080",
            "url": "http://www.elsevier.com/locate/neunet",
            "alternate_urls": [
                "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                "http://www.sciencedirect.com/science/journal/08936080"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e95d3934e51107da7610acd0b1bcb6551671f9f1",
        "title": "A Practical Guide to Training Restricted Boltzmann Machines",
        "abstract": null,
        "venue": "Neural Networks",
        "year": 2012,
        "referenceCount": 27,
        "citationCount": 2999,
        "influentialCitationCount": 327,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This guide is an attempt to share expertise at training restricted Boltzmann machines with other machine learning researchers."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "pages": "599-619"
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Hinton2012APG,\n author = {Geoffrey E. Hinton},\n booktitle = {Neural Networks},\n pages = {599-619},\n title = {A Practical Guide to Training Restricted Boltzmann Machines},\n year = {2012}\n}\n"
        }
    },
    "408_rnn_(sgd+clr)_(ptb)": {
        "paperId": "ded103d0613e1a8f51f586cc1678aee3ff26e811",
        "externalIds": {
            "ArXiv": "1212.0901",
            "MAG": "2949427968",
            "DBLP": "conf/icassp/BengioBP13",
            "DOI": "10.1109/ICASSP.2013.6639349",
            "CorpusId": 12485056
        },
        "corpusId": 12485056,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/ded103d0613e1a8f51f586cc1678aee3ff26e811",
        "title": "Advances in optimizing recurrent networks",
        "abstract": "After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2012,
        "referenceCount": 34,
        "citationCount": 495,
        "influentialCitationCount": 36,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1212.0901",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "publicationDate": "2012-12-04",
        "journal": {
            "name": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "pages": "8624-8628"
        },
        "citationStyles": {
            "bibtex": "@Article{Bengio2012AdvancesIO,\n author = {Yoshua Bengio and Nicolas Boulanger-Lewandowski and Razvan Pascanu},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},\n pages = {8624-8628},\n title = {Advances in optimizing recurrent networks},\n year = {2012}\n}\n"
        }
    },
    "409_cogvlm": {
        "paperId": "2313afae52d98e569da2dedbf14daf9efc74e7cf",
        "externalIds": {
            "DBLP": "journals/corr/abs-2311-03079",
            "ArXiv": "2311.03079",
            "DOI": "10.48550/arXiv.2311.03079",
            "CorpusId": 265034288
        },
        "corpusId": 265034288,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/2313afae52d98e569da2dedbf14daf9efc74e7cf",
        "title": "CogVLM: Visual Expert for Pretrained Language Models",
        "abstract": "We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. Codes and checkpoints are available at https://github.com/THUDM/CogVLM.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 66,
        "citationCount": 44,
        "influentialCitationCount": 5,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces CogVLM, a powerful open-source visual language foundation model that bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers, enabling deep fusion of vision language features without sacrificing any performance on NLP tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-11-06",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2311.03079"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2023CogVLMVE,\n author = {Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {CogVLM: Visual Expert for Pretrained Language Models},\n volume = {abs/2311.03079},\n year = {2023}\n}\n"
        }
    },
    "410_generative_bst": {
        "paperId": "9b539d413393047b28bb7be9b195f142aaf7a80e",
        "externalIds": {
            "ACL": "2021.eacl-main.24",
            "MAG": "3023786569",
            "DBLP": "journals/corr/abs-2004-13637",
            "ArXiv": "2004.13637",
            "DOI": "10.18653/v1/2021.eacl-main.24",
            "CorpusId": 216562425
        },
        "corpusId": 216562425,
        "publicationVenue": {
            "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
            "name": "Conference of the European Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Conf Eur Chapter Assoc Comput Linguistics",
                "EACL"
            ],
            "url": "https://www.aclweb.org/anthology/venues/eacl/"
        },
        "url": "https://www.semanticscholar.org/paper/9b539d413393047b28bb7be9b195f142aaf7a80e",
        "title": "Recipes for Building an Open-Domain Chatbot",
        "abstract": "Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",
        "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
        "year": 2020,
        "referenceCount": 84,
        "citationCount": 819,
        "influentialCitationCount": 139,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.eacl-main.24.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Human evaluations show the best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements, and the limitations of this work are discussed by analyzing failure cases of the models."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-04-28",
        "journal": {
            "pages": "300-325"
        },
        "citationStyles": {
            "bibtex": "@Article{Roller2020RecipesFB,\n author = {Stephen Roller and Emily Dinan and Naman Goyal and Da Ju and Mary Williamson and Yinhan Liu and Jing Xu and Myle Ott and Kurt Shuster and Eric Michael Smith and Y-Lan Boureau and J. Weston},\n booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},\n pages = {300-325},\n title = {Recipes for Building an Open-Domain Chatbot},\n year = {2020}\n}\n"
        }
    },
    "411_tape_transformer": {
        "paperId": "ec7c9b201fc1ce18b4e0131691c9418f519a71c5",
        "externalIds": {
            "MAG": "2951433247",
            "DBLP": "conf/nips/RaoBTDCCAS19",
            "ArXiv": "1906.08230",
            "DOI": "10.1101/676825",
            "CorpusId": 195069360,
            "PubMed": "33390682"
        },
        "corpusId": 195069360,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/ec7c9b201fc1ce18b4e0131691c9418f519a71c5",
        "title": "Evaluating Protein Transfer Learning with TAPE",
        "abstract": "Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We bench-mark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences. TAPE will help the machine learning community focus effort on scientifically relevant problems. Toward this end, all data and code used to run these experiments are available at https://github.com/songlab-cal/tape.",
        "venue": "bioRxiv",
        "year": 2019,
        "referenceCount": 79,
        "citationCount": 577,
        "influentialCitationCount": 92,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2019/06/20/676825.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science",
            "Biology",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases and suggesting a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-06-19",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Rao2019EvaluatingPT,\n author = {Roshan Rao and Nicholas Bhattacharya and Neil Thomas and Yan Duan and Xi Chen and J. Canny and P. Abbeel and Yun S. Song},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {Evaluating Protein Transfer Learning with TAPE},\n year = {2019}\n}\n"
        }
    },
    "412_stacked_denoising_autoencoders": {
        "paperId": "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd",
        "externalIds": {
            "MAG": "2997574889",
            "DBLP": "journals/jmlr/VincentLLBM10",
            "DOI": "10.5555/1756006.1953039",
            "CorpusId": 17804904
        },
        "corpusId": 17804904,
        "publicationVenue": {
            "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
            "name": "Journal of machine learning research",
            "type": "journal",
            "alternate_names": [
                "Journal of Machine Learning Research",
                "J mach learn res",
                "J Mach Learn Res"
            ],
            "issn": "1532-4435",
            "alternate_issns": [
                "1533-7928"
            ],
            "url": "http://www.ai.mit.edu/projects/jmlr/",
            "alternate_urls": [
                "http://jmlr.csail.mit.edu/",
                "http://www.jmlr.org/",
                "http://portal.acm.org/affiliated/jmlr"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e2b7f37cd97a7907b1b8a41138721ed06a0b76cd",
        "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
        "abstract": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.",
        "venue": "Journal of machine learning research",
        "year": 2010,
        "referenceCount": 60,
        "citationCount": 6649,
        "influentialCitationCount": 567,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2010-03-01",
        "journal": {
            "name": "J. Mach. Learn. Res.",
            "pages": "3371-3408",
            "volume": "11"
        },
        "citationStyles": {
            "bibtex": "@Article{Vincent2010StackedDA,\n author = {Pascal Vincent and H. Larochelle and Isabelle Lajoie and Yoshua Bengio and Pierre-Antoine Manzagol},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {3371-3408},\n title = {Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},\n volume = {11},\n year = {2010}\n}\n"
        }
    },
    "413_neuro-symbolic_concept_learner": {
        "paperId": "50f76736c3090c6effac25400e5e40cc0b7b5ad9",
        "externalIds": {
            "ArXiv": "1904.12584",
            "MAG": "2908791737",
            "DBLP": "journals/corr/abs-1904-12584",
            "CorpusId": 108296442
        },
        "corpusId": 108296442,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/50f76736c3090c6effac25400e5e40cc0b7b5ad9",
        "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
        "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 54,
        "citationCount": 559,
        "influentialCitationCount": 63,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, the model learns by simply looking at images and reading paired questions and answers."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-04-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1904.12584"
        },
        "citationStyles": {
            "bibtex": "@Article{Mao2019TheNC,\n author = {Jiayuan Mao and Chuang Gan and Pushmeet Kohli and J. Tenenbaum and Jiajun Wu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},\n volume = {abs/1904.12584},\n year = {2019}\n}\n"
        }
    },
    "414_super-vector_coding": {
        "paperId": "4e65c9f0a64b6a4333b12e2adc3861ad75aca83b",
        "externalIds": {
            "MAG": "3032013330",
            "DBLP": "conf/eccv/ZhouYZH10",
            "DOI": "10.1007/978-3-642-15555-0_11",
            "CorpusId": 7405065
        },
        "corpusId": 7405065,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/4e65c9f0a64b6a4333b12e2adc3861ad75aca83b",
        "title": "Image Classification Using Super-Vector Coding of Local Image Descriptors",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2010,
        "referenceCount": 27,
        "citationCount": 563,
        "influentialCitationCount": 44,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-15555-0_11.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper introduces a new framework for image classification using local visual descriptors that first performs a non-linear feature transformation on descriptors, then aggregates the results together to form image-level representations, and finally applies a classification model."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2010-09-05",
        "journal": {
            "pages": "141-154"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhou2010ImageCU,\n author = {Xi Zhou and Kai Yu and Tong Zhang and Thomas S. Huang},\n booktitle = {European Conference on Computer Vision},\n pages = {141-154},\n title = {Image Classification Using Super-Vector Coding of Local Image Descriptors},\n year = {2010}\n}\n"
        }
    },
    "415_bpe": {
        "paperId": "1518039b5001f1836565215eb047526b3ac7f462",
        "externalIds": {
            "DBLP": "conf/acl/SennrichHB16a",
            "ACL": "P16-1162",
            "MAG": "1816313093",
            "ArXiv": "1508.07909",
            "DOI": "10.18653/v1/P16-1162",
            "CorpusId": 1114678
        },
        "corpusId": 1114678,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/1518039b5001f1836565215eb047526b3ac7f462",
        "title": "Neural Machine Translation of Rare Words with Subword Units",
        "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2015,
        "referenceCount": 42,
        "citationCount": 6602,
        "influentialCitationCount": 925,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P16-1162.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper introduces a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units, and empirically shows that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.3 BLEU."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-08-31",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1508.07909"
        },
        "citationStyles": {
            "bibtex": "@Article{Sennrich2015NeuralMT,\n author = {Rico Sennrich and B. Haddow and Alexandra Birch},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Neural Machine Translation of Rare Words with Subword Units},\n volume = {abs/1508.07909},\n year = {2015}\n}\n"
        }
    },
    "417_gpt-3_175b_(davinci)": {
        "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc",
        "externalIds": {
            "ArXiv": "2005.14165",
            "DBLP": "journals/corr/abs-2005-14165",
            "MAG": "3030163527",
            "CorpusId": 218971783
        },
        "corpusId": 218971783,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/6b85b63579a916f705a8e10a49bd8d849d91b1fc",
        "title": "Language Models are Few-Shot Learners",
        "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "referenceCount": 143,
        "citationCount": 21916,
        "influentialCitationCount": 2738,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-05-28",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2005.14165"
        },
        "citationStyles": {
            "bibtex": "@Article{Brown2020LanguageMA,\n author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and J. Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. Henighan and R. Child and A. Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and S. Gray and B. Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and I. Sutskever and Dario Amodei},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Language Models are Few-Shot Learners},\n volume = {abs/2005.14165},\n year = {2020}\n}\n"
        }
    },
    "420_ct-mos_(wt2)": {
        "paperId": "f617f7ba4040d6e85b384685da09fed35c841280",
        "externalIds": {
            "DBLP": "journals/corr/abs-2012-13575",
            "ArXiv": "2012.13575",
            "MAG": "2994888541",
            "CorpusId": 214250287
        },
        "corpusId": 214250287,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/f617f7ba4040d6e85b384685da09fed35c841280",
        "title": "Contextual Temperature for Language Modeling",
        "abstract": "Temperature scaling has been widely used as an effective approach to control the smoothness of a distribution, which helps the model performance in various tasks. Current practices to apply temperature scaling assume either a fixed, or a manually-crafted dynamically changing schedule. However, our studies indicate that the individual optimal trajectory for each class can change with the context. To this end, we propose contextual temperature, a generalized approach that learns an optimal temperature trajectory for each vocabulary over the context. Experimental results confirm that the proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2, respectively. In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties. These evidences further justify the need for the proposed method and its advantages over fixed temperature schedules.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 25,
        "citationCount": 7,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties, which justify the need for the proposed method and its advantages over fixed temperature schedules."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-09-25",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2012.13575"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2019ContextualTF,\n author = {Pei-Hsin Wang and Sheng-Iou Hsieh and Shih-Chieh Chang and Yu-Ting Chen and Jia-Yu Pan and Wei Wei and Da-Chang Juan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Contextual Temperature for Language Modeling},\n volume = {abs/2012.13575},\n year = {2019}\n}\n"
        }
    },
    "421_inflated_3d_convnet": {
        "paperId": "b61a3f8b80bbd44f24544dc915f52fd30bbdf485",
        "externalIds": {
            "ArXiv": "1705.07750",
            "MAG": "2619082050",
            "DBLP": "conf/cvpr/CarreiraZ17",
            "DOI": "10.1109/CVPR.2017.502",
            "CorpusId": 206596127
        },
        "corpusId": 206596127,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/b61a3f8b80bbd44f24544dc915f52fd30bbdf485",
        "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2% on HMDB-51 and 97.9% on UCF-101.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "referenceCount": 41,
        "citationCount": 6447,
        "influentialCitationCount": 1532,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1705.07750",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2% on HMDB-51 and 97.9% on UCF-101 after pre-training on Kinetics, and a new Two-Stream Inflated 3D Conv net that is based on 2D ConvNet inflation is introduced."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-05-22",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "4724-4733"
        },
        "citationStyles": {
            "bibtex": "@Article{Carreira2017QuoVA,\n author = {Jo\u00e3o Carreira and Andrew Zisserman},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4724-4733},\n title = {Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset},\n year = {2017}\n}\n"
        }
    },
    "422_msra_(c,_prelu)": {
        "paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
        "externalIds": {
            "MAG": "1677182931",
            "DBLP": "conf/iccv/HeZRS15",
            "ArXiv": "1502.01852",
            "DOI": "10.1109/ICCV.2015.123",
            "CorpusId": 13740328
        },
        "corpusId": 13740328,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
        "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "referenceCount": 46,
        "citationCount": 16426,
        "influentialCitationCount": 1169,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1502.01852",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit and derives a robust initialization method that particularly considers the rectifier nonlinearities."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-02-06",
        "journal": {
            "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
            "pages": "1026-1034"
        },
        "citationStyles": {
            "bibtex": "@Article{He2015DelvingDI,\n author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {1026-1034},\n title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},\n year = {2015}\n}\n"
        }
    },
    "423_(ensemble):_awd-lstm-doc_(fin)_\u00d7_5_(wt2)": {
        "paperId": "0be9ca65ad318ee3729928882ef2c403d4b6d24e",
        "externalIds": {
            "DBLP": "journals/corr/abs-1808-10143",
            "MAG": "2888799392",
            "ACL": "D18-1489",
            "ArXiv": "1808.10143",
            "DOI": "10.18653/v1/D18-1489",
            "CorpusId": 52138320
        },
        "corpusId": 52138320,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/0be9ca65ad318ee3729928882ef2c403d4b6d24e",
        "title": "Direct Output Connection for a High-Rank Language Model",
        "abstract": "This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also middle layers. This method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). Our proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to application tasks: machine translation and headline generation.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2018,
        "referenceCount": 45,
        "citationCount": 34,
        "influentialCitationCount": 3,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D18-1489.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also middle layers, and indicates the proposed method contributes to application tasks: machine translation and headline generation."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-08-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1808.10143"
        },
        "citationStyles": {
            "bibtex": "@Article{Takase2018DirectOC,\n author = {Sho Takase and Jun Suzuki and M. Nagata},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Direct Output Connection for a High-Rank Language Model},\n volume = {abs/1808.10143},\n year = {2018}\n}\n"
        }
    },
    "424_rnn_+_char4-ms-vec": {
        "paperId": "0aef56962035a79101821480f51897fdc4443945",
        "externalIds": {
            "MAG": "2905559537",
            "DBLP": "journals/corr/abs-1906-05506",
            "ArXiv": "1906.05506",
            "DOI": "10.1609/AAAI.V33I01.33015074",
            "CorpusId": 69615502
        },
        "corpusId": 69615502,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/0aef56962035a79101821480f51897fdc4443945",
        "title": "Character n-gram Embeddings to Improve RNN Language Models",
        "abstract": "This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "referenceCount": 48,
        "citationCount": 22,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4440/4318",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel Recurrent Neural Network (RNN) language model that takes advantage of character information based on research in the field of word embedding construction and combines them with ordinary word embeddings is proposed."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-06-13",
        "journal": {
            "pages": "5074-5082"
        },
        "citationStyles": {
            "bibtex": "@Article{Takase2019CharacterNE,\n author = {Sho Takase and Jun Suzuki and M. Nagata},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {5074-5082},\n title = {Character n-gram Embeddings to Improve RNN Language Models},\n year = {2019}\n}\n"
        }
    },
    "425_deepseekmoe-16b": {
        "paperId": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
        "externalIds": {
            "DBLP": "journals/corr/abs-2401-06066",
            "ArXiv": "2401.06066",
            "DOI": "10.48550/arXiv.2401.06066",
            "CorpusId": 266933338
        },
        "corpusId": 266933338,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
        "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
        "abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.",
        "venue": "arXiv.org",
        "year": 2024,
        "referenceCount": 61,
        "citationCount": 8,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2024-01-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2401.06066"
        },
        "citationStyles": {
            "bibtex": "@Article{Dai2024DeepSeekMoETU,\n author = {Damai Dai and Chengqi Deng and Chenggang Zhao and R. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Yu Wu and Zhenda Xie and Y. K. Li and Panpan Huang and Fuli Luo and Chong Ruan and Zhifang Sui and Wenfeng Liang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models},\n volume = {abs/2401.06066},\n year = {2024}\n}\n"
        }
    },
    "427_t0-xxl": {
        "paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca",
        "externalIds": {
            "ArXiv": "2110.08207",
            "DBLP": "conf/iclr/SanhWRBSACSRDBX22",
            "CorpusId": 239009562
        },
        "corpusId": 239009562,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/17dd3555fd1ccf1141cf984347fa1b3fd6b009ca",
        "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
        "abstract": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "referenceCount": 0,
        "citationCount": 1135,
        "influentialCitationCount": 107,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A system for easily mapping any natural language tasks into a human-readable prompted form and fine-tune a pretrained encoder-decoder model on this multitask mixture covering a wide variety of tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2110.08207"
        },
        "citationStyles": {
            "bibtex": "@Article{Sanh2021MultitaskPT,\n author = {Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal V. Nayak and Debajyoti Datta and Jonathan D. Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng-Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault F\u00e9vry and Jason Alan Fries and Ryan Teehan and Stella Biderman and Leo Gao and T. Bers and Thomas Wolf and Alexander M. Rush},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Multitask Prompted Training Enables Zero-Shot Task Generalization},\n volume = {abs/2110.08207},\n year = {2021}\n}\n"
        }
    },
    "428_gpt-2_(1.5b)": {
        "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
        "externalIds": {
            "MAG": "2955855238",
            "CorpusId": 160025533
        },
        "corpusId": 160025533,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
        "title": "Language Models are Unsupervised Multitask Learners",
        "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
        "venue": "",
        "year": 2019,
        "referenceCount": 75,
        "citationCount": 14760,
        "influentialCitationCount": 2643,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText, suggesting a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
        }
    },
    "429_convnet_similarity_metric": {
        "paperId": "cfaae9b6857b834043606df3342d8dc97524aa9d",
        "externalIds": {
            "DBLP": "conf/cvpr/ChopraHL05",
            "MAG": "2157364932",
            "DOI": "10.1109/CVPR.2005.202",
            "CorpusId": 5555257
        },
        "corpusId": 5555257,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/cfaae9b6857b834043606df3342d8dc97524aa9d",
        "title": "Learning a similarity metric discriminatively, with application to face verification",
        "abstract": "We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "referenceCount": 23,
        "citationCount": 3829,
        "influentialCitationCount": 222,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2005-06-20",
        "journal": {
            "name": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)",
            "pages": "539-546 vol. 1",
            "volume": "1"
        },
        "citationStyles": {
            "bibtex": "@Article{Chopra2005LearningAS,\n author = {S. Chopra and R. Hadsell and Yann LeCun},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},\n pages = {539-546 vol. 1},\n title = {Learning a similarity metric discriminatively, with application to face verification},\n volume = {1},\n year = {2005}\n}\n"
        }
    },
    "430_talk_convolution": {
        "paperId": "af34ea4242ca8725ea739ec1bef674bec10c1fa9",
        "externalIds": {
            "MAG": "3004484424",
            "DBLP": "journals/corr/abs-2002-03184",
            "ArXiv": "2002.03184",
            "CorpusId": 211068793
        },
        "corpusId": 211068793,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/af34ea4242ca8725ea739ec1bef674bec10c1fa9",
        "title": "Time-aware Large Kernel Convolutions",
        "abstract": "To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of $O(n^2)$. Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size $k$ acting as a limited-window self-attention, resulting in time complexity of $O(k{\\cdot}n)$. In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of $O(n)$, effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches.",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "referenceCount": 75,
        "citationCount": 28,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Time-aware Large Kernel (TaLK) Convolutions is introduced, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-02-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2002.03184"
        },
        "citationStyles": {
            "bibtex": "@Article{Lioutas2020TimeawareLK,\n author = {Vasileios Lioutas and Yuhong Guo},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Time-aware Large Kernel Convolutions},\n volume = {abs/2002.03184},\n year = {2020}\n}\n"
        }
    },
    "431_gpipe_(transformer)": {
        "paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38",
        "externalIds": {
            "MAG": "2991040477",
            "DBLP": "conf/nips/HuangCBFCCLNLWC19",
            "CorpusId": 53670168
        },
        "corpusId": 53670168,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/d79a26226393f687ddbc375e32055b40b8ad8d38",
        "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
        "abstract": "Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "referenceCount": 68,
        "citationCount": 1139,
        "influentialCitationCount": 141,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "GPipe is introduced, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers by pipelining different sub-sequences of layers on separate accelerators, resulting in almost linear speedup when a model is partitioned across multiple accelerators."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-11-16",
        "journal": {
            "pages": "103-112"
        },
        "citationStyles": {
            "bibtex": "@Article{Huang2018GPipeET,\n author = {Yanping Huang and Yonglong Cheng and Dehao Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Z. Chen},\n booktitle = {Neural Information Processing Systems},\n pages = {103-112},\n title = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},\n year = {2018}\n}\n"
        }
    },
    "432_word2vec_(small)": {
        "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
        "externalIds": {
            "ArXiv": "1310.4546",
            "MAG": "2950133940",
            "DBLP": "conf/nips/MikolovSCCD13",
            "CorpusId": 16447573
        },
        "corpusId": 16447573,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "referenceCount": 24,
        "citationCount": 31045,
        "influentialCitationCount": 3959,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2013-10-16",
        "journal": {
            "pages": "3111-3119"
        },
        "citationStyles": {
            "bibtex": "@Article{Mikolov2013DistributedRO,\n author = {Tomas Mikolov and I. Sutskever and Kai Chen and G. Corrado and J. Dean},\n booktitle = {Neural Information Processing Systems},\n pages = {3111-3119},\n title = {Distributed Representations of Words and Phrases and their Compositionality},\n year = {2013}\n}\n"
        }
    },
    "434_trocr": {
        "paperId": "de688c6e73ccf6ed33ff1cc7919d24456a1f74e2",
        "externalIds": {
            "DBLP": "journals/corr/abs-2109-10282",
            "ArXiv": "2109.10282",
            "DOI": "10.1609/aaai.v37i11.26538",
            "CorpusId": 237581568
        },
        "corpusId": 237581568,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/de688c6e73ccf6ed33ff1cc7919d24456a1f74e2",
        "title": "TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models",
        "abstract": "Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at https://aka.ms/trocr.",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2021,
        "referenceCount": 74,
        "citationCount": 146,
        "influentialCitationCount": 23,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/26538/26310",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets, and outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-09-21",
        "journal": {
            "pages": "13094-13102"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2021TrOCRTO,\n author = {Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and D. Flor\u00eancio and Cha Zhang and Zhoujun Li and Furu Wei},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {13094-13102},\n title = {TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models},\n year = {2021}\n}\n"
        }
    },
    "435_restricted_bolzmann_machines": {
        "paperId": "1626c940a64ad96a7ed53d7d6c0df63c6696956b",
        "externalIds": {
            "MAG": "2099866409",
            "DBLP": "conf/icml/SalakhutdinovMH07",
            "DOI": "10.1145/1273496.1273596",
            "CorpusId": 7285098
        },
        "corpusId": 7285098,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/1626c940a64ad96a7ed53d7d6c0df63c6696956b",
        "title": "Restricted Boltzmann machines for collaborative filtering",
        "abstract": "Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.",
        "venue": "International Conference on Machine Learning",
        "year": 2007,
        "referenceCount": 15,
        "citationCount": 1971,
        "influentialCitationCount": 150,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper shows how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies, and demonstrates that RBM's can be successfully applied to the Netflix data set."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2007-06-20",
        "journal": {
            "pages": "791-798"
        },
        "citationStyles": {
            "bibtex": "@Article{Salakhutdinov2007RestrictedBM,\n author = {R. Salakhutdinov and A. Mnih and Geoffrey E. Hinton},\n booktitle = {International Conference on Machine Learning},\n pages = {791-798},\n title = {Restricted Boltzmann machines for collaborative filtering},\n year = {2007}\n}\n"
        }
    },
    "436_megatron-turing_nlg_530b": {
        "paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19",
        "externalIds": {
            "ArXiv": "2201.11990",
            "DBLP": "journals/corr/abs-2201-11990",
            "CorpusId": 246411325
        },
        "corpusId": 246411325,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/7cbc2a7843411a1768ab762930707af0a3c33a19",
        "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
        "abstract": "Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 78,
        "citationCount": 519,
        "influentialCitationCount": 35,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The infrastructure as well as the 3D parallelism methodology used to train the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters is presented."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-01-28",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2201.11990"
        },
        "citationStyles": {
            "bibtex": "@Article{Smith2022UsingDA,\n author = {Shaden Smith and M. Patwary and Brandon Norick and P. LeGresley and Samyam Rajbhandari and J. Casper and Zhun Liu and Shrimai Prabhumoye and George Zerveas and V. Korthikanti and Elton Zhang and R. Child and Reza Yazdani Aminabadi and J. Bernauer and Xia Song and M. Shoeybi and Yuxiong He and Michael Houston and Saurabh Tiwary and Bryan Catanzaro},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},\n volume = {abs/2201.11990},\n year = {2022}\n}\n"
        }
    },
    "438_textual_imager": {
        "paperId": "755e9f43ce398ae8737366720c5f82685b0c253e",
        "externalIds": {
            "DBLP": "conf/nips/SocherGMN13",
            "MAG": "2927762331",
            "ArXiv": "1301.3666",
            "CorpusId": 2808203
        },
        "corpusId": 2808203,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/755e9f43ce398ae8737366720c5f82685b0c253e",
        "title": "Zero-Shot Learning Through Cross-Modal Transfer",
        "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "referenceCount": 36,
        "citationCount": 1379,
        "influentialCitationCount": 156,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces a model that can recognize objects in images even if no training data is available for the object class, and uses novelty detection methods to differentiate unseen classes from seen classes."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2013-01-16",
        "journal": {
            "pages": "935-943"
        },
        "citationStyles": {
            "bibtex": "@Article{Socher2013ZeroShotLT,\n author = {R. Socher and M. Ganjoo and Christopher D. Manning and A. Ng},\n booktitle = {Neural Information Processing Systems},\n pages = {935-943},\n title = {Zero-Shot Learning Through Cross-Modal Transfer},\n year = {2013}\n}\n"
        }
    },
    "439_mamba-24m_(sc09)": {
        "paperId": "432bef8e34014d726c674bc458008ac895297b51",
        "externalIds": {
            "DBLP": "journals/corr/abs-2312-00752",
            "ArXiv": "2312.00752",
            "DOI": "10.48550/arXiv.2312.00752",
            "CorpusId": 265551773
        },
        "corpusId": 265551773,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/432bef8e34014d726c674bc458008ac895297b51",
        "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 0,
        "citationCount": 102,
        "influentialCitationCount": 33,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba)."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-12-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2312.00752"
        },
        "citationStyles": {
            "bibtex": "@Article{Gu2023MambaLS,\n author = {Albert Gu and Tri Dao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},\n volume = {abs/2312.00752},\n year = {2023}\n}\n"
        }
    },
    "440_mlp_as_bayesian_approximator": {
        "paperId": "b217788dd6d274ad391ee950e6f6a34033bd2fc7",
        "externalIds": {
            "DBLP": "journals/tnn/RuckRKOS90",
            "MAG": "2141278204",
            "DOI": "10.1109/72.80266",
            "CorpusId": 13199036,
            "PubMed": "18282850"
        },
        "corpusId": 13199036,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/b217788dd6d274ad391ee950e6f6a34033bd2fc7",
        "title": "The multilayer perceptron as an approximation to a Bayes optimal discriminant function",
        "abstract": "The multilayer perceptron, when trained as a classifier using backpropagation, is shown to approximate the Bayes optimal discriminant function. The result is demonstrated for both the two-class problem and multiple classes. It is shown that the outputs of the multilayer perceptron approximate the a posteriori probability functions of the classes being trained. The proof applies to any number of layers and any type of unit activation function, linear or nonlinear.",
        "venue": "IEEE Trans. Neural Networks",
        "year": 1990,
        "referenceCount": 6,
        "citationCount": 860,
        "influentialCitationCount": 29,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The multilayer perceptron, when trained as a classifier using backpropagation, is shown to approximate the Bayes optimal discriminant function."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1990-12-01",
        "journal": {
            "name": "IEEE transactions on neural networks",
            "pages": "\n          296-8\n        ",
            "volume": "1 4"
        },
        "citationStyles": {
            "bibtex": "@Article{Ruck1990TheMP,\n author = {D. Ruck and S. Rogers and M. Kabrisky and M. Oxley and B. Suter},\n booktitle = {IEEE Trans. Neural Networks},\n journal = {IEEE transactions on neural networks},\n pages = {\n          296-8\n        },\n title = {The multilayer perceptron as an approximation to a Bayes optimal discriminant function},\n volume = {1 4},\n year = {1990}\n}\n"
        }
    },
    "441_adaptive_broom_balancer": {
        "paperId": "2ebde07a28c390b5787df6946d4729037f37c9c0",
        "externalIds": {
            "DBLP": "conf/icnn/TolatW88",
            "MAG": "2030101996",
            "DOI": "10.1109/ICNN.1988.23982",
            "CorpusId": 15088931
        },
        "corpusId": 15088931,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/2ebde07a28c390b5787df6946d4729037f37c9c0",
        "title": "An adaptive 'broom balancer' with visual inputs",
        "abstract": "An adaptive network with visual inputs has been trained to balance an inverted pendulum. Simulation results show that the network is capable of extracting the necessary state information from time sequences of crude visual images. A single linear adaptive threshold element (ADALINE) was adequate for this task. When tested by simulation, the performance achieved was sufficient to keep the pendulum from falling. The adaptive network's ability to generalize made this possible since the training set encompassed only a fraction of all possible states.<<ETX>>",
        "venue": "IEEE 1988 International Conference on Neural Networks",
        "year": 1988,
        "referenceCount": 7,
        "citationCount": 55,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www-isl.stanford.edu/people/widrow/papers/c1988anadaptive.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An adaptive network with visual inputs has been trained to balance an inverted pendulum and simulation results show that the network is capable of extracting the necessary state information from time sequences of crude visual images."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "1988-07-24",
        "journal": {
            "name": "IEEE 1988 International Conference on Neural Networks",
            "pages": "641-647 vol.2"
        },
        "citationStyles": {
            "bibtex": "@Article{Tolat1988AnA,\n author = {V. V. Tolat and B. Widrow},\n booktitle = {IEEE 1988 International Conference on Neural Networks},\n journal = {IEEE 1988 International Conference on Neural Networks},\n pages = {641-647 vol.2},\n title = {An adaptive 'broom balancer' with visual inputs},\n year = {1988}\n}\n"
        }
    },
    "443_permuteformer": {
        "paperId": "bb363c8c5bc1c473f0801c647c88d0c071792858",
        "externalIds": {
            "ACL": "2021.emnlp-main.828",
            "DBLP": "conf/emnlp/Chen21",
            "ArXiv": "2109.02377",
            "DOI": "10.18653/v1/2021.emnlp-main.828",
            "CorpusId": 237421288
        },
        "corpusId": 237421288,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/bb363c8c5bc1c473f0801c647c88d0c071792858",
        "title": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences",
        "abstract": "A recent variation of Transformer, Performer, scales Transformer to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to Performer. Based on the analysis, we propose PermuteFormer, a Performer-based model with relative position encoding that scales linearly on long sequences. PermuteFormer applies position-dependent transformation on queries and keys to encode positional information into the attention module. This transformation is carefully crafted so that the final output of self-attention is not affected by absolute positions of tokens. PermuteFormer introduces negligible computational overhead by design that it runs as fast as Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long sequences, as well as WikiText-103, a language modeling dataset. The experiments show that PermuteFormer uniformly improves the performance of Performer with almost no computational overhead and outperforms vanilla Transformer on most of the tasks.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "referenceCount": 30,
        "citationCount": 15,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.828.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "PermuteFormer is proposed, a Performer-based model with relative position encoding that scales linearly on long sequences and uniformly improves the performance of Performer with almost no computational overhead and outperforms vanilla Transformer on most of the tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-09-06",
        "journal": {
            "pages": "10606-10618"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2021PermuteFormerER,\n author = {Peng-Jen Chen},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {10606-10618},\n title = {PermuteFormer: Efficient Relative Position Encoding for Long Sequences},\n year = {2021}\n}\n"
        }
    },
    "444_biggan-deep_512x512": {
        "paperId": "22aab110058ebbd198edb1f1e7b4f69fb13c0613",
        "externalIds": {
            "ArXiv": "1809.11096",
            "DBLP": "journals/corr/abs-1809-11096",
            "MAG": "2893749619",
            "CorpusId": 52889459
        },
        "corpusId": 52889459,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/22aab110058ebbd198edb1f1e7b4f69fb13c0613",
        "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
        "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 60,
        "citationCount": 4340,
        "influentialCitationCount": 578,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-09-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1809.11096"
        },
        "citationStyles": {
            "bibtex": "@Article{Brock2018LargeSG,\n author = {Andrew Brock and Jeff Donahue and K. Simonyan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Large Scale GAN Training for High Fidelity Natural Image Synthesis},\n volume = {abs/1809.11096},\n year = {2018}\n}\n"
        }
    },
    "445_population-based_drl": {
        "paperId": "ea64dbd29ab90d8944bcc6a05680aeaacc86fc0a",
        "externalIds": {
            "MAG": "2947862328",
            "ArXiv": "1807.01281",
            "DBLP": "journals/corr/abs-1807-01281",
            "DOI": "10.1126/science.aau6249",
            "CorpusId": 49561741,
            "PubMed": "31147514"
        },
        "corpusId": 49561741,
        "publicationVenue": {
            "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
            "name": "Science",
            "type": "journal",
            "issn": "0193-4511",
            "alternate_issns": [
                "0036-8075"
            ],
            "url": "https://www.jstor.org/journal/science",
            "alternate_urls": [
                "https://www.sciencemag.org/",
                "http://www.sciencemag.org/",
                "http://www.jstor.org/journals/00368075.html",
                "http://www.sciencemag.org/archive/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/ea64dbd29ab90d8944bcc6a05680aeaacc86fc0a",
        "title": "Human-level performance in 3D multiplayer games with population-based reinforcement learning",
        "abstract": "Artificial teamwork Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans. Science, this issue p. 859 Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode. Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.",
        "venue": "Science",
        "year": 2018,
        "referenceCount": 99,
        "citationCount": 607,
        "influentialCitationCount": 25,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://science.sciencemag.org/content/sci/364/6443/859.full.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A tournament-style evaluation is used to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-07-03",
        "journal": {
            "name": "Science",
            "pages": "859 - 865",
            "volume": "364"
        },
        "citationStyles": {
            "bibtex": "@Article{Jaderberg2018HumanlevelPI,\n author = {Max Jaderberg and Wojciech M. Czarnecki and Iain Dunning and Luke Marris and Guy Lever and Antonio Garc\u00eda Casta\u00f1eda and Charlie Beattie and Neil C. Rabinowitz and Ari S. Morcos and Avraham Ruderman and Nicolas Sonnerat and Tim Green and Louise Deason and Joel Z. Leibo and David Silver and D. Hassabis and K. Kavukcuoglu and T. Graepel},\n booktitle = {Science},\n journal = {Science},\n pages = {859 - 865},\n title = {Human-level performance in 3D multiplayer games with population-based reinforcement learning},\n volume = {364},\n year = {2018}\n}\n"
        }
    },
    "446_alibi_(l=3072,_lvalid_=_3072)": {
        "paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd",
        "externalIds": {
            "ArXiv": "2108.12409",
            "DBLP": "journals/corr/abs-2108-12409",
            "CorpusId": 237347130
        },
        "corpusId": 237347130,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9ca329408813d209b1dcb36936f7f9cba82506bd",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "referenceCount": 48,
        "citationCount": 317,
        "influentialCitationCount": 51,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work shows that extrapolation can be enabled by simply changing the position representation method, though it finds that current methods do not allow for efficient extrapolation, and introduces a simpler and more efficient position method, Attention with Linear Biases (ALiBi)."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-08-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2108.12409"
        },
        "citationStyles": {
            "bibtex": "@Article{Press2021TrainST,\n author = {Ofir Press and Noah A. Smith and M. Lewis},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},\n volume = {abs/2108.12409},\n year = {2021}\n}\n"
        }
    },
    "448_llama_2-7b": {
        "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "externalIds": {
            "ArXiv": "2307.09288",
            "DBLP": "journals/corr/abs-2307-09288",
            "CorpusId": 259950998
        },
        "corpusId": 259950998,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/104b0bb1da562d53cbda87aec79ef6a2827d191a",
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 131,
        "citationCount": 2639,
        "influentialCitationCount": 430,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work develops and releases Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters, which may be a suitable substitute for closed-source models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-07-18",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2307.09288"
        },
        "citationStyles": {
            "bibtex": "@Article{Touvron2023Llama2O,\n author = {Hugo Touvron and Louis Martin and Kevin R. Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and D. Bikel and Lukas Blecher and Cristian Cant\u00f3n Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and A. Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel M. Kloumann and A. Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and R. Subramanian and Xia Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zhengxu Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},\n volume = {abs/2307.09288},\n year = {2023}\n}\n"
        }
    },
    "450_neocognitron": {
        "paperId": "69e68bfaadf2dccff800158749f5a50fe82d173b",
        "externalIds": {
            "MAG": "2101926813",
            "DOI": "10.1007/BF00344251",
            "CorpusId": 206775608,
            "PubMed": "7370364"
        },
        "corpusId": 206775608,
        "publicationVenue": {
            "id": "57cada26-a03e-494e-929e-a71ac35f2ad0",
            "name": "Biological cybernetics",
            "type": "journal",
            "alternate_names": [
                "Biological cybern",
                "Biological Cybern",
                "Biological Cybernetics"
            ],
            "issn": "0340-1200",
            "url": "http://link.springer.com/journal/422",
            "alternate_urls": [
                "https://link.springer.com/journal/volumesAndIssues/422"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/69e68bfaadf2dccff800158749f5a50fe82d173b",
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "abstract": null,
        "venue": "Biological cybernetics",
        "year": 1980,
        "referenceCount": 17,
        "citationCount": 4289,
        "influentialCitationCount": 226,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A neural network model for a mechanism of visual pattern recognition that is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity of their shapes without affected by their positions."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1980-04-01",
        "journal": {
            "name": "Biological Cybernetics",
            "pages": "193-202",
            "volume": "36"
        },
        "citationStyles": {
            "bibtex": "@Article{Fukushima1980NeocognitronAS,\n author = {K. Fukushima},\n booktitle = {Biological cybernetics},\n journal = {Biological Cybernetics},\n pages = {193-202},\n title = {Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},\n volume = {36},\n year = {1980}\n}\n"
        }
    },
    "451_llava-1.5": {
        "paperId": "124d4d374fbef2016fa9880489871a58a7450644",
        "externalIds": {
            "ArXiv": "2310.03744",
            "DBLP": "journals/corr/abs-2310-03744",
            "DOI": "10.48550/arXiv.2310.03744",
            "CorpusId": 263672058
        },
        "corpusId": 263672058,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/124d4d374fbef2016fa9880489871a58a7450644",
        "title": "Improved Baselines with Visual Instruction Tuning",
        "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 55,
        "citationCount": 311,
        "influentialCitationCount": 81,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.03744",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient, and can make state-of-the-art LMM research more accessible."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-10-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2310.03744"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2023ImprovedBW,\n author = {Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Improved Baselines with Visual Instruction Tuning},\n volume = {abs/2310.03744},\n year = {2023}\n}\n"
        }
    },
    "452_xlnet": {
        "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "externalIds": {
            "DBLP": "conf/nips/YangDYCSL19",
            "MAG": "2950813464",
            "ArXiv": "1906.08237",
            "CorpusId": 195069387
        },
        "corpusId": 195069387,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
        "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "referenceCount": 47,
        "citationCount": 6966,
        "influentialCitationCount": 852,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "XLNet is proposed, a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autore progressive formulation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-06-19",
        "journal": {
            "pages": "5754-5764"
        },
        "citationStyles": {
            "bibtex": "@Article{Yang2019XLNetGA,\n author = {Zhilin Yang and Zihang Dai and Yiming Yang and J. Carbonell and R. Salakhutdinov and Quoc V. Le},\n booktitle = {Neural Information Processing Systems},\n pages = {5754-5764},\n title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},\n year = {2019}\n}\n"
        }
    },
    "454_basic-l": {
        "paperId": "d257547d681f3a153f4bbf85f5955b5a0189e500",
        "externalIds": {
            "DBLP": "journals/corr/abs-2111-10050",
            "ArXiv": "2111.10050",
            "DOI": "10.1016/j.neucom.2023.126658",
            "CorpusId": 244463149
        },
        "corpusId": 244463149,
        "publicationVenue": {
            "id": "df12d289-f447-47d3-8846-75e39de3ab57",
            "name": "Neurocomputing",
            "type": "journal",
            "issn": "0925-2312",
            "url": "http://www.elsevier.com/locate/neucom",
            "alternate_urls": [
                "http://www.sciencedirect.com/science/journal/09252312"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/d257547d681f3a153f4bbf85f5955b5a0189e500",
        "title": "Combined Scaling for Zero-shot Transfer Learning",
        "abstract": null,
        "venue": "Neurocomputing",
        "year": 2021,
        "referenceCount": 121,
        "citationCount": 120,
        "influentialCitationCount": 22,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A theoretical framework is developed which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as BASIC, and sheds light on the benefits of large contrastivebatch sizes."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-11-19",
        "journal": {
            "name": "Neurocomputing",
            "pages": "126658",
            "volume": "555"
        },
        "citationStyles": {
            "bibtex": "@Article{Pham2021CombinedSF,\n author = {Hieu Pham and Zihang Dai and Golnaz Ghiasi and Hanxiao Liu and Adams Wei Yu and Minh-Thang Luong and Mingxing Tan and Quoc V. Le},\n booktitle = {Neurocomputing},\n journal = {Neurocomputing},\n pages = {126658},\n title = {Combined Scaling for Zero-shot Transfer Learning},\n volume = {555},\n year = {2021}\n}\n"
        }
    },
    "455_immediate_trihead": {
        "paperId": "436772d9a916f0382800cf18581cfdfd4f83c457",
        "externalIds": {
            "ACL": "P01-1017",
            "DBLP": "conf/acl/Charniak01",
            "MAG": "2155693943",
            "DOI": "10.3115/1073012.1073029",
            "CorpusId": 457176
        },
        "corpusId": 457176,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/436772d9a916f0382800cf18581cfdfd4f83c457",
        "title": "Immediate-Head Parsing for Language Models",
        "abstract": "We present two language models based upon an \"immediate-head\" parser --- our name for a parser that conditions all events below a constituent c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model. For the better of our two models these improvements are 24% and 14% respectively. We also suggest that improvement of the underlying parser should significantly improve the model's perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2001,
        "referenceCount": 22,
        "citationCount": 358,
        "influentialCitationCount": 34,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1073012.1073029",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is suggested that improvement of the underlying parser should significantly improve the model's perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2001-07-06",
        "journal": {
            "pages": "116-123"
        },
        "citationStyles": {
            "bibtex": "@Article{Charniak2001ImmediateHeadPF,\n author = {Eugene Charniak},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {116-123},\n title = {Immediate-Head Parsing for Language Models},\n year = {2001}\n}\n"
        }
    },
    "456_tcn_(148m)": {
        "paperId": "3bb63fdb4670745f8c97d8cad1a8a9603b1c16f5",
        "externalIds": {
            "DBLP": "conf/iclr/BaiKK18",
            "MAG": "2786228682",
            "CorpusId": 3291855
        },
        "corpusId": 3291855,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/3bb63fdb4670745f8c97d8cad1a8a9603b1c16f5",
        "title": "Convolutional Sequence Modeling Revisited",
        "abstract": "Although both convolutional and recurrent architectures have a long history in sequence prediction, the current \u201cdefault\u201d mindset in much of the deep learning community is that generic sequence modeling is best handled using recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should a practitioner use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. In particular, the models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We further show that the potential \u201cinfinite memory\u201d advantage that RNNs have over TCNs is largely absent in practice: TCNs indeed exhibit longer effective history sizes than their recurrent counterparts. As a whole, we argue that it may be time to (re)consider ConvNets as the default \u201cgo to\u201d architecture for sequence modeling.",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 11,
        "citationCount": 55,
        "influentialCitationCount": 11,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is argued that it may be time to (re)consider ConvNets as the default \u201cgo to\u201d architecture for sequence modeling, and the potential \u201cinfinite memory\u201d advantage that RNNs have over TCNs is largely absent in practice."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-02-12",
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Bai2018ConvolutionalSM,\n author = {Shaojie Bai and J. Z. Kolter and V. Koltun},\n booktitle = {International Conference on Learning Representations},\n title = {Convolutional Sequence Modeling Revisited},\n year = {2018}\n}\n"
        }
    },
    "457_memoreader": {
        "paperId": "b42e784ee14709828d9f1028f4147fb904d0be7c",
        "externalIds": {
            "MAG": "2898858752",
            "DBLP": "conf/emnlp/BackYIKC18",
            "ACL": "D18-1237",
            "DOI": "10.18653/v1/D18-1237",
            "CorpusId": 53082056
        },
        "corpusId": 53082056,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/b42e784ee14709828d9f1028f4147fb904d0be7c",
        "title": "MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller",
        "abstract": "Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text. Existing approaches made a significant progress comparable to human-level performance, but they are still limited in understanding, up to a few paragraphs, failing to properly comprehend lengthy document. In this paper, we propose a novel deep neural network architecture to handle a long-range dependency in RC tasks. In detail, our method has two novel aspects: (1) an advanced memory-augmented architecture and (2) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory. Our proposed architecture is widely applicable to other models. We have performed extensive experiments with well-known benchmark datasets such as TriviaQA, QUASAR-T, and SQuAD. The experimental results demonstrate that the proposed method outperforms existing methods, especially for lengthy documents.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2018,
        "referenceCount": 29,
        "citationCount": 13,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D18-1237.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel deep neural network architecture to handle a long-range dependency in RC tasks with two novel aspects: an advanced memory-augmented architecture and an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "2131-2140"
        },
        "citationStyles": {
            "bibtex": "@Article{Back2018MemoReaderLR,\n author = {Seohyun Back and Seunghak Yu and S. Indurthi and J. Kim and J. Choo},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {2131-2140},\n title = {MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller},\n year = {2018}\n}\n"
        }
    },
    "458_m6-10t": {
        "paperId": "24e775b20adf21e9b5b95c6a9b7a5c164d055849",
        "externalIds": {
            "DBLP": "journals/corr/abs-2110-03888",
            "ArXiv": "2110.03888",
            "CorpusId": 238531482
        },
        "corpusId": 238531482,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/24e775b20adf21e9b5b95c6a9b7a5c164d055849",
        "title": "M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining",
        "abstract": "Recent expeditious developments in deep learning algorithms, distributed training, and even hardware design for large models have enabled training extreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of billions or even trillions of parameters. However, under limited resources, extreme-scale model training that requires enormous amounts of computes and memory footprint suffers from frustratingly low efficiency in model convergence. In this paper, we propose a simple training strategy called\"Pseudo-to-Real\"for high-memory-footprint-required large models. Pseudo-to-Real is compatible with large models with architecture of sequential layers. We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the application of Pseudo-to-Real, we also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities. Fast training of extreme-scale models on a decent amount of resources can bring much smaller carbon footprint and contribute to greener AI.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 52,
        "citationCount": 30,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Pseudo-to-Real is a simple training strategy for high-memory-footprint-required large models that is compatible with large models with architecture of sequential layers and demonstrates a practice of pretraining unprecedented 10-trillion-parameter model on solely 512 GPUs within 10 days."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2110.03888"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2021M610TAS,\n author = {Junyang Lin and An Yang and Jinze Bai and Chang Zhou and Le Jiang and Xianyan Jia and Ang Wang and J. Zhang and Yong Li and Wei Lin and Jingren Zhou and Hongxia Yang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining},\n volume = {abs/2110.03888},\n year = {2021}\n}\n"
        }
    },
    "460_ctr-bert": {
        "paperId": "32ead456a7316d26893861169f57ddaeb512dabb",
        "externalIds": {
            "CorpusId": 243844336
        },
        "corpusId": 243844336,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/32ead456a7316d26893861169f57ddaeb512dabb",
        "title": "CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models",
        "abstract": "While pre-trained large language models (LLM) like BERT have achieved state-of-the-art in several NLP tasks, their performance on tasks with additional grounding e.g. with numeric and categorical features is less studied. In this paper, we study the application of pre-trained LLM for Click-through-rate (CTR) prediction for product advertisement in e-commerce. This is challenging because the model needs to a) learn from language as well as tabular data features, b) maintain low-latency (<5 ms) at inference time, and c) adapt to constantly changing advertisement distribution. We \ufb01rst show that scaling the pre-trained language model to 1.5 billion parameters signi\ufb01cantly improves performance over conventional CTR baselines. We then present CTR-BERT, a novel lightweight cache-friendly factorized model for CTR prediction that consists of twin-structured BERT-like encoders for text with a mechanism for late fusion for text and tabular features. We train the CTR-BERT model using cross-architecture knowledge distillation (KD) and empirically study the interaction between KD and distribution shift in this setting, by a) experimenting with pre-training, distillation pre-\ufb01netuning and \ufb01ne-tuning strategies b) factorizing features based on their distribution shift time scales, that allows the model to readily adapt and be re-trained. Finally, we show that CTR-BERT signi\ufb01cantly outperforms a traditional CTR baseline with a 2.3% relative ROC-AUC lift in of\ufb02ine experiments and a 2% CTR lift in an online experiment.",
        "venue": "",
        "year": 2021,
        "referenceCount": 21,
        "citationCount": 28,
        "influentialCitationCount": 4,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": null,
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Muhamed2021CTRBERTCK,\n author = {Aashiq Muhamed and I. Keivanloo and Sujan Perera and J. Mracek and Yi Xu and Qi Cui and Santosh Rajagopalan and Belinda Zeng and Trishul M. Chilimbi},\n title = {CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models},\n year = {2021}\n}\n"
        }
    },
    "461_awd-lstm": {
        "paperId": "2397ce306e5d7f3d0492276e357fb1833536b5d8",
        "externalIds": {
            "DBLP": "conf/iclr/MelisDB18",
            "ArXiv": "1707.05589",
            "MAG": "2739255396",
            "CorpusId": 33513311
        },
        "corpusId": 33513311,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/2397ce306e5d7f3d0492276e357fb1833536b5d8",
        "title": "On the State of the Art of Evaluation in Neural Language Models",
        "abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "referenceCount": 27,
        "citationCount": 495,
        "influentialCitationCount": 44,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrives at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-07-18",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1707.05589"
        },
        "citationStyles": {
            "bibtex": "@Article{Melis2017OnTS,\n author = {G\u00e1bor Melis and Chris Dyer and Phil Blunsom},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On the State of the Art of Evaluation in Neural Language Models},\n volume = {abs/1707.05589},\n year = {2017}\n}\n"
        }
    },
    "463_dmn": {
        "paperId": "452059171226626718eb677358836328f884298e",
        "externalIds": {
            "MAG": "2949215902",
            "ArXiv": "1506.07285",
            "DBLP": "conf/icml/KumarIOIBGZPS16",
            "CorpusId": 2319779
        },
        "corpusId": 2319779,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/452059171226626718eb677358836328f884298e",
        "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",
        "abstract": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "referenceCount": 52,
        "citationCount": 1131,
        "influentialCitationCount": 106,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers, is introduced."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-06-24",
        "journal": {
            "pages": "1378-1387"
        },
        "citationStyles": {
            "bibtex": "@Article{Kumar2015AskMA,\n author = {A. Kumar and Ozan Irsoy and Peter Ondruska and Mohit Iyyer and James Bradbury and Ishaan Gulrajani and Victor Zhong and Romain Paulus and R. Socher},\n booktitle = {International Conference on Machine Learning},\n pages = {1378-1387},\n title = {Ask Me Anything: Dynamic Memory Networks for Natural Language Processing},\n year = {2015}\n}\n"
        }
    },
    "465_muzero": {
        "paperId": "3507bd62a14bd0e8ead28cdedb1c33ba83c39c6b",
        "externalIds": {
            "DBLP": "journals/nature/SchrittwieserAH20",
            "ArXiv": "1911.08265",
            "MAG": "2989847975",
            "DOI": "10.1038/s41586-020-03051-4",
            "CorpusId": 208158225,
            "PubMed": "33361790"
        },
        "corpusId": 208158225,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/3507bd62a14bd0e8ead28cdedb1c33ba83c39c6b",
        "title": "Mastering Atari, Go, chess and shogi by planning with a learned model",
        "abstract": null,
        "venue": "Nature",
        "year": 2019,
        "referenceCount": 55,
        "citationCount": 1471,
        "influentialCitationCount": 123,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1911.08265",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The MuZero algorithm is presented, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-11-19",
        "journal": {
            "name": "Nature",
            "pages": "604 - 609",
            "volume": "588"
        },
        "citationStyles": {
            "bibtex": "@Article{Schrittwieser2019MasteringAG,\n author = {Julian Schrittwieser and Ioannis Antonoglou and T. Hubert and K. Simonyan and L. Sifre and Simon Schmitt and A. Guez and Edward Lockhart and D. Hassabis and T. Graepel and T. Lillicrap and David Silver},\n booktitle = {Nature},\n journal = {Nature},\n pages = {604 - 609},\n title = {Mastering Atari, Go, chess and shogi by planning with a learned model},\n volume = {588},\n year = {2019}\n}\n"
        }
    },
    "466_capsnet_(mnist)": {
        "paperId": "c4c06578f4870e4b126e6837907929f3c900b99f",
        "externalIds": {
            "ArXiv": "1710.09829",
            "MAG": "2963703618",
            "DBLP": "journals/corr/abs-1710-09829",
            "CorpusId": 3603485
        },
        "corpusId": 3603485,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/c4c06578f4870e4b126e6837907929f3c900b99f",
        "title": "Dynamic Routing Between Capsules",
        "abstract": "A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "referenceCount": 22,
        "citationCount": 4001,
        "influentialCitationCount": 875,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-10-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1710.09829"
        },
        "citationStyles": {
            "bibtex": "@Article{Sabour2017DynamicRB,\n author = {S. Sabour and Nicholas Frosst and Geoffrey E. Hinton},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Dynamic Routing Between Capsules},\n volume = {abs/1710.09829},\n year = {2017}\n}\n"
        }
    },
    "467_heuristic_problem_solving_for_ai": {
        "paperId": "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b",
        "externalIds": {
            "MAG": "2045031658",
            "DOI": "10.1109/JRPROC.1961.287775",
            "CorpusId": 14250548
        },
        "corpusId": 14250548,
        "publicationVenue": {
            "id": "82b56675-f0f3-487f-b83e-155a762e2855",
            "name": "Proceedings of the IRE",
            "alternate_names": [
                "Proc IRE"
            ],
            "issn": "0096-8390",
            "alternate_issns": [
                "2162-6634"
            ],
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=10933"
        },
        "url": "https://www.semanticscholar.org/paper/b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b",
        "title": "Steps toward Artificial Intelligence",
        "abstract": "The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction. A computer can do, in a sense, only what it is told to do. But even when we do not know how to solve a certain problem, we may program a machine (computer) to Search through some large space of solution attempts. Unfortunately, this usually leads to an enormously inefficient process. With Pattern-Recognition techniques, efficiency can often be improved, by restricting the application of the machine's methods to appropriate problems. Pattern-Recognition, together with Learning, can be used to exploit generalizations based on accumulated experience, further reducing search. By analyzing the situation, using Planning methods, we may obtain a fundamental improvement by replacing the given search with a much smaller, more appropriate exploration. To manage broad classes of problems, machines will need to construct models of their environments, using some scheme for Induction. Wherever appropriate, the discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date.",
        "venue": "Proceedings of the IRE",
        "year": 1995,
        "referenceCount": 94,
        "citationCount": 1414,
        "influentialCitationCount": 57,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://cumincad.architexturez.net/system/files/pdf/1d2b.content.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date."
        },
        "publicationTypes": null,
        "publicationDate": "1995-10-26",
        "journal": {
            "name": "Proceedings of the IRE",
            "pages": "8-30",
            "volume": "49"
        },
        "citationStyles": {
            "bibtex": "@Article{Minsky1995StepsTA,\n author = {M. Minsky},\n booktitle = {Proceedings of the IRE},\n journal = {Proceedings of the IRE},\n pages = {8-30},\n title = {Steps toward Artificial Intelligence},\n volume = {49},\n year = {1995}\n}\n"
        }
    },
    "468_w2v-bert": {
        "paperId": "ebe259796870ebccf26577044d0087884209b884",
        "externalIds": {
            "DBLP": "conf/asru/ChungZHCQPW21",
            "ArXiv": "2108.06209",
            "DOI": "10.1109/ASRU51503.2021.9688253",
            "CorpusId": 237048255
        },
        "corpusId": 237048255,
        "publicationVenue": {
            "id": "29014a7c-861f-43bd-b4d6-63edf4cd57ef",
            "name": "Automatic Speech Recognition & Understanding",
            "type": "conference",
            "alternate_names": [
                "IEEE Automatic Speech Recognition and Understanding Workshop",
                "Autom Speech Recognit  Underst",
                "ASRU",
                "IEEE Autom Speech Recognit Underst Workshop"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/ebe259796870ebccf26577044d0087884209b884",
        "title": "w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
        "abstract": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
        "venue": "Automatic Speech Recognition & Understanding",
        "year": 2021,
        "referenceCount": 42,
        "citationCount": 234,
        "influentialCitationCount": 23,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2108.06209",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-08-07",
        "journal": {
            "name": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "pages": "244-250"
        },
        "citationStyles": {
            "bibtex": "@Article{Chung2021w2vBERTCC,\n author = {Yu-An Chung and Yu Zhang and Wei Han and Chung-Cheng Chiu and James Qin and Ruoming Pang and Yonghui Wu},\n booktitle = {Automatic Speech Recognition & Understanding},\n journal = {2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},\n pages = {244-250},\n title = {w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training},\n year = {2021}\n}\n"
        }
    },
    "469_qwen-audio-chat": {
        "paperId": "f90595f99a0c66d2bb6d0f230f17c7cd8c58f44d",
        "externalIds": {
            "DBLP": "journals/corr/abs-2311-07919",
            "ArXiv": "2311.07919",
            "DOI": "10.48550/arXiv.2311.07919",
            "CorpusId": 265157993
        },
        "corpusId": 265157993,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/f90595f99a0c66d2bb6d0f230f17c7cd8c58f44d",
        "title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models",
        "abstract": "Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 63,
        "citationCount": 14,
        "influentialCitationCount": 2,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Qwen-Audio model, a multi-task training framework that achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-11-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2311.07919"
        },
        "citationStyles": {
            "bibtex": "@Article{Chu2023QwenAudioAU,\n author = {Yunfei Chu and Jin Xu and Xiaohuan Zhou and Qian Yang and Shiliang Zhang and Zhijie Yan and Chang Zhou and Jingren Zhou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\n volume = {abs/2311.07919},\n year = {2023}\n}\n"
        }
    },
    "470_kngpt2": {
        "paperId": "b6f616e9305e59c9dc7ccf33c311ede47584caf6",
        "externalIds": {
            "ArXiv": "2110.08152",
            "ACL": "2022.acl-short.24",
            "DBLP": "journals/corr/abs-2110-08152",
            "DOI": "10.18653/v1/2022.acl-short.24",
            "CorpusId": 239009526
        },
        "corpusId": 239009526,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/b6f616e9305e59c9dc7ccf33c311ede47584caf6",
        "title": "Kronecker Decomposition for GPT Compression",
        "abstract": "GPT is an auto-regressive Transformer-based pre-trained language model which has attracted a lot of attention in the natural language processing (NLP) domain. The success of GPT is mostly attributed to its pre-training on huge amount of data and its large number of parameters. Despite the superior performance of GPT, this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory. This problem can be mitigated using model compression techniques; however, compressing GPT models has not been investigated much in the literature. In this work, we use Kronecker decomposition to compress the linear mappings of the GPT-2 model. Our Kronecker GPT-2 model (KnGPT2) is initialized based on the Kronecker decomposed version of the GPT-2 model and then is undergone a very light pre- training on only a small portion of the training data with intermediate layer knowledge distillation (ILKD). Finally, our KnGPT2 is fine-tuned on downstream tasks using ILKD as well. We evaluate our model on both language modeling and General Language Understanding Evaluation benchmark tasks and show that with more efficient pre-training and similar number of parameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 38,
        "citationCount": 20,
        "influentialCitationCount": 4,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.acl-short.24.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work uses Kronecker decomposition to compress the linear mappings of the GPT-2 model and demonstrates that with more efficient pre-training and similar number of parameters, the KnGPT2 outperforms the existing DistilG PT2 model significantly."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-10-15",
        "journal": {
            "pages": "219-226"
        },
        "citationStyles": {
            "bibtex": "@Article{Edalati2021KroneckerDF,\n author = {A. Edalati and Marzieh S. Tahaei and Ahmad Rashid and V. Nia and J. Clark and Mehdi Rezagholizadeh},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {219-226},\n title = {Kronecker Decomposition for GPT Compression},\n year = {2021}\n}\n"
        }
    },
    "471_rnmt+": {
        "paperId": "bb669de2fce407df2f5cb2f8c51dedee3f467e04",
        "externalIds": {
            "MAG": "2949877738",
            "DBLP": "journals/corr/abs-1804-09849",
            "ACL": "P18-1008",
            "ArXiv": "1804.09849",
            "DOI": "10.18653/v1/P18-1008",
            "CorpusId": 13747425
        },
        "corpusId": 13747425,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/bb669de2fce407df2f5cb2f8c51dedee3f467e04",
        "title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
        "abstract": "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT\u201914 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "referenceCount": 47,
        "citationCount": 425,
        "influentialCitationCount": 28,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-1008.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper identifies several key modeling and training techniques, and applies them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT\u201914 English to French and English to German tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-04-26",
        "journal": {
            "pages": "76-86"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2018TheBO,\n author = {M. Chen and Orhan Firat and Ankur Bapna and Melvin Johnson and Wolfgang Macherey and George F. Foster and Llion Jones and Niki Parmar and M. Schuster and Zhifeng Chen and Yonghui Wu and Macduff Hughes},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {76-86},\n title = {The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation},\n year = {2018}\n}\n"
        }
    },
    "472_social_and_content-based_classification": {
        "paperId": "ae30f8fc5a969d2d14ae066db4cd07d86fadbf42",
        "externalIds": {
            "MAG": "138016529",
            "DBLP": "conf/aaai/BasuHC98",
            "CorpusId": 14176567
        },
        "corpusId": 14176567,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/ae30f8fc5a969d2d14ae066db4cd07d86fadbf42",
        "title": "Recommendation as Classification: Using Social and Content-Based Information in Recommendation",
        "abstract": "Methionine5-enkephalin sulfoxides and sulfones having agonist activity at opiate receptors are disclosed herein. These sulfoxides and sulfones are useful as analgesics, non-addicting narcotic antagonists and anti-diarrheal agents.",
        "venue": "AAAI/IAAI",
        "year": 1998,
        "referenceCount": 13,
        "citationCount": 1186,
        "influentialCitationCount": 44,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "These sulfoxides and sulfones are useful as analgesics, non-addicting narcotic antagonists and anti-diarrheal agents and having agonist activity at opiate receptors are disclosed herein."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1998-07-01",
        "journal": {
            "pages": "714-720"
        },
        "citationStyles": {
            "bibtex": "@Article{Basu1998RecommendationAC,\n author = {C. Basu and H. Hirsh and William W. Cohen},\n booktitle = {AAAI/IAAI},\n pages = {714-720},\n title = {Recommendation as Classification: Using Social and Content-Based Information in Recommendation},\n year = {1998}\n}\n"
        }
    },
    "473_dna_fine-tuned_language_model_(dflm)": {
        "paperId": "55858cf544b8c063f5c1f3701d81c08b6062ac41",
        "externalIds": {
            "DBLP": "journals/tcbb/HeZWCCGH23",
            "DOI": "10.1109/TCBB.2022.3165592",
            "CorpusId": 248024697,
            "PubMed": "35389869"
        },
        "corpusId": 248024697,
        "publicationVenue": {
            "id": "dc4a9aad-72db-4530-a183-eaa4bf1d4490",
            "name": "IEEE/ACM Transactions on Computational Biology & Bioinformatics",
            "type": "journal",
            "alternate_names": [
                "IEEE/ACM Trans Comput Biology  Bioinform",
                "IEEE/ACM Trans Comput Biology Bioinform",
                "IEEE/ACM Transactions on Computational Biology and Bioinformatics"
            ],
            "issn": "1545-5963",
            "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=8857",
            "alternate_urls": [
                "http://www.computer.org/portal/web/tcbb/home"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/55858cf544b8c063f5c1f3701d81c08b6062ac41",
        "title": "Predicting the Sequence Specificities of DNA-Binding Proteins by DNA Fine-Tuned Language Model With Decaying Learning Rates",
        "abstract": "DNA-binding proteins (DBPs) play vital roles in the regulation of biological systems. Although there are already many deep learning methods for predicting the sequence specificities of DBPs, they face two challenges as follows. Classic deep learning methods for DBPs prediction usually fail to capture the dependencies between genomic sequences since their commonly used one-hot codes are mutually orthogonal. Besides, these methods usually perform poorly when samples are inadequate. To address these two challenges, we developed a novel language model for mining DBPs using human genomic data and ChIP-seq datasets with decaying learning rates, named DNA Fine-tuned Language Model (DFLM). It can capture the dependencies between genome sequences based on the context of human genomic data and then fine-tune the features of DBPs tasks using different ChIP-seq datasets. First, we compared DFLM with the existing widely used methods on 69 datasets and we achieved excellent performance. Moreover, we conducted comparative experiments on complex DBPs and small datasets. The results show that DFLM still achieved a significant improvement. Finally, through visualization analysis of one-hot encoding and DFLM, we found that one-hot encoding completely cut off the dependencies of DNA sequences themselves, while DFLM using language models can well represent the dependency of DNA sequences. Source code are available at: https://github.com/Deep-Bioinfo/DFLM.",
        "venue": "IEEE/ACM Transactions on Computational Biology & Bioinformatics",
        "year": 2022,
        "referenceCount": 43,
        "citationCount": 1,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel language model for mining DBPs using human genomic data and ChIP-seq datasets with decaying learning rates, named DNA Fine-tuned Language Model (DFLM), which can capture the dependencies between genome sequences based on the context of human genomicData and then fine-tune the features of DBPs tasks using different ChIP -seq datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-04-07",
        "journal": {
            "name": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
            "pages": "616-624",
            "volume": "20"
        },
        "citationStyles": {
            "bibtex": "@Article{He2022PredictingTS,\n author = {Ying He and Qinhu Zhang and Siguo Wang and Zhanheng Chen and Zhen Cui and Zhen-Hao Guo and Desheng Huang},\n booktitle = {IEEE/ACM Transactions on Computational Biology & Bioinformatics},\n journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},\n pages = {616-624},\n title = {Predicting the Sequence Specificities of DNA-Binding Proteins by DNA Fine-Tuned Language Model With Decaying Learning Rates},\n volume = {20},\n year = {2022}\n}\n"
        }
    },
    "474_lrso-gan": {
        "paperId": "f9b7783448f65205e085bd4e6fdfa2c8bfa9a4df",
        "externalIds": {
            "MAG": "2585635281",
            "DBLP": "conf/iccv/ZhengZY17",
            "ArXiv": "1701.07717",
            "DOI": "10.1109/ICCV.2017.405",
            "CorpusId": 2683207
        },
        "corpusId": 2683207,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/f9b7783448f65205e085bd4e6fdfa2c8bfa9a4df",
        "title": "Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro",
        "abstract": "The main contribution of this paper is a simple semisupervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market- 1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at https://github.com/layumi/ Person-reID_GAN.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "referenceCount": 63,
        "citationCount": 1674,
        "influentialCitationCount": 362,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://opus.lib.uts.edu.au/bitstream/10453/118067/4/FF67E427-6528-4081-B0B7-C3EB797E0421.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simple semisupervised pipeline that only uses the original training set without collecting extra data, which effectively improves the discriminative ability of learned CNN embeddings and proposes the label smoothing regularization for outliers (LSRO)."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-01-26",
        "journal": {
            "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
            "pages": "3774-3782"
        },
        "citationStyles": {
            "bibtex": "@Article{Zheng2017UnlabeledSG,\n author = {Zhedong Zheng and Liang Zheng and Yi Yang},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {3774-3782},\n title = {Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro},\n year = {2017}\n}\n"
        }
    },
    "475_dmpfold": {
        "paperId": "0eb078b7d36d587a546ab786e4a81569310c168a",
        "externalIds": {
            "MAG": "2972238299",
            "ArXiv": "1811.12355",
            "PubMedCentral": "6726615",
            "DOI": "10.1038/s41467-019-11994-0",
            "CorpusId": 201814691,
            "PubMed": "31484923"
        },
        "corpusId": 201814691,
        "publicationVenue": {
            "id": "43b3f0f9-489a-4566-8164-02fafde3cd98",
            "name": "Nature Communications",
            "type": "journal",
            "alternate_names": [
                "Nat Commun"
            ],
            "issn": "2041-1723",
            "url": "https://www.nature.com/ncomms/",
            "alternate_urls": [
                "http://www.nature.com/ncomms/about/index.html",
                "http://www.nature.com/ncomms/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/0eb078b7d36d587a546ab786e4a81569310c168a",
        "title": "Deep learning extends de novo protein modelling coverage of genomes using iteratively predicted structural constraints",
        "abstract": null,
        "venue": "Nature Communications",
        "year": 2018,
        "referenceCount": 49,
        "citationCount": 137,
        "influentialCitationCount": 8,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.nature.com/articles/s41467-019-11994-0.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Chemistry",
            "Biology",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Chemistry",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "DMPfold is introduced, which uses deep learning to predict inter-atomic distance bounds, the main chain hydrogen bond network, and torsion angles, which it uses to build models in an iterative fashion to predict the structures of 1475 Pfam domains."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-11-29",
        "journal": {
            "name": "Nature Communications",
            "volume": "10"
        },
        "citationStyles": {
            "bibtex": "@Article{Greener2018DeepLE,\n author = {Joe G. Greener and Shaun M. Kandathil and David T. Jones},\n booktitle = {Nature Communications},\n journal = {Nature Communications},\n title = {Deep learning extends de novo protein modelling coverage of genomes using iteratively predicted structural constraints},\n volume = {10},\n year = {2018}\n}\n"
        }
    },
    "478_pspnet": {
        "paperId": "1031a69923b80ad01cf3fbb703d10757a80e699b",
        "externalIds": {
            "ArXiv": "1612.01105",
            "DBLP": "journals/corr/ZhaoSQWJ16",
            "MAG": "2560023338",
            "DOI": "10.1109/CVPR.2017.660",
            "CorpusId": 5299559
        },
        "corpusId": 5299559,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/1031a69923b80ad01cf3fbb703d10757a80e699b",
        "title": "Pyramid Scene Parsing Network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 47,
        "citationCount": 9548,
        "influentialCitationCount": 1442,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.01105",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper exploits the capability of global context information by different-region-based context aggregation through the pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet) to produce good quality results on the scene parsing task."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-12-04",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "6230-6239"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhao2016PyramidSP,\n author = {Hengshuang Zhao and Jianping Shi and Xiaojuan Qi and Xiaogang Wang and Jiaya Jia},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {6230-6239},\n title = {Pyramid Scene Parsing Network},\n year = {2016}\n}\n"
        }
    },
    "479_delight": {
        "paperId": "b08c360ddf899923aebf25913706b4f03e54eccd",
        "externalIds": {
            "DBLP": "conf/iclr/MehtaGIZH21",
            "CorpusId": 235613336
        },
        "corpusId": 235613336,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/b08c360ddf899923aebf25913706b4f03e54eccd",
        "title": "DeLighT: Deep and Light-weight Transformer",
        "abstract": null,
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "referenceCount": 0,
        "citationCount": 61,
        "influentialCitationCount": 8,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            },
            {
                "category": "Materials Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": null,
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": null,
        "citationStyles": {
            "bibtex": "@Article{Mehta2021DeLighTDA,\n author = {Sachin Mehta and Marjan Ghazvininejad and Srini Iyer and Luke Zettlemoyer and Hannaneh Hajishirzi},\n booktitle = {International Conference on Learning Representations},\n title = {DeLighT: Deep and Light-weight Transformer},\n year = {2021}\n}\n"
        }
    },
    "480_proxylessnas": {
        "paperId": "f323407464c4cd492d3fc1afd7170eab08f44d9b",
        "externalIds": {
            "DBLP": "conf/iclr/CaiZH19",
            "MAG": "2950483360",
            "ArXiv": "1812.00332",
            "CorpusId": 54438210
        },
        "corpusId": 54438210,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/f323407464c4cd492d3fc1afd7170eab08f44d9b",
        "title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
        "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours) makes it difficult to \\emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \\emph{ProxylessNAS} that can \\emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6$\\times$ fewer parameters. On ImageNet, our model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being 1.2$\\times$ faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 39,
        "citationCount": 1614,
        "influentialCitationCount": 341,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "ProxylessNAS is presented, which can directly learn the architectures for large-scale target tasks and target hardware platforms and apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-09-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1812.00332"
        },
        "citationStyles": {
            "bibtex": "@Article{Cai2018ProxylessNASDN,\n author = {Han Cai and Ligeng Zhu and Song Han},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware},\n volume = {abs/1812.00332},\n year = {2018}\n}\n"
        }
    },
    "482_llama-7b_(protein-oriented_instructions_finetuned)": {
        "paperId": "f86aa25603d1f2e4066db9b6a9a6d311b4e8c491",
        "externalIds": {
            "ArXiv": "2306.08018",
            "DBLP": "journals/corr/abs-2306-08018",
            "DOI": "10.48550/arXiv.2306.08018",
            "CorpusId": 259164901
        },
        "corpusId": 259164901,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/f86aa25603d1f2e4066db9b6a9a6d311b4e8c491",
        "title": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models",
        "abstract": "Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 82,
        "citationCount": 20,
        "influentialCitationCount": 3,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2306.08018",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Through extensive instruction tuning experiments on LLMs, the effectiveness of Mol-Instructions is demonstrated in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolescular research community."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-06-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2306.08018"
        },
        "citationStyles": {
            "bibtex": "@Article{Fang2023MolInstructionsAL,\n author = {Yin Fang and Xiaozhuan Liang and Ningyu Zhang and Kangwei Liu and Rui Huang and Zhuo Chen and Xiaohui Fan and Huajun Chen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models},\n volume = {abs/2306.08018},\n year = {2023}\n}\n"
        }
    },
    "483_sparse_energy-based_model": {
        "paperId": "932c2a02d462abd75af018125413b1ceaa1ee3f4",
        "externalIds": {
            "DBLP": "conf/nips/RanzatoPCL06",
            "MAG": "2172174689",
            "DOI": "10.7551/mitpress/7503.003.0147",
            "CorpusId": 819006
        },
        "corpusId": 819006,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/932c2a02d462abd75af018125413b1ceaa1ee3f4",
        "title": "Efficient Learning of Sparse Representations with an Energy-Based Model",
        "abstract": "We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces \"stroke detectors\" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.",
        "venue": "Neural Information Processing Systems",
        "year": 2006,
        "referenceCount": 16,
        "citationCount": 1291,
        "influentialCitationCount": 64,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel unsupervised method for learning sparse, overcomplete features using a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2006-12-04",
        "journal": {
            "pages": "1137-1144"
        },
        "citationStyles": {
            "bibtex": "@Article{Ranzato2006EfficientLO,\n author = {Marc'Aurelio Ranzato and Christopher S. Poultney and S. Chopra and Yann LeCun},\n booktitle = {Neural Information Processing Systems},\n pages = {1137-1144},\n title = {Efficient Learning of Sparse Representations with an Energy-Based Model},\n year = {2006}\n}\n"
        }
    },
    "486_perfusion": {
        "paperId": "314047a5aad780af9efef7ebd4a41e6995666543",
        "externalIds": {
            "DBLP": "journals/corr/abs-2305-01644",
            "ArXiv": "2305.01644",
            "DOI": "10.1145/3588432.3591506",
            "CorpusId": 258436985
        },
        "corpusId": 258436985,
        "publicationVenue": {
            "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
            "name": "International Conference on Computer Graphics and Interactive Techniques",
            "type": "conference",
            "alternate_names": [
                "Int Conf Comput Graph Interact Tech",
                "SIGGRAPH"
            ],
            "url": "http://www.siggraph.org/"
        },
        "url": "https://www.semanticscholar.org/paper/314047a5aad780af9efef7ebd4a41e6995666543",
        "title": "Key-Locked Rank One Editing for Text-to-Image Personalization",
        "abstract": "Text-to-image models (T2I) offer a new level of flexibility by allowing users to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a challenging problem. The task of T2I personalization poses multiple hard challenges, such as maintaining high visual fidelity while allowing creative control, combining multiple personalized concepts in a single image, and keeping a small model size. We present Perfusion, a T2I personalization method that addresses these challenges using dynamic rank-1 updates to the underlying T2I model. Perfusion avoids overfitting by introducing a new mechanism that \u201clocks\u201d new concepts\u2019 cross-attention Keys to their superordinate category. Additionally, we develop a gated rank-1 approach that enables us to control the influence of a learned concept during inference time and to combine multiple concepts. This allows runtime efficient balancing of visual-fidelity and textual-alignment with a single 100KB trained model. Importantly, it can span different operating points across the Pareto front without additional training. We compare our approach to strong baselines and demonstrate its qualitative and quantitative strengths.",
        "venue": "International Conference on Computer Graphics and Interactive Techniques",
        "year": 2023,
        "referenceCount": 52,
        "citationCount": 54,
        "influentialCitationCount": 5,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.01644",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Perfusion avoids overfitting by introducing a new mechanism that \u201clocks\u201d new concepts\u2019 cross-attention Keys to their superordinate category and develops a gated rank-1 approach that enables us to control the influence of a learned concept during inference time and to combine multiple concepts."
        },
        "publicationTypes": [
            "JournalArticle",
            "Book",
            "Conference"
        ],
        "publicationDate": "2023-05-02",
        "journal": {
            "name": "ACM SIGGRAPH 2023 Conference Proceedings"
        },
        "citationStyles": {
            "bibtex": "@Article{Tewel2023KeyLockedRO,\n author = {Yoad Tewel and Rinon Gal and Gal Chechik and Y. Atzmon},\n booktitle = {International Conference on Computer Graphics and Interactive Techniques},\n journal = {ACM SIGGRAPH 2023 Conference Proceedings},\n title = {Key-Locked Rank One Editing for Text-to-Image Personalization},\n year = {2023}\n}\n"
        }
    },
    "487_markov-driven_pos_tagger": {
        "paperId": "4614650c3bb3e835c80612d3bca9586f81db95a3",
        "externalIds": {
            "MAG": "2112861996",
            "DBLP": "journals/coling/Merialdo94",
            "ACL": "J94-2001",
            "CorpusId": 2727455
        },
        "corpusId": 2727455,
        "publicationVenue": {
            "id": "30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
            "name": "International Conference on Computational Logic",
            "type": "conference",
            "alternate_names": [
                "CL",
                "Int Conf Comput Log"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/4614650c3bb3e835c80612d3bca9586f81db95a3",
        "title": "Tagging English Text with a Probabilistic Model",
        "abstract": "In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined:using text that has been tagged by hand and computing relative frequency counts,using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.Experminents show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.",
        "venue": "International Conference on Computational Logic",
        "year": 1994,
        "referenceCount": 67,
        "citationCount": 627,
        "influentialCitationCount": 50,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experminents show that the best training is obtained by using as much tagged text as possible, and show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1994-06-01",
        "journal": {
            "name": "Comput. Linguistics",
            "pages": "155-171",
            "volume": "20"
        },
        "citationStyles": {
            "bibtex": "@Article{M\u00e9rialdo1994TaggingET,\n author = {B. M\u00e9rialdo},\n booktitle = {International Conference on Computational Logic},\n journal = {Comput. Linguistics},\n pages = {155-171},\n title = {Tagging English Text with a Probabilistic Model},\n volume = {20},\n year = {1994}\n}\n"
        }
    },
    "488_darts": {
        "paperId": "c1f457e31b611da727f9aef76c283a18157dfa83",
        "externalIds": {
            "DBLP": "journals/corr/abs-1806-09055",
            "MAG": "2810075754",
            "ArXiv": "1806.09055",
            "CorpusId": 49411844
        },
        "corpusId": 49411844,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/c1f457e31b611da727f9aef76c283a18157dfa83",
        "title": "DARTS: Differentiable Architecture Search",
        "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 48,
        "citationCount": 3602,
        "influentialCitationCount": 1148,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The proposed algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-06-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1806.09055"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2018DARTSDA,\n author = {Hanxiao Liu and K. Simonyan and Yiming Yang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {DARTS: Differentiable Architecture Search},\n volume = {abs/1806.09055},\n year = {2018}\n}\n"
        }
    },
    "489_deeplab": {
        "paperId": "39ad6c911f3351a3b390130a6e4265355b4d593b",
        "externalIds": {
            "MAG": "2964288706",
            "ArXiv": "1412.7062",
            "DBLP": "journals/corr/ChenPKMY14",
            "CorpusId": 1996665
        },
        "corpusId": 1996665,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/39ad6c911f3351a3b390130a6e4265355b4d593b",
        "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs",
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "referenceCount": 58,
        "citationCount": 4368,
        "influentialCitationCount": 448,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF)."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-12-22",
        "journal": {
            "name": "CoRR",
            "volume": "abs/1412.7062"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2014SemanticIS,\n author = {Liang-Chieh Chen and G. Papandreou and Iasonas Kokkinos and K. Murphy and A. Yuille},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs},\n volume = {abs/1412.7062},\n year = {2014}\n}\n"
        }
    },
    "490_mobilebert": {
        "paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "externalIds": {
            "MAG": "3034457371",
            "DBLP": "journals/corr/abs-2004-02984",
            "ArXiv": "2004.02984",
            "ACL": "2020.acl-main.195",
            "DOI": "10.18653/v1/2020.acl-main.195",
            "CorpusId": 215238853
        },
        "corpusId": 215238853,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices",
        "abstract": "Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "referenceCount": 67,
        "citationCount": 608,
        "influentialCitationCount": 94,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.195.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks that can be generically applied to various downstream NLP tasks via simple fine-tuning."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-04-06",
        "journal": {
            "pages": "2158-2170"
        },
        "citationStyles": {
            "bibtex": "@Article{Sun2020MobileBERTAC,\n author = {Zhiqing Sun and Hongkun Yu and Xiaodan Song and Renjie Liu and Yiming Yang and Denny Zhou},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2158-2170},\n title = {MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},\n year = {2020}\n}\n"
        }
    },
    "491_lanet-l_(cifar-10)": {
        "paperId": "f3c7e853bb77d1ad360464aea81676cc9e3ca1fe",
        "externalIds": {
            "DBLP": "journals/corr/abs-1906-06832",
            "MAG": "2952490571",
            "ArXiv": "1906.06832",
            "CorpusId": 189928297
        },
        "corpusId": 189928297,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/f3c7e853bb77d1ad360464aea81676cc9e3ca1fe",
        "title": "Sample-Efficient Neural Architecture Search by Learning Action Space",
        "abstract": "Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. However, existing NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy). As a result, using manually designed action space to perform NAS often leads to sample-inefficient explorations of architectures and thus can be sub-optimal. In order to improve sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS) that learns the action space to recursively partition the architecture search space into regions, each with concentrated performance metrics (\\emph{i.e.}, low variance). During the search phase, as different architecture search action sequences lead to regions of different performance, the search efficiency can be significantly improved by biasing towards the regions with good performance. On the largest NAS dataset NasBench-101, our experimental results demonstrated that LaNAS is 22x, 14.6x and 12.4x more sample-efficient than random search, regularized evolution, and Monte Carlo Tree Search (MCTS) respectively. When applied to the open domain, LaNAS finds an architecture that achieves SoTA 98.0% accuracy on CIFAR-10 and 75.0% top1 accuracy on ImageNet (mobile setting), after exploring only 6,000 architectures.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 64,
        "citationCount": 42,
        "influentialCitationCount": 8,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes Latent Action Neural Architecture Search (LaNAS) that learns the action space to recursively partition the architecture search space into regions, each with concentrated performance metrics (\\emph{i.e., low variance)."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-06-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1906.06832"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2019SampleEfficientNA,\n author = {Linnan Wang and Saining Xie and Teng Li and Rodrigo Fonseca and Yuandong Tian},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Sample-Efficient Neural Architecture Search by Learning Action Space},\n volume = {abs/1906.06832},\n year = {2019}\n}\n"
        }
    },
    "492_rnn_(sgd+clr)": {
        "paperId": "ded103d0613e1a8f51f586cc1678aee3ff26e811",
        "externalIds": {
            "ArXiv": "1212.0901",
            "MAG": "2949427968",
            "DBLP": "conf/icassp/BengioBP13",
            "DOI": "10.1109/ICASSP.2013.6639349",
            "CorpusId": 12485056
        },
        "corpusId": 12485056,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/ded103d0613e1a8f51f586cc1678aee3ff26e811",
        "title": "Advances in optimizing recurrent networks",
        "abstract": "After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2012,
        "referenceCount": 34,
        "citationCount": 495,
        "influentialCitationCount": 36,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1212.0901",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "publicationDate": "2012-12-04",
        "journal": {
            "name": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "pages": "8624-8628"
        },
        "citationStyles": {
            "bibtex": "@Article{Bengio2012AdvancesIO,\n author = {Yoshua Bengio and Nicolas Boulanger-Lewandowski and Razvan Pascanu},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},\n pages = {8624-8628},\n title = {Advances in optimizing recurrent networks},\n year = {2012}\n}\n"
        }
    },
    "493_framenet_role_labeling": {
        "paperId": "c274b8aac56e49e65a3827c570b2496b14429166",
        "externalIds": {
            "MAG": "2151170651",
            "DBLP": "journals/coling/GildeaJ02",
            "ACL": "P00-1065",
            "DOI": "10.3115/1075218.1075283",
            "CorpusId": 62182406
        },
        "corpusId": 62182406,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/c274b8aac56e49e65a3827c570b2496b14429166",
        "title": "Automatic Labeling of Semantic Roles",
        "abstract": "We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as Agent or Patient, or more domain-specific semantic roles, such as Speaker, Message, and Topic. The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers. Our system achieves 82 accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65 precision and 61 recall. Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2000,
        "referenceCount": 55,
        "citationCount": 1391,
        "influentialCitationCount": 122,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1075218.1075283",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame, based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2000-10-03",
        "journal": {
            "name": "Computational Linguistics",
            "pages": "245-288",
            "volume": "28"
        },
        "citationStyles": {
            "bibtex": "@Article{Gildea2000AutomaticLO,\n author = {D. Gildea and Dan Jurafsky},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {Computational Linguistics},\n pages = {245-288},\n title = {Automatic Labeling of Semantic Roles},\n volume = {28},\n year = {2000}\n}\n"
        }
    },
    "494_stargan_v2": {
        "paperId": "5474ddca920f59c4ec3c243345a5b9248e64065b",
        "externalIds": {
            "DBLP": "conf/cvpr/ChoiUYH20",
            "MAG": "2991997773",
            "ArXiv": "1912.01865",
            "DOI": "10.1109/cvpr42600.2020.00821",
            "CorpusId": 208617800
        },
        "corpusId": 208617800,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/5474ddca920f59c4ec3c243345a5b9248e64065b",
        "title": "StarGAN v2: Diverse Image Synthesis for Multiple Domains",
        "abstract": "A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset are available at https://github.com/clovaai/stargan-v2.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "referenceCount": 58,
        "citationCount": 1280,
        "influentialCitationCount": 312,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.01865",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "StarGAN v2, a single framework that tackles image-to-image translation models with limited diversity and multiple models for all domains, is proposed and shows significantly improved results over the baselines."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-12-04",
        "journal": {
            "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "8185-8194"
        },
        "citationStyles": {
            "bibtex": "@Article{Choi2019StarGANVD,\n author = {Yunjey Choi and Youngjung Uh and Jaejun Yoo and Jung-Woo Ha},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {8185-8194},\n title = {StarGAN v2: Diverse Image Synthesis for Multiple Domains},\n year = {2019}\n}\n"
        }
    },
    "495_or-wideresnet": {
        "paperId": "1a970066e726c74484a6ba943959f81fe57c6f96",
        "externalIds": {
            "MAG": "2952439645",
            "DBLP": "conf/cvpr/ZhouYQJ17",
            "ArXiv": "1701.01833",
            "DOI": "10.1109/CVPR.2017.527",
            "CorpusId": 11054363
        },
        "corpusId": 11054363,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/1a970066e726c74484a6ba943959f81fe57c6f96",
        "title": "Oriented Response Networks",
        "abstract": "Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During back-propagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "referenceCount": 48,
        "citationCount": 238,
        "influentialCitationCount": 30,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1701.01833",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, it is consistently observed that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-01-07",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "4961-4970"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhou2017OrientedRN,\n author = {Yanzhao Zhou and Qixiang Ye and Qiang Qiu and Jianbin Jiao},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4961-4970},\n title = {Oriented Response Networks},\n year = {2017}\n}\n"
        }
    },
    "497_adaptive_agent": {
        "paperId": "bfe6fd05f09647b001c7eb6e333a95c881c88344",
        "externalIds": {
            "DBLP": "conf/icml/BauerBBBBCCCDGG23",
            "ArXiv": "2301.07608",
            "DOI": "10.48550/arXiv.2301.07608",
            "CorpusId": 255998274
        },
        "corpusId": 255998274,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/bfe6fd05f09647b001c7eb6e333a95c881c88344",
        "title": "Human-Timescale Adaptation in an Open-Ended Task Space",
        "abstract": "Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "referenceCount": 133,
        "citationCount": 56,
        "influentialCitationCount": 6,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2301.07608",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-01-18",
        "journal": {
            "pages": "1887-1935"
        },
        "citationStyles": {
            "bibtex": "@Article{Team2023HumanTimescaleAI,\n author = {Adaptive Agent Team and Jakob Bauer and Kate Baumli and Satinder Baveja and Feryal M. P. Behbahani and Avishkar Bhoopchand and N. Bradley-Schmieg and Michael Chang and Natalie Clay and Adrian Collister and Vibhavari Dasagi and Lucy Gonzalez and Karol Gregor and Edward Hughes and Sheleem Kashem and Maria Loks-Thompson and Hannah Openshaw and Jack Parker-Holder and Shreyaan Pathak and Nicolas Perez Nieves and Nemanja Rakicevic and Tim Rockt\u00e4schel and Yannick Schroecker and Jakub Sygnowski and K. Tuyls and Sarah York and Alexander Zacherl and Lei M. Zhang},\n booktitle = {International Conference on Machine Learning},\n pages = {1887-1935},\n title = {Human-Timescale Adaptation in an Open-Ended Task Space},\n year = {2023}\n}\n"
        }
    },
    "498_metamimic": {
        "paperId": "776f3d2250285ac03b2019ecf18668fcdd72a9ce",
        "externalIds": {
            "ArXiv": "1810.05017",
            "MAG": "2896924405",
            "DBLP": "journals/corr/abs-1810-05017",
            "CorpusId": 52961547
        },
        "corpusId": 52961547,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/776f3d2250285ac03b2019ecf18668fcdd72a9ce",
        "title": "One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL",
        "abstract": "Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt. Humans use this ability to quickly solve a task instance, and to bootstrap learning of new tasks. Achieving these abilities in autonomous agents is an open problem. In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap. MetaMimic can learn both (i) policies for high-fidelity one-shot imitation of diverse novel skills, and (ii) policies that enable the agent to solve tasks more efficiently than the demonstrators. MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL. This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task. The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.",
        "venue": "arXiv.org",
        "year": 2018,
        "referenceCount": 59,
        "citationCount": 25,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The MetaMimic algorithm is introduced, to the best of the knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-09-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1810.05017"
        },
        "citationStyles": {
            "bibtex": "@Article{Paine2018OneShotHI,\n author = {T. Paine and Sergio Gomez Colmenarejo and Ziyun Wang and Scott E. Reed and Y. Aytar and T. Pfaff and Matthew W. Hoffman and Gabriel Barth-Maron and Serkan Cabi and D. Budden and Nando de Freitas},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL},\n volume = {abs/1810.05017},\n year = {2018}\n}\n"
        }
    },
    "500_lep-ad": {
        "paperId": "d6686d2d96958b24a86319344557648c5b41ba3a",
        "externalIds": {
            "DOI": "10.1101/2023.03.14.532563",
            "CorpusId": 257624267
        },
        "corpusId": 257624267,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/d6686d2d96958b24a86319344557648c5b41ba3a",
        "title": "LEP-AD: Language Embedding of Proteins and Attention to Drugs predicts drug target interactions",
        "abstract": "Predicting drug-target interactions is a tremendous challenge for drug development and lead optimization. Recent advances include training algorithms to learn drug-target interactions from data and molecular simulations. Here we utilize Evolutionary Scale Modeling (ESM-2) models to establish a Transformer protein language model for drug-target interaction predictions. Our architecture, LEP-AD, combines pre-trained ESM-2 and Transformer-GCN models predicting binding affinity values. We report new best-in-class state-of-the-art results compared to competing methods such as SimBoost, DeepCPI, Attention-DTA, GraphDTA, and more using multiple datasets, including Davis, KIBA, DTC, Metz, ToxCast, and STITCH. Finally, we find that a pre-trained model with embedding of proteins (the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins (e.g., LEP-AD supervised by Alphafold). The LEP-AD model scales favorably in performance with the size of training data. Code available at https://github.com/adaga06/LEP-AD",
        "venue": "bioRxiv",
        "year": 2023,
        "referenceCount": 33,
        "citationCount": 1,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2023/03/15/2023.03.14.532563.full.pdf",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work utilizes Evolutionary Scale Modeling (ESM-2) models to establish a Transformer protein language model for drug-target interaction predictions, and finds that a pre-trained model with embedding of proteins (the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins."
        },
        "publicationTypes": null,
        "publicationDate": "2023-03-15",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Daga2023LEPADLE,\n author = {Anuj Daga and S. Khan and D. Cabrero and R. Hoehndorf and N. Kiani and J. Tegn\u00e9r},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {LEP-AD: Language Embedding of Proteins and Attention to Drugs predicts drug target interactions},\n year = {2023}\n}\n"
        }
    },
    "502_musiclm": {
        "paperId": "428854d9e75f94f0e61f37c6887c77800437d516",
        "externalIds": {
            "ArXiv": "2301.11325",
            "DBLP": "journals/corr/abs-2301-11325",
            "DOI": "10.48550/arXiv.2301.11325",
            "CorpusId": 256274504
        },
        "corpusId": 256274504,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/428854d9e75f94f0e61f37c6887c77800437d516",
        "title": "MusicLM: Generating Music From Text",
        "abstract": "We introduce MusicLM, a model generating high-fidelity music from text descriptions such as\"a calming violin melody backed by a distorted guitar riff\". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 51,
        "citationCount": 169,
        "influentialCitationCount": 24,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2301.11325",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "MusicLM is introduced, a model generating high-fidelity music from text descriptions such as \"a calming violin melody backed by a distorted guitar riff\" that can transform whistled and hummed melodies according to the style described in a text caption."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-01-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2301.11325"
        },
        "citationStyles": {
            "bibtex": "@Article{Agostinelli2023MusicLMGM,\n author = {A. Agostinelli and Timo I. Denk and Zal\u00e1n Borsos and Jesse Engel and Mauro Verzetti and Antoine Caillon and Qingqing Huang and A. Jansen and Adam Roberts and M. Tagliasacchi and Matthew Sharifi and Neil Zeghidour and C. Frank},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {MusicLM: Generating Music From Text},\n volume = {abs/2301.11325},\n year = {2023}\n}\n"
        }
    },
    "503_2nd_order_fofe-fnnlm": {
        "paperId": "63c490f2a3330e3685e7df50973278296905f63b",
        "externalIds": {
            "DBLP": "journals/corr/ZhangJXHD15",
            "MAG": "1943583106",
            "ArXiv": "1505.01504",
            "CorpusId": 7735706
        },
        "corpusId": 7735706,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/63c490f2a3330e3685e7df50973278296905f63b",
        "title": "A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models",
        "abstract": "In this paper, we propose the new fixed-size ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation. FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs). Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular RNN-LMs.",
        "venue": "arXiv.org",
        "year": 2015,
        "referenceCount": 23,
        "citationCount": 13,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform not only the standard fixed-input FNNs but also the popular RNNs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2015-05-06",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1505.01504"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2015AFE,\n author = {Shiliang Zhang and Hui Jiang and Mingbin Xu and Junfeng Hou and Lirong Dai},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models},\n volume = {abs/1505.01504},\n year = {2015}\n}\n"
        }
    },
    "504_monarch-gpt-2-medium": {
        "paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135",
        "externalIds": {
            "DBLP": "journals/corr/abs-2204-00595",
            "ArXiv": "2204.00595",
            "DOI": "10.48550/arXiv.2204.00595",
            "CorpusId": 247922732
        },
        "corpusId": 247922732,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/8326dba15f6b8ee6e43c23eea3265a05e59e8135",
        "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training",
        "abstract": "Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called\"reverse sparsification,\"Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "referenceCount": 124,
        "citationCount": 45,
        "influentialCitationCount": 4,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2204.00595",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution and can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-04-01",
        "journal": {
            "pages": "4690-4721"
        },
        "citationStyles": {
            "bibtex": "@Article{Dao2022MonarchES,\n author = {Tri Dao and Beidi Chen and N. Sohoni and Arjun D Desai and Michael Poli and Jessica Grogan and Alexander Liu and Aniruddh Rao and A. Rudra and Christopher R\u00e9},\n booktitle = {International Conference on Machine Learning},\n pages = {4690-4721},\n title = {Monarch: Expressive Structured Matrices for Efficient and Accurate Training},\n year = {2022}\n}\n"
        }
    },
    "505_mobilenetv2": {
        "paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4",
        "externalIds": {
            "MAG": "2796438033",
            "DBLP": "conf/cvpr/SandlerHZZC18",
            "ArXiv": "1801.04381",
            "DOI": "10.1109/CVPR.2018.00474",
            "CorpusId": 4555207
        },
        "corpusId": 4555207,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4",
        "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
        "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.",
        "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
        "year": 2018,
        "referenceCount": 51,
        "citationCount": 14310,
        "influentialCitationCount": 2464,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1801.04381",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new mobile architecture, MobileNetV2, is described that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes and allows decoupling of the input/output domains from the expressiveness of the transformation."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-01-13",
        "journal": {
            "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "pages": "4510-4520"
        },
        "citationStyles": {
            "bibtex": "@Article{Sandler2018MobileNetV2IR,\n author = {M. Sandler and Andrew G. Howard and Menglong Zhu and A. Zhmoginov and Liang-Chieh Chen},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {4510-4520},\n title = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},\n year = {2018}\n}\n"
        }
    },
    "506_multi-task_cascaded_cnn": {
        "paperId": "9e60942aa15670ed9ee03af3c0ae011fa4966b7c",
        "externalIds": {
            "DBLP": "journals/spl/ZhangZLQ16",
            "MAG": "2341528187",
            "ArXiv": "1604.02878",
            "DOI": "10.1109/LSP.2016.2603342",
            "CorpusId": 10585115
        },
        "corpusId": 10585115,
        "publicationVenue": {
            "id": "d5da7004-7b61-450a-9c7d-a39500de7acf",
            "name": "IEEE Signal Processing Letters",
            "type": "journal",
            "alternate_names": [
                "IEEE Signal Process Lett"
            ],
            "issn": "1070-9908",
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=97"
        },
        "url": "https://www.semanticscholar.org/paper/9e60942aa15670ed9ee03af3c0ae011fa4966b7c",
        "title": "Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks",
        "abstract": "Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations, and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this letter, we propose a deep cascaded multitask framework that exploits the inherent correlation between detection and alignment to boost up their performance. In particular, our framework leverages a cascaded architecture with three stages of carefully designed deep convolutional networks to predict face and landmark location in a coarse-to-fine manner. In addition, we propose a new online hard sample mining strategy that further improves the performance in practice. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging face detection dataset and benchmark and WIDER FACE benchmarks for face detection, and annotated facial landmarks in the wild benchmark for face alignment, while keeps real-time performance.",
        "venue": "IEEE Signal Processing Letters",
        "year": 2016,
        "referenceCount": 31,
        "citationCount": 4254,
        "influentialCitationCount": 458,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1604.02878",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A deep cascaded multitask framework that exploits the inherent correlation between detection and alignment to boost up their performance and achieves superior accuracy over the state-of-the-art techniques on the challenging face detection dataset and benchmark."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-04-11",
        "journal": {
            "name": "IEEE Signal Processing Letters",
            "pages": "1499-1503",
            "volume": "23"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2016JointFD,\n author = {Kaipeng Zhang and Zhanpeng Zhang and Zhifeng Li and Y. Qiao},\n booktitle = {IEEE Signal Processing Letters},\n journal = {IEEE Signal Processing Letters},\n pages = {1499-1503},\n title = {Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks},\n volume = {23},\n year = {2016}\n}\n"
        }
    },
    "507_rcan": {
        "paperId": "9775f8964a2eea1c9e35a02b1b906487396ea1f5",
        "externalIds": {
            "DBLP": "journals/corr/abs-1807-02758",
            "ArXiv": "1807.02758",
            "MAG": "2866634454",
            "DOI": "10.1007/978-3-030-01234-2_18",
            "CorpusId": 49657846
        },
        "corpusId": 49657846,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/9775f8964a2eea1c9e35a02b1b906487396ea1f5",
        "title": "Image Super-Resolution Using Very Deep Residual Channel Attention Networks",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "referenceCount": 46,
        "citationCount": 3323,
        "influentialCitationCount": 750,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1807.02758",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections, and proposes a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-07-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1807.02758"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2018ImageSU,\n author = {Yulun Zhang and Kunpeng Li and Kai Li and Lichen Wang and Bineng Zhong and Y. Fu},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {Image Super-Resolution Using Very Deep Residual Channel Attention Networks},\n volume = {abs/1807.02758},\n year = {2018}\n}\n"
        }
    },
    "509_big-little_net_(vision)": {
        "paperId": "425032e4fdd531cd7a5d3e7effc12d16f9c076d9",
        "externalIds": {
            "MAG": "2949431202",
            "DBLP": "conf/iclr/ChenFMSF19",
            "ArXiv": "1807.03848",
            "CorpusId": 49671490
        },
        "corpusId": 49671490,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/425032e4fdd531cd7a5d3e7effc12d16f9c076d9",
        "title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
        "abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at this https URL",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 62,
        "citationCount": 80,
        "influentialCitationCount": 8,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a novel Convolutional Neural Network architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy by using a multi-branch network, which has different computational complexity at different branches."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-07-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1807.03848"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2018BigLittleNA,\n author = {Chun-Fu Chen and Quanfu Fan and Neil Rohit Mallinar and Tom Sercu and R. Feris},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},\n volume = {abs/1807.03848},\n year = {2018}\n}\n"
        }
    },
    "510_protgpt2": {
        "paperId": "dcb31b98ec58f3fff9f94f148e2952595f017fd9",
        "externalIds": {
            "PubMedCentral": "9329459",
            "DOI": "10.1038/s41467-022-32007-7",
            "CorpusId": 247439606,
            "PubMed": "35896542"
        },
        "corpusId": 247439606,
        "publicationVenue": {
            "id": "43b3f0f9-489a-4566-8164-02fafde3cd98",
            "name": "Nature Communications",
            "type": "journal",
            "alternate_names": [
                "Nat Commun"
            ],
            "issn": "2041-1723",
            "url": "https://www.nature.com/ncomms/",
            "alternate_urls": [
                "http://www.nature.com/ncomms/about/index.html",
                "http://www.nature.com/ncomms/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/dcb31b98ec58f3fff9f94f148e2952595f017fd9",
        "title": "ProtGPT2 is a deep unsupervised language model for protein design",
        "abstract": null,
        "venue": "Nature Communications",
        "year": 2022,
        "referenceCount": 82,
        "citationCount": 209,
        "influentialCitationCount": 9,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.nature.com/articles/s41467-022-32007-7.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Some of the latest advances in natural language processing, generative Transformers, are applied to train ProtGPT2, a language model that explores unseen regions of the protein space while designing proteins with nature-like properties."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-07-27",
        "journal": {
            "name": "Nature Communications",
            "volume": "13"
        },
        "citationStyles": {
            "bibtex": "@Article{Ferruz2022ProtGPT2IA,\n author = {Noelia Ferruz and Steffen Schmidt and B. H\u00f6cker},\n booktitle = {Nature Communications},\n journal = {Nature Communications},\n title = {ProtGPT2 is a deep unsupervised language model for protein design},\n volume = {13},\n year = {2022}\n}\n"
        }
    },
    "511_subformer_(122m)": {
        "paperId": "d1870f667cbd309df45a244c170d1d4ba36bac03",
        "externalIds": {
            "DBLP": "journals/corr/abs-2101-00234",
            "ArXiv": "2101.00234",
            "DOI": "10.18653/v1/2021.findings-emnlp.344",
            "CorpusId": 230435784
        },
        "corpusId": 230435784,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/d1870f667cbd309df45a244c170d1d4ba36bac03",
        "title": "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers",
        "abstract": "Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "referenceCount": 58,
        "citationCount": 31,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.findings-emnlp.344.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE) and shows that the Subformer can outperform the Transformer even when using significantly fewer parameters."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-01-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2101.00234"
        },
        "citationStyles": {
            "bibtex": "@Article{Reid2021SubformerEW,\n author = {Machel Reid and Edison Marrese-Taylor and Y. Matsuo},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers},\n volume = {abs/2101.00234},\n year = {2021}\n}\n"
        }
    },
    "512_esrgan": {
        "paperId": "1bdd30a8acc75c58a1bdd4daa4545d5f3971a826",
        "externalIds": {
            "MAG": "2952773607",
            "DBLP": "conf/eccv/WangYWGLDQL18",
            "ArXiv": "1809.00219",
            "DOI": "10.1007/978-3-030-11021-5_5",
            "CorpusId": 52154773
        },
        "corpusId": 52154773,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/1bdd30a8acc75c58a1bdd4daa4545d5f3971a826",
        "title": "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks",
        "abstract": null,
        "venue": "ECCV Workshops",
        "year": 2018,
        "referenceCount": 50,
        "citationCount": 2766,
        "influentialCitationCount": 573,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1809.00219",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work thoroughly study three key components of SRGAN \u2013 network architecture, adversarial loss and perceptual loss, and improves each of them to derive an Enhanced SRGAN (ESRGAN), which achieves consistently better visual quality with more realistic and natural textures than SRGAN."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-09-01",
        "journal": {
            "pages": "63-79"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2018ESRGANES,\n author = {Xintao Wang and Ke Yu and Shixiang Wu and Jinjin Gu and Yihao Liu and Chao Dong and Chen Change Loy and Y. Qiao and Xiaoou Tang},\n booktitle = {ECCV Workshops},\n pages = {63-79},\n title = {ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks},\n year = {2018}\n}\n"
        }
    },
    "513_resnext-101_32x48d": {
        "paperId": "0f885fd46064d271d4404cf9bb3d758e1a6f8d55",
        "externalIds": {
            "MAG": "2799269579",
            "DBLP": "conf/eccv/MahajanGRHPLBM18",
            "ArXiv": "1805.00932",
            "DOI": "10.1007/978-3-030-01216-8_12",
            "CorpusId": 13751202
        },
        "corpusId": 13751202,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/0f885fd46064d271d4404cf9bb3d758e1a6f8d55",
        "title": "Exploring the Limits of Weakly Supervised Pretraining",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "referenceCount": 58,
        "citationCount": 1198,
        "influentialCitationCount": 91,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1805.00932",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images and shows improvements on several image classification and object detection tasks, and reports the highest ImageNet-1k single-crop, top-1 accuracy to date."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-05-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1805.00932"
        },
        "citationStyles": {
            "bibtex": "@Article{Mahajan2018ExploringTL,\n author = {D. Mahajan and Ross B. Girshick and Vignesh Ramanathan and Kaiming He and Manohar Paluri and Yixuan Li and Ashwin R. Bharambe and L. Maaten},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {Exploring the Limits of Weakly Supervised Pretraining},\n volume = {abs/1805.00932},\n year = {2018}\n}\n"
        }
    },
    "514_codegen2": {
        "paperId": "886e0962479ec6dac563666399ca4c96a468fcaa",
        "externalIds": {
            "ArXiv": "2305.02309",
            "DBLP": "journals/corr/abs-2305-02309",
            "DOI": "10.48550/arXiv.2305.02309",
            "CorpusId": 258461229
        },
        "corpusId": 258461229,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/886e0962479ec6dac563666399ca4c96a468fcaa",
        "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
        "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly. In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a\"free lunch\"hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored. We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: https://github.com/salesforce/CodeGen.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 31,
        "citationCount": 55,
        "influentialCitationCount": 8,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.02309",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This study attempts to render the training of LLMs for program synthesis more efficient by unifying four key components: model architectures, learning methods, infill sampling, and, (4) data distributions."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-05-03",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2305.02309"
        },
        "citationStyles": {
            "bibtex": "@Article{Nijkamp2023CodeGen2LF,\n author = {Erik Nijkamp and Hiroaki Hayashi and Caiming Xiong and S. Savarese and Yingbo Zhou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {CodeGen2: Lessons for Training LLMs on Programming and Natural Languages},\n volume = {abs/2305.02309},\n year = {2023}\n}\n"
        }
    },
    "515_perceiver_io": {
        "paperId": "9933a5af7895354087baf6c96b64dc8a8973eaed",
        "externalIds": {
            "DBLP": "journals/corr/abs-2107-14795",
            "ArXiv": "2107.14795",
            "CorpusId": 236635379
        },
        "corpusId": 236635379,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9933a5af7895354087baf6c96b64dc8a8973eaed",
        "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs",
        "abstract": "A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain&task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "referenceCount": 104,
        "citationCount": 376,
        "influentialCitationCount": 46,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs and augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-07-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2107.14795"
        },
        "citationStyles": {
            "bibtex": "@Article{Jaegle2021PerceiverIA,\n author = {Andrew Jaegle and Sebastian Borgeaud and Jean-Baptiste Alayrac and Carl Doersch and Catalin Ionescu and David Ding and Skanda Koppula and Andrew Brock and Evan Shelhamer and Olivier J. H'enaff and M. Botvinick and Andrew Zisserman and O. Vinyals and Jo\u00e3o Carreira},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Perceiver IO: A General Architecture for Structured Inputs & Outputs},\n volume = {abs/2107.14795},\n year = {2021}\n}\n"
        }
    },
    "517_ldm-1.45b": {
        "paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "externalIds": {
            "ArXiv": "2112.10752",
            "DBLP": "journals/corr/abs-2112-10752",
            "DOI": "10.1109/CVPR52688.2022.01042",
            "CorpusId": 245335280
        },
        "corpusId": 245335280,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "referenceCount": 110,
        "citationCount": 5469,
        "influentialCitationCount": 1662,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2112.10752",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "These latent diffusion models achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-12-20",
        "journal": {
            "name": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "10674-10685"
        },
        "citationStyles": {
            "bibtex": "@Article{Rombach2021HighResolutionIS,\n author = {Robin Rombach and A. Blattmann and Dominik Lorenz and Patrick Esser and B. Ommer},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {10674-10685},\n title = {High-Resolution Image Synthesis with Latent Diffusion Models},\n year = {2021}\n}\n"
        }
    },
    "519_alphamissense": {
        "paperId": "a82137965925d7712dfb720e1f2ad39251d799eb",
        "externalIds": {
            "DOI": "10.1126/science.adg7492",
            "CorpusId": 262084527,
            "PubMed": "37733863"
        },
        "corpusId": 262084527,
        "publicationVenue": {
            "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
            "name": "Science",
            "type": "journal",
            "issn": "0193-4511",
            "alternate_issns": [
                "0036-8075"
            ],
            "url": "https://www.jstor.org/journal/science",
            "alternate_urls": [
                "https://www.sciencemag.org/",
                "http://www.sciencemag.org/",
                "http://www.jstor.org/journals/00368075.html",
                "http://www.sciencemag.org/archive/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/a82137965925d7712dfb720e1f2ad39251d799eb",
        "title": "Accurate proteome-wide missense variant effect prediction with AlphaMissense",
        "abstract": "The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89% of missense variants as either likely benign or likely pathogenic. Description Editor\u2019s summary Single\u2013amino acid changes in proteins sometimes have little effect but can often lead to problems in protein folding, activity, or stability. Only a small fraction of variants have been experimentally investigated, but there are vast amounts of biological sequence data that are suitable for use as training data for machine learning approaches. Cheng et al. developed AlphaMissense, a deep learning model that builds on the protein structure prediction tool AlphaFold2 (see the Perspective by Marsh and Teichmann). The model is trained on population frequency data and uses sequence and predicted structural context, all of which contribute to its performance. The authors evaluated the model against related methods using clinical databases not included in the training and demonstrated agreement with multiplexed assays of variant effect. Predictions for all single\u2013amino acid substitutions in the human proteome are provided as a community resource. \u2014Michael A. Funk AlphaFold fine-tuned on human and primate population variant frequency databases predicts variant pathogenicity. INTRODUCTION Genome sequencing has revealed extensive genetic variation in human populations. Missense variants are genetic variants that alter the amino acid sequence of proteins. Pathogenic missense variants disrupt protein function and reduce organismal fitness, while benign missense variants have limited effect. RATIONALE Classifying these variants is an important ongoing challenge in human genetics. Of more than 4 million observed missense variants, only an estimated 2% have been clinically classified as pathogenic or benign, while the vast majority of them are of unknown clinical significance. This limits the diagnosis of rare diseases, as well as the development or application of clinical treatments that target the underlying genetic cause. Machine learning approaches could close the variant interpretation gap by exploiting patterns in biological data to predict the pathogenicity of unannotated variants. Specifically, AlphaFold, which accurately predicts protein structure from protein sequence, may be used as a foundation to predict the pathogenicity of variants on proteins. RESULTS We developed AlphaMissense to leverage advances on multiple fronts: (i) unsupervised protein language modeling to learn amino acid distributions conditioned on sequence context; (ii) incorporating structural context by using an AlphaFold-derived system; and (iii) fine-tuning on weak labels from population frequency data, thereby avoiding bias from human-curated annotations. AlphaMissense achieves state-of-the-art missense pathogenicity predictions in clinical annotation, de novo disease variants, and experimental assay benchmarks without explicitly training on such data. As a resource to the community, we provide a database of predictions for all possible single amino acid substitutions in the human proteome. We classify 32% of all missense variants as likely pathogenic and 57% as likely benign using a cutoff yielding 90% precision on the ClinVar dataset, thereby providing a confident prediction for most human missense variants. We show how this resource can be used to accelerate research in multiple fields. Molecular biologists could use the database as a starting point for designing and interpreting experiments that probe saturating amino acid substitutions across the human proteome. Human geneticists could combine gene-level AlphaMissense predictions with population cohort\u2013based approaches to quantify the functional significance of genes, especially for shorter human genes where cohort-based approaches lack statistical power. Finally, clinicians could benefit from the boost in coverage of confidently classified pathogenic variants when prioritizing de novo variants for rare disease diagnostics, and AlphaMissense predictions could inform studies of complex trait genetics that use annotations of rare, likely deleterious variants. CONCLUSION AlphaMissense predictions may illuminate the molecular effects of variants on protein function, contribute to the identification of pathogenic missense mutations and previously unknown disease-causing genes, and increase the diagnostic yield of rare genetic diseases. AlphaMissense will also foster further development of specialized protein variant effect predictors from structure prediction models. AlphaMissense pathogenicity prediction. AlphaMissense takes as input a missense variant and predicts its pathogenicity. We fine-tuned AlphaFold on human and primate variant population frequency data and calibrated the confidence on known disease variants. AlphaMissense predicts the probability of a missense variant being pathogenic and classifies it as either likely benign, likely pathogenic, or uncertain. We provide predictions for all possible human missense variants as a resource for the community.",
        "venue": "Science",
        "year": 2023,
        "referenceCount": 99,
        "citationCount": 112,
        "influentialCitationCount": 9,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.science.org/history/c51c2d58-93ae-4963-a7e4-c0d7c3bc4342/science.adg7492.v1.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity, achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-09-19",
        "journal": {
            "name": "Science",
            "volume": "381"
        },
        "citationStyles": {
            "bibtex": "@Article{Cheng2023AccuratePM,\n author = {Jun Cheng and Guido Novati and Joshua Pan and Clare Bycroft and Akvil\u0117 \u017demgulyt\u0117 and Taylor Applebaum and A. Pritzel and Lai Hong Wong and Michal Zielinski and Tobias Sargeant and Rosalia G. Schneider and Andrew Senior and J. Jumper and D. Hassabis and Pushmeet Kohli and \u017diga Avsec},\n booktitle = {Science},\n journal = {Science},\n title = {Accurate proteome-wide missense variant effect prediction with AlphaMissense},\n volume = {381},\n year = {2023}\n}\n"
        }
    },
    "520_error_propagation": {
        "paperId": "319f22bd5abfd67ac15988aa5c7f705f018c3ccd",
        "externalIds": {
            "MAG": "2154642048",
            "DOI": "10.1016/B978-1-4832-1446-7.50035-2",
            "CorpusId": 62245742
        },
        "corpusId": 62245742,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/319f22bd5abfd67ac15988aa5c7f705f018c3ccd",
        "title": "Learning internal representations by error propagation",
        "abstract": null,
        "venue": "",
        "year": 1986,
        "referenceCount": 40,
        "citationCount": 20381,
        "influentialCitationCount": 961,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion."
        },
        "publicationTypes": null,
        "publicationDate": "1986-01-03",
        "journal": {
            "name": "",
            "pages": "673-695",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Rumelhart1986LearningIR,\n author = {D. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},\n pages = {673-695},\n title = {Learning internal representations by error propagation},\n year = {1986}\n}\n"
        }
    },
    "521_roberta_large": {
        "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "externalIds": {
            "DBLP": "journals/corr/abs-1907-11692",
            "MAG": "2965373594",
            "ArXiv": "1907.11692",
            "CorpusId": 198953378
        },
        "corpusId": 198953378,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 68,
        "citationCount": 17524,
        "influentialCitationCount": 4323,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that BERT was significantly undertrained, and can match or exceed the performance of every model published after it, and the best model achieves state-of-the-art results on GLUE, RACE and SQuAD."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-07-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1907.11692"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2019RoBERTaAR,\n author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and M. Lewis and Luke Zettlemoyer and Veselin Stoyanov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},\n volume = {abs/1907.11692},\n year = {2019}\n}\n"
        }
    },
    "522_overfeat": {
        "paperId": "1109b663453e78a59e4f66446d71720ac58cec25",
        "externalIds": {
            "MAG": "1487583988",
            "DBLP": "journals/corr/SermanetEZMFL13",
            "ArXiv": "1312.6229",
            "CorpusId": 4071727
        },
        "corpusId": 4071727,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/1109b663453e78a59e4f66446d71720ac58cec25",
        "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "referenceCount": 37,
        "citationCount": 4807,
        "influentialCitationCount": 327,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This integrated framework for using Convolutional Networks for classification, localization and detection is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 and obtained very competitive results for the detection and classifications tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2013-12-21",
        "journal": {
            "name": "CoRR",
            "volume": "abs/1312.6229"
        },
        "citationStyles": {
            "bibtex": "@Article{Sermanet2013OverFeatIR,\n author = {P. Sermanet and D. Eigen and Xiang Zhang and Micha\u00ebl Mathieu and R. Fergus and Yann LeCun},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks},\n volume = {abs/1312.6229},\n year = {2013}\n}\n"
        }
    },
    "523_coedit-xxl": {
        "paperId": "9c827a18a2a865e68c848b6823aec1768f9f9300",
        "externalIds": {
            "DBLP": "conf/emnlp/Raheja0KK23",
            "ArXiv": "2305.09857",
            "DOI": "10.48550/arXiv.2305.09857",
            "CorpusId": 258741409
        },
        "corpusId": 258741409,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/9c827a18a2a865e68c848b6823aec1768f9f9300",
        "title": "CoEdIT: Text Editing by Task-Specific Instruction Tuning",
        "abstract": "We introduce CoEdIT, a state-of-the-art text editing system for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as\"Make the sentence simpler\"or\"Write it in a more neutral style,\"and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly available largest-sized LLMs trained on instructions while being nearly 60x smaller, (3) is capable of generalizing to unseen edit instructions, and (4) exhibits abilities to generalize to composite instructions containing different combinations of edit actions. Through extensive qualitative and quantitative analysis, we show that writers prefer the edits suggested by CoEdIT relative to other state-of-the-art text editing models. Our code, data, and models are publicly available at https://github.com/vipulraheja/coedit.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2023,
        "referenceCount": 70,
        "citationCount": 18,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.09857",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that writers prefer the edits suggested by CoEdIT relative to other state-of-the-art text editing models."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-05-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2305.09857"
        },
        "citationStyles": {
            "bibtex": "@Article{Raheja2023CoEdITTE,\n author = {Vipul Raheja and Dhruv Kumar and Ryan Koo and Dongyeop Kang},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {CoEdIT: Text Editing by Task-Specific Instruction Tuning},\n volume = {abs/2305.09857},\n year = {2023}\n}\n"
        }
    },
    "524_cogvideo": {
        "paperId": "707bd332d2c21dc5eb1f02a52d4a0506199aae76",
        "externalIds": {
            "ArXiv": "2205.15868",
            "DBLP": "journals/corr/abs-2205-15868",
            "DOI": "10.48550/arXiv.2205.15868",
            "CorpusId": 249209614
        },
        "corpusId": 249209614,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/707bd332d2c21dc5eb1f02a52d4a0506199aae76",
        "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
        "abstract": "Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 45,
        "citationCount": 226,
        "influentialCitationCount": 47,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2205.15868",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2, and proposes multi-frame-rate hierarchical training strategy to better align text and video clips."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-05-29",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2205.15868"
        },
        "citationStyles": {
            "bibtex": "@Article{Hong2022CogVideoLP,\n author = {Wenyi Hong and Ming Ding and Wendi Zheng and Xinghan Liu and Jie Tang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers},\n volume = {abs/2205.15868},\n year = {2022}\n}\n"
        }
    },
    "525_transformer-xl+wn+adamp": {
        "paperId": "e11e8d81c68cc782f564ed78e595b66790719804",
        "externalIds": {
            "MAG": "3092043977",
            "DBLP": "conf/iclr/HeoCOHYKUH21",
            "CorpusId": 222295578
        },
        "corpusId": 222295578,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/e11e8d81c68cc782f564ed78e595b66790719804",
        "title": "AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights",
        "abstract": "Normalization techniques are a boon for modern deep learning. They let weights converge more quickly with often better generalization performances. It has been argued that the normalization-induced scale invariance among the weights provides an advantageous ground for gradient descent (GD) optimizers: the effective step sizes are automatically reduced over time, stabilizing the overall training procedure. It is often overlooked, however, that the additional introduction of momentum in GD optimizers results in a far more rapid reduction in effective step sizes for scale-invariant weights, a phenomenon that has not yet been studied and may have caused unwanted side effects in the current practice. This is a crucial issue because arguably the vast majority of modern deep neural networks consist of (1) momentum-based GD (e.g. SGD or Adam) and (2) scale-invariant parameters. In this paper, we verify that the widely-adopted combination of the two ingredients lead to the premature decay of effective step sizes and sub-optimal model performances. We propose a simple and effective remedy, SGDP and AdamP: get rid of the radial component, or the norm-increasing direction, at each optimizer step. Because of the scale invariance, this modification only alters the effective step sizes without changing the effective update directions, thus enjoying the original convergence properties of GD optimizers. Given the ubiquity of momentum GD and scale invariance in machine learning, we have evaluated our methods against the baselines on 13 benchmarks. They range from vision tasks like classification (e.g. ImageNet), retrieval (e.g. CUB and SOP), and detection (e.g. COCO) to language modelling (e.g. WikiText) and audio classification (e.g. DCASE) tasks. We verify that our solution brings about uniform gains in those benchmarks. Source code is available at this https URL.",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "referenceCount": 77,
        "citationCount": 97,
        "influentialCitationCount": 10,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a simple and effective remedy, SGDP and AdamP: get rid of the radial component, or the norm-increasing direction, at each optimizer step, which alters the effective step sizes without changing the effective update directions, thus enjoying the original convergence properties of GD optimizers."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-06-15",
        "journal": {
            "name": "arXiv: Learning",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Heo2020AdamPSD,\n author = {Byeongho Heo and Sanghyuk Chun and Seong Joon Oh and Dongyoon Han and Sangdoo Yun and Gyuwan Kim and Youngjung Uh and Jung-Woo Ha},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Learning},\n title = {AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights},\n year = {2020}\n}\n"
        }
    },
    "526_codegen-mono_16.1b": {
        "paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878",
        "externalIds": {
            "DBLP": "conf/iclr/NijkampPHTWZSX23",
            "ArXiv": "2203.13474",
            "CorpusId": 252668917
        },
        "corpusId": 252668917,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/38115e80d805fb0fb8f090dc88ced4b24be07878",
        "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis",
        "abstract": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 51,
        "citationCount": 385,
        "influentialCitationCount": 68,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work trains and releases a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER and model checkpoints, and investigates the multi-step paradigm for program synthesis."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-03-25",
        "journal": null,
        "citationStyles": {
            "bibtex": "@Article{Nijkamp2022CodeGenAO,\n author = {Erik Nijkamp and Bo Pang and Hiroaki Hayashi and Lifu Tu and Haiquan Wang and Yingbo Zhou and S. Savarese and Caiming Xiong},\n booktitle = {International Conference on Learning Representations},\n title = {CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},\n year = {2022}\n}\n"
        }
    },
    "527_$\\infty$-former_(sm)": {
        "paperId": "83238f3402fc4a7c4efa48bae180d81cabed23ee",
        "externalIds": {
            "DBLP": "journals/corr/abs-2109-00301",
            "ACL": "2022.acl-long.375",
            "ArXiv": "2109.00301",
            "CorpusId": 237412971
        },
        "corpusId": 237412971,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/83238f3402fc4a7c4efa48bae180d81cabed23ee",
        "title": "\\infty-former: Infinite Memory Transformer",
        "abstract": "Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the \\infty-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \\infty-former\u2019s attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important, \\infty-former maintains \u201csticky memories,\u201d being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the \\infty-former\u2019s ability to retain information from long sequences.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 54,
        "citationCount": 14,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The \\infty-former is proposed, which extends the vanilla transformer with an unbounded long-term memory, and maintains \u201csticky memories,\u201d being able to model arbitrarily long contexts while keeping the computation budget fixed."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-09-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2109.00301"
        },
        "citationStyles": {
            "bibtex": "@Article{Martins2021inftyformerIM,\n author = {Pedro Henrique Martins and Zita Marinho and Andr\u00e9 F. T. Martins},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {\\infty-former: Infinite Memory Transformer},\n volume = {abs/2109.00301},\n year = {2021}\n}\n"
        }
    },
    "529_chinchilla": {
        "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1",
        "externalIds": {
            "ArXiv": "2203.15556",
            "DBLP": "journals/corr/abs-2203-15556",
            "DOI": "10.48550/arXiv.2203.15556",
            "CorpusId": 247778764
        },
        "corpusId": 247778764,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/8342b592fe238f3d230e4959b06fd10153c45db1",
        "title": "Training Compute-Optimal Large Language Models",
        "abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 78,
        "citationCount": 947,
        "influentialCitationCount": 93,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.15556",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work trains a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data, and reaches a state-of-the-art average accuracy, greater than a 7% improvement over Gopher."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-03-29",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2203.15556"
        },
        "citationStyles": {
            "bibtex": "@Article{Hoffmann2022TrainingCL,\n author = {Jordan Hoffmann and Sebastian Borgeaud and A. Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and K. Simonyan and Erich Elsen and Jack W. Rae and O. Vinyals and L. Sifre},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Compute-Optimal Large Language Models},\n volume = {abs/2203.15556},\n year = {2022}\n}\n"
        }
    },
    "530_align": {
        "paperId": "141a5033d9994242b18bb3b217e79582f1ee9306",
        "externalIds": {
            "DBLP": "conf/icml/JiaYXCPPLSLD21",
            "ArXiv": "2102.05918",
            "CorpusId": 231879586
        },
        "corpusId": 231879586,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/141a5033d9994242b18bb3b217e79582f1ee9306",
        "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
        "abstract": "Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 81,
        "citationCount": 2078,
        "influentialCitationCount": 274,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A noisy dataset of over one billion image alt-text pairs is leverage, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset and it is shown that the scale of the corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-02-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2102.05918"
        },
        "citationStyles": {
            "bibtex": "@Article{Jia2021ScalingUV,\n author = {Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yun-Hsuan Sung and Zhen Li and Tom Duerig},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},\n volume = {abs/2102.05918},\n year = {2021}\n}\n"
        }
    },
    "532_perceptron_for_large_margin_classification": {
        "paperId": "2479a5cf6cefefb83166c612564787414e47131f",
        "externalIds": {
            "DBLP": "journals/ml/FreundS99",
            "MAG": "1979711143",
            "DOI": "10.1023/A:1007662407062",
            "CorpusId": 5885617
        },
        "corpusId": 5885617,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/2479a5cf6cefefb83166c612564787414e47131f",
        "title": "Large Margin Classification Using the Perceptron Algorithm",
        "abstract": null,
        "venue": "COLT' 98",
        "year": 1998,
        "referenceCount": 26,
        "citationCount": 1435,
        "influentialCitationCount": 100,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1023/A:1007662407062.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new algorithm for linear classification which combines Rosenblatt's perceptron algorithm with Helmbold and Warmuth's leave-one-out method is introduced, which is much simpler to implement, and much more efficient in terms of computation time."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1998-07-24",
        "journal": {
            "name": "Machine Learning",
            "pages": "277-296",
            "volume": "37"
        },
        "citationStyles": {
            "bibtex": "@Article{Freund1998LargeMC,\n author = {Y. Freund and R. Schapire},\n booktitle = {COLT' 98},\n journal = {Machine Learning},\n pages = {277-296},\n title = {Large Margin Classification Using the Perceptron Algorithm},\n volume = {37},\n year = {1998}\n}\n"
        }
    },
    "534_d-lsrc(200)+kn5": {
        "paperId": "eb329aab0d7d48b982dde62a5db6deaafa7de07d",
        "externalIds": {
            "MAG": "2963160216",
            "ACL": "D16-1154",
            "ArXiv": "1708.06555",
            "DBLP": "conf/emnlp/OualilSGK16",
            "DOI": "10.18653/v1/D16-1154",
            "CorpusId": 1045792
        },
        "corpusId": 1045792,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/eb329aab0d7d48b982dde62a5db6deaafa7de07d",
        "title": "Long-Short Range Context Neural Networks for Language Modeling",
        "abstract": "The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora. This task typically involves the learning of short range dependencies, which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature. We propose in this paper a new multi-span architecture, which separately models the short and long context information while it dynamically merges them to perform the language modeling task. This is done through a novel recurrent Long-Short Range Context (LSRC) network, which explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time. This new architecture is an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art language modeling techniques.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2016,
        "referenceCount": 21,
        "citationCount": 9,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D16-1154.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new multi-span architecture is proposed, which separately models the short and long context information while it dynamically merges them to perform the language modeling task, and an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-11-01",
        "journal": {
            "pages": "1473-1481"
        },
        "citationStyles": {
            "bibtex": "@Article{Oualil2016LongShortRC,\n author = {Youssef Oualil and Mittul Singh and Clayton Greenberg and D. Klakow},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1473-1481},\n title = {Long-Short Range Context Neural Networks for Language Modeling},\n year = {2016}\n}\n"
        }
    },
    "535_histograms_of_oriented_gradients": {
        "paperId": "e8b12467bdc20bde976750b8a28decdb33246d1d",
        "externalIds": {
            "DBLP": "conf/cvpr/DalalT05",
            "MAG": "2161969291",
            "DOI": "10.1109/CVPR.2005.177",
            "CorpusId": 206590483
        },
        "corpusId": 206590483,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e8b12467bdc20bde976750b8a28decdb33246d1d",
        "title": "Histograms of oriented gradients for human detection",
        "abstract": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "referenceCount": 26,
        "citationCount": 32550,
        "influentialCitationCount": 4817,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://hal.inria.fr/inria-00548512/file/hog_cvpr2005.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "publicationDate": "2005-06-20",
        "journal": {
            "name": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)",
            "pages": "886-893 vol. 1",
            "volume": "1"
        },
        "citationStyles": {
            "bibtex": "@Article{Dalal2005HistogramsOO,\n author = {Navneet Dalal and B. Triggs},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},\n pages = {886-893 vol. 1},\n title = {Histograms of oriented gradients for human detection},\n volume = {1},\n year = {2005}\n}\n"
        }
    },
    "536_esm1-670m_(ur50_d)": {
        "paperId": "18a93dc1558bf9d7534d0b416633cebaf75c1145",
        "externalIds": {
            "DBLP": "journals/pnas/RivesMSGLLGOZMF21",
            "PubMedCentral": "8053943",
            "MAG": "2943495267",
            "DOI": "10.1101/622803",
            "CorpusId": 155162335,
            "PubMed": "33876751"
        },
        "corpusId": 155162335,
        "publicationVenue": {
            "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
            "name": "Proceedings of the National Academy of Sciences of the United States of America",
            "type": "journal",
            "alternate_names": [
                "PNAS",
                "PNAS online",
                "Proceedings of the National Academy of Sciences of the United States of America.",
                "Proc National Acad Sci",
                "Proceedings of the National Academy of Sciences",
                "Proc National Acad Sci u s Am"
            ],
            "issn": "0027-8424",
            "alternate_issns": [
                "1091-6490"
            ],
            "url": "https://www.jstor.org/journal/procnatiacadscie",
            "alternate_urls": [
                "http://www.pnas.org/",
                "https://www.pnas.org/",
                "http://www.jstor.org/journals/00278424.html",
                "www.pnas.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/18a93dc1558bf9d7534d0b416633cebaf75c1145",
        "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
        "abstract": "Significance Learning biological properties from sequence data is a logical step toward generative and predictive artificial intelligence for biology. Here, we propose scaling a deep contextual language model with unsupervised learning to sequences spanning evolutionary diversity. We find that without prior knowledge, information emerges in the learned representations on fundamental properties of proteins such as secondary structure, contacts, and biological activity. We show the learned representations are useful across benchmarks for remote homology detection, prediction of secondary structure, long-range residue\u2013residue contacts, and mutational effect. Unsupervised representation learning enables state-of-the-art supervised prediction of mutational effect and secondary structure and improves state-of-the-art features for long-range contact prediction. In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",
        "venue": "Proceedings of the National Academy of Sciences of the United States of America",
        "year": 2019,
        "referenceCount": 150,
        "citationCount": 1250,
        "influentialCitationCount": 145,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2019/05/29/622803.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Biology",
            "Medicine",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work uses unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity, and finds that without prior knowledge, information emerges in the learned representations on fundamental properties of proteins such as secondary structure, contacts, and biological activity."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-04-29",
        "journal": {
            "name": "Proceedings of the National Academy of Sciences of the United States of America",
            "volume": "118"
        },
        "citationStyles": {
            "bibtex": "@Article{Rives2019BiologicalSA,\n author = {Alexander Rives and Siddharth Goyal and Joshua Meier and Demi Guo and Myle Ott and C. L. Zitnick and Jerry Ma and R. Fergus},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences of the United States of America},\n title = {Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},\n volume = {118},\n year = {2019}\n}\n"
        }
    },
    "537_variational_(untied_weights,_mc)_lstm_(large)": {
        "paperId": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
        "externalIds": {
            "MAG": "2963266340",
            "DBLP": "conf/nips/GalG16",
            "ArXiv": "1512.05287",
            "CorpusId": 15953218
        },
        "corpusId": 15953218,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
        "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
        "abstract": "Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "referenceCount": 36,
        "citationCount": 1547,
        "influentialCitationCount": 229,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work applies a new variational inference based dropout technique in LSTM and GRU models, which outperforms existing techniques, and to the best of the knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-12-16",
        "journal": {
            "pages": "1019-1027"
        },
        "citationStyles": {
            "bibtex": "@Article{Gal2015ATG,\n author = {Y. Gal and Zoubin Ghahramani},\n booktitle = {Neural Information Processing Systems},\n pages = {1019-1027},\n title = {A Theoretically Grounded Application of Dropout in Recurrent Neural Networks},\n year = {2015}\n}\n"
        }
    },
    "538_learning_past_tenses": {
        "paperId": "4fa569625b5ab35e955a8d5be11a4aa9f59ca424",
        "externalIds": {
            "MAG": "1939511740",
            "DOI": "10.7551/mitpress/5782.003.0031",
            "CorpusId": 56499839
        },
        "corpusId": 56499839,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/4fa569625b5ab35e955a8d5be11a4aa9f59ca424",
        "title": "On learning the past-tenses of English verbs: implicit rules or parallel distributed processing",
        "abstract": "Abstract : This paper presents an alternative to the standard rule based account of a child's acquisition of the past tense in English. Children are typically said to pass through a three-phase acquisition process in which they first learn past tense by rote, then learn the past tense rule and over regularize, and then finally learn the exceptions to the rule. We show that the acquisition data can be accounted for in more detail by dispensing with the assumption that the child learns rules and substituting in its place a simple homogeneous learning procedure. We show how rule-like behavior can emerge from the interactions among a network of units encoding the root form to past tense mapping. A large computer simulation of the learning process demonstrates the operating principles of our alternative account, shows how details of the acquisition process not captured by the rule account emerge, and makes predictions about other details of the acquisition process not yet observed. Keywords: Learning; networks; Language; Verbs; Perceptions; Morphology.",
        "venue": "",
        "year": 1986,
        "referenceCount": 26,
        "citationCount": 1114,
        "influentialCitationCount": 39,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown how rule-like behavior can emerge from the interactions among a network of units encoding the root form to past tense mapping, and how details of the acquisition process not captured by the rule account emerge."
        },
        "publicationTypes": null,
        "publicationDate": "1986-01-03",
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Rumelhart1986OnLT,\n author = {D. Rumelhart and James L. McClelland},\n title = {On learning the past-tenses of English verbs: implicit rules or parallel distributed processing},\n year = {1986}\n}\n"
        }
    },
    "539_search-proven_best_lstm": {
        "paperId": "5b8364c21155d3d2cd38ea4c8b8580beba9a3250",
        "externalIds": {
            "DBLP": "conf/icml/JozefowiczZS15",
            "MAG": "581956982",
            "CorpusId": 9668607
        },
        "corpusId": 9668607,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/5b8364c21155d3d2cd38ea4c8b8580beba9a3250",
        "title": "An Empirical Exploration of Recurrent Network Architectures",
        "abstract": "The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. \n \nIn this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "referenceCount": 30,
        "citationCount": 1602,
        "influentialCitationCount": 112,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that adding a bias of 1 to the LSTM's forget gate closes the gap between the L STM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-07-06",
        "journal": {
            "pages": "2342-2350"
        },
        "citationStyles": {
            "bibtex": "@Article{J\u00f3zefowicz2015AnEE,\n author = {R. J\u00f3zefowicz and Wojciech Zaremba and I. Sutskever},\n booktitle = {International Conference on Machine Learning},\n pages = {2342-2350},\n title = {An Empirical Exploration of Recurrent Network Architectures},\n year = {2015}\n}\n"
        }
    },
    "540_aragpt2-mega": {
        "paperId": "c342798bafc1eaaa60c652fc90fd738941542133",
        "externalIds": {
            "DBLP": "conf/wanlp/AntounBH21a",
            "ACL": "2021.wanlp-1.21",
            "ArXiv": "2012.15520",
            "CorpusId": 229923851
        },
        "corpusId": 229923851,
        "publicationVenue": {
            "id": "7008bb6f-4a70-4ba5-b6fc-2d691c1557ca",
            "name": "Workshop on Arabic Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "WANLP",
                "Workshop Arab Nat Lang Process"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/c342798bafc1eaaa60c652fc90fd738941542133",
        "title": "AraGPT2: Pre-Trained Transformer for Arabic Language Generation",
        "abstract": "Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.",
        "venue": "Workshop on Arabic Natural Language Processing",
        "year": 2020,
        "referenceCount": 43,
        "citationCount": 61,
        "influentialCitationCount": 7,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles is developed and an automatic discriminator model with a 98% percent accuracy in detecting model-generated text is released."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-12-31",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2012.15520"
        },
        "citationStyles": {
            "bibtex": "@Article{Antoun2020AraGPT2PT,\n author = {Wissam Antoun and Fady Baly and Hazem M. Hajj},\n booktitle = {Workshop on Arabic Natural Language Processing},\n journal = {ArXiv},\n title = {AraGPT2: Pre-Trained Transformer for Arabic Language Generation},\n volume = {abs/2012.15520},\n year = {2020}\n}\n"
        }
    },
    "541_glam": {
        "paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14",
        "externalIds": {
            "DBLP": "conf/icml/DuHDTLXKZYFZFBZ22",
            "ArXiv": "2112.06905",
            "CorpusId": 245124124
        },
        "corpusId": 245124124,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/80d0116d77beeded0c23cf48946d9d10d4faee14",
        "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
        "abstract": "Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 108,
        "citationCount": 395,
        "influentialCitationCount": 38,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes and develops a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-12-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2112.06905"
        },
        "citationStyles": {
            "bibtex": "@Article{Du2021GLaMES,\n author = {Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and M. Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat and Barret Zoph and L. Fedus and Maarten Bosma and Zongwei Zhou and Tao Wang and Yu Emma Wang and Kellie Webster and Marie Pellat and Kevin Robinson and K. Meier-Hellstern and Toju Duke and Lucas Dixon and Kun Zhang and Quoc V. Le and Yonghui Wu and Z. Chen and Claire Cui},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},\n volume = {abs/2112.06905},\n year = {2021}\n}\n"
        }
    },
    "542_lstm-3-layer+gadam": {
        "paperId": "f1d527d069cd370428a40ec58bff5e85cd7f9715",
        "externalIds": {
            "ArXiv": "2003.01247",
            "CorpusId": 236169710
        },
        "corpusId": 236169710,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/f1d527d069cd370428a40ec58bff5e85cd7f9715",
        "title": "Iterative Averaging in the Quest for Best Test Error",
        "abstract": "We analyse and explain the increased generalisation performance of iterate averaging using a Gaussian process perturbation model between the true and batch risk surface on the high dimensional quadratic. We derive three phenomena \\latestEdits{from our theoretical results:} (1) The importance of combining iterate averaging (IA) with large learning rates and regularisation for improved regularisation. (2) Justification for less frequent averaging. (3) That we expect adaptive gradient methods to work equally well, or better, with iterate averaging than their non-adaptive counterparts. Inspired by these results\\latestEdits{, together with} empirical investigations of the importance of appropriate regularisation for the solution diversity of the iterates, we propose two adaptive algorithms with iterate averaging. These give significantly better results compared to stochastic gradient descent (SGD), require less tuning and do not require early stopping or validation set monitoring. We showcase the efficacy of our approach on the CIFAR-10/100, ImageNet and Penn Treebank datasets on a variety of modern and classical network architectures.",
        "venue": "",
        "year": 2020,
        "referenceCount": 92,
        "citationCount": 5,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Two adaptive algorithms with iterate averaging are proposed that give significantly better results compared to stochastic gradient descent (SGD), require less tuning and do not require early stopping or validation set monitoring."
        },
        "publicationTypes": null,
        "publicationDate": "2020-03-02",
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Granziol2020IterativeAI,\n author = {Diego Granziol and Xingchen Wan and Samuel Albanie and Stephen J. Roberts},\n title = {Iterative Averaging in the Quest for Best Test Error},\n year = {2020}\n}\n"
        }
    },
    "543_big_transfer_(bit-l)": {
        "paperId": "d3cd88dda7475e13662deb5c65662ef79e318050",
        "externalIds": {
            "MAG": "2997972888",
            "DBLP": "journals/corr/abs-1912-11370",
            "CorpusId": 209460680
        },
        "corpusId": 209460680,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/d3cd88dda7475e13662deb5c65662ef79e318050",
        "title": "Large Scale Learning of General Visual Representations for Transfer",
        "abstract": ",",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 56,
        "citationCount": 84,
        "influentialCitationCount": 11,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-12-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1912.11370"
        },
        "citationStyles": {
            "bibtex": "@Article{Kolesnikov2019LargeSL,\n author = {A. Kolesnikov and Lucas Beyer and Xiaohua Zhai and J. Puigcerver and Jessica Yung and S. Gelly and N. Houlsby},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Large Scale Learning of General Visual Representations for Transfer},\n volume = {abs/1912.11370},\n year = {2019}\n}\n"
        }
    },
    "544_scibert": {
        "paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028",
        "externalIds": {
            "ACL": "D19-1371",
            "DBLP": "conf/emnlp/BeltagyLC19",
            "MAG": "2973154071",
            "ArXiv": "1903.10676",
            "DOI": "10.18653/v1/D19-1371",
            "CorpusId": 202558505
        },
        "corpusId": 202558505,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/156d217b0a911af97fa1b5a71dc909ccef7a8028",
        "title": "SciBERT: A Pretrained Language Model for Scientific Text",
        "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "referenceCount": 32,
        "citationCount": 2162,
        "influentialCitationCount": 423,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1371.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks and demonstrates statistically significant improvements over BERT."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-03-01",
        "journal": {
            "pages": "3613-3618"
        },
        "citationStyles": {
            "bibtex": "@Article{Beltagy2019SciBERTAP,\n author = {Iz Beltagy and Kyle Lo and Arman Cohan},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {3613-3618},\n title = {SciBERT: A Pretrained Language Model for Scientific Text},\n year = {2019}\n}\n"
        }
    },
    "545_coatnet": {
        "paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0",
        "externalIds": {
            "DBLP": "conf/nips/DaiLLT21",
            "ArXiv": "2106.04803",
            "CorpusId": 235376986
        },
        "corpusId": 235376986,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9f4b69762ffb1ba42b573fd4ced996f3153e21c0",
        "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
        "abstract": "Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced\"coat\"nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 55,
        "citationCount": 789,
        "influentialCitationCount": 88,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents CoAtNets, a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention and (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-06-09",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2106.04803"
        },
        "citationStyles": {
            "bibtex": "@Article{Dai2021CoAtNetMC,\n author = {Zihang Dai and Hanxiao Liu and Quoc V. Le and Mingxing Tan},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {CoAtNet: Marrying Convolution and Attention for All Data Sizes},\n volume = {abs/2106.04803},\n year = {2021}\n}\n"
        }
    },
    "546_alphago_fan": {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "externalIds": {
            "DBLP": "journals/nature/SilverHMGSDSAPL16",
            "MAG": "2257979135",
            "DOI": "10.1038/nature16961",
            "CorpusId": 515925,
            "PubMed": "26819042"
        },
        "corpusId": 515925,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search",
        "abstract": null,
        "venue": "Nature",
        "year": 2016,
        "referenceCount": 72,
        "citationCount": 14771,
        "influentialCitationCount": 522,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.nature.com/articles/nature16961.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Using this search algorithm, the program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0.5, the first time that a computer program has defeated a human professional player in the full-sized game of Go."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-01-27",
        "journal": {
            "name": "Nature",
            "pages": "484-489",
            "volume": "529"
        },
        "citationStyles": {
            "bibtex": "@Article{Silver2016MasteringTG,\n author = {David Silver and Aja Huang and Chris J. Maddison and A. Guez and L. Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and S. Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and I. Sutskever and T. Lillicrap and M. Leach and K. Kavukcuoglu and T. Graepel and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {484-489},\n title = {Mastering the game of Go with deep neural networks and tree search},\n volume = {529},\n year = {2016}\n}\n"
        }
    },
    "547_megatron-lm_(1t)": {
        "paperId": "77e73174e606c0820a52a940088832b32d9a033e",
        "externalIds": {
            "CorpusId": 233204537
        },
        "corpusId": 233204537,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/77e73174e606c0820a52a940088832b32d9a033e",
        "title": "Efficient Large-Scale Language Model Training on GPU Clusters",
        "abstract": "Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these large models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on a single GPU or even on a multi-GPU server; and b) the number of compute operations required to train these models can result in unrealistically long training times. New methods of model parallelism such as tensor and pipeline parallelism have been proposed to address these challenges; unfortunately, naive usage leads to fundamental scaling issues at thousands of GPUs due to various reasons, e.g., expensive cross-node communication or idle periods waiting on other devices. In this work, we show how to compose different types of parallelism methods (tensor, pipeline, and data paralleism) to scale to thousands of GPUs, achieving a two-order-of-magnitude increase in the sizes of models we can efficiently train compared to existing systems. We discuss various implementations of pipeline parallelism and propose a novel schedule that can improve throughput by more than 10% with comparable memory footprint compared to previously-proposed approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. The composition of these techniques allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of peak; previous efforts to train similar-sized models achieve much lower throughput (36% of theoretical peak). Our code has been open-sourced at https://github.com/nvidia/megatron-lm.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 11,
        "citationCount": 79,
        "influentialCitationCount": 15,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work shows how to compose different types of parallelism methods (tensor, pipeline, and data paralleism) to scale to thousands of GPUs, achieving a two-order-of-magnitude increase in the sizes of models the authors can efficiently train compared to existing systems."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2104.04473"
        },
        "citationStyles": {
            "bibtex": "@Article{Narayanan2021EfficientLL,\n author = {D. Narayanan and M. Shoeybi and Jared Casper and P. LeGresley and M. Patwary and V. Korthikanti and Dmitri Vainbrand and Prethvi Kashinkunti and Julie Bernauer and Bryan Catanzaro and Amar Phanishayee and Matei Zaharia and Fiodar Kazhamiaka},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Efficient Large-Scale Language Model Training on GPU Clusters},\n volume = {abs/2104.04473},\n year = {2021}\n}\n"
        }
    },
    "548_ctm_(cifar-10)": {
        "paperId": "2ea9167ca0dd536923ea0968d6b89e2b6cc34f21",
        "externalIds": {
            "DBLP": "journals/corr/abs-2310-02279",
            "ArXiv": "2310.02279",
            "DOI": "10.48550/arXiv.2310.02279",
            "CorpusId": 263622294
        },
        "corpusId": 263622294,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/2ea9167ca0dd536923ea0968d6b89e2b6cc34f21",
        "title": "Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion",
        "abstract": "Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the score accommodates all diffusion model inference techniques, including exact likelihood computation.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 68,
        "citationCount": 14,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.02279",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 and ImageNet at 64X64 resolution."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-10-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2310.02279"
        },
        "citationStyles": {
            "bibtex": "@Article{Kim2023ConsistencyTM,\n author = {Dongjun Kim and Chieh-Hsin Lai and Wei-Hsiang Liao and Naoki Murata and Yuhta Takida and Toshimitsu Uesaka and Yutong He and Yuki Mitsufuji and Stefano Ermon},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion},\n volume = {abs/2310.02279},\n year = {2023}\n}\n"
        }
    },
    "549_bpl": {
        "paperId": "815c84ab906e43f3e6322f2ca3fd5e1360c64285",
        "externalIds": {
            "MAG": "2341708432",
            "DOI": "10.1126/science.aab3050",
            "CorpusId": 11790493,
            "PubMed": "26659050"
        },
        "corpusId": 11790493,
        "publicationVenue": {
            "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
            "name": "Science",
            "type": "journal",
            "issn": "0193-4511",
            "alternate_issns": [
                "0036-8075"
            ],
            "url": "https://www.jstor.org/journal/science",
            "alternate_urls": [
                "https://www.sciencemag.org/",
                "http://www.sciencemag.org/",
                "http://www.jstor.org/journals/00368075.html",
                "http://www.sciencemag.org/archive/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/815c84ab906e43f3e6322f2ca3fd5e1360c64285",
        "title": "Human-level concept learning through probabilistic program induction",
        "abstract": "Handwritten characters drawn by a model Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look \u201cright\u201d as judged by Turing-like tests of the model's output in comparison to what real humans produce. Science, this issue p. 1332 Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms\u2014for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world\u2019s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several \u201cvisual Turing tests\u201d probing the model\u2019s creative generalization abilities, which in many cases are indistinguishable from human behavior.",
        "venue": "Science",
        "year": 2015,
        "referenceCount": 96,
        "citationCount": 2692,
        "influentialCitationCount": 264,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://science.sciencemag.org/content/sci/350/6266/1332.full.pdf",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A computational model is described that learns in a similar fashion and does so better than current deep learning algorithms and can generate new letters of the alphabet that look \u201cright\u201d as judged by Turing-like tests of the model's output in comparison to what real humans produce."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2015-12-11",
        "journal": {
            "name": "Science",
            "pages": "1332 - 1338",
            "volume": "350"
        },
        "citationStyles": {
            "bibtex": "@Article{Lake2015HumanlevelCL,\n author = {B. Lake and R. Salakhutdinov and J. Tenenbaum},\n booktitle = {Science},\n journal = {Science},\n pages = {1332 - 1338},\n title = {Human-level concept learning through probabilistic program induction},\n volume = {350},\n year = {2015}\n}\n"
        }
    },
    "550_sparrow": {
        "paperId": "74eae12620bd1c1393e268bddcb6f129a5025166",
        "externalIds": {
            "DBLP": "journals/corr/abs-2209-14375",
            "ArXiv": "2209.14375",
            "DOI": "10.48550/arXiv.2209.14375",
            "CorpusId": 252596089
        },
        "corpusId": 252596089,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/74eae12620bd1c1393e268bddcb6f129a5025166",
        "title": "Improving alignment of dialogue agents via targeted human judgements",
        "abstract": "We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 106,
        "citationCount": 288,
        "influentialCitationCount": 17,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2209.14375",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Sparks, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines, is presented and it is demonstrated that though the model learns to follow the authors' rules it can exhibit distributional biases."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-09-28",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2209.14375"
        },
        "citationStyles": {
            "bibtex": "@Article{Glaese2022ImprovingAO,\n author = {A. Glaese and Nathan McAleese and Maja Trkebacz and John Aslanides and Vlad Firoiu and Timo Ewalds and M. Rauh and Laura Weidinger and Martin Chadwick and Phoebe Thacker and Lucy Campbell-Gillingham and J. Uesato and Po-Sen Huang and Ramona Comanescu and Fan Yang and A. See and Sumanth Dathathri and Rory Greig and Charlie Chen and Doug Fritz and Jaume Sanchez Elias and Richard Green and Sovna Mokr'a and Nicholas Fernando and Boxi Wu and Rachel Foley and Susannah Young and Iason Gabriel and William S. Isaac and John F. J. Mellor and D. Hassabis and K. Kavukcuoglu and Lisa Anne Hendricks and G. Irving},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Improving alignment of dialogue agents via targeted human judgements},\n volume = {abs/2209.14375},\n year = {2022}\n}\n"
        }
    },
    "552_vqgan_+_clip": {
        "paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c",
        "externalIds": {
            "ArXiv": "2012.09841",
            "MAG": "3111551570",
            "DBLP": "journals/corr/abs-2012-09841",
            "DOI": "10.1109/CVPR46437.2021.01268",
            "CorpusId": 229297973
        },
        "corpusId": 229297973,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/47f7ec3d0a5e6e83b6768ece35206a94dc81919c",
        "title": "Taming Transformers for High-Resolution Image Synthesis",
        "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "referenceCount": 82,
        "citationCount": 1493,
        "influentialCitationCount": 326,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2012.09841",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-12-17",
        "journal": {
            "name": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "12868-12878"
        },
        "citationStyles": {
            "bibtex": "@Article{Esser2020TamingTF,\n author = {Patrick Esser and Robin Rombach and B. Ommer},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {12868-12878},\n title = {Taming Transformers for High-Resolution Image Synthesis},\n year = {2020}\n}\n"
        }
    },
    "553_phrase-based_translation": {
        "paperId": "a4b828609b60b06e61bea7a4029cc9e1cad5df87",
        "externalIds": {
            "ACL": "N03-1017",
            "MAG": "2153653739",
            "DBLP": "conf/naacl/KoehnOM03",
            "DOI": "10.3115/1073445.1073462",
            "CorpusId": 8884845
        },
        "corpusId": 8884845,
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "url": "https://www.semanticscholar.org/paper/a4b828609b60b06e61bea7a4029cc9e1cad5df87",
        "title": "Statistical Phrase-Based Translation",
        "abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2003,
        "referenceCount": 15,
        "citationCount": 3830,
        "influentialCitationCount": 597,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1073445.1073462",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The empirical results suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translation."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2003-05-27",
        "journal": {
            "name": "",
            "pages": "48-54",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Koehn2003StatisticalPT,\n author = {Philipp Koehn and F. Och and Daniel Marcu},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {48-54},\n title = {Statistical Phrase-Based Translation},\n year = {2003}\n}\n"
        }
    },
    "554_equidock": {
        "paperId": "b190df7856f71856cbd98e1394c6513df279d53e",
        "externalIds": {
            "DBLP": "journals/corr/abs-2111-07786",
            "ArXiv": "2111.07786",
            "CorpusId": 244117004
        },
        "corpusId": 244117004,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/b190df7856f71856cbd98e1394c6513df279d53e",
        "title": "Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking",
        "abstract": "Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational change within the proteins happens during binding. We design a novel pairwise-independent SE(3)-equivariant graph matching network to predict the rotation and translation to place one of the proteins at the right docked position relative to the second protein. We mathematically guarantee a basic principle: the predicted complex is always identical regardless of the initial locations and orientations of the two structures. Our model, named EquiDock, approximates the binding pockets and predicts the docking poses using keypoint matching and alignment, achieved through optimal transport and a differentiable Kabsch algorithm. Empirically, we achieve significant running time improvements and often outperform existing docking software despite not relying on heavy candidate sampling, structure refinement, or templates.",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "referenceCount": 82,
        "citationCount": 107,
        "influentialCitationCount": 12,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel pairwise-independent SE(3)-equivariant graph matching network is designed to predict the rotation and translation to place one of the proteins at the right docked position relative to the second protein."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-11-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2111.07786"
        },
        "citationStyles": {
            "bibtex": "@Article{Ganea2021IndependentSM,\n author = {O. Ganea and Xinyuan Huang and Charlotte Bunne and Yatao Bian and R. Barzilay and T. Jaakkola and A. Krause},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking},\n volume = {abs/2111.07786},\n year = {2021}\n}\n"
        }
    },
    "556_fisher_kernel_gmm": {
        "paperId": "23694b6d61668e62bb11f17c1d75dde3b4951948",
        "externalIds": {
            "MAG": "2147238549",
            "DBLP": "conf/cvpr/PerronninD07",
            "DOI": "10.1109/CVPR.2007.383266",
            "CorpusId": 12795415
        },
        "corpusId": 12795415,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/23694b6d61668e62bb11f17c1d75dde3b4951948",
        "title": "Fisher Kernels on Visual Vocabularies for Image Categorization",
        "abstract": "Within the field of pattern classification, the Fisher kernel is a powerful framework which combines the strengths of generative and discriminative approaches. The idea is to characterize a signal with a gradient vector derived from a generative probability model and to subsequently feed this representation to a discriminative classifier. We propose to apply this framework to image categorization where the input signals are images and where the underlying generative model is a visual vocabulary: a Gaussian mixture model which approximates the distribution of low-level features in images. We show that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms. Our approach demonstrates excellent performance on two challenging databases: an in-house database of 19 object/scene categories and the recently released VOC 2006 database. It is also very practical: it has low computational needs both at training and test time and vocabularies trained on one set of categories can be applied to another set without any significant loss in performance.",
        "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition",
        "year": 2007,
        "referenceCount": 21,
        "citationCount": 1689,
        "influentialCitationCount": 176,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work shows that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms, and proposes to apply this framework to image categorization where the input signals are images and where the underlying generative model is a visual vocabulary: a Gaussian mixture model which approximates the distribution of low-level features in images."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2007-06-17",
        "journal": {
            "name": "2007 IEEE Conference on Computer Vision and Pattern Recognition",
            "pages": "1-8"
        },
        "citationStyles": {
            "bibtex": "@Article{Perronnin2007FisherKO,\n author = {Florent Perronnin and C. Dance},\n booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2007 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1-8},\n title = {Fisher Kernels on Visual Vocabularies for Image Categorization},\n year = {2007}\n}\n"
        }
    },
    "557_rnn+weight_noise+dynamic_eval": {
        "paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17",
        "externalIds": {
            "DBLP": "journals/corr/Graves13",
            "ArXiv": "1308.0850",
            "MAG": "1810943226",
            "CorpusId": 1697424
        },
        "corpusId": 1697424,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17",
        "title": "Generating Sequences With Recurrent Neural Networks",
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",
        "venue": "arXiv.org",
        "year": 2013,
        "referenceCount": 38,
        "citationCount": 3728,
        "influentialCitationCount": 329,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2013-08-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1308.0850"
        },
        "citationStyles": {
            "bibtex": "@Article{Graves2013GeneratingSW,\n author = {Alex Graves},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Generating Sequences With Recurrent Neural Networks},\n volume = {abs/1308.0850},\n year = {2013}\n}\n"
        }
    },
    "558_phenaki": {
        "paperId": "aa509ec67f311cd09d109356f7fa37a40072aabb",
        "externalIds": {
            "DBLP": "journals/corr/abs-2210-02399",
            "ArXiv": "2210.02399",
            "DOI": "10.48550/arXiv.2210.02399",
            "CorpusId": 252715594
        },
        "corpusId": 252715594,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/aa509ec67f311cd09d109356f7fa37a40072aabb",
        "title": "Phenaki: Variable Length Video Generation From Open Domain Textual Description",
        "abstract": "We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the per-frame baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 62,
        "citationCount": 190,
        "influentialCitationCount": 19,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.02399",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts, and demonstrates how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.02399"
        },
        "citationStyles": {
            "bibtex": "@Article{Villegas2022PhenakiVL,\n author = {Ruben Villegas and M. Babaeizadeh and Pieter-Jan Kindermans and Hernan Moraldo and Han Zhang and M. Saffar and Santiago Castro and Julius Kunze and D. Erhan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Phenaki: Variable Length Video Generation From Open Domain Textual Description},\n volume = {abs/2210.02399},\n year = {2022}\n}\n"
        }
    },
    "559_parti": {
        "paperId": "1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe",
        "externalIds": {
            "DBLP": "journals/tmlr/YuXKLBWVKYAHHPLZBW22",
            "ArXiv": "2206.10789",
            "DOI": "10.48550/arXiv.2206.10789",
            "CorpusId": 249926846
        },
        "corpusId": 249926846,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe",
        "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
        "abstract": "We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",
        "venue": "Trans. Mach. Learn. Res.",
        "year": 2022,
        "referenceCount": 115,
        "citationCount": 612,
        "influentialCitationCount": 64,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2206.10789",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Pathways Autoregressive Text-to-Image (Parti) model is presented, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge and explores and highlights limitations of the models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-06-22",
        "journal": {
            "name": "Trans. Mach. Learn. Res.",
            "volume": "2022"
        },
        "citationStyles": {
            "bibtex": "@Article{Yu2022ScalingAM,\n author = {Jiahui Yu and Yuanzhong Xu and Jing Yu Koh and Thang Luong and Gunjan Baid and Zirui Wang and Vijay Vasudevan and Alexander Ku and Yinfei Yang and Burcu Karagol Ayan and B. Hutchinson and Wei Han and Zarana Parekh and Xin Li and Han Zhang and Jason Baldridge and Yonghui Wu},\n booktitle = {Trans. Mach. Learn. Res.},\n journal = {Trans. Mach. Learn. Res.},\n title = {Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},\n volume = {2022},\n year = {2022}\n}\n"
        }
    },
    "560_awd-lstm_+_mos_+_partial_shuffled": {
        "paperId": "e84d754564c9e2ce993596370e0a1493c9c6e4b1",
        "externalIds": {
            "MAG": "2950000469",
            "ArXiv": "1906.03805",
            "DBLP": "conf/icml/WangG019",
            "CorpusId": 174800362
        },
        "corpusId": 174800362,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/e84d754564c9e2ce993596370e0a1493c9c6e4b1",
        "title": "Improving Neural Language Modeling via Adversarial Training",
        "abstract": "Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "referenceCount": 65,
        "citationCount": 100,
        "influentialCitationCount": 15,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simple yet highly effective adversarial training mechanism for regularizing neural language models by introducing adversarial noise to the output embedding layer while training the models, allowing for a simple and time efficient algorithm."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-05-24",
        "journal": {
            "pages": "6555-6565"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2019ImprovingNL,\n author = {Dilin Wang and Chengyue Gong and Qiang Liu},\n booktitle = {International Conference on Machine Learning},\n pages = {6555-6565},\n title = {Improving Neural Language Modeling via Adversarial Training},\n year = {2019}\n}\n"
        }
    },
    "561_ablang": {
        "paperId": "95a3237bfbde1d312e943ab965351537c46643f7",
        "externalIds": {
            "PubMedCentral": "9710568",
            "DOI": "10.1093/bioadv/vbac046",
            "CorpusId": 246226399,
            "PubMed": "36699403"
        },
        "corpusId": 246226399,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/95a3237bfbde1d312e943ab965351537c46643f7",
        "title": "AbLang: an antibody language model for completing antibody sequences",
        "abstract": "Motivation General protein language models have been shown to summarise the semantics of protein sequences into representations that are useful for state-of-the-art predictive methods. However, for antibody specific problems, such as restoring residues lost due to sequencing errors, a model trained solely on antibodies may be more powerful. Antibodies are one of the few protein types where the volume of sequence data needed for such language models is available, for example in the Observed Antibody Space (OAS) database. Results Here, we introduce AbLang, a language model trained on the antibody sequences in the OAS database. We demonstrate the power of AbLang by using it to restore missing residues in antibody sequence data, a key issue with B-cell receptor repertoire sequencing, over 40% of OAS sequences are missing the first 15 amino acids. AbLang restores the missing residues of antibody sequences better than using IMGT germlines or the general protein language model ESM-1b. Further, AbLang does not require knowledge of the germline of the antibody and is seven times faster than ESM-1b. Availability and Implementation AbLang is a python package available at https://github.com/oxpig/AbLang.",
        "venue": "bioRxiv",
        "year": 2022,
        "referenceCount": 17,
        "citationCount": 65,
        "influentialCitationCount": 5,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://academic.oup.com/bioinformaticsadvances/article-pdf/2/1/vbac046/44395223/vbac046.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Biology",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The power of AbLang is demonstrated by using it to restore missing residues in antibody sequence data, a key issue with B-cell receptor repertoire sequencing, over 40% of OAS sequences are missing the first 15 amino acids."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-01-22",
        "journal": {
            "name": "Bioinformatics Advances",
            "volume": "2"
        },
        "citationStyles": {
            "bibtex": "@Article{Olsen2022AbLangAA,\n author = {T. H. Olsen and I. Moal and C. Deane},\n booktitle = {bioRxiv},\n journal = {Bioinformatics Advances},\n title = {AbLang: an antibody language model for completing antibody sequences},\n volume = {2},\n year = {2022}\n}\n"
        }
    },
    "562_dall\u00b7e_3": {
        "paperId": "cfee1826dd4743eab44c6e27a0cc5970effa4d80",
        "externalIds": {
            "CorpusId": 264403242
        },
        "corpusId": 264403242,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/cfee1826dd4743eab44c6e27a0cc5970effa4d80",
        "title": "Improving Image Generation with Better Captions",
        "abstract": "We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions. Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. We address this by training a bespoke image captioner and use it to recaption the training dataset. We then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, we use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.",
        "venue": "",
        "year": null,
        "referenceCount": 32,
        "citationCount": 136,
        "influentialCitationCount": 16,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": null,
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions, and is benchmarked on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": null,
        "citationStyles": {
            "bibtex": "@Misc{None,\n author = {James Betker and Gabriel Goh and Li Jing and \u2020. TimBrooks and Jianfeng Wang and Linjie Li and \u2020. LongOuyang and \u2020. JuntangZhuang and \u2020. JoyceLee and \u2020. YufeiGuo and \u2020. WesamManassra and \u2020. PrafullaDhariwal and \u2020. CaseyChu and \u2020. YunxinJiao and Aditya Ramesh},\n title = {Improving Image Generation with Better Captions}\n}\n"
        }
    },
    "565_enhanced_neighborhood-based_filtering": {
        "paperId": "17296302ffb402ba3ec4b49a883224c7ceaa1ae7",
        "externalIds": {
            "DBLP": "conf/icdm/BellK07",
            "MAG": "2172249709",
            "DOI": "10.1109/ICDM.2007.90",
            "CorpusId": 206784101
        },
        "corpusId": 206784101,
        "publicationVenue": {
            "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
            "name": "Industrial Conference on Data Mining",
            "type": "conference",
            "alternate_names": [
                "Ind Conf Data Min",
                "ICDM"
            ],
            "url": "http://www.data-mining-forum.de/"
        },
        "url": "https://www.semanticscholar.org/paper/17296302ffb402ba3ec4b49a883224c7ceaa1ae7",
        "title": "Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights",
        "abstract": "Recommender systems based on collaborative filtering predict user preferences for products or services by learning past user-item relationships. A predominant approach to collaborative filtering is neighborhood based (\"k-nearest neighbors\"), where a user-item preference rating is interpolated from ratings of similar items and/or users. We enhance the neighborhood-based approach leading to substantial improvement of prediction accuracy, without a meaningful increase in running time. First, we remove certain so-called \"global effects\" from the data to make the ratings more comparable, thereby improving interpolation accuracy. Second, we show how to simultaneously derive interpolation weights for all nearest neighbors, unlike previous approaches where each weight is computed separately. By globally solving a suitable optimization problem, this simultaneous interpolation accounts for the many interactions between neighbors leading to improved accuracy. Our method is very fast in practice, generating a prediction in about 0.2 milliseconds. Importantly, it does not require training many parameters or a lengthy preprocessing, making it very practical for large scale applications. Finally, we show how to apply these methods to the perceivably much slower user-oriented approach. To this end, we suggest a novel scheme for low dimensional embedding of the users. We evaluate these methods on the netflix dataset, where they deliver significantly better results than the commercial netflix cinematch recommender system.",
        "venue": "Industrial Conference on Data Mining",
        "year": 2007,
        "referenceCount": 15,
        "citationCount": 566,
        "influentialCitationCount": 52,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work enhances the neighborhood-based approach leading to substantial improvement of prediction accuracy, without a meaningful increase in running time, and suggests a novel scheme for low dimensional embedding of the users."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2007-10-28",
        "journal": {
            "name": "Seventh IEEE International Conference on Data Mining (ICDM 2007)",
            "pages": "43-52"
        },
        "citationStyles": {
            "bibtex": "@Article{Bell2007ScalableCF,\n author = {Robert M. Bell and Y. Koren},\n booktitle = {Industrial Conference on Data Mining},\n journal = {Seventh IEEE International Conference on Data Mining (ICDM 2007)},\n pages = {43-52},\n title = {Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights},\n year = {2007}\n}\n"
        }
    },
    "566_gpt-4": {
        "paperId": "a8e1f42412639275fd59e7ac9b702787ab59016a",
        "externalIds": {
            "ArXiv": "2303.08774",
            "CorpusId": 257532815
        },
        "corpusId": 257532815,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/a8e1f42412639275fd59e7ac9b702787ab59016a",
        "title": "GPT-4 Technical Report",
        "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
        "venue": "",
        "year": 2023,
        "referenceCount": 0,
        "citationCount": 2888,
        "influentialCitationCount": 416,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs, is developed, a Transformer-based model pre-trained to predict the next token in a document which exhibits human-level performance on various professional and academic benchmarks."
        },
        "publicationTypes": null,
        "publicationDate": "2023-03-15",
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Achiam2023GPT4TR,\n author = {OpenAI Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and S. Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mo Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and B. Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and L. Fedus and Niko Felix and Sim'on Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Raphael Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and S. Gray and Ryan Greene and Joshua Gross and S. Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Lukasz Kaiser and Ali Kamali and I. Kanitscheider and N. Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and J. Kiros and Matthew Knight and Daniel Kokotajlo and Lukasz Kondraciuk and A. Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and J. Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and A. Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and S. McKinney and C. McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel P. Mossing and Tong Mu and Mira Murati and O. Murk and David M'ely and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Ouyang Long and Cullen O'Keefe and J. Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alexandre Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Pond\u00e9 de Oliveira Pinto and Michael Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and M. Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and T. Sherbakov and Jessica Shieh and S. Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin D. Sokolowsky and Yang Song and Natalie Staudacher and F. Such and Natalie Summers and I. Sutskever and Jie Tang and N. Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer'on Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and P. Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},\n title = {GPT-4 Technical Report},\n year = {2023}\n}\n"
        }
    },
    "567_dlrm-2020": {
        "paperId": "6e13e111e85d499d781386b182fd855fbb053771",
        "externalIds": {
            "DBLP": "journals/corr/abs-1906-00091",
            "ArXiv": "1906.00091",
            "MAG": "2947737663",
            "CorpusId": 173990641
        },
        "corpusId": 173990641,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/6e13e111e85d499d781386b182fd855fbb053771",
        "title": "Deep Learning Recommendation Model for Personalization and Recommendation Systems",
        "abstract": "With the advent of deep learning, neural network-based recommendation models have emerged as an important tool for tackling personalization and recommendation tasks. These networks differ significantly from other deep learning networks due to their need to handle categorical features and are not well studied or understood. In this paper, we develop a state-of-the-art deep learning recommendation model (DLRM) and provide its implementation in both PyTorch and Caffe2 frameworks. In addition, we design a specialized parallelization scheme utilizing model parallelism on the embedding tables to mitigate memory constraints while exploiting data parallelism to scale-out compute from the fully-connected layers. We compare DLRM against existing recommendation models and characterize its performance on the Big Basin AI platform, demonstrating its usefulness as a benchmark for future algorithmic experimentation and system co-design.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 30,
        "citationCount": 523,
        "influentialCitationCount": 108,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A state-of-the-art deep learning recommendation model (DLRM) is developed and its implementation in both PyTorch and Caffe2 frameworks is provided and a specialized parallelization scheme utilizing model parallelism on the embedding tables to mitigate memory constraints while exploiting data parallelism to scale-out compute from the fully-connected layers is designed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-05-31",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1906.00091"
        },
        "citationStyles": {
            "bibtex": "@Article{Naumov2019DeepLR,\n author = {M. Naumov and Dheevatsa Mudigere and H. Shi and Jianyu Huang and Narayanan Sundaraman and Jongsoo Park and Xiaodong Wang and Udit Gupta and Carole-Jean Wu and A. Azzolini and Dmytro Dzhulgakov and Andrey Mallevich and I. Cherniavskii and Yinghai Lu and Raghuraman Krishnamoorthi and Ansha Yu and Volodymyr Kondratenko and Stephanie Pereira and Xianjie Chen and Wenlin Chen and Vijay Rao and Bill Jia and Liang Xiong and M. Smelyanskiy},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Learning Recommendation Model for Personalization and Recommendation Systems},\n volume = {abs/1906.00091},\n year = {2019}\n}\n"
        }
    },
    "568_td(0)": {
        "paperId": "0f2d0e9c57d268fc1d05ce657eaf64eaaeb323c7",
        "externalIds": {
            "DBLP": "journals/iandc/Witten77",
            "MAG": "2054940200",
            "DOI": "10.1016/S0019-9958(77)90354-0",
            "CorpusId": 30145758
        },
        "corpusId": 30145758,
        "publicationVenue": {
            "id": "feadd011-44f6-4649-bc71-8c9a604eee42",
            "name": "Information and Control",
            "alternate_names": [
                "Inf Control"
            ],
            "issn": "0019-9958",
            "url": "https://www.sciencedirect.com/journal/information-and-control",
            "alternate_urls": [
                "http://www.sciencedirect.com/science/journal/00199958",
                "http://www.sciencedirect.com/science/journal/00199958/1/1"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/0f2d0e9c57d268fc1d05ce657eaf64eaaeb323c7",
        "title": "An Adaptive Optimal Controller for Discrete-Time Markov Environments",
        "abstract": null,
        "venue": "Information and Control",
        "year": 1977,
        "referenceCount": 7,
        "citationCount": 193,
        "influentialCitationCount": 10,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that, under certain conditions, the adaptive controller's actions eventually become optimal for the particular control task with which it is faced, in the sense that they maximize the expected reward obtained in the future."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1977-08-01",
        "journal": {
            "name": "Inf. Control.",
            "pages": "286-295",
            "volume": "34"
        },
        "citationStyles": {
            "bibtex": "@Article{Witten1977AnAO,\n author = {I. Witten},\n booktitle = {Information and Control},\n journal = {Inf. Control.},\n pages = {286-295},\n title = {An Adaptive Optimal Controller for Discrete-Time Markov Environments},\n volume = {34},\n year = {1977}\n}\n"
        }
    },
    "569_r-fcn": {
        "paperId": "b724c3f7ff395235b62537203ddeb710f0eb27bb",
        "externalIds": {
            "MAG": "2950800384",
            "DBLP": "journals/corr/DaiLHS16",
            "ArXiv": "1605.06409",
            "CorpusId": 7428689
        },
        "corpusId": 7428689,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/b724c3f7ff395235b62537203ddeb710f0eb27bb",
        "title": "R-FCN: Object Detection via Region-based Fully Convolutional Networks",
        "abstract": "We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 times faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "referenceCount": 29,
        "citationCount": 5120,
        "influentialCitationCount": 638,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents region-based, fully convolutional networks for accurate and efficient object detection, and proposes position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-05-20",
        "journal": {
            "pages": "379-387"
        },
        "citationStyles": {
            "bibtex": "@Article{Dai2016RFCNOD,\n author = {Jifeng Dai and Yi Li and Kaiming He and Jian Sun},\n booktitle = {Neural Information Processing Systems},\n pages = {379-387},\n title = {R-FCN: Object Detection via Region-based Fully Convolutional Networks},\n year = {2016}\n}\n"
        }
    },
    "570_cogview": {
        "paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa",
        "externalIds": {
            "DBLP": "conf/nips/DingYHZZYLZSYT21",
            "ArXiv": "2105.13290",
            "CorpusId": 235212350
        },
        "corpusId": 235212350,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa",
        "title": "CogView: Mastering Text-to-Image Generation via Transformers",
        "abstract": "Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 58,
        "citationCount": 461,
        "influentialCitationCount": 37,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-05-26",
        "journal": {
            "pages": "19822-19835"
        },
        "citationStyles": {
            "bibtex": "@Article{Ding2021CogViewMT,\n author = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},\n booktitle = {Neural Information Processing Systems},\n pages = {19822-19835},\n title = {CogView: Mastering Text-to-Image Generation via Transformers},\n year = {2021}\n}\n"
        }
    },
    "571_codet5+": {
        "paperId": "9ada8fa11b1cdece31f253acae50b62df8d5f823",
        "externalIds": {
            "DBLP": "journals/corr/abs-2305-07922",
            "ArXiv": "2305.07922",
            "DOI": "10.48550/arXiv.2305.07922",
            "CorpusId": 258685677
        },
        "corpusId": 258685677,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/9ada8fa11b1cdece31f253acae50b62df8d5f823",
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
        "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2023,
        "referenceCount": 71,
        "citationCount": 134,
        "influentialCitationCount": 19,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.07922",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks, and proposes a mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-05-13",
        "journal": {
            "pages": "1069-1088"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2023CodeT5OC,\n author = {Yue Wang and Hung Le and Akhilesh Deepak Gotmare and Nghi D. Q. Bui and Junnan Li and Steven C. H. Hoi},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1069-1088},\n title = {CodeT5+: Open Code Large Language Models for Code Understanding and Generation},\n year = {2023}\n}\n"
        }
    },
    "572_tpm-lvd": {
        "paperId": "af6e8ef9352ed2a82b168032c6d35655afc54f57",
        "externalIds": {
            "DBLP": "journals/corr/abs-2210-04398",
            "ArXiv": "2210.04398",
            "DOI": "10.48550/arXiv.2210.04398",
            "CorpusId": 252781163
        },
        "corpusId": 252781163,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/af6e8ef9352ed2a82b168032c6d35655afc54f57",
        "title": "Scaling Up Probabilistic Circuits by Latent Variable Distillation",
        "abstract": "Probabilistic Circuits (PCs) are a unified framework for tractable probabilistic models that support efficient computation of various probabilistic queries (e.g., marginal probabilities). One key challenge is to scale PCs to model large and high-dimensional real-world datasets: we observe that as the number of parameters in PCs increases, their performance immediately plateaus. This phenomenon suggests that the existing optimizers fail to exploit the full expressive power of large PCs. We propose to overcome such bottleneck by latent variable distillation: we leverage the less tractable but more expressive deep generative models to provide extra supervision over the latent variables of PCs. Specifically, we extract information from Transformer-based generative models to assign values to latent variables of PCs, providing guidance to PC optimizers. Experiments on both image and language modeling benchmarks (e.g., ImageNet and WikiText-2) show that latent variable distillation substantially boosts the performance of large PCs compared to their counterparts without latent variable distillation. In particular, on the image modeling benchmarks, PCs achieve competitive performance against some of the widely-used deep generative models, including variational autoencoders and flow-based models, opening up new avenues for tractable generative modeling.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 37,
        "citationCount": 14,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.04398",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work extracts information from Transformer-based generative models to assign values to latent variables of PCs, providing guidance to PC optimizers, and shows that latent variable distillation substantially boosts the performance of large PCs compared to their counterparts without latent variables distillation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.04398"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2022ScalingUP,\n author = {Anji Liu and Honghua Zhang and Guy Van den Broeck},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Scaling Up Probabilistic Circuits by Latent Variable Distillation},\n volume = {abs/2210.04398},\n year = {2022}\n}\n"
        }
    },
    "573_yolov3": {
        "paperId": "ebc96892b9bcbf007be9a1d7844e4b09fde9d961",
        "externalIds": {
            "ArXiv": "1804.02767",
            "DBLP": "journals/corr/abs-1804-02767",
            "MAG": "2796347433",
            "CorpusId": 4714433
        },
        "corpusId": 4714433,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/ebc96892b9bcbf007be9a1d7844e4b09fde9d961",
        "title": "YOLOv3: An Incremental Improvement",
        "abstract": "We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL",
        "venue": "arXiv.org",
        "year": 2018,
        "referenceCount": 20,
        "citationCount": 16850,
        "influentialCitationCount": 2370,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-04-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1804.02767"
        },
        "citationStyles": {
            "bibtex": "@Article{Redmon2018YOLOv3AI,\n author = {Joseph Redmon and Ali Farhadi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {YOLOv3: An Incremental Improvement},\n volume = {abs/1804.02767},\n year = {2018}\n}\n"
        }
    },
    "574_nlp_from_scratch": {
        "paperId": "bc1022b031dc6c7019696492e8116598097a8c12",
        "externalIds": {
            "MAG": "2158899491",
            "DBLP": "journals/jmlr/CollobertWBKKK11",
            "ArXiv": "1103.0398",
            "DOI": "10.5555/1953048.2078186",
            "CorpusId": 351666
        },
        "corpusId": 351666,
        "publicationVenue": {
            "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
            "name": "Journal of machine learning research",
            "type": "journal",
            "alternate_names": [
                "Journal of Machine Learning Research",
                "J mach learn res",
                "J Mach Learn Res"
            ],
            "issn": "1532-4435",
            "alternate_issns": [
                "1533-7928"
            ],
            "url": "http://www.ai.mit.edu/projects/jmlr/",
            "alternate_urls": [
                "http://jmlr.csail.mit.edu/",
                "http://www.jmlr.org/",
                "http://portal.acm.org/affiliated/jmlr"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/bc1022b031dc6c7019696492e8116598097a8c12",
        "title": "Natural Language Processing (Almost) from Scratch",
        "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",
        "venue": "Journal of machine learning research",
        "year": 2011,
        "referenceCount": 101,
        "citationCount": 7380,
        "influentialCitationCount": 643,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling is proposed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2011-02-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1103.0398"
        },
        "citationStyles": {
            "bibtex": "@Article{Collobert2011NaturalLP,\n author = {R. Collobert and J. Weston and L. Bottou and Michael Karlen and K. Kavukcuoglu and Pavel P. Kuksa},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {Natural Language Processing (Almost) from Scratch},\n volume = {abs/1103.0398},\n year = {2011}\n}\n"
        }
    },
    "575_semi-supervised_embedding_for_dl": {
        "paperId": "7ee368e60d0b826e78f965aad8d6c7d406127104",
        "externalIds": {
            "MAG": "2407712691",
            "DBLP": "conf/icml/WestonRC08",
            "DOI": "10.1145/1390156.1390303",
            "CorpusId": 740114
        },
        "corpusId": 740114,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7ee368e60d0b826e78f965aad8d6c7d406127104",
        "title": "Deep learning via semi-supervised embedding",
        "abstract": "We show how nonlinear embedding algorithms popular for use with shallow semi-supervised learning techniques such as kernel methods can be applied to deep multilayer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.",
        "venue": "International Conference on Machine Learning",
        "year": 2008,
        "referenceCount": 33,
        "citationCount": 1014,
        "influentialCitationCount": 102,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://infoscience.epfl.ch/record/192705/files/Weston_SPRINGER_2012.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown how nonlinear embedding algorithms popular for use with shallow semi-supervised learning techniques such as kernel methods can be applied to deep multilayer architectures, either as a regularizer at the output layer, or on each layer of the architecture."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2008-07-05",
        "journal": {
            "pages": "639-655"
        },
        "citationStyles": {
            "bibtex": "@Article{Weston2008DeepLV,\n author = {J. Weston and F. Ratle and H. Mobahi and R. Collobert},\n booktitle = {International Conference on Machine Learning},\n pages = {639-655},\n title = {Deep learning via semi-supervised embedding},\n year = {2008}\n}\n"
        }
    },
    "576_universal_approximation_via_feedforward_networks": {
        "paperId": "f22f6972e66bdd2e769fa64b0df0a13063c0c101",
        "externalIds": {
            "DBLP": "journals/nn/HornikSW89",
            "MAG": "2137983211",
            "DOI": "10.1016/0893-6080(89)90020-8",
            "CorpusId": 2757547
        },
        "corpusId": 2757547,
        "publicationVenue": {
            "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
            "name": "Neural Networks",
            "type": "journal",
            "alternate_names": [
                "Neural Netw"
            ],
            "issn": "0893-6080",
            "url": "http://www.elsevier.com/locate/neunet",
            "alternate_urls": [
                "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                "http://www.sciencedirect.com/science/journal/08936080"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/f22f6972e66bdd2e769fa64b0df0a13063c0c101",
        "title": "Multilayer feedforward networks are universal approximators",
        "abstract": null,
        "venue": "Neural Networks",
        "year": 1989,
        "referenceCount": 25,
        "citationCount": 20156,
        "influentialCitationCount": 485,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is rigorously established that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1989-07-01",
        "journal": {
            "name": "Neural Networks",
            "pages": "359-366",
            "volume": "2"
        },
        "citationStyles": {
            "bibtex": "@Article{Hornik1989MultilayerFN,\n author = {K. Hornik and M. Stinchcombe and H. White},\n booktitle = {Neural Networks},\n journal = {Neural Networks},\n pages = {359-366},\n title = {Multilayer feedforward networks are universal approximators},\n volume = {2},\n year = {1989}\n}\n"
        }
    },
    "578_transformer_elmo": {
        "paperId": "ac11062f1f368d97f4c826c317bf50dcc13fdb59",
        "externalIds": {
            "DBLP": "conf/emnlp/PetersNZY18",
            "ArXiv": "1808.08949",
            "MAG": "2950405925",
            "ACL": "D18-1179",
            "DOI": "10.18653/v1/D18-1179",
            "CorpusId": 52098907
        },
        "corpusId": 52098907,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/ac11062f1f368d97f4c826c317bf50dcc13fdb59",
        "title": "Dissecting Contextual Word Embeddings: Architecture and Representation",
        "abstract": "Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2018,
        "referenceCount": 56,
        "citationCount": 354,
        "influentialCitationCount": 68,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D18-1179.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "There is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks, suggesting that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-08-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1808.08949"
        },
        "citationStyles": {
            "bibtex": "@Article{Peters2018DissectingCW,\n author = {Matthew E. Peters and Mark Neumann and Luke Zettlemoyer and Wen-tau Yih},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Dissecting Contextual Word Embeddings: Architecture and Representation},\n volume = {abs/1808.08949},\n year = {2018}\n}\n"
        }
    },
    "580_semexp": {
        "paperId": "62516303058a1322450b58e4cd778ab873b5e531",
        "externalIds": {
            "MAG": "3040041096",
            "DBLP": "journals/corr/abs-2007-00643",
            "ArXiv": "2007.00643",
            "CorpusId": 220280504
        },
        "corpusId": 220280504,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/62516303058a1322450b58e4cd778ab873b5e531",
        "title": "Object Goal Navigation using Goal-Oriented Semantic Exploration",
        "abstract": "This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "referenceCount": 52,
        "citationCount": 331,
        "influentialCitationCount": 61,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category and outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map- based methods."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-07-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2007.00643"
        },
        "citationStyles": {
            "bibtex": "@Article{Chaplot2020ObjectGN,\n author = {Devendra Singh Chaplot and Dhiraj Gandhi and A. Gupta and R. Salakhutdinov},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Object Goal Navigation using Goal-Oriented Semantic Exploration},\n volume = {abs/2007.00643},\n year = {2020}\n}\n"
        }
    },
    "581_rasor": {
        "paperId": "24c26cb896e79391b6fbbc99db37345081b9da28",
        "externalIds": {
            "DBLP": "conf/acl/LeeSKC20",
            "ArXiv": "2012.12624",
            "ACL": "2021.acl-long.518",
            "DOI": "10.18653/v1/2021.acl-long.518",
            "CorpusId": 229363636
        },
        "corpusId": 229363636,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/24c26cb896e79391b6fbbc99db37345081b9da28",
        "title": "Learning Dense Representations of Phrases at Scale",
        "abstract": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "referenceCount": 52,
        "citationCount": 92,
        "influentialCitationCount": 15,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.acl-long.518.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work shows for the first time that it can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA and proposes a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-12-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2012.12624"
        },
        "citationStyles": {
            "bibtex": "@Article{Lee2020LearningDR,\n author = {Jinhyuk Lee and Mujeen Sung and Jaewoo Kang and Danqi Chen},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Learning Dense Representations of Phrases at Scale},\n volume = {abs/2012.12624},\n year = {2020}\n}\n"
        }
    },
    "582_bart-large": {
        "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "externalIds": {
            "MAG": "2982399380",
            "DBLP": "conf/acl/LewisLGGMLSZ20",
            "ACL": "2020.acl-main.703",
            "ArXiv": "1910.13461",
            "DOI": "10.18653/v1/2020.acl-main.703",
            "CorpusId": 204960716
        },
        "corpusId": 204960716,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
        "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "referenceCount": 36,
        "citationCount": 7511,
        "influentialCitationCount": 1708,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.703.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "BART is presented, a denoising autoencoder for pretraining sequence-to-sequence models, which matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-10-29",
        "journal": {
            "pages": "7871-7880"
        },
        "citationStyles": {
            "bibtex": "@Article{Lewis2019BARTDS,\n author = {M. Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdel-rahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {7871-7880},\n title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},\n year = {2019}\n}\n"
        }
    },
    "583_table-gpt": {
        "paperId": "3f413dca2607d68301143770e599b59d461a569e",
        "externalIds": {
            "ArXiv": "2310.09263",
            "DBLP": "journals/corr/abs-2310-09263",
            "DOI": "10.48550/arXiv.2310.09263",
            "CorpusId": 264127877
        },
        "corpusId": 264127877,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/3f413dca2607d68301143770e599b59d461a569e",
        "title": "Table-GPT: Table-tuned GPT for Diverse Table Tasks",
        "abstract": "Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However, when probing language models using a range of basic table-understanding tasks, we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on \\emph{one-dimensional} natural-language texts, whereas relational tables are \\emph{two-dimensional} objects. In this work, we propose a new\"\\emph{table-tuning}\"paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, with the goal of enhancing language models' ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better \\emph{table-understanding} capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout unseen tasks, and (2) strong \\emph{generalizability}, in its ability to respond to diverse human instructions to perform new table-tasks, in a manner similar to GPT-3.5 and ChatGPT.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 71,
        "citationCount": 9,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.09263",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a new \"\\emph{table-tuning}\" paradigm, where language models like GPT-3.5 and ChatGPT are trained/fine-tune using diverse table-tasks synthesized from real tables as training data, with the goal of enhancing language models' ability to understand tables and perform table tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-10-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2310.09263"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2023TableGPTTG,\n author = {Peng Li and Yeye He and Dror Yashar and Weiwei Cui and Song Ge and Haidong Zhang and D. Fainman and Dongmei Zhang and Surajit Chaudhuri},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Table-GPT: Table-tuned GPT for Diverse Table Tasks},\n volume = {abs/2310.09263},\n year = {2023}\n}\n"
        }
    },
    "584_dall\u00b7e_2": {
        "paperId": "c57293882b2561e1ba03017902df9fc2f289dea2",
        "externalIds": {
            "ArXiv": "2204.06125",
            "DBLP": "journals/corr/abs-2204-06125",
            "DOI": "10.48550/arXiv.2204.06125",
            "CorpusId": 248097655
        },
        "corpusId": 248097655,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/c57293882b2561e1ba03017902df9fc2f289dea2",
        "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
        "abstract": "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 66,
        "citationCount": 3556,
        "influentialCitationCount": 346,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2204.06125",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity, and the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-04-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2204.06125"
        },
        "citationStyles": {
            "bibtex": "@Article{Ramesh2022HierarchicalTI,\n author = {A. Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},\n volume = {abs/2204.06125},\n year = {2022}\n}\n"
        }
    },
    "585_densely_connected_lstm_+_var._dropout": {
        "paperId": "24a3133c3b3c903bba407059c7a3193c370f34af",
        "externalIds": {
            "MAG": "2963839582",
            "DBLP": "conf/rep4nlp/GodinDN17",
            "ArXiv": "1707.06130",
            "ACL": "W17-2622",
            "DOI": "10.18653/v1/W17-2622",
            "CorpusId": 11744937
        },
        "corpusId": 11744937,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/24a3133c3b3c903bba407059c7a3193c370f34af",
        "title": "Improving Language Modeling using Densely Connected Recurrent Neural Networks",
        "abstract": "In this paper, we introduce the novel concept of densely connected layers into recurrent neural networks. We evaluate our proposed architecture on the Penn Treebank language modeling task. We show that we can obtain similar perplexity scores with six times fewer parameters compared to a standard stacked 2-layer LSTM model trained with dropout (Zaremba et al., 2014). In contrast with the current usage of skip connections, we show that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions.",
        "venue": "Rep4NLP@ACL",
        "year": 2017,
        "referenceCount": 15,
        "citationCount": 5,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/W17-2622.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The novel concept of densely connected layers into recurrent neural networks is introduced and it is shown that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-07-19",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1707.06130"
        },
        "citationStyles": {
            "bibtex": "@Article{Godin2017ImprovingLM,\n author = {Fr\u00e9deric Godin and J. Dambre and W. D. Neve},\n booktitle = {Rep4NLP@ACL},\n journal = {ArXiv},\n title = {Improving Language Modeling using Densely Connected Recurrent Neural Networks},\n volume = {abs/1707.06130},\n year = {2017}\n}\n"
        }
    },
    "586_a3c_ff_hs": {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "externalIds": {
            "DBLP": "journals/corr/MnihBMGLHSK16",
            "MAG": "2964043796",
            "ArXiv": "1602.01783",
            "CorpusId": 6875312
        },
        "corpusId": 6875312,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning",
        "abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "referenceCount": 43,
        "citationCount": 7531,
        "influentialCitationCount": 1178,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers and shows that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-02-04",
        "journal": {
            "pages": "1928-1937"
        },
        "citationStyles": {
            "bibtex": "@Article{Mnih2016AsynchronousMF,\n author = {Volodymyr Mnih and Adri\u00e0 Puigdom\u00e8nech Badia and Mehdi Mirza and Alex Graves and T. Lillicrap and Tim Harley and David Silver and K. Kavukcuoglu},\n booktitle = {International Conference on Machine Learning},\n pages = {1928-1937},\n title = {Asynchronous Methods for Deep Reinforcement Learning},\n year = {2016}\n}\n"
        }
    },
    "587_motion-driven_3d_feature_tracking": {
        "paperId": "6818668fb895d95861a2eb9673ddc3a41e27b3b3",
        "externalIds": {
            "DBLP": "conf/bmvc/HarrisS88",
            "MAG": "2111308925",
            "DOI": "10.5244/C.2.23",
            "CorpusId": 1694378
        },
        "corpusId": 1694378,
        "publicationVenue": {
            "id": "67ceef4f-3b48-4a6c-ac54-c29c57589d33",
            "name": "Alvey Vision Conference",
            "type": "conference",
            "alternate_names": [
                "AVC",
                "Alvey Vis Conf"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/6818668fb895d95861a2eb9673ddc3a41e27b3b3",
        "title": "A Combined Corner and Edge Detector",
        "abstract": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed.",
        "venue": "Alvey Vision Conference",
        "year": 1988,
        "referenceCount": 10,
        "citationCount": 14771,
        "influentialCitationCount": 848,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.bmva.org/bmvc/1988/avc-88-023.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The problem the authors are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "1-6"
        },
        "citationStyles": {
            "bibtex": "@Article{Harris1988ACC,\n author = {C. G. Harris and M. Stephens},\n booktitle = {Alvey Vision Conference},\n pages = {1-6},\n title = {A Combined Corner and Edge Detector},\n year = {1988}\n}\n"
        }
    },
    "588_fnetar_medium": {
        "paperId": "f09252262b7d45db4311ac29f517057004ee0ffc",
        "externalIds": {
            "ArXiv": "2107.10932",
            "DBLP": "journals/corr/abs-2107-10932",
            "CorpusId": 236318364
        },
        "corpusId": 236318364,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/f09252262b7d45db4311ac29f517057004ee0ffc",
        "title": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms",
        "abstract": "In this note we examine the autoregressive generalization of the FNet algorithm, in which self-attention layers from the standard Transformer architecture are substituted with a trivial sparse-uniformsampling procedure based on Fourier transforms. Using the Wikitext-103 benchmark, we demonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the task of causal language modelingcompared to a Transformer-XL baseline (24.2 ppl) with only half the number self-attention layers,thus providing further evidence for the superfluity of deep neural networks with heavily compoundedattention mechanisms. The autoregressive Fourier transform could likely be used for parameterreduction on most Transformer-based time-series prediction models.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 15,
        "citationCount": 2,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The autoregressive generalization of the FNet algorithm is examined, in which self-attention layers from the standard Transformer architecture are substituted with a trivial sparse-uniformsampling procedure based on Fourier transforms, providing further evidence for the superfluity of deep neural networks with heavily compounded attention mechanisms."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-07-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2107.10932"
        },
        "citationStyles": {
            "bibtex": "@Article{Lou2021FNetARMT,\n author = {Tim Lou and M. Park and M. Ramezanali and Vincent Tang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {FNetAR: Mixing Tokens with Autoregressive Fourier Transforms},\n volume = {abs/2107.10932},\n year = {2021}\n}\n"
        }
    },
    "589_fractional_max-pooling": {
        "paperId": "55dda8f230566867acbfaa7bdd08fd8c7b8721ed",
        "externalIds": {
            "DBLP": "journals/corr/Graham14a",
            "ArXiv": "1412.6071",
            "MAG": "2133319764",
            "CorpusId": 16452744
        },
        "corpusId": 16452744,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/55dda8f230566867acbfaa7bdd08fd8c7b8721ed",
        "title": "Fractional Max-Pooling",
        "abstract": "Convolutional networks almost always incorporate some form of spatial pooling, and very often it is max-pooling with = 2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor . The amazing by product of discarding 75% of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of max-pooling where is allowed to take non-integer values. Our version of maxpooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state of the art for CIFAR-100 without even using dropout.",
        "venue": "arXiv.org",
        "year": 2014,
        "referenceCount": 17,
        "citationCount": 497,
        "influentialCitationCount": 59,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The form of fractional max-pooling formulated is found to reduce overfitting on a variety of datasets: for instance, it improves on the state of the art for CIFAR-100 without even using dropout."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-12-18",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1412.6071"
        },
        "citationStyles": {
            "bibtex": "@Article{Graham2014FractionalM,\n author = {Benjamin Graham},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Fractional Max-Pooling},\n volume = {abs/1412.6071},\n year = {2014}\n}\n"
        }
    },
    "590_rt-trajectory": {
        "paperId": "1ba3adf8f2049e672c0b8786c18a1f2ffcd21fa0",
        "externalIds": {
            "ArXiv": "2311.01977",
            "DBLP": "journals/corr/abs-2311-01977",
            "DOI": "10.48550/arXiv.2311.01977",
            "CorpusId": 265018996
        },
        "corpusId": 265018996,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/1ba3adf8f2049e672c0b8786c18a1f2ffcd21fa0",
        "title": "RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches",
        "abstract": "Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies: they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 37,
        "citationCount": 8,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a policy conditioning method using rough trajectory sketches that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform, and finds that RT-Trajectory is able to perform a wider range of tasks when provided the same training data."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-11-03",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2311.01977"
        },
        "citationStyles": {
            "bibtex": "@Article{Gu2023RTTrajectoryRT,\n author = {Jiayuan Gu and Sean Kirmani and Paul Wohlhart and Yao Lu and Montse Gonzalez Arenas and Kanishka Rao and Wenhao Yu and Chuyuan Fu and K. Gopalakrishnan and Zhuo Xu and Priya Sundaresan and Peng Xu and Hao Su and Karol Hausman and Chelsea Finn and Q. Vuong and Ted Xiao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches},\n volume = {abs/2311.01977},\n year = {2023}\n}\n"
        }
    },
    "591_dblstm": {
        "paperId": "1149888d75af4ed5dffc25731b875651c3ccdeb2",
        "externalIds": {
            "DBLP": "conf/asru/GravesJM13",
            "MAG": "2005708641",
            "DOI": "10.1109/ASRU.2013.6707742",
            "CorpusId": 3338763
        },
        "corpusId": 3338763,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/1149888d75af4ed5dffc25731b875651c3ccdeb2",
        "title": "Hybrid speech recognition with Deep Bidirectional LSTM",
        "abstract": "Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We find that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.",
        "venue": "2013 IEEE Workshop on Automatic Speech Recognition and Understanding",
        "year": 2013,
        "referenceCount": 22,
        "citationCount": 1526,
        "influentialCitationCount": 131,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates, and the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2013-12-01",
        "journal": {
            "name": "2013 IEEE Workshop on Automatic Speech Recognition and Understanding",
            "pages": "273-278"
        },
        "citationStyles": {
            "bibtex": "@Article{Graves2013HybridSR,\n author = {Alex Graves and N. Jaitly and Abdel-rahman Mohamed},\n booktitle = {2013 IEEE Workshop on Automatic Speech Recognition and Understanding},\n journal = {2013 IEEE Workshop on Automatic Speech Recognition and Understanding},\n pages = {273-278},\n title = {Hybrid speech recognition with Deep Bidirectional LSTM},\n year = {2013}\n}\n"
        }
    },
    "592_starcoder": {
        "paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e",
        "externalIds": {
            "DBLP": "journals/corr/abs-2305-06161",
            "ArXiv": "2305.06161",
            "DOI": "10.48550/arXiv.2305.06161",
            "CorpusId": 258588247
        },
        "corpusId": 258588247,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/3e4085e5869f1b7959707a1e1d7d273b6057eb4e",
        "title": "StarCoder: may the source be with you!",
        "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 113,
        "citationCount": 230,
        "influentialCitationCount": 31,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.06161",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work performs the most comprehensive evaluation of Code LLMs to date and shows that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-05-09",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2305.06161"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2023StarCoderMT,\n author = {Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and J. Lamy-Poirier and Jo\u00e3o Monteiro and Oleh Shliazhko and Nicolas Gontier and Nicholas Meade and Armel Zebaze and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Benjamin Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and J. Stillerman and S. Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and N. Fahmy and Urvashi Bhattacharyya and W. Yu and Swayam Singh and Sasha Luccioni and Paulo Villegas and M. Kunakov and Fedor Zhdanov and Manuel Romero and Tony Lee and Nadav Timor and Jennifer Ding and Claire Schlesinger and Hailey Schoelkopf and Jana Ebert and Tri Dao and Mayank Mishra and A. Gu and Jennifer Robinson and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Mu\u00f1oz Ferrandis and Sean M. Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and H. D. Vries},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {StarCoder: may the source be with you!},\n volume = {abs/2305.06161},\n year = {2023}\n}\n"
        }
    },
    "595_dcn+": {
        "paperId": "0342f5bf7f7b8f26ff4380846f9e577ae6fdd88a",
        "externalIds": {
            "MAG": "2951447432",
            "ArXiv": "1711.00106",
            "DBLP": "conf/iclr/XiongZS18",
            "CorpusId": 21196492
        },
        "corpusId": 21196492,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/0342f5bf7f7b8f26ff4380846f9e577ae6fdd88a",
        "title": "DCN+: Mixed Objective and Deep Residual Coattention for Question Answering",
        "abstract": "Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning. The objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we improve dynamic coattention networks (DCN) with a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1.",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "referenceCount": 32,
        "citationCount": 100,
        "influentialCitationCount": 12,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a mixed objective that combines cross entropy loss with self-critical policy learning, and improves dynamic coattention networks (DCN) with a deep residual coatt attention encoder that is inspired by recent work in deep self-attention and residual networks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-10-31",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1711.00106"
        },
        "citationStyles": {
            "bibtex": "@Article{Xiong2017DCNMO,\n author = {Caiming Xiong and Victor Zhong and R. Socher},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {DCN+: Mixed Objective and Deep Residual Coattention for Question Answering},\n volume = {abs/1711.00106},\n year = {2017}\n}\n"
        }
    },
    "596_phi-1.5": {
        "paperId": "e26888285436bc7998e5c95102a9beb60144be5e",
        "externalIds": {
            "DBLP": "journals/corr/abs-2309-05463",
            "ArXiv": "2309.05463",
            "DOI": "10.48550/arXiv.2309.05463",
            "CorpusId": 261696657
        },
        "corpusId": 261696657,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/e26888285436bc7998e5c95102a9beb60144be5e",
        "title": "Textbooks Are All You Need II: phi-1.5 technical report",
        "abstract": "We continue the investigation into the power of smaller Transformer-based language models as initiated by \\textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality\"data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need\"approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \\textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step\"or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \\textbf{phi-1.5} to promote further research on these urgent topics.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 39,
        "citationCount": 79,
        "influentialCitationCount": 6,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.05463",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Education",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new 1.3 billion parameter model namedphi-1.5 is created, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-09-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2309.05463"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2023TextbooksAA,\n author = {Yuan-Fang Li and S\u00e9bastien Bubeck and Ronen Eldan and Allison Del Giorno and Suriya Gunasekar and Yin Tat Lee},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Textbooks Are All You Need II: phi-1.5 technical report},\n volume = {abs/2309.05463},\n year = {2023}\n}\n"
        }
    },
    "597_vgg19": {
        "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
        "externalIds": {
            "MAG": "2949429431",
            "ArXiv": "1409.1556",
            "DBLP": "journals/corr/SimonyanZ14a",
            "CorpusId": 14124313
        },
        "corpusId": 14124313,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108",
        "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "referenceCount": 43,
        "citationCount": 86491,
        "influentialCitationCount": 13402,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-09-04",
        "journal": {
            "name": "CoRR",
            "volume": "abs/1409.1556"
        },
        "citationStyles": {
            "bibtex": "@Article{Simonyan2014VeryDC,\n author = {K. Simonyan and Andrew Zisserman},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},\n volume = {abs/1409.1556},\n year = {2014}\n}\n"
        }
    },
    "598_diffractive_deep_neural_network": {
        "paperId": "5c7e5248d9eb7f373f10277410bf8506160907ea",
        "externalIds": {
            "MAG": "2798701005",
            "ArXiv": "1804.08711",
            "DBLP": "journals/corr/abs-1804-08711",
            "DOI": "10.1126/science.aat8084",
            "CorpusId": 13753997,
            "PubMed": "30049787"
        },
        "corpusId": 13753997,
        "publicationVenue": {
            "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
            "name": "Science",
            "type": "journal",
            "issn": "0193-4511",
            "alternate_issns": [
                "0036-8075"
            ],
            "url": "https://www.jstor.org/journal/science",
            "alternate_urls": [
                "https://www.sciencemag.org/",
                "http://www.sciencemag.org/",
                "http://www.jstor.org/journals/00368075.html",
                "http://www.sciencemag.org/archive/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/5c7e5248d9eb7f373f10277410bf8506160907ea",
        "title": "All-optical machine learning using diffractive deep neural networks",
        "abstract": "All-optical deep learning Deep learning uses multilayered artificial neural networks to learn digitally from large datasets. It then performs advanced identification and classification tasks. To date, these multilayered neural networks have been implemented on a computer. Lin et al. demonstrate all-optical machine learning that uses passive optical components that can be patterned and fabricated with 3D-printing. Their hardware approach comprises stacked layers of diffractive optical elements analogous to an artificial neural network that can be trained to execute complex functions at the speed of light. Science, this issue p. 1004 All-optical deep learning can be implemented with 3D-printed passive optical components. Deep learning has been transforming our ability to execute advanced inference tasks using computers. Here we introduce a physical mechanism to perform machine learning by demonstrating an all-optical diffractive deep neural network (D2NN) architecture that can implement various functions following the deep learning\u2013based design of passive diffractive layers that work collectively. We created 3D-printed D2NNs that implement classification of images of handwritten digits and fashion products, as well as the function of an imaging lens at a terahertz spectrum. Our all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can execute; will find applications in all-optical image analysis, feature detection, and object classification; and will also enable new camera designs and optical components that perform distinctive tasks using D2NNs.",
        "venue": "Science",
        "year": 2018,
        "referenceCount": 55,
        "citationCount": 1135,
        "influentialCitationCount": 53,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.science.org/cms/asset/e0345a34-e2a8-46ff-81d8-e68e47e223c6/pap.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine",
            "Physics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Physics",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            },
            {
                "category": "Physics",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "3D-printed D2NNs are created that implement classification of images of handwritten digits and fashion products, as well as the function of an imaging lens at a terahertz spectrum."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-04-14",
        "journal": {
            "name": "Science",
            "pages": "1004 - 1008",
            "volume": "361"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2018AllopticalML,\n author = {Xing Lin and Y. Rivenson and N. Yardimci and Muhammed Veli and Yilin Luo and M. Jarrahi and A. Ozcan},\n booktitle = {Science},\n journal = {Science},\n pages = {1004 - 1008},\n title = {All-optical machine learning using diffractive deep neural networks},\n volume = {361},\n year = {2018}\n}\n"
        }
    },
    "599_yuyan_11b": {
        "paperId": "ba68248488a92114d5136263d284e109395abb5e",
        "externalIds": {
            "ACL": "2022.naacl-industry.8",
            "DBLP": "conf/naacl/LiXDWLZLFMZ22",
            "ArXiv": "2104.12470",
            "DOI": "10.18653/v1/2022.naacl-industry.8",
            "CorpusId": 236511698
        },
        "corpusId": 236511698,
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "url": "https://www.semanticscholar.org/paper/ba68248488a92114d5136263d284e109395abb5e",
        "title": "Easy and Efficient Transformer: Scalable Inference Solution For Large NLP Model",
        "abstract": "Recently, large-scale transformer-based models have been proven to be effective over various tasks across many domains. Nevertheless, applying them in industrial production requires tedious and heavy works to reduce inference costs. To fill such a gap, we introduce a scalable inference solution: Easy and Efficient Transformer (EET), including a series of transformer inference optimization at the algorithm and implementation levels. First, we design highly optimized kernels for long inputs and large hidden sizes. Second, we propose a flexible CUDA memory manager to reduce the memory footprint when deploying a large model. Compared with the state-of-the-art transformer inference library (Faster Transformer v4.0), EET can achieve an average of 1.40-4.20x speedup on the transformer decoder layer with an A100 GPU.",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 28,
        "citationCount": 5,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.naacl-industry.8.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A scalable inference solution: Easy and Efficient Transformer (EET), including a series of transformer inference optimization at the algorithm and implementation levels, and a flexible CUDA memory manager to reduce the memory footprint when deploying a large model."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-04-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2104.12470"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2021EasyAE,\n author = {GongZheng Li and Yadong Xi and Jingzhen Ding and Duan Wang and Bai Liu and Changjie Fan and Xiaoxi Mao and Zeng Zhao},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Easy and Efficient Transformer: Scalable Inference Solution For Large NLP Model},\n volume = {abs/2104.12470},\n year = {2021}\n}\n"
        }
    },
    "600_fairseq_adaptive_inputs": {
        "paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "externalIds": {
            "ArXiv": "1904.01038",
            "MAG": "2952509486",
            "DBLP": "journals/corr/abs-1904-01038",
            "ACL": "N19-4009",
            "DOI": "10.18653/v1/N19-4009",
            "CorpusId": 91184134
        },
        "corpusId": 91184134,
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "url": "https://www.semanticscholar.org/paper/faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
        "abstract": "fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "referenceCount": 51,
        "citationCount": 2673,
        "influentialCitationCount": 244,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.01038",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks and supports distributed training across multiple GPUs and machines."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-04-01",
        "journal": {
            "pages": "48-53"
        },
        "citationStyles": {
            "bibtex": "@Article{Ott2019fairseqAF,\n author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {48-53},\n title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},\n year = {2019}\n}\n"
        }
    },
    "602_diffusion-gan": {
        "paperId": "9c3ceae3cf605f934cc5f04a44feae23b5252faa",
        "externalIds": {
            "DBLP": "journals/corr/abs-2206-02262",
            "ArXiv": "2206.02262",
            "DOI": "10.48550/arXiv.2206.02262",
            "CorpusId": 249395201
        },
        "corpusId": 249395201,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9c3ceae3cf605f934cc5f04a44feae23b5252faa",
        "title": "Diffusion-GAN: Training GANs with Diffusion",
        "abstract": "Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussian-mixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminator's feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator's timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 76,
        "citationCount": 91,
        "influentialCitationCount": 15,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2206.02262",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Diffusion-GAN is proposed, a novel GAN framework that leverages a forward diffusion chain to generate Gaussian-mixture distributed instance noise and can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-06-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2206.02262"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2022DiffusionGANTG,\n author = {Zhendong Wang and Huangjie Zheng and Pengcheng He and Weizhu Chen and Mingyuan Zhou},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Diffusion-GAN: Training GANs with Diffusion},\n volume = {abs/2206.02262},\n year = {2022}\n}\n"
        }
    },
    "603_faster_r-cnn": {
        "paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
        "externalIds": {
            "MAG": "2953106684",
            "ArXiv": "1506.01497",
            "DBLP": "journals/pami/RenHG017",
            "DOI": "10.1109/TPAMI.2016.2577031",
            "CorpusId": 10328909,
            "PubMed": "27295650"
        },
        "corpusId": 10328909,
        "publicationVenue": {
            "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
            "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Pattern Anal Mach Intell"
            ],
            "issn": "0162-8828",
            "url": "http://www.computer.org/tpami/",
            "alternate_urls": [
                "http://www.computer.org/portal/web/tpami",
                "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
        "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "year": 2015,
        "referenceCount": 47,
        "citationCount": 50532,
        "influentialCitationCount": 8423,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1506.01497",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2015-06-04",
        "journal": {
            "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "pages": "1137-1149",
            "volume": "39"
        },
        "citationStyles": {
            "bibtex": "@Article{Ren2015FasterRT,\n author = {Shaoqing Ren and Kaiming He and Ross B. Girshick and Jian Sun},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1137-1149},\n title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},\n volume = {39},\n year = {2015}\n}\n"
        }
    },
    "604_dropout_(mnist)": {
        "paperId": "0060745e006c5f14ec326904119dca19c6545e51",
        "externalIds": {
            "ArXiv": "1207.0580",
            "MAG": "1904365287",
            "DBLP": "journals/corr/abs-1207-0580",
            "CorpusId": 14832074
        },
        "corpusId": 14832074,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0060745e006c5f14ec326904119dca19c6545e51",
        "title": "Improving neural networks by preventing co-adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",
        "venue": "arXiv.org",
        "year": 2012,
        "referenceCount": 26,
        "citationCount": 7141,
        "influentialCitationCount": 641,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2012-07-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1207.0580"
        },
        "citationStyles": {
            "bibtex": "@Article{Hinton2012ImprovingNN,\n author = {Geoffrey E. Hinton and Nitish Srivastava and A. Krizhevsky and I. Sutskever and R. Salakhutdinov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Improving neural networks by preventing co-adaptation of feature detectors},\n volume = {abs/1207.0580},\n year = {2012}\n}\n"
        }
    },
    "606_code_llama-34b": {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "externalIds": {
            "DBLP": "journals/corr/abs-2308-12950",
            "ArXiv": "2308.12950",
            "DOI": "10.48550/arXiv.2308.12950",
            "CorpusId": 261100919
        },
        "corpusId": 261100919,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code",
        "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 92,
        "citationCount": 389,
        "influentialCitationCount": 62,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2308.12950",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-08-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2308.12950"
        },
        "citationStyles": {
            "bibtex": "@Article{Rozi\u00e8re2023CodeLO,\n author = {Baptiste Rozi\u00e8re and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Tan and Yossi Adi and Jingyu Liu and Tal Remez and J. Rapin and Artyom Kozhevnikov and I. Evtimov and Joanna Bitton and Manish P Bhatt and Cristian Cant\u00f3n Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre D'efossez and Jade Copet and F. Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Code Llama: Open Foundation Models for Code},\n volume = {abs/2308.12950},\n year = {2023}\n}\n"
        }
    },
    "607_mnemonic_reader": {
        "paperId": "e0222a1ae6874f7fff128c3da8769ab95963da04",
        "externalIds": {
            "MAG": "2962808855",
            "DBLP": "conf/ijcai/HuPHQW018",
            "DOI": "10.24963/ijcai.2018/570",
            "CorpusId": 13559921
        },
        "corpusId": 13559921,
        "publicationVenue": {
            "id": "67f7f831-711a-43c8-8785-1e09005359b5",
            "name": "International Joint Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "Int Jt Conf Artif Intell",
                "IJCAI"
            ],
            "url": "http://www.ijcai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/e0222a1ae6874f7fff128c3da8769ab95963da04",
        "title": "Reinforced Mnemonic Reader for Machine Reading Comprehension",
        "abstract": "In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture, so as to avoid the problems of attention redundancy and attention deficiency. Second, a new optimization approach, called dynamic-critical reinforcement learning, is introduced to extend the standard supervised method. It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms. Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets.",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2017,
        "referenceCount": 47,
        "citationCount": 206,
        "influentialCitationCount": 27,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2018/0570.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects: a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-05-08",
        "journal": {
            "pages": "4099-4106"
        },
        "citationStyles": {
            "bibtex": "@Article{Hu2017ReinforcedMR,\n author = {Minghao Hu and Yuxing Peng and Zhen Huang and Xipeng Qiu and Furu Wei and M. Zhou},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {4099-4106},\n title = {Reinforced Mnemonic Reader for Machine Reading Comprehension},\n year = {2017}\n}\n"
        }
    },
    "608_flan-t5_11b": {
        "paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "externalIds": {
            "DBLP": "journals/corr/abs-2210-11416",
            "ArXiv": "2210.11416",
            "DOI": "10.48550/arXiv.2210.11416",
            "CorpusId": 253018554
        },
        "corpusId": 253018554,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "title": "Scaling Instruction-Finetuned Language Models",
        "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 106,
        "citationCount": 1466,
        "influentialCitationCount": 210,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.11416",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups, and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation)."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.11416"
        },
        "citationStyles": {
            "bibtex": "@Article{Chung2022ScalingIL,\n author = {Hyung Won Chung and Le Hou and S. Longpre and Barret Zoph and Yi Tay and W. Fedus and Eric Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and S. Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Wei Yu and Vincent Zhao and Yanping Huang and Andrew M. Dai and Hongkun Yu and Slav Petrov and E. Chi and J. Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Scaling Instruction-Finetuned Language Models},\n volume = {abs/2210.11416},\n year = {2022}\n}\n"
        }
    },
    "609_inception_v3": {
        "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
        "externalIds": {
            "MAG": "2183341477",
            "DBLP": "conf/cvpr/SzegedyVISW16",
            "ArXiv": "1512.00567",
            "DOI": "10.1109/CVPR.2016.308",
            "CorpusId": 206593880
        },
        "corpusId": 206593880,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/23ffaa0fe06eae05817f527a47ac3291077f9e58",
        "title": "Rethinking the Inception Architecture for Computer Vision",
        "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "referenceCount": 24,
        "citationCount": 23119,
        "influentialCitationCount": 2919,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1512.00567",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work is exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-12-02",
        "journal": {
            "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "2818-2826"
        },
        "citationStyles": {
            "bibtex": "@Article{Szegedy2015RethinkingTI,\n author = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Z. Wojna},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2818-2826},\n title = {Rethinking the Inception Architecture for Computer Vision},\n year = {2015}\n}\n"
        }
    },
    "610_automated_wsd_via_wordnet": {
        "paperId": "1aac9a51700b4f548ed4d406d3987c8008876521",
        "externalIds": {
            "MAG": "2121147707",
            "ACL": "P04-1036",
            "DBLP": "conf/acl/McCarthyKWC04",
            "DOI": "10.3115/1218955.1218991",
            "CorpusId": 1044865
        },
        "corpusId": 1044865,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/1aac9a51700b4f548ed4d406d3987c8008876521",
        "title": "Finding Predominant Word Senses in Untagged Text",
        "abstract": "In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of hand-tagged data. Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL-2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domain-specific corpora.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2004,
        "referenceCount": 29,
        "citationCount": 368,
        "influentialCitationCount": 40,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://dl.acm.org/ft_gateway.cfm?id=1218991&type=pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically, and demonstrates that this method discovers appropriate predominant senses for words from two domain-specific corpora."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2004-07-21",
        "journal": {
            "pages": "279-286"
        },
        "citationStyles": {
            "bibtex": "@Article{McCarthy2004FindingPW,\n author = {Diana McCarthy and R. Koeling and Julie Weeds and John A. Carroll},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {279-286},\n title = {Finding Predominant Word Senses in Untagged Text},\n year = {2004}\n}\n"
        }
    },
    "611_decision_tree_(classification)": {
        "paperId": "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63",
        "externalIds": {
            "MAG": "2164598857",
            "DBLP": "conf/cvpr/ViolaJ01",
            "DOI": "10.1109/CVPR.2001.990517",
            "CorpusId": 2715202
        },
        "corpusId": 2715202,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63",
        "title": "Rapid object detection using a boosted cascade of simple features",
        "abstract": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.",
        "venue": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001",
        "year": 2001,
        "referenceCount": 16,
        "citationCount": 19232,
        "influentialCitationCount": 1878,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates and the introduction of a new image representation called the \"integral image\" which allows the features used by the detector to be computed very quickly."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2001-12-08",
        "journal": {
            "name": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001",
            "pages": "I-I",
            "volume": "1"
        },
        "citationStyles": {
            "bibtex": "@Article{Viola2001RapidOD,\n author = {Paul A. Viola and Michael J. Jones},\n booktitle = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},\n journal = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},\n pages = {I-I},\n title = {Rapid object detection using a boosted cascade of simple features},\n volume = {1},\n year = {2001}\n}\n"
        }
    },
    "612_solar-10.7b": {
        "paperId": "ab7d320cbae173aef86c31faa087780cba44551f",
        "externalIds": {
            "DBLP": "journals/corr/abs-2312-15166",
            "ArXiv": "2312.15166",
            "DOI": "10.48550/arXiv.2312.15166",
            "CorpusId": 266550918
        },
        "corpusId": 266550918,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/ab7d320cbae173aef86c31faa087780cba44551f",
        "title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling",
        "abstract": "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 51,
        "citationCount": 18,
        "influentialCitationCount": 8,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks, and presents DUS, a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-12-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2312.15166"
        },
        "citationStyles": {
            "bibtex": "@Article{Kim2023SOLAR1S,\n author = {Dahyun Kim and Chanjun Park and Sanghoon Kim and Wonsung Lee and Wonho Song and Yunsu Kim and Hyeonwoo Kim and Yungi Kim and Hyeonju Lee and Jihoo Kim and Changbae Ahn and Seonghoon Yang and Sukyung Lee and Hyunbyung Park and Gyoungjin Gim and Mikyoung Cha and Hwalsuk Lee and Sunghun Kim},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling},\n volume = {abs/2312.15166},\n year = {2023}\n}\n"
        }
    },
    "613_reinforce_in_stochastic_connectionism": {
        "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "externalIds": {
            "DBLP": "journals/ml/Williams92",
            "MAG": "2119717200",
            "DOI": "10.1007/BF00992696",
            "CorpusId": 2332513
        },
        "corpusId": 2332513,
        "publicationVenue": {
            "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
            "name": "Machine-mediated learning",
            "type": "journal",
            "alternate_names": [
                "Mach learn",
                "Machine Learning",
                "Mach Learn"
            ],
            "issn": "0732-6718",
            "alternate_issns": [
                "0885-6125"
            ],
            "url": "http://www.springer.com/computer/artificial/journal/10994",
            "alternate_urls": [
                "https://link.springer.com/journal/10994",
                "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614",
        "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
        "abstract": null,
        "venue": "Machine-mediated learning",
        "year": 1992,
        "referenceCount": 38,
        "citationCount": 7802,
        "influentialCitationCount": 1019,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/BF00992696.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units that are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reInforcement tasks, and they do this without explicitly computing gradient estimates."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1992-05-01",
        "journal": {
            "name": "Machine Learning",
            "pages": "229-256",
            "volume": "8"
        },
        "citationStyles": {
            "bibtex": "@Article{Williams1992SimpleSG,\n author = {Ronald J. Williams},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {229-256},\n title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},\n volume = {8},\n year = {1992}\n}\n"
        }
    },
    "615_en^2as_with_performance_reward": {
        "paperId": "93c07b1aaaaa617709024f3ee2654c91efde8260",
        "externalIds": {
            "ArXiv": "1907.09109",
            "DBLP": "journals/corr/abs-1907-09109",
            "MAG": "2963656711",
            "CorpusId": 198147833
        },
        "corpusId": 198147833,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/93c07b1aaaaa617709024f3ee2654c91efde8260",
        "title": "Efficient Novelty-Driven Neural Architecture Search",
        "abstract": "One-Shot Neural architecture search (NAS) attracts broad attention recently due to its capacity to reduce the computational hours through weight sharing. However, extensive experiments on several recent works show that there is no positive correlation between the validation accuracy with inherited weights from the supernet and the test accuracy after re-training for One-Shot NAS. Different from devising a controller to find the best performing architecture with inherited weights, this paper focuses on how to sample architectures to train the supernet to make it more predictive. A single-path supernet is adopted, where only a small part of weights are optimized in each step, to reduce the memory demand greatly. Furthermore, we abandon devising complicated reward based architecture sampling controller, and sample architectures to train supernet based on novelty search. An efficient novelty search method for NAS is devised in this paper, and extensive experiments demonstrate the effectiveness and efficiency of our novelty search based architecture sampling method. The best architecture obtained by our algorithm with the same search space achieves the state-of-the-art test error rate of 2.51\\% on CIFAR-10 with only 7.5 hours search time in a single GPU, and a validation perplexity of 60.02 and a test perplexity of 57.36 on PTB. We also transfer these search cell structures to larger datasets ImageNet and WikiText-2, respectively.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 33,
        "citationCount": 1,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An efficient novelty search method for NAS is devised in this paper, and extensive experiments demonstrate the effectiveness and efficiency of the novelty search based architecture sampling method."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-07-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1907.09109"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2019EfficientNN,\n author = {Miao Zhang and Huiqi Li and Shirui Pan and Taoping Liu and S. Su},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Efficient Novelty-Driven Neural Architecture Search},\n volume = {abs/1907.09109},\n year = {2019}\n}\n"
        }
    },
    "617_empirical_evaluation_of_deep_architectures": {
        "paperId": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
        "externalIds": {
            "MAG": "1994197834",
            "DBLP": "conf/icml/LarochelleECBB07",
            "DOI": "10.1145/1273496.1273556",
            "CorpusId": 14805281
        },
        "corpusId": 14805281,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
        "title": "An empirical evaluation of deep architectures on problems with many factors of variation",
        "abstract": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.",
        "venue": "International Conference on Machine Learning",
        "year": 2007,
        "referenceCount": 13,
        "citationCount": 1122,
        "influentialCitationCount": 109,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A series of experiments indicate that these models with deep architectures show promise in solving harder learning problems that exhibit many factors of variation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2007-06-20",
        "journal": {
            "pages": "473-480"
        },
        "citationStyles": {
            "bibtex": "@Article{Larochelle2007AnEE,\n author = {H. Larochelle and D. Erhan and Aaron C. Courville and J. Bergstra and Yoshua Bengio},\n booktitle = {International Conference on Machine Learning},\n pages = {473-480},\n title = {An empirical evaluation of deep architectures on problems with many factors of variation},\n year = {2007}\n}\n"
        }
    },
    "618_diabetic_retinopathy_detection_net": {
        "paperId": "5c45a5d05ac564adb67811eeb9d41d6460c70135",
        "externalIds": {
            "MAG": "2557738935",
            "DOI": "10.1001/jama.2016.17216",
            "CorpusId": 26657811,
            "PubMed": "27898976"
        },
        "corpusId": 26657811,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/5c45a5d05ac564adb67811eeb9d41d6460c70135",
        "title": "Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.",
        "abstract": "Importance\nDeep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.\n\n\nObjective\nTo apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.\n\n\nDesign and Setting\nA specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128\u202f175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.\n\n\nExposure\nDeep learning-trained algorithm.\n\n\nMain Outcomes and Measures\nThe sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.\n\n\nResults\nThe EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2% women; prevalence of RDR, 683/8878 fully gradable images [7.8%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6% women; prevalence of RDR, 254/1745 fully gradable images [14.6%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3% (95% CI, 87.5%-92.7%) and the specificity was 98.1% (95% CI, 97.8%-98.5%). For Messidor-2, the sensitivity was 87.0% (95% CI, 81.1%-91.0%) and the specificity was 98.5% (95% CI, 97.7%-99.1%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5% and specificity was 93.4% and for Messidor-2 the sensitivity was 96.1% and specificity was 93.9%.\n\n\nConclusions and Relevance\nIn this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.",
        "venue": "Journal of the American Medical Association (JAMA)",
        "year": 2016,
        "referenceCount": 28,
        "citationCount": 4994,
        "influentialCitationCount": 161,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://jamanetwork.com/journals/jama/articlepdf/2588763/joi160132.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy and diabetic macular edema in retinal fundus photographs from adults with diabetes."
        },
        "publicationTypes": [
            "Study",
            "JournalArticle"
        ],
        "publicationDate": "2016-12-13",
        "journal": {
            "name": "JAMA",
            "pages": "\n          2402-2410\n        ",
            "volume": "316 22"
        },
        "citationStyles": {
            "bibtex": "@Article{Gulshan2016DevelopmentAV,\n author = {Varun Gulshan and L. Peng and Marc Coram and Martin C. Stumpe and Derek J. Wu and Arunachalam Narayanaswamy and Subhashini Venugopalan and Kasumi Widner and T. Madams and Jorge A Cuadros and R. Kim and R. Raman and Philip Nelson and J. Mega and D. Webster},\n booktitle = {Journal of the American Medical Association (JAMA)},\n journal = {JAMA},\n pages = {\n          2402-2410\n        },\n title = {Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.},\n volume = {316 22},\n year = {2016}\n}\n"
        }
    },
    "619_deep_deterministic_policy_gradients": {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "externalIds": {
            "MAG": "2173248099",
            "ArXiv": "1509.02971",
            "DBLP": "journals/corr/LillicrapHPHETS15",
            "CorpusId": 16326763
        },
        "corpusId": 16326763,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning",
        "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "referenceCount": 34,
        "citationCount": 10768,
        "influentialCitationCount": 1938,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces, and demonstrates that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2015-09-09",
        "journal": {
            "name": "CoRR",
            "volume": "abs/1509.02971"
        },
        "citationStyles": {
            "bibtex": "@Article{Lillicrap2015ContinuousCW,\n author = {T. Lillicrap and Jonathan J. Hunt and A. Pritzel and N. Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Continuous control with deep reinforcement learning},\n volume = {abs/1509.02971},\n year = {2015}\n}\n"
        }
    },
    "620_bellkor_2008": {
        "paperId": "b31fdda107475278a46ea26f98e862cea1994b9d",
        "externalIds": {
            "MAG": "54392637",
            "CorpusId": 8456387
        },
        "corpusId": 8456387,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/b31fdda107475278a46ea26f98e862cea1994b9d",
        "title": "The BellKor 2008 Solution to the Netflix Prize",
        "abstract": "Our RMSE=0.8643 solution is a linear blend of over 100 results. Some of them are new to this year, whereas many others belong to the set that was reported a year ago in our 2007 Progress Prize report [3]. This report is structured accordingly. In Section 2 we detail methods new to this year. In general, our view is that those newer methods deliver a superior performance compared to the methods we used a year ago. Throughout the description of the methods, we highlight the specific predictors that participated in the final blended solution. Nonetheless, the older methods still play a role in the blend, and thus in Section 3 we list those methods repeated from a year ago. Finally, we conclude with general thoughts in Section 4.",
        "venue": "",
        "year": 2008,
        "referenceCount": 18,
        "citationCount": 160,
        "influentialCitationCount": 7,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Psychology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Psychology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Throughout the description of the methods, the specific predictors that participated in the final blended solution are highlighted and the view is that those newer methods deliver a superior performance compared to the methods the authors used a year ago."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Bell2008TheB2,\n author = {Robert M. Bell and Y. Koren and C. Volinsky},\n title = {The BellKor 2008 Solution to the Netflix Prize},\n year = {2008}\n}\n"
        }
    },
    "621_ernie_3.0_titan": {
        "paperId": "a3184d40d390793232c99c89b57b8f65c16320b2",
        "externalIds": {
            "ArXiv": "2112.12731",
            "DBLP": "journals/corr/abs-2112-12731",
            "CorpusId": 245425057
        },
        "corpusId": 245425057,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/a3184d40d390793232c99c89b57b8f65c16320b2",
        "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
        "abstract": "Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 104,
        "citationCount": 46,
        "influentialCitationCount": 14,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters is trained, which is the largest Chinese dense pre-trained model so far and outperforms the state-of-the-art models on 68 NLP datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-12-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2112.12731"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2021ERNIE3T,\n author = {Shuohuan Wang and Yu Sun and Yang Xiang and Zhihua Wu and Siyu Ding and Weibao Gong and Shi Feng and Junyuan Shang and Yanbin Zhao and Chao Pang and Jiaxiang Liu and Xuyi Chen and Yuxiang Lu and Weixin Liu and Xi Wang and Yangfan Bai and Qiuliang Chen and Li Zhao and Shiyong Li and Peng Sun and Dianhai Yu and Yanjun Ma and Hao Tian and Hua Wu and Tian Wu and Wei Zeng and Ge Li and Wen Gao and Haifeng Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation},\n volume = {abs/2112.12731},\n year = {2021}\n}\n"
        }
    },
    "622_kn5_lm_+_rnn_400_10_(wsj)": {
        "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
        "externalIds": {
            "MAG": "179875071",
            "DBLP": "conf/interspeech/MikolovKBCK10",
            "DOI": "10.21437/Interspeech.2010-343",
            "CorpusId": 17048224
        },
        "corpusId": 17048224,
        "publicationVenue": {
            "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
            "name": "Interspeech",
            "type": "conference",
            "alternate_names": [
                "Conf Int Speech Commun Assoc",
                "INTERSPEECH",
                "Conference of the International Speech Communication Association"
            ],
            "issn": "2308-457X",
            "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
            "alternate_urls": [
                "http://www.isca-speech.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/9819b600a828a57e1cde047bbe710d3446b30da5",
        "title": "Recurrent neural network based language model",
        "abstract": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition",
        "venue": "Interspeech",
        "year": 2010,
        "referenceCount": 17,
        "citationCount": 5608,
        "influentialCitationCount": 490,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "1045-1048"
        },
        "citationStyles": {
            "bibtex": "@Article{Mikolov2010RecurrentNN,\n author = {Tomas Mikolov and M. Karafi\u00e1t and L. Burget and J. \u010cernock\u00fd and S. Khudanpur},\n booktitle = {Interspeech},\n pages = {1045-1048},\n title = {Recurrent neural network based language model},\n year = {2010}\n}\n"
        }
    },
    "624_metnet": {
        "paperId": "088488af28a93fac590827e538a1ebc0cea26e6f",
        "externalIds": {
            "MAG": "3013229294",
            "DBLP": "journals/corr/abs-2003-12140",
            "ArXiv": "2003.12140",
            "CorpusId": 214693028
        },
        "corpusId": 214693028,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/088488af28a93fac590827e538a1ebc0cea26e6f",
        "title": "MetNet: A Neural Weather Model for Precipitation Forecasting",
        "abstract": "Weather forecasting is a long standing scientific challenge with direct social and economic impact. The task is suitable for deep neural networks due to vast amounts of continuously collected data and a rich spatial and temporal structure that presents long range dependencies. We introduce MetNet, a neural network that forecasts precipitation up to 8 hours into the future at the high spatial resolution of 1 km$^2$ and at the temporal resolution of 2 minutes with a latency in the order of seconds. MetNet takes as input radar and satellite data and forecast lead time and produces a probabilistic precipitation map. The architecture uses axial self-attention to aggregate the global context from a large input patch corresponding to a million square kilometers. We evaluate the performance of MetNet at various precipitation thresholds and find that MetNet outperforms Numerical Weather Prediction at forecasts of up to 7 to 8 hours on the scale of the continental United States.",
        "venue": "arXiv.org",
        "year": 2020,
        "referenceCount": 32,
        "citationCount": 213,
        "influentialCitationCount": 16,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Physics",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Physics",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Environmental Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces MetNet, a neural network that forecasts precipitation up to 8 hours into the future at the high spatial resolution of 1 km$^2$ and at the temporal resolution of 2 minutes with a latency in the order of seconds, and finds that MetNet outperforms Numerical Weather Prediction at forecasts of up to 7 to8 hours on the scale of the continental United States."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-03-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2003.12140"
        },
        "citationStyles": {
            "bibtex": "@Article{S\u00f8nderby2020MetNetAN,\n author = {C. S\u00f8nderby and L. Espeholt and J. Heek and Mostafa Dehghani and Avital Oliver and Tim Salimans and Shreya Agrawal and Jason Hickey and Nal Kalchbrenner},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {MetNet: A Neural Weather Model for Precipitation Forecasting},\n volume = {abs/2003.12140},\n year = {2020}\n}\n"
        }
    },
    "625_imagen_video": {
        "paperId": "498ac9b2e494601d20a3d0211c16acf2b7954a54",
        "externalIds": {
            "DBLP": "journals/corr/abs-2210-02303",
            "ArXiv": "2210.02303",
            "DOI": "10.48550/arXiv.2210.02303",
            "CorpusId": 252715883
        },
        "corpusId": 252715883,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/498ac9b2e494601d20a3d0211c16acf2b7954a54",
        "title": "Imagen Video: High Definition Video Generation with Diffusion Models",
        "abstract": "We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See https://imagen.research.google/video/ for samples.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 51,
        "citationCount": 615,
        "influentialCitationCount": 61,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.02303",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Imagen Video is found to be capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.02303"
        },
        "citationStyles": {
            "bibtex": "@Article{Ho2022ImagenVH,\n author = {Jonathan Ho and William Chan and Chitwan Saharia and Jay Whang and Ruiqi Gao and A. Gritsenko and Diederik P. Kingma and Ben Poole and Mohammad Norouzi and David J. Fleet and Tim Salimans},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Imagen Video: High Definition Video Generation with Diffusion Models},\n volume = {abs/2210.02303},\n year = {2022}\n}\n"
        }
    },
    "626_top-down_frozen_classifier": {
        "paperId": "c5be209482624dcf0e0de6d0b2d5a727d65351f9",
        "externalIds": {
            "DBLP": "journals/corr/abs-2102-04697",
            "ArXiv": "2102.04697",
            "DOI": "10.1109/ICASSP39728.2021.9413565",
            "CorpusId": 231855351
        },
        "corpusId": 231855351,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/c5be209482624dcf0e0de6d0b2d5a727d65351f9",
        "title": "Train Your Classifier First: Cascade Neural Networks Training from Upper Layers to Lower Layers",
        "abstract": "Although the lower layers of a deep neural network learn features which are transferable across datasets, these layers are not transferable within the same dataset. That is, in general, freezing the trained feature extractor (the lower layers) and retraining the classifier (the upper layers) on the same dataset leads to worse performance. In this paper, for the first time, we show that the frozen classifier is transferable within the same dataset. We develop a novel top-down training method which can be viewed as an algorithm for searching for high-quality classifiers. We tested this method on automatic speech recognition (ASR) tasks and language modelling tasks. The proposed method consistently improves recurrent neural network ASR models on Wall Street Journal, self-attention ASR models on Switchboard, and AWD-LSTM language models on WikiText-2.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2021,
        "referenceCount": 36,
        "citationCount": 2,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.pure.ed.ac.uk/ws/files/198503294/Train_Your_Classifier_ZHANG_DOA30012021_AFV.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper develops a novel top-down training method which can be viewed as an algorithm for searching for high-quality classifiers and consistently improves recurrent neural network ASR models on Wall Street Journal, self-attention ASR model on Switchboard, and AWD-LSTM language models on WikiText-2."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-02-09",
        "journal": {
            "name": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "pages": "2750-2754"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2021TrainYC,\n author = {Shucong Zhang and Cong-Thanh Do and R. Doddipatla and Erfan Loweimi and P. Bell and S. Renals},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {2750-2754},\n title = {Train Your Classifier First: Cascade Neural Networks Training from Upper Layers to Lower Layers},\n year = {2021}\n}\n"
        }
    },
    "627_flm-101b": {
        "paperId": "f8afa4bd5b05f52ffa304f56aed7a5792a42ef1f",
        "externalIds": {
            "ArXiv": "2309.03852",
            "DBLP": "journals/corr/abs-2309-03852",
            "DOI": "10.48550/arXiv.2309.03852",
            "CorpusId": 261582615
        },
        "corpusId": 261582615,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/f8afa4bd5b05f52ffa304f56aed7a5792a42ef1f",
        "title": "FLM-101B: An Open LLM and How to Train It with $100K Budget",
        "abstract": "Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks, among others. Despite these successes, two main challenges remain in developing LLMs: (i) high computational cost, and (ii) fair and objective evaluations. In this paper, we report a solution to significantly reduce LLM training cost through a growth strategy. We demonstrate that a 101B-parameter LLM with 0.31T tokens can be trained with a budget of 100K US dollars. Inspired by IQ tests, we also consolidate an additional range of evaluations on top of existing evaluations that focus on knowledge-oriented abilities. These IQ evaluations include symbolic mapping, rule understanding, pattern mining, and anti-interference. Such evaluations minimize the potential impact of memorization. Experimental results show that our model, named FLM-101B, trained with a budget of 100K US dollars, achieves performance comparable to powerful and well-known models, e.g., GPT-3 and GLM-130B, especially on the additional range of IQ evaluations. The checkpoint of FLM-101B is released at https://huggingface.co/CofeAI/FLM-101B.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 86,
        "citationCount": 6,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.03852",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated that a 101B-parameter LLM with 0.31T tokens can be trained with a budget of 100K US dollars, and Inspired by IQ tests, an additional range of evaluations are consolidated on top of existing evaluations that focus on knowledge-oriented abilities."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-09-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2309.03852"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2023FLM101BAO,\n author = {Xiang Li and Yiqun Yao and Xin Jiang and Xuezhi Fang and Xuying Meng and Siqi Fan and Peng Han and Jing Li and LI DU and Bowen Qin and Zheng Zhang and Aixin Sun and Yequan Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {FLM-101B: An Open LLM and How to Train It with $100K Budget},\n volume = {abs/2309.03852},\n year = {2023}\n}\n"
        }
    },
    "628_distilbert": {
        "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
        "externalIds": {
            "DBLP": "journals/corr/abs-1910-01108",
            "ArXiv": "1910.01108",
            "MAG": "2978017171",
            "CorpusId": 203626972
        },
        "corpusId": 203626972,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/a54b56af24bb4873ed0163b77df63b92bd018ddc",
        "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
        "abstract": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 27,
        "citationCount": 5097,
        "influentialCitationCount": 815,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can be fine-tuned with good performances on a wide range of tasks like its larger counterparts, and introduces a triple loss combining language modeling, distillation and cosine-distance losses."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-10-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1910.01108"
        },
        "citationStyles": {
            "bibtex": "@Article{Sanh2019DistilBERTAD,\n author = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n volume = {abs/1910.01108},\n year = {2019}\n}\n"
        }
    },
    "629_6-layer_mlp_(mnist)": {
        "paperId": "b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd",
        "externalIds": {
            "DBLP": "journals/corr/abs-1003-0358",
            "ArXiv": "1003.0358",
            "MAG": "2132424367",
            "DOI": "10.1162/NECO_a_00052",
            "CorpusId": 1918673,
            "PubMed": "20858131"
        },
        "corpusId": 1918673,
        "publicationVenue": {
            "id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
            "name": "Neural Computation",
            "type": "journal",
            "alternate_names": [
                "Neural Comput"
            ],
            "issn": "0899-7667",
            "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667",
            "alternate_urls": [
                "http://ieeexplore.ieee.org/servlet/opac?punumber=6720226",
                "http://www.mitpressjournals.org/loi/neco",
                "https://www.mitpressjournals.org/loi/neco"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd",
        "title": "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",
        "abstract": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.",
        "venue": "Neural Computation",
        "year": 2010,
        "referenceCount": 29,
        "citationCount": 961,
        "influentialCitationCount": 43,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1003.0358",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2010-03-01",
        "journal": {
            "name": "Neural Computation",
            "pages": "3207-3220",
            "volume": "22"
        },
        "citationStyles": {
            "bibtex": "@Article{Ciresan2010DeepBS,\n author = {D. Ciresan and U. Meier and L. Gambardella and J. Schmidhuber},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {3207-3220},\n title = {Deep, Big, Simple Neural Nets for Handwritten Digit Recognition},\n volume = {22},\n year = {2010}\n}\n"
        }
    },
    "631_transformer-xl_large_+_phrase_induction": {
        "paperId": "9ae4aa0ccb45c564af422d2bc419f141c74eefd8",
        "externalIds": {
            "MAG": "2948961293",
            "ACL": "P19-1144",
            "DBLP": "conf/acl/LuoJBG19",
            "ArXiv": "1906.01702",
            "DOI": "10.18653/v1/P19-1144",
            "CorpusId": 174799480
        },
        "corpusId": 174799480,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/9ae4aa0ccb45c564af422d2bc419f141c74eefd8",
        "title": "Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",
        "abstract": "Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "referenceCount": 34,
        "citationCount": 7,
        "influentialCitationCount": 3,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1144.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a method that improves language modeling by learning to align the given context and the following phrase by defining syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-06-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1906.01702"
        },
        "citationStyles": {
            "bibtex": "@Article{Luo2019ImprovingNL,\n author = {Hongyin Luo and Lan Jiang and Yonatan Belinkov and James R. Glass},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Improving Neural Language Models by Segmenting, Attending, and Predicting the Future},\n volume = {abs/1906.01702},\n year = {2019}\n}\n"
        }
    },
    "632_lstm": {
        "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
        "externalIds": {
            "MAG": "2064675550",
            "DBLP": "journals/neco/HochreiterS97",
            "DOI": "10.1162/neco.1997.9.8.1735",
            "CorpusId": 1915014,
            "PubMed": "9377276"
        },
        "corpusId": 1915014,
        "publicationVenue": {
            "id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
            "name": "Neural Computation",
            "type": "journal",
            "alternate_names": [
                "Neural Comput"
            ],
            "issn": "0899-7667",
            "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667",
            "alternate_urls": [
                "http://ieeexplore.ieee.org/servlet/opac?punumber=6720226",
                "http://www.mitpressjournals.org/loi/neco",
                "https://www.mitpressjournals.org/loi/neco"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/2e9d221c206e9503ceb452302d68d10e293f2a10",
        "title": "Long Short-Term Memory",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "venue": "Neural Computation",
        "year": 1997,
        "referenceCount": 48,
        "citationCount": 73808,
        "influentialCitationCount": 9196,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
        },
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "publicationDate": "1997-11-01",
        "journal": {
            "name": "Neural Computation",
            "pages": "1735-1780",
            "volume": "9"
        },
        "citationStyles": {
            "bibtex": "@Article{Hochreiter1997LongSM,\n author = {Sepp Hochreiter and J. Schmidhuber},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {1735-1780},\n title = {Long Short-Term Memory},\n volume = {9},\n year = {1997}\n}\n"
        }
    },
    "633_rnnlm_+_dynamic_kl_regularization_(wt2)": {
        "paperId": "6c5860bb9b2e2469e6cf9deb0ba2baf460871067",
        "externalIds": {
            "MAG": "2788031431",
            "DBLP": "conf/aaai/NorasetDD18",
            "DOI": "10.1609/aaai.v32i1.11993",
            "CorpusId": 13746544
        },
        "corpusId": 13746544,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/6c5860bb9b2e2469e6cf9deb0ba2baf460871067",
        "title": "Controlling Global Statistics in Recurrent Neural Network Text Generation",
        "abstract": "\n \n Recurrent neural network language models (RNNLMs) are an essential component for many language generation tasks such as machine translation, summarization, and automated conversation. Often, we would like to subject the text generated by the RNNLM to constraints, in order to overcome systemic errors (e.g. word repetition) or achieve application-specific goals (e.g. more positive sentiment). In this paper, we present a method for training RNNLMs to simultaneously optimize likelihood and follow a given set of statistical constraints on text generation.\u00a0 The problem is challenging because the statistical constraints are defined over aggregate model behavior, rather than model parameters, meaning that a straightforward parameter regularization approach is insufficient.\u00a0 We solve this problem using a dynamic regularizer that updates as training proceeds, based on the generative behavior of the RNNLMs.\u00a0 Our experiments show that the dynamic regularizer outperforms both generic training and a static regularization baseline.\u00a0 The approach is successful at improving word-level repetition statistics by a factor of four in RNNLMs on a definition modeling task.\u00a0 It also improves model perplexity when the statistical constraints are $n$-gram statistics taken from a large corpus.\n \n",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "referenceCount": 41,
        "citationCount": 6,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11993/11852",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A dynamic regularizer that updates as training proceeds, based on the generative behavior of the RNNLMs is presented, which is successful at improving word-level repetition statistics and improves model perplexity when the statistical constraints are $n$-gram statistics taken from a large corpus."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-04-27",
        "journal": {
            "pages": "5333-5341"
        },
        "citationStyles": {
            "bibtex": "@Article{Noraset2018ControllingGS,\n author = {Thanapon Noraset and David Demeter and Doug Downey},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {5333-5341},\n title = {Controlling Global Statistics in Recurrent Neural Network Text Generation},\n year = {2018}\n}\n"
        }
    },
    "634_scrn(structurally_constrained_recurrent_network)": {
        "paperId": "9665247ea3421929f9b6ad721f139f11edb1dbb8",
        "externalIds": {
            "MAG": "2118776487",
            "ArXiv": "1412.7753",
            "DBLP": "journals/corr/MikolovJCMR14",
            "CorpusId": 14715110
        },
        "corpusId": 14715110,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9665247ea3421929f9b6ad721f139f11edb1dbb8",
        "title": "Learning Longer Memory in Recurrent Neural Networks",
        "abstract": "Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming kind of a longer term memory. We evaluate our model in language modeling experiments, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997).",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "referenceCount": 32,
        "citationCount": 238,
        "influentialCitationCount": 27,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper shows that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent, by using a slight structural modification of the simple recurrent neural network architecture."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-12-24",
        "journal": {
            "name": "arXiv: Neural and Evolutionary Computing",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Mikolov2014LearningLM,\n author = {Tomas Mikolov and Armand Joulin and S. Chopra and Micha\u00ebl Mathieu and Marc'Aurelio Ranzato},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Neural and Evolutionary Computing},\n title = {Learning Longer Memory in Recurrent Neural Networks},\n year = {2014}\n}\n"
        }
    },
    "635_mt5-xxl": {
        "paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a",
        "externalIds": {
            "ArXiv": "2010.11934",
            "DBLP": "conf/naacl/XueCRKASBR21",
            "MAG": "3169483174",
            "ACL": "2021.naacl-main.41",
            "DOI": "10.18653/V1/2021.NAACL-MAIN.41",
            "CorpusId": 225040574
        },
        "corpusId": 225040574,
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "url": "https://www.semanticscholar.org/paper/74276a37bfa50f90dfae37f767b2b67784bd402a",
        "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
        "abstract": "The recent \u201cText-to-Text Transfer Transformer\u201d (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \u201caccidental translation\u201d in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2020,
        "referenceCount": 55,
        "citationCount": 1531,
        "influentialCitationCount": 189,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.naacl-main.41.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-10-22",
        "journal": {
            "pages": "483-498"
        },
        "citationStyles": {
            "bibtex": "@Article{Xue2020mT5AM,\n author = {Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and Aditya Barua and Colin Raffel},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {483-498},\n title = {mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer},\n year = {2020}\n}\n"
        }
    },
    "636_nucleotide_transformer": {
        "paperId": "b06e1a2c84fb3bff03b10283bc863f007f5483b6",
        "externalIds": {
            "DOI": "10.1101/2023.01.11.523679",
            "CorpusId": 255943445
        },
        "corpusId": 255943445,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/b06e1a2c84fb3bff03b10283bc863f007f5483b6",
        "title": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics",
        "abstract": "Closing the gap between measurable genetic information and observable traits is a longstand-ing challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, rang-ing from 50M up to 2.5B parameters and integrating information from 3,202 diverse human genomes, as well as 850 genomes selected across diverse phyla, including both model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the developed models can be fine-tuned at low cost and despite low available data regime to solve a variety of genomics applications. Despite no supervision, the transformer models learned to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model rep-resentations can improve the prioritization of functional genetic variants. The training and ap-plication of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence. Code and weights available at: https://github.com/instadeepai/nucleotide-transformer in Jax and https://huggingface.co/InstaDeepAI in Pytorch. Example notebooks to apply these models to any downstream task are available on HuggingFace.",
        "venue": "bioRxiv",
        "year": 2023,
        "referenceCount": 75,
        "citationCount": 51,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2023/03/09/2023.01.11.523679.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, ringing from 50M up to 2.5B parameters and integrating information from 3,202 diverse human genomes, as well as 850 genomes selected across diverse phyla, including both model and non-model organisms are presented."
        },
        "publicationTypes": null,
        "publicationDate": "2023-09-19",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Dalla-Torre2023TheNT,\n author = {Hugo Dalla-Torre and Liam Gonzalez and Javier Mendoza Revilla and Nicolas Lopez Carranza and Adam Henryk Grzywaczewski and Francesco Oteri and Christian Dallago and Evan Trop and Hassan Sirelkhatim and Guillaume Richard and Marcin J. Skwark and Karim Beguir and Marie Lopez and Thomas Pierrot},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics},\n year = {2023}\n}\n"
        }
    },
    "637_dimensionality_reduction": {
        "paperId": "7c59908c946a4157abc030cdbe2b63d08ba97db3",
        "externalIds": {
            "MAG": "2338600138",
            "CorpusId": 262637400
        },
        "corpusId": 262637400,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/7c59908c946a4157abc030cdbe2b63d08ba97db3",
        "title": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks",
        "abstract": null,
        "venue": "",
        "year": 2006,
        "referenceCount": 8,
        "citationCount": 7942,
        "influentialCitationCount": 464,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": null,
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Hinton2006SupportingOM,\n author = {Geoffrey E. Hinton and R. Salakhutdinov},\n title = {Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks},\n year = {2006}\n}\n"
        }
    },
    "638_squeezebert": {
        "paperId": "eb1602ecba96beadeb7d2f05e1b57fa6b339fc69",
        "externalIds": {
            "DBLP": "journals/corr/abs-2006-11316",
            "ArXiv": "2006.11316",
            "ACL": "2020.sustainlp-1.17",
            "MAG": "3036463250",
            "DOI": "10.18653/v1/2020.sustainlp-1.17",
            "CorpusId": 219966938
        },
        "corpusId": 219966938,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/eb1602ecba96beadeb7d2f05e1b57fa6b339fc69",
        "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?",
        "abstract": "Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant opportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. Toward this end, we consider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today\u2019s highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. To begin to address this problem, we draw inspiration from the computer vision community, where work such as MobileNet has demonstrated that grouped convolutions (e.g. depthwise convolutions) can enable speedups without sacrificing accuracy. We demonstrate how to replace several operations in self-attention layers with grouped convolutions, and we use this technique in a novel network architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set. A PyTorch-based implementation of SqueezeBERT is available as part of the Hugging Face Transformers library: https://huggingface.co/squeezebert",
        "venue": "SUSTAINLP",
        "year": 2020,
        "referenceCount": 105,
        "citationCount": 104,
        "influentialCitationCount": 11,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.sustainlp-1.17.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated how to replace several operations in self-attention layers with grouped convolutions, and this technique is used in a novel network architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-06-19",
        "journal": {
            "pages": "124-135"
        },
        "citationStyles": {
            "bibtex": "@Article{Iandola2020SqueezeBERTWC,\n author = {F. Iandola and Albert Eaton Shaw and Ravi Krishna and K. Keutzer},\n booktitle = {SUSTAINLP},\n pages = {124-135},\n title = {SqueezeBERT: What can computer vision teach NLP about efficient neural networks?},\n year = {2020}\n}\n"
        }
    },
    "640_hopfield_networks_(2020)": {
        "paperId": "804a6d7c23335bbca6eec3b7d3c8366dcbe395a5",
        "externalIds": {
            "DBLP": "journals/corr/abs-2008-02217",
            "ArXiv": "2008.02217",
            "MAG": "3047517563",
            "CorpusId": 220968978
        },
        "corpusId": 220968978,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/804a6d7c23335bbca6eec3b7d3c8366dcbe395a5",
        "title": "Hopfield Networks is All You Need",
        "abstract": "We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called \"Hopfield\", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: this https URL",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "referenceCount": 107,
        "citationCount": 262,
        "influentialCitationCount": 33,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new PyTorch layer is provided, called \"Hopfield\", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-07-16",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2008.02217"
        },
        "citationStyles": {
            "bibtex": "@Article{Ramsauer2020HopfieldNI,\n author = {Hubert Ramsauer and Bernhard Schafl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Milena Pavlovi'c and G. K. Sandve and V. Greiff and David P. Kreil and Michael Kopp and G. Klambauer and Johannes Brandstetter and Sepp Hochreiter},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Hopfield Networks is All You Need},\n volume = {abs/2008.02217},\n year = {2020}\n}\n"
        }
    },
    "641_mamba-2.8b": {
        "paperId": "432bef8e34014d726c674bc458008ac895297b51",
        "externalIds": {
            "DBLP": "journals/corr/abs-2312-00752",
            "ArXiv": "2312.00752",
            "DOI": "10.48550/arXiv.2312.00752",
            "CorpusId": 265551773
        },
        "corpusId": 265551773,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/432bef8e34014d726c674bc458008ac895297b51",
        "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 0,
        "citationCount": 102,
        "influentialCitationCount": 33,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba)."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-12-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2312.00752"
        },
        "citationStyles": {
            "bibtex": "@Article{Gu2023MambaLS,\n author = {Albert Gu and Tri Dao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},\n volume = {abs/2312.00752},\n year = {2023}\n}\n"
        }
    },
    "642_data2vec_(speech)": {
        "paperId": "8f2bca9d684005675e294b33c26481e36f528cdb",
        "externalIds": {
            "ArXiv": "2202.03555",
            "DBLP": "conf/icml/BaevskiHXBGA22",
            "CorpusId": 246652264
        },
        "corpusId": 246652264,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/8f2bca9d684005675e294b33c26481e36f528cdb",
        "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
        "abstract": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "referenceCount": 86,
        "citationCount": 523,
        "influentialCitationCount": 93,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Data2vec is a framework that uses the same learning method for either speech, NLP or computer vision to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-02-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2202.03555"
        },
        "citationStyles": {
            "bibtex": "@Article{Baevski2022data2vecAG,\n author = {Alexei Baevski and Wei-Ning Hsu and Qiantong Xu and Arun Babu and Jiatao Gu and Michael Auli},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language},\n volume = {abs/2202.03555},\n year = {2022}\n}\n"
        }
    },
    "644_visualizing_cnns": {
        "paperId": "1a2a770d23b4a171fa81de62a78a3deb0588f238",
        "externalIds": {
            "DBLP": "conf/eccv/ZeilerF14",
            "ArXiv": "1311.2901",
            "MAG": "1849277567",
            "DOI": "10.1007/978-3-319-10590-1_53",
            "CorpusId": 3960646
        },
        "corpusId": 3960646,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/1a2a770d23b4a171fa81de62a78a3deb0588f238",
        "title": "Visualizing and Understanding Convolutional Networks",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2013,
        "referenceCount": 32,
        "citationCount": 14477,
        "influentialCitationCount": 966,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10590-1_53.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel visualization technique is introduced that gives insight into the function of intermediate feature layers and the operation of the classifier in large Convolutional Network models, used in a diagnostic role to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2013-11-12",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1311.2901"
        },
        "citationStyles": {
            "bibtex": "@Article{Zeiler2013VisualizingAU,\n author = {Matthew D. Zeiler and R. Fergus},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {Visualizing and Understanding Convolutional Networks},\n volume = {abs/1311.2901},\n year = {2013}\n}\n"
        }
    },
    "645_unifiedqa": {
        "paperId": "ad5970584754cc7a1d91c95ab84a1e210258183a",
        "externalIds": {
            "DBLP": "conf/emnlp/KhashabiMKSTCH20",
            "MAG": "3099655892",
            "ArXiv": "2005.00700",
            "ACL": "2020.findings-emnlp.171",
            "DOI": "10.18653/v1/2020.findings-emnlp.171",
            "CorpusId": 218487109
        },
        "corpusId": 218487109,
        "publicationVenue": {
            "id": "479d5605-51be-4346-b1d6-4334084504df",
            "name": "Findings",
            "type": "journal",
            "issn": "2652-8800",
            "url": "https://findingspress.org/"
        },
        "url": "https://www.semanticscholar.org/paper/ad5970584754cc7a1d91c95ab84a1e210258183a",
        "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System",
        "abstract": "Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.",
        "venue": "Findings",
        "year": 2020,
        "referenceCount": 58,
        "citationCount": 574,
        "influentialCitationCount": 115,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2005.00700",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work uses the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats, and results in a new state of the art on 10 factoid and commonsense question answering datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-05-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2005.00700"
        },
        "citationStyles": {
            "bibtex": "@Article{Khashabi2020UnifiedQACF,\n author = {Daniel Khashabi and Sewon Min and Tushar Khot and Ashish Sabharwal and Oyvind Tafjord and Peter Clark and Hannaneh Hajishirzi},\n booktitle = {Findings},\n journal = {ArXiv},\n title = {UnifiedQA: Crossing Format Boundaries With a Single QA System},\n volume = {abs/2005.00700},\n year = {2020}\n}\n"
        }
    },
    "646_awd-lstm-mos_+_dynamic_evaluation_(wt2,_2018)": {
        "paperId": "a9a5d671271fff45429084e184a788f611b6f194",
        "externalIds": {
            "DBLP": "journals/corr/abs-1809-06858",
            "ArXiv": "1809.06858",
            "MAG": "2890560993",
            "CorpusId": 52301591
        },
        "corpusId": 52301591,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/a9a5d671271fff45429084e184a788f611b6f194",
        "title": "FRAGE: Frequency-Agnostic Word Representation",
        "abstract": "Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In this paper, we develop a neat, simple yet effective way to learn \\emph{FRequency-AGnostic word Embedding} (FRAGE) using adversarial training. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification. Results show that with FRAGE, we achieve higher performance than the baselines in all tasks.",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "referenceCount": 46,
        "citationCount": 130,
        "influentialCitationCount": 14,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper develops a neat, simple yet effective way to learn FRequency-AGnostic word Embedding (FRAGE) using adversarial training and shows that with FRAGE, the model achieves higher performance than the baselines in all tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-09-18",
        "journal": {
            "pages": "1341-1352"
        },
        "citationStyles": {
            "bibtex": "@Article{Gong2018FRAGEFW,\n author = {Chengyue Gong and Di He and Xu Tan and Tao Qin and Liwei Wang and Tie-Yan Liu},\n booktitle = {Neural Information Processing Systems},\n pages = {1341-1352},\n title = {FRAGE: Frequency-Agnostic Word Representation},\n year = {2018}\n}\n"
        }
    },
    "647_lstm-300units": {
        "paperId": "f9a1b3850dfd837793743565a8af95973d395a4e",
        "externalIds": {
            "MAG": "2402268235",
            "DBLP": "conf/interspeech/SundermeyerSN12",
            "DOI": "10.21437/Interspeech.2012-65",
            "CorpusId": 18939716
        },
        "corpusId": 18939716,
        "publicationVenue": {
            "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
            "name": "Interspeech",
            "type": "conference",
            "alternate_names": [
                "Conf Int Speech Commun Assoc",
                "INTERSPEECH",
                "Conference of the International Speech Communication Association"
            ],
            "issn": "2308-457X",
            "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
            "alternate_urls": [
                "http://www.isca-speech.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/f9a1b3850dfd837793743565a8af95973d395a4e",
        "title": "LSTM Neural Networks for Language Modeling",
        "abstract": "Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a fixed context length to predict the next word of a sequence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difficult to train and therefore are unlikely to show the full potential of recurrent models. These problems are addressed by a the Long Short-Term Memory neural network architecture. In this work, we analyze this type of network on an English and a large French language modeling task. Experiments show improvements of about 8 % relative in perplexity over standard recurrent neural network LMs. In addition, we gain considerable improvements in WER on top of a state-of-the-art speech recognition system.",
        "venue": "Interspeech",
        "year": 2012,
        "referenceCount": 19,
        "citationCount": 1804,
        "influentialCitationCount": 129,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www-i6.informatik.rwth-aachen.de/publications/downloader.php?id=820&row=pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work analyzes the Long Short-Term Memory neural network architecture on an English and a large French language modeling task and gains considerable improvements in WER on top of a state-of-the-art speech recognition system."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "194-197"
        },
        "citationStyles": {
            "bibtex": "@Article{Sundermeyer2012LSTMNN,\n author = {M. Sundermeyer and R. Schl\u00fcter and H. Ney},\n booktitle = {Interspeech},\n pages = {194-197},\n title = {LSTM Neural Networks for Language Modeling},\n year = {2012}\n}\n"
        }
    },
    "648_nmt_transformer_437m": {
        "paperId": "88bd75ce3ce22ed85bf9271877aa85da7b7bb312",
        "externalIds": {
            "ArXiv": "1903.00089",
            "ACL": "N19-1388",
            "DBLP": "conf/naacl/AharoniJF19",
            "MAG": "2919290281",
            "DOI": "10.18653/v1/N19-1388",
            "CorpusId": 67855815
        },
        "corpusId": 67855815,
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "url": "https://www.semanticscholar.org/paper/88bd75ce3ce22ed85bf9271877aa85da7b7bb312",
        "title": "Massively Multilingual Neural Machine Translation",
        "abstract": "Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "referenceCount": 37,
        "citationCount": 417,
        "influentialCitationCount": 50,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1903.00089",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-02-28",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1903.00089"
        },
        "citationStyles": {
            "bibtex": "@Article{Aharoni2019MassivelyMN,\n author = {Roee Aharoni and Melvin Johnson and Orhan Firat},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Massively Multilingual Neural Machine Translation},\n volume = {abs/1903.00089},\n year = {2019}\n}\n"
        }
    },
    "649_rwkv-4_14b": {
        "paperId": "026b3396a63ed5772329708b7580d633bb86bec9",
        "externalIds": {
            "ArXiv": "2305.13048",
            "DBLP": "conf/emnlp/PengAAAABCCCDDG23",
            "DOI": "10.48550/arXiv.2305.13048",
            "CorpusId": 258832459
        },
        "corpusId": 258832459,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/026b3396a63ed5772329708b7580d633bb86bec9",
        "title": "RWKV: Reinventing RNNs for the Transformer Era",
        "abstract": "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2023,
        "referenceCount": 105,
        "citationCount": 106,
        "influentialCitationCount": 13,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.13048",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs, and presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-05-22",
        "journal": {
            "pages": "14048-14077"
        },
        "citationStyles": {
            "bibtex": "@Article{Peng2023RWKVRR,\n author = {Bo Peng and Eric Alcaide and Quentin G. Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and G. Kranthikiran and Xuming He and Haowen Hou and Przemyslaw Kazienko and Jan Koco\u0144 and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Xiangru Tang and Bolun Wang and J. S. Wind and Stansilaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and P. Zhou and Jian Zhu and Rui Zhu},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {14048-14077},\n title = {RWKV: Reinventing RNNs for the Transformer Era},\n year = {2023}\n}\n"
        }
    },
    "650_efficientzero": {
        "paperId": "3b3d7adb9047d01af6dfa2975ad8addd69715e96",
        "externalIds": {
            "DBLP": "journals/corr/abs-2111-00210",
            "ArXiv": "2111.00210",
            "CorpusId": 240354728
        },
        "corpusId": 240354728,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/3b3d7adb9047d01af6dfa2975ad8addd69715e96",
        "title": "Mastering Atari Games with Limited Data",
        "abstract": "Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3% mean human performance and 109.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 49,
        "citationCount": 122,
        "influentialCitationCount": 14,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a sample efficient model-based visual RL algorithm built on MuZero, which it is hoped will accelerate the research of MCTS-based RL algorithms in the wider community and achieves super-human performance on Atari games with such little data."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2111.00210"
        },
        "citationStyles": {
            "bibtex": "@Article{Ye2021MasteringAG,\n author = {Weirui Ye and Shao-Wei Liu and Thanard Kurutach and P. Abbeel and Yang Gao},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Mastering Atari Games with Limited Data},\n volume = {abs/2111.00210},\n year = {2021}\n}\n"
        }
    },
    "651_jais": {
        "paperId": "5c577988ccebfea96de86678d04fd94fad367d2e",
        "externalIds": {
            "ArXiv": "2308.16149",
            "DBLP": "journals/corr/abs-2308-16149",
            "DOI": "10.48550/arXiv.2308.16149",
            "CorpusId": 261339576
        },
        "corpusId": 261339576,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/5c577988ccebfea96de86678d04fd94fad367d2e",
        "title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
        "abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 119,
        "citationCount": 11,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2308.16149",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Jais and Jais-chat are introduced, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs) based on the GPT-3 decoder-only architecture that demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-08-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2308.16149"
        },
        "citationStyles": {
            "bibtex": "@Article{Sengupta2023JaisAJ,\n author = {Neha Sengupta and Sunil Kumar Sahu and Bokang Jia and Satheesh Katipomu and Haonan Li and Fajri Koto and Osama Mohammed Afzal and Samta Kamboj and O. Pandit and Rahul Pal and Lalit Pradhan and Zainul Mujahid and Massa Baali and Xudong Han and Alham Fikri Aji and Zhengzhong Liu and Andy Hock and Andrew Feldman and Jonathan Lee and A. Jackson and Preslav Nakov and Timothy Baldwin and Eric P. Xing},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models},\n volume = {abs/2308.16149},\n year = {2023}\n}\n"
        }
    },
    "652_audiogen": {
        "paperId": "ebb85974e06c4879b451fdfcb4f472a09471935b",
        "externalIds": {
            "DBLP": "conf/iclr/KreukSPSDCPTA23",
            "ArXiv": "2209.15352",
            "DOI": "10.48550/arXiv.2209.15352",
            "CorpusId": 252668761
        },
        "corpusId": 252668761,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/ebb85974e06c4879b451fdfcb4f472a09471935b",
        "title": "AudioGen: Textually Guided Audio Generation",
        "abstract": "We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: https://felixkreuk.github.io/audiogen",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 70,
        "citationCount": 115,
        "influentialCitationCount": 18,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2209.15352",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs that outperforms over both objective and subjective metrics."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-09-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2209.15352"
        },
        "citationStyles": {
            "bibtex": "@Article{Kreuk2022AudioGenTG,\n author = {Felix Kreuk and Gabriel Synnaeve and Adam Polyak and Uriel Singer and Alexandre D'efossez and Jade Copet and Devi Parikh and Yaniv Taigman and Yossi Adi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {AudioGen: Textually Guided Audio Generation},\n volume = {abs/2209.15352},\n year = {2022}\n}\n"
        }
    },
    "653_tagging_via_viterbi_decoding": {
        "paperId": "5a7958b418bceb48a315384568091ab1898b1640",
        "externalIds": {
            "DBLP": "conf/emnlp/Collins02",
            "MAG": "2008652694",
            "ACL": "W02-1001",
            "DOI": "10.3115/1118693.1118694",
            "CorpusId": 10888973
        },
        "corpusId": 10888973,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/5a7958b418bceb48a315384568091ab1898b1640",
        "title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms",
        "abstract": "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2002,
        "referenceCount": 15,
        "citationCount": 2257,
        "influentialCitationCount": 347,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1118693.1118694",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experimental results on part-of-speech tagging and base noun phrase chunking are given, in both cases showing improvements over results for a maximum-entropy tagger."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2002-07-06",
        "journal": {
            "pages": "1-8"
        },
        "citationStyles": {
            "bibtex": "@Article{Collins2002DiscriminativeTM,\n author = {M. Collins},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1-8},\n title = {Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms},\n year = {2002}\n}\n"
        }
    },
    "654_deep_belief_nets": {
        "paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
        "externalIds": {
            "DBLP": "journals/neco/HintonOT06",
            "MAG": "2136922672",
            "DOI": "10.1162/neco.2006.18.7.1527",
            "CorpusId": 2309950,
            "PubMed": "16764513"
        },
        "corpusId": 2309950,
        "publicationVenue": {
            "id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
            "name": "Neural Computation",
            "type": "journal",
            "alternate_names": [
                "Neural Comput"
            ],
            "issn": "0899-7667",
            "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667",
            "alternate_urls": [
                "http://ieeexplore.ieee.org/servlet/opac?punumber=6720226",
                "http://www.mitpressjournals.org/loi/neco",
                "https://www.mitpressjournals.org/loi/neco"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/8978cf7574ceb35f4c3096be768c7547b28a35d0",
        "title": "A Fast Learning Algorithm for Deep Belief Nets",
        "abstract": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.",
        "venue": "Neural Computation",
        "year": 2006,
        "referenceCount": 39,
        "citationCount": 15156,
        "influentialCitationCount": 1189,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2006-07-01",
        "journal": {
            "name": "Neural Computation",
            "pages": "1527-1554",
            "volume": "18"
        },
        "citationStyles": {
            "bibtex": "@Article{Hinton2006AFL,\n author = {Geoffrey E. Hinton and Simon Osindero and Y. Teh},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {1527-1554},\n title = {A Fast Learning Algorithm for Deep Belief Nets},\n volume = {18},\n year = {2006}\n}\n"
        }
    },
    "655_mnasnet-a1_+_ssdlite": {
        "paperId": "693c97ecedb0a84539b7162c95e89fa3cd84ca73",
        "externalIds": {
            "ArXiv": "1807.11626",
            "DBLP": "conf/cvpr/TanCPVSHL19",
            "MAG": "2886953980",
            "DOI": "10.1109/CVPR.2019.00293",
            "CorpusId": 51891697
        },
        "corpusId": 51891697,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/693c97ecedb0a84539b7162c95e89fa3cd84ca73",
        "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
        "abstract": "Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8\u00d7 faster than MobileNetV2 with 0.5% higher accuracy and 2.3\u00d7 faster than NASNet with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "referenceCount": 43,
        "citationCount": 2519,
        "influentialCitationCount": 342,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1807.11626",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-07-31",
        "journal": {
            "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "2815-2823"
        },
        "citationStyles": {
            "bibtex": "@Article{Tan2018MnasNetPN,\n author = {Mingxing Tan and Bo Chen and Ruoming Pang and Vijay Vasudevan and Quoc V. Le},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2815-2823},\n title = {MnasNet: Platform-Aware Neural Architecture Search for Mobile},\n year = {2018}\n}\n"
        }
    },
    "656_lstm_with_forget_gates": {
        "paperId": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
        "externalIds": {
            "MAG": "2116261113",
            "DBLP": "journals/neco/GersSC00",
            "DOI": "10.1162/089976600300015015",
            "CorpusId": 11598600,
            "PubMed": "11032042"
        },
        "corpusId": 11598600,
        "publicationVenue": {
            "id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
            "name": "Neural Computation",
            "type": "journal",
            "alternate_names": [
                "Neural Comput"
            ],
            "issn": "0899-7667",
            "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667",
            "alternate_urls": [
                "http://ieeexplore.ieee.org/servlet/opac?punumber=6720226",
                "http://www.mitpressjournals.org/loi/neco",
                "https://www.mitpressjournals.org/loi/neco"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/11540131eae85b2e11d53df7f1360eeb6476e7f4",
        "title": "Learning to Forget: Continual Prediction with LSTM",
        "abstract": "Long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive forget gate that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.",
        "venue": "Neural Computation",
        "year": 2000,
        "referenceCount": 38,
        "citationCount": 5826,
        "influentialCitationCount": 372,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine",
            "Psychology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Psychology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work identifies a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset, and proposes a novel, adaptive forget gate that enables an LSTm cell to learn to reset itself at appropriate times, thus releasing internal resources."
        },
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "publicationDate": "2000-10-01",
        "journal": {
            "name": "Neural Computation",
            "pages": "2451-2471",
            "volume": "12"
        },
        "citationStyles": {
            "bibtex": "@Article{Gers2000LearningTF,\n author = {Felix Alexander Gers and J. Schmidhuber and Fred Cummins},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {2451-2471},\n title = {Learning to Forget: Continual Prediction with LSTM},\n volume = {12},\n year = {2000}\n}\n"
        }
    },
    "657_q-learning": {
        "paperId": "59b50a775542e87f078db35b868ac10ab43d4c75",
        "externalIds": {
            "MAG": "2021247827",
            "DBLP": "journals/ras/Krose95",
            "DOI": "10.1016/0921-8890(95)00026-C",
            "CorpusId": 40597581
        },
        "corpusId": 40597581,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/59b50a775542e87f078db35b868ac10ab43d4c75",
        "title": "Learning from delayed rewards",
        "abstract": null,
        "venue": "Robotics Auton. Syst.",
        "year": 1995,
        "referenceCount": 0,
        "citationCount": 4878,
        "influentialCitationCount": 629,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The invention relates to a circuit for use in a receiver which can receive two-tone/stereo signals which is intended to make a choice between mono or stereo reproduction of signal A or of signal B and vice versa."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1995-10-01",
        "journal": {
            "name": "Robotics Auton. Syst.",
            "pages": "233-235",
            "volume": "15"
        },
        "citationStyles": {
            "bibtex": "@Article{Kr\u00f6se1995LearningFD,\n author = {B. Kr\u00f6se},\n booktitle = {Robotics Auton. Syst.},\n journal = {Robotics Auton. Syst.},\n pages = {233-235},\n title = {Learning from delayed rewards},\n volume = {15},\n year = {1995}\n}\n"
        }
    },
    "659_vrns-rnn-3-3-5": {
        "paperId": "f3ff2da07cc9873a838687c17704be4e1fc6743b",
        "externalIds": {
            "DBLP": "conf/iclr/DuSell023",
            "ArXiv": "2210.01343",
            "DOI": "10.48550/arXiv.2210.01343",
            "CorpusId": 252693138
        },
        "corpusId": 252693138,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/f3ff2da07cc9873a838687c17704be4e1fc6743b",
        "title": "The Surprising Computational Power of Nondeterministic Stack RNNs",
        "abstract": "Traditional recurrent neural networks (RNNs) have a fixed, finite number of memory cells. In theory (assuming bounded range and precision), this limits their formal language recognition power to regular languages, and in practice, RNNs have been shown to be unable to learn many context-free languages (CFLs). In order to expand the class of languages RNNs recognize, prior work has augmented RNNs with a nondeterministic stack data structure, putting them on par with pushdown automata and increasing their language recognition power to CFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic CFLs), but in this paper, we show that nondeterminism and the neural controller interact to produce two more unexpected abilities. First, the nondeterministic stack RNN can recognize not only CFLs, but also many non-context-free languages. Second, it can recognize languages with much larger alphabet sizes than one might expect given the size of its stack alphabet. Finally, to increase the information capacity in the stack and allow it to solve more complicated tasks with large alphabet sizes, we propose a new version of the nondeterministic stack that simulates stacks of vectors rather than discrete symbols. We demonstrate perplexity improvements with this new model on the Penn Treebank language modeling benchmark.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 34,
        "citationCount": 1,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.01343",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper shows that nondeterminism and the neural controller interact to produce two more unexpected abilities of the nondeterministic stack RNN, which can recognize not only CFLs, but also many non-context-free languages."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.01343"
        },
        "citationStyles": {
            "bibtex": "@Article{DuSell2022TheSC,\n author = {Brian DuSell and David Chiang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {The Surprising Computational Power of Nondeterministic Stack RNNs},\n volume = {abs/2210.01343},\n year = {2022}\n}\n"
        }
    },
    "661_decay_rnn": {
        "paperId": "5132a2ebbbe3090d73cc825a2608a69694eefc8b",
        "externalIds": {
            "DBLP": "conf/acl/BhattBSA20",
            "MAG": "3037371671",
            "ArXiv": "2005.08199",
            "ACL": "2020.acl-srw.33",
            "DOI": "10.18653/v1/2020.acl-srw.33",
            "CorpusId": 218673912
        },
        "corpusId": 218673912,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/5132a2ebbbe3090d73cc825a2608a69694eefc8b",
        "title": "How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?",
        "abstract": "Long short-term memory (LSTM) networks and their variants are capable of encapsulating long-range dependencies, which is evident from their performance on a variety of linguistic tasks. On the other hand, simple recurrent networks (SRNs), which appear more biologically grounded in terms of synaptic connections, have generally been less successful at capturing long-range dependencies as well as the loci of grammatical errors in an unsupervised setting. In this paper, we seek to develop models that bridge the gap between biological plausibility and linguistic competence. We propose a new architecture, the Decay RNN, which incorporates the decaying nature of neuronal activations and models the excitatory and inhibitory connections in a population of neurons. Besides its biological inspiration, our model also shows competitive performance relative to LSTMs on subject-verb agreement, sentence grammaticality, and language modeling tasks. These results provide some pointers towards probing the nature of the inductive biases required for RNN architectures to model linguistic phenomena successfully.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "referenceCount": 37,
        "citationCount": 4,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-srw.33.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new architecture is proposed, the Decay RNN, which incorporates the decaying nature of neuronal activations and models the excitatory and inhibitory connections in a population of neurons and shows competitive performance relative to LSTMs on subject-verb agreement, sentence grammaticality, and language modeling tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-05-17",
        "journal": {
            "pages": "244-254"
        },
        "citationStyles": {
            "bibtex": "@Article{Bhatt2020HowMC,\n author = {Gantavya Bhatt and Hritik Bansal and Rishu Singh and Sumeet Agarwal},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {244-254},\n title = {How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?},\n year = {2020}\n}\n"
        }
    },
    "662_self-attention_and_convolutional_layers": {
        "paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614",
        "externalIds": {
            "ArXiv": "1911.03584",
            "MAG": "2989226908",
            "DBLP": "conf/iclr/CordonnierLJ20",
            "CorpusId": 207852415
        },
        "corpusId": 207852415,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/bb713d56a39a040b35e4f9e036fb4422f543e614",
        "title": "On the Relationship between Self-Attention and Convolutional Layers",
        "abstract": "Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 26,
        "citationCount": 428,
        "influentialCitationCount": 21,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proves that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer, which provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-11-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1911.03584"
        },
        "citationStyles": {
            "bibtex": "@Article{Cordonnier2019OnTR,\n author = {Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On the Relationship between Self-Attention and Convolutional Layers},\n volume = {abs/1911.03584},\n year = {2019}\n}\n"
        }
    },
    "663_4-gram_+_8_denn": {
        "paperId": "9e8beebda6b5ce46ffd16f54bf69526483c9089c",
        "externalIds": {
            "MAG": "1931184598",
            "DBLP": "journals/corr/AudhkhasiSR14",
            "ArXiv": "1412.7063",
            "CorpusId": 18275776
        },
        "corpusId": 18275776,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9e8beebda6b5ce46ffd16f54bf69526483c9089c",
        "title": "Diverse Embedding Neural Network Language Models",
        "abstract": "We propose Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "referenceCount": 14,
        "citationCount": 1,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs), which projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher- dimensional sub-space as in conventional feed-forward neural network LMs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-12-01",
        "journal": {
            "name": "CoRR",
            "volume": "abs/1412.7063"
        },
        "citationStyles": {
            "bibtex": "@Article{Audhkhasi2014DiverseEN,\n author = {Kartik Audhkhasi and A. Sethy and B. Ramabhadran},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Diverse Embedding Neural Network Language Models},\n volume = {abs/1412.7063},\n year = {2014}\n}\n"
        }
    },
    "664_rnn_500_10_+_rt09_lm_(nist_rt05)": {
        "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
        "externalIds": {
            "MAG": "179875071",
            "DBLP": "conf/interspeech/MikolovKBCK10",
            "DOI": "10.21437/Interspeech.2010-343",
            "CorpusId": 17048224
        },
        "corpusId": 17048224,
        "publicationVenue": {
            "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
            "name": "Interspeech",
            "type": "conference",
            "alternate_names": [
                "Conf Int Speech Commun Assoc",
                "INTERSPEECH",
                "Conference of the International Speech Communication Association"
            ],
            "issn": "2308-457X",
            "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
            "alternate_urls": [
                "http://www.isca-speech.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/9819b600a828a57e1cde047bbe710d3446b30da5",
        "title": "Recurrent neural network based language model",
        "abstract": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition",
        "venue": "Interspeech",
        "year": 2010,
        "referenceCount": 17,
        "citationCount": 5608,
        "influentialCitationCount": 490,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "1045-1048"
        },
        "citationStyles": {
            "bibtex": "@Article{Mikolov2010RecurrentNN,\n author = {Tomas Mikolov and M. Karafi\u00e1t and L. Burget and J. \u010cernock\u00fd and S. Khudanpur},\n booktitle = {Interspeech},\n pages = {1045-1048},\n title = {Recurrent neural network based language model},\n year = {2010}\n}\n"
        }
    },
    "666_gen-1": {
        "paperId": "07be0ec1f45e21a1032616535d0290ee6bfe0f6b",
        "externalIds": {
            "DBLP": "conf/iccv/EsserCAGG23",
            "ArXiv": "2302.03011",
            "DOI": "10.1109/ICCV51070.2023.00675",
            "CorpusId": 256615582
        },
        "corpusId": 256615582,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/07be0ec1f45e21a1032616535d0290ee6bfe0f6b",
        "title": "Structure and Content-Guided Video Synthesis with Diffusion Models",
        "abstract": "Text-guided generative diffusion models unlock powerful image creation and editing tools. Recent approaches that edit the content of footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames.In this work, we present a structure and content-guided video diffusion model that edits videos based on descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. A novel guidance method, enabled by joint video and image training, exposes explicit control over temporal consistency. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2023,
        "referenceCount": 69,
        "citationCount": 210,
        "influentialCitationCount": 27,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2302.03011",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents a structure and content-guided video diffusion model that edits videos based on descriptions of the desired output, and shows that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-02-06",
        "journal": {
            "name": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "pages": "7312-7322"
        },
        "citationStyles": {
            "bibtex": "@Article{Esser2023StructureAC,\n author = {Patrick Esser and Johnathan Chiu and Parmida Atighehchian and Jonathan Granskog and Anastasis Germanidis},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {7312-7322},\n title = {Structure and Content-Guided Video Synthesis with Diffusion Models},\n year = {2023}\n}\n"
        }
    },
    "667_trellisnet": {
        "paperId": "a14af711aaa3ae83eb64d1f517b024b8c3094a8a",
        "externalIds": {
            "MAG": "2896528354",
            "DBLP": "conf/iclr/BaiKK19",
            "ArXiv": "1810.06682",
            "CorpusId": 53111329
        },
        "corpusId": 53111329,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/a14af711aaa3ae83eb64d1f517b024b8c3094a8a",
        "title": "Trellis Networks for Sequence Modeling",
        "abstract": "We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling tasks, and stress tests designed to evaluate long-term memory retention. The code is available at this https URL .",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 59,
        "citationCount": 130,
        "influentialCitationCount": 19,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Trellis networks are presented, a new architecture for sequence modeling that outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character- level language modeling tasks, and stress tests designed to evaluate long-term memory retention."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-09-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1810.06682"
        },
        "citationStyles": {
            "bibtex": "@Article{Modeling2018TrellisNF,\n author = {Sequence Modeling and Shaojie Bai and J. Z. Kolter and V. Koltun},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Trellis Networks for Sequence Modeling},\n volume = {abs/1810.06682},\n year = {2018}\n}\n"
        }
    },
    "668_residual_dense_network": {
        "paperId": "4ef1476dec02c62227187edbba88615278b3edba",
        "externalIds": {
            "MAG": "2964101377",
            "DBLP": "conf/cvpr/ZhangTKZ018",
            "ArXiv": "1802.08797",
            "DOI": "10.1109/CVPR.2018.00262",
            "CorpusId": 3619954
        },
        "corpusId": 3619954,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/4ef1476dec02c62227187edbba88615278b3edba",
        "title": "Residual Dense Network for Image Super-Resolution",
        "abstract": "A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.",
        "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
        "year": 2018,
        "referenceCount": 44,
        "citationCount": 2712,
        "influentialCitationCount": 482,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1802.08797",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes residual dense block (RDB) to extract abundant local features via dense connected convolutional layers and uses global feature fusion in RDB to jointly and adaptively learn global hierarchical features in a holistic way."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-02-24",
        "journal": {
            "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "pages": "2472-2481"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2018ResidualDN,\n author = {Yulun Zhang and Yapeng Tian and Yu Kong and Bineng Zhong and Y. Fu},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {2472-2481},\n title = {Residual Dense Network for Image Super-Resolution},\n year = {2018}\n}\n"
        }
    },
    "669_lstm_(2018)": {
        "paperId": "921196c32213a229245a9705ee4768bc941e7a26",
        "externalIds": {
            "MAG": "2792764867",
            "DBLP": "journals/corr/abs-1803-01271",
            "ArXiv": "1803.01271",
            "CorpusId": 4747877
        },
        "corpusId": 4747877,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/921196c32213a229245a9705ee4768bc941e7a26",
        "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
        "abstract": "For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at this http URL .",
        "venue": "arXiv.org",
        "year": 2018,
        "referenceCount": 84,
        "citationCount": 3411,
        "influentialCitationCount": 536,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A systematic evaluation of generic convolutional and recurrent architectures for sequence modeling concludes that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutionals should be regarded as a natural starting point for sequence modeled tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-03-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1803.01271"
        },
        "citationStyles": {
            "bibtex": "@Article{Bai2018AnEE,\n author = {Shaojie Bai and J. Z. Kolter and V. Koltun},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},\n volume = {abs/1803.01271},\n year = {2018}\n}\n"
        }
    },
    "670_ngram_corpus": {
        "paperId": "ba3f84c45807ef82ee096868159974066600761d",
        "externalIds": {
            "MAG": "2106279089",
            "DBLP": "conf/acl/LinMLOBP12",
            "ACL": "P12-3029",
            "CorpusId": 17707301
        },
        "corpusId": 17707301,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/ba3f84c45807ef82ee096868159974066600761d",
        "title": "Syntactic Annotations for the Google Books NGram Corpus",
        "abstract": "We present a new edition of the Google Books Ngram Corpus, which describes how often words and phrases were used over a period of five centuries, in eight languages; it reflects 6% of all books ever published. This new edition introduces syntactic annotations: words are tagged with their part-of-speech, and head-modifier relationships are recorded. The annotations are produced automatically with statistical models that are specifically adapted to historical text. The corpus will facilitate the study of linguistic trends, especially those related to the evolution of syntax.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2012,
        "referenceCount": 17,
        "citationCount": 426,
        "influentialCitationCount": 40,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new edition of the Google Books Ngram Corpus, which describes how often words and phrases were used over a period of five centuries, in eight languages, is presented, which will facilitate the study of linguistic trends, especially those related to the evolution of syntax."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2012-07-10",
        "journal": {
            "pages": "169-174"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2012SyntacticAF,\n author = {Yuri Lin and Jean-Baptiste Michel and Erez Aiden Lieberman and Jon Orwant and W. Brockman and Slav Petrov},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {169-174},\n title = {Syntactic Annotations for the Google Books NGram Corpus},\n year = {2012}\n}\n"
        }
    },
    "671_conformer_+_wav2vec_2.0_+_noisy_student": {
        "paperId": "9ff525d1ebd389c359ddbf06df3e99c433c2bf9e",
        "externalIds": {
            "MAG": "3093579165",
            "ArXiv": "2010.10504",
            "DBLP": "journals/corr/abs-2010-10504",
            "CorpusId": 224803233
        },
        "corpusId": 224803233,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/9ff525d1ebd389c359ddbf06df3e99c433c2bf9e",
        "title": "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition",
        "abstract": "We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7%/3.3%.",
        "venue": "arXiv.org",
        "year": 2020,
        "referenceCount": 54,
        "citationCount": 261,
        "influentialCitationCount": 40,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-10-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2010.10504"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2020PushingTL,\n author = {Yu Zhang and James Qin and Daniel S. Park and Wei Han and Chung-Cheng Chiu and Ruoming Pang and Quoc V. Le and Yonghui Wu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition},\n volume = {abs/2010.10504},\n year = {2020}\n}\n"
        }
    },
    "672_googlenet___inceptionv1": {
        "paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
        "externalIds": {
            "DBLP": "journals/corr/SzegedyLJSRAEVR14",
            "MAG": "2097117768",
            "ArXiv": "1409.4842",
            "DOI": "10.1109/CVPR.2015.7298594",
            "CorpusId": 206592484
        },
        "corpusId": 206592484,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e15cf50aa89fee8535703b9f9512fca5bfc43327",
        "title": "Going deeper with convolutions",
        "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "referenceCount": 264,
        "citationCount": 38921,
        "influentialCitationCount": 4074,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1409.4842",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A deep convolutional neural network architecture codenamed Inception is proposed that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-09-16",
        "journal": {
            "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "1-9"
        },
        "citationStyles": {
            "bibtex": "@Article{Szegedy2014GoingDW,\n author = {Christian Szegedy and Wei Liu and Yangqing Jia and P. Sermanet and Scott E. Reed and Dragomir Anguelov and D. Erhan and Vincent Vanhoucke and Andrew Rabinovich},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1-9},\n title = {Going deeper with convolutions},\n year = {2014}\n}\n"
        }
    },
    "673_pretrans-3l-250h": {
        "paperId": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d",
        "externalIds": {
            "DBLP": "journals/corr/abs-1303-5778",
            "MAG": "2950689855",
            "ArXiv": "1303.5778",
            "DOI": "10.1109/ICASSP.2013.6638947",
            "CorpusId": 206741496
        },
        "corpusId": 206741496,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/4177ec52d1b80ed57f2e72b0f9a42365f1a8598d",
        "title": "Speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2013,
        "referenceCount": 30,
        "citationCount": 7981,
        "influentialCitationCount": 404,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1303.5778",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2013-03-22",
        "journal": {
            "name": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "pages": "6645-6649"
        },
        "citationStyles": {
            "bibtex": "@Article{Graves2013SpeechRW,\n author = {Alex Graves and Abdel-rahman Mohamed and Geoffrey E. Hinton},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},\n pages = {6645-6649},\n title = {Speech recognition with deep recurrent neural networks},\n year = {2013}\n}\n"
        }
    },
    "675_learnability_theory_of_language_development": {
        "paperId": "fb0641ea5b2c74b0e1fc5163f87d14a886c50e9e",
        "externalIds": {
            "MAG": "2094249282",
            "DOI": "10.2307/414499",
            "CorpusId": 62166684
        },
        "corpusId": 62166684,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/fb0641ea5b2c74b0e1fc5163f87d14a886c50e9e",
        "title": "Language learnability and language development",
        "abstract": "Language learnability and language devlopment revisited the acquisition theory - assumptions and postulates phrase structure rules phrase stucture rules - developmental considerations inflection complementation and control auxiliaries lexical entries and lexical rules.",
        "venue": "",
        "year": 1985,
        "referenceCount": 0,
        "citationCount": 2305,
        "influentialCitationCount": 156,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Psychology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Psychology",
                "source": "external"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Language learnability and language devlopment revisited the acquisition theory - assumptions and postulates phrase structure rules phrase stucture rules - developmental considerations inflection complementation and control auxiliaries lexical entries and lexical rules."
        },
        "publicationTypes": null,
        "publicationDate": "1985-12-01",
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Pinker1985LanguageLA,\n author = {S. Pinker},\n title = {Language learnability and language development},\n year = {1985}\n}\n"
        }
    },
    "676_one-peace": {
        "paperId": "bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f",
        "externalIds": {
            "DBLP": "journals/corr/abs-2305-11172",
            "ArXiv": "2305.11172",
            "DOI": "10.48550/arXiv.2305.11172",
            "CorpusId": 258762390
        },
        "corpusId": 258762390,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f",
        "title": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
        "abstract": "In this work, we explore a scalable way for building a general representation model toward unlimited modalities. We release ONE-PEACE, a highly extensible model with 4B parameters that can seamlessly align and integrate representations across vision, audio, and language modalities. The architecture of ONE-PEACE comprises modality adapters, shared self-attention layers, and modality FFNs. This design allows for the easy extension of new modalities by adding adapters and FFNs, while also enabling multi-modal fusion through self-attention layers. To pretrain ONE-PEACE, we develop two modality-agnostic pretraining tasks, cross-modal aligning contrast and intra-modal denoising contrast, which align the semantic space of different modalities and capture fine-grained details within modalities concurrently. With the scaling-friendly architecture and pretraining tasks, ONE-PEACE has the potential to expand to unlimited modalities. Without using any vision or language pretrained model for initialization, ONE-PEACE achieves leading results on a wide range of uni-modal and multi-modal tasks, including image classification (ImageNet), semantic segmentation (ADE20K), audio-text retrieval (AudioCaps, Clotho), audio classification (ESC-50, FSD50K, VGGSound), audio question answering (AVQA), image-text retrieval (MSCOCO, Flickr30K), and visual grounding (RefCOCO/+/g). Code is available at https://github.com/OFA-Sys/ONE-PEACE.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 171,
        "citationCount": 25,
        "influentialCitationCount": 3,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.11172",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work releases ONE-PEACE, a highly extensible model with 4B parameters that can seamlessly align and integrate representations across vision, audio, and language modalities, and develops two modality-agnostic pretraining tasks, which align the semantic space of different modalities and capture fine-grained details within modalities concurrently."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-05-18",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2305.11172"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2023ONEPEACEEO,\n author = {Peng Wang and Shijie Wang and Junyang Lin and Shuai Bai and Xiaohuan Zhou and Jingren Zhou and Xinggang Wang and Chang Zhou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities},\n volume = {abs/2305.11172},\n year = {2023}\n}\n"
        }
    },
    "677_memsizer": {
        "paperId": "e2ee883fca5f8f32a1dfa2dc06c742d57f2c38b9",
        "externalIds": {
            "DBLP": "conf/emnlp/Zhang022",
            "ArXiv": "2203.12644",
            "ACL": "2022.emnlp-main.24",
            "DOI": "10.18653/v1/2022.emnlp-main.24",
            "CorpusId": 248965487
        },
        "corpusId": 248965487,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/e2ee883fca5f8f32a1dfa2dc06c742d57f2c38b9",
        "title": "Linearizing Transformer with Key-Value Memory",
        "abstract": "Efficient transformer variants with linear time complexity have been developed to mitigate the quadratic computational overhead of the vanilla transformer. Among them are low-rank projection methods such as Linformer and kernel-based Transformers. Despite their unique merits, they usually suffer from a performance drop comparing with the vanilla transformer on many sequence generation tasks, and often fail to obtain computation gain when the generation is short. We propose Memsizer, an approach towards closing the performance gap while improving the efficiency even with short generation. It projects the source sequences into lower dimension representations like Linformer, while enjoying efficient recurrent-style incremental computation similar to kernel-based transformers. This yields linear computation time and constant memory complexity at inference time. Memsizer also employs a lightweight multi-head mechanism which renders the computation as light as a single-head model. We demonstrate that Memsizer provides an improved balance between efficiency and accuracy over the vanilla transformer and other efficient transformer variants in three typical sequence generation tasks, including machine translation, abstractive text summarization, and language modeling.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2022,
        "referenceCount": 64,
        "citationCount": 3,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.emnlp-main.24.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated that Memsizer provides an improved balance between efficiency and accuracy over the vanilla transformer and other efficient transformer variants in three typical sequence generation tasks, including machine translation, abstractive text summarization, and language modeling."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-03-23",
        "journal": {
            "pages": "346-359"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2022LinearizingTW,\n author = {Yizhe Zhang and Deng Cai},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {346-359},\n title = {Linearizing Transformer with Key-Value Memory},\n year = {2022}\n}\n"
        }
    },
    "678_adaline": {
        "paperId": "2e14b2ff9dc2234df94fc24d89fc25e797d0e9e7",
        "externalIds": {
            "MAG": "1535810436",
            "DOI": "10.21236/ad0241531",
            "CorpusId": 60830585
        },
        "corpusId": 60830585,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/2e14b2ff9dc2234df94fc24d89fc25e797d0e9e7",
        "title": "Adaptive switching circuits",
        "abstract": null,
        "venue": "",
        "year": 1988,
        "referenceCount": 0,
        "citationCount": 2212,
        "influentialCitationCount": 29,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.dtic.mil/dtic/tr/fulltext/u2/241531.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": null,
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "pages": "123-134",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Widrow1988AdaptiveSC,\n author = {B. Widrow and M. Hoff},\n pages = {123-134},\n title = {Adaptive switching circuits},\n year = {1988}\n}\n"
        }
    },
    "679_deq-transformer_(post-ln)_+_jacobian_regularisation": {
        "paperId": "7ea5d034a98776207fd6e1ae021016ea0c1dab5d",
        "externalIds": {
            "ArXiv": "2106.14342",
            "DBLP": "conf/icml/BaiKK21",
            "CorpusId": 235632013
        },
        "corpusId": 235632013,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7ea5d034a98776207fd6e1ae021016ea0c1dab5d",
        "title": "Stabilizing Equilibrium Models by Jacobian Regularization",
        "abstract": "Deep equilibrium networks (DEQs) are a new class of models that eschews traditional depth in favor of finding the fixed point of a single nonlinear layer. These models have been shown to achieve performance competitive with the state-of-the-art deep networks while using significantly less memory. Yet they are also slower, brittle to architectural choices, and introduce potential instability to the model. In this paper, we propose a regularization scheme for DEQ models that explicitly regularizes the Jacobian of the fixed-point update equations to stabilize the learning of equilibrium models. We show that this regularization adds only minimal computational cost, significantly stabilizes the fixed-point convergence in both forward and backward passes, and scales well to high-dimensional, realistic domains (e.g., WikiText-103 language modeling and ImageNet classification). Using this method, we demonstrate, for the first time, an implicit-depth model that runs with approximately the same speed and level of performance as popular conventional deep networks such as ResNet-101, while still maintaining the constant memory footprint and architectural simplicity of DEQs. Code is available at https://github.com/locuslab/deq .",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 60,
        "citationCount": 44,
        "influentialCitationCount": 9,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a regularization scheme for DEQ models that explicitly regularizes the Jacobian of the fixed-point update equations to stabilize the learning of equilibrium models and demonstrates, for the first time, an implicit-depth model that runs with approximately the same speed and level of performance as popular conventional deep networks such as ResNet-101."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-06-28",
        "journal": {
            "pages": "554-565"
        },
        "citationStyles": {
            "bibtex": "@Article{Bai2021StabilizingEM,\n author = {Shaojie Bai and V. Koltun and J. Z. Kolter},\n booktitle = {International Conference on Machine Learning},\n pages = {554-565},\n title = {Stabilizing Equilibrium Models by Jacobian Regularization},\n year = {2021}\n}\n"
        }
    },
    "680_dqn-2015": {
        "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
        "externalIds": {
            "DBLP": "journals/nature/MnihKSRVBGRFOPB15",
            "MAG": "2145339207",
            "DOI": "10.1038/nature14236",
            "CorpusId": 205242740,
            "PubMed": "25719670"
        },
        "corpusId": 205242740,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/340f48901f72278f6bf78a04ee5b01df208cc508",
        "title": "Human-level control through deep reinforcement learning",
        "abstract": null,
        "venue": "Nature",
        "year": 2015,
        "referenceCount": 37,
        "citationCount": 23012,
        "influentialCitationCount": 2938,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2015-02-25",
        "journal": {
            "name": "Nature",
            "pages": "529-533",
            "volume": "518"
        },
        "citationStyles": {
            "bibtex": "@Article{Mnih2015HumanlevelCT,\n author = {Volodymyr Mnih and K. Kavukcuoglu and David Silver and Andrei A. Rusu and J. Veness and Marc G. Bellemare and Alex Graves and Martin A. Riedmiller and A. Fidjeland and Georg Ostrovski and Stig Petersen and Charlie Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and D. Kumaran and Daan Wierstra and S. Legg and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {529-533},\n title = {Human-level control through deep reinforcement learning},\n volume = {518},\n year = {2015}\n}\n"
        }
    },
    "681_efficientnetv2": {
        "paperId": "8f8f73f0f208302546c825ed474432389ed63be4",
        "externalIds": {
            "DBLP": "journals/corr/abs-2104-00298",
            "ArXiv": "2104.00298",
            "CorpusId": 232478903
        },
        "corpusId": 232478903,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/8f8f73f0f208302546c825ed474432389ed63be4",
        "title": "EfficientNetV2: Smaller Models and Faster Training",
        "abstract": "This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy. With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 50,
        "citationCount": 1418,
        "influentialCitationCount": 188,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models that were searched from the search space enriched with new ops such as Fused-MBConv."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-04-01",
        "journal": {
            "pages": "10096-10106"
        },
        "citationStyles": {
            "bibtex": "@Article{Tan2021EfficientNetV2SM,\n author = {Mingxing Tan and Quoc V. Le},\n booktitle = {International Conference on Machine Learning},\n pages = {10096-10106},\n title = {EfficientNetV2: Smaller Models and Faster Training},\n year = {2021}\n}\n"
        }
    },
    "682_inceptionv4": {
        "paperId": "b5c26ab8767d046cb6e32d959fdf726aee89bb62",
        "externalIds": {
            "DBLP": "conf/aaai/SzegedyIVA17",
            "MAG": "2952505871",
            "ArXiv": "1602.07261",
            "DOI": "10.1609/aaai.v31i1.11231",
            "CorpusId": 1023605
        },
        "corpusId": 1023605,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/b5c26ab8767d046cb6e32d959fdf726aee89bb62",
        "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
        "abstract": "\n \n Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.\n \n",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "referenceCount": 23,
        "citationCount": 12076,
        "influentialCitationCount": 1138,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11231/11090",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly is given and several new streamlined architectures for both residual and non-residual Inception Networks are presented."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-02-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1602.07261"
        },
        "citationStyles": {
            "bibtex": "@Article{Szegedy2016Inceptionv4IA,\n author = {Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alexander A. Alemi},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},\n volume = {abs/1602.07261},\n year = {2016}\n}\n"
        }
    },
    "685_tensorized_transformer_(257m)": {
        "paperId": "62dc8ddb4907db4b889c5e93673d9b3c189d1f25",
        "externalIds": {
            "MAG": "2970213198",
            "DBLP": "journals/corr/abs-1906-09777",
            "ArXiv": "1906.09777",
            "CorpusId": 195345467
        },
        "corpusId": 195345467,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/62dc8ddb4907db4b889c5e93673d9b3c189d1f25",
        "title": "A Tensorized Transformer for Language Modeling",
        "abstract": "Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "referenceCount": 42,
        "citationCount": 128,
        "influentialCitationCount": 10,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD) with tensor train decomposition is proposed, which can not only largely compress the model parameters but also obtain performance improvements."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-06-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1906.09777"
        },
        "citationStyles": {
            "bibtex": "@Article{Ma2019ATT,\n author = {Xindian Ma and Peng Zhang and Shuai Zhang and Nan Duan and Yuexian Hou and D. Song and M. Zhou},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {A Tensorized Transformer for Language Modeling},\n volume = {abs/1906.09777},\n year = {2019}\n}\n"
        }
    },
    "687_noisynet-dueling": {
        "paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "externalIds": {
            "MAG": "2724169821",
            "ArXiv": "1706.10295",
            "DBLP": "journals/corr/FortunatoAPMOGM17",
            "CorpusId": 5176587
        },
        "corpusId": 5176587,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/3fab4d28be5ff73f42c8b307117aa8d680345ad9",
        "title": "Noisy Networks for Exploration",
        "abstract": "We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and $\\epsilon$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "referenceCount": 49,
        "citationCount": 772,
        "influentialCitationCount": 81,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that replacing the conventional exploration heuristics for A3C, DQN and dueling agents with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-06-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1706.10295"
        },
        "citationStyles": {
            "bibtex": "@Article{Fortunato2017NoisyNF,\n author = {Meire Fortunato and M. G. Azar and Bilal Piot and Jacob Menick and Ian Osband and Alex Graves and Vlad Mnih and R. Munos and D. Hassabis and O. Pietquin and C. Blundell and S. Legg},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Noisy Networks for Exploration},\n volume = {abs/1706.10295},\n year = {2017}\n}\n"
        }
    },
    "688_dropout_(timit)": {
        "paperId": "0060745e006c5f14ec326904119dca19c6545e51",
        "externalIds": {
            "ArXiv": "1207.0580",
            "MAG": "1904365287",
            "DBLP": "journals/corr/abs-1207-0580",
            "CorpusId": 14832074
        },
        "corpusId": 14832074,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0060745e006c5f14ec326904119dca19c6545e51",
        "title": "Improving neural networks by preventing co-adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",
        "venue": "arXiv.org",
        "year": 2012,
        "referenceCount": 26,
        "citationCount": 7141,
        "influentialCitationCount": 641,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2012-07-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1207.0580"
        },
        "citationStyles": {
            "bibtex": "@Article{Hinton2012ImprovingNN,\n author = {Geoffrey E. Hinton and Nitish Srivastava and A. Krizhevsky and I. Sutskever and R. Salakhutdinov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Improving neural networks by preventing co-adaptation of feature detectors},\n volume = {abs/1207.0580},\n year = {2012}\n}\n"
        }
    },
    "689_bidaf": {
        "paperId": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
        "externalIds": {
            "DBLP": "conf/iclr/SeoKFH17",
            "MAG": "2551396370",
            "ArXiv": "1611.01603",
            "CorpusId": 8535316
        },
        "corpusId": 8535316,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
        "title": "Bidirectional Attention Flow for Machine Comprehension",
        "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "referenceCount": 40,
        "citationCount": 2016,
        "influentialCitationCount": 483,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The BIDAF network is introduced, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-11-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1611.01603"
        },
        "citationStyles": {
            "bibtex": "@Article{Seo2016BidirectionalAF,\n author = {Minjoon Seo and Aniruddha Kembhavi and Ali Farhadi and Hannaneh Hajishirzi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Bidirectional Attention Flow for Machine Comprehension},\n volume = {abs/1611.01603},\n year = {2016}\n}\n"
        }
    },
    "690_persia": {
        "paperId": "25d12dc62ae62812085384196dfe465c9cfbd6e0",
        "externalIds": {
            "ArXiv": "2111.05897",
            "DBLP": "conf/kdd/LianYZWHWSLLDLL22",
            "DOI": "10.1145/3534678.3539070",
            "CorpusId": 244488711
        },
        "corpusId": 244488711,
        "publicationVenue": {
            "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
            "name": "Knowledge Discovery and Data Mining",
            "type": "conference",
            "alternate_names": [
                "KDD",
                "Knowl Discov Data Min"
            ],
            "url": "http://www.acm.org/sigkdd/"
        },
        "url": "https://www.semanticscholar.org/paper/25d12dc62ae62812085384196dfe465c9cfbd6e0",
        "title": "Persia: An Open, Hybrid System Scaling Deep Learning-based Recommenders up to 100 Trillion Parameters",
        "abstract": "Recent years have witnessed an exponential growth of model scale in deep learning-based recommender systems---from Google's 2016 model with 1 billion parameters to the latest Facebook's model with 12 trillion parameters. Significant quality boost has come with each jump of the model capacity, which makes us believe the era of 100 trillion parameters is around the corner. However, the training of such models is challenging even within industrial scale data centers. We resolve this challenge by careful co-design of both optimization algorithm and distributed system architecture. Specifically, to ensure both the training efficiency and the training accuracy, we design a novel hybrid training algorithm, where the embedding layer and the dense neural network are handled by different synchronization mechanisms; then we build a system called Persia (short for parallel recommendation training system with hybrid acceleration) to support this hybrid training algorithm. Both theoretical demonstrations and empirical studies with up to 100 trillion parameters have been conducted to justify the system design and implementation of Persia. We make Persia publicly available (at github.com/PersiaML/Persia) so that anyone can easily train a recommender model at the scale of 100 trillion parameters.",
        "venue": "Knowledge Discovery and Data Mining",
        "year": 2021,
        "referenceCount": 119,
        "citationCount": 17,
        "influentialCitationCount": 3,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2111.05897",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work design a novel hybrid training algorithm, where the embedding layer and the dense neural network are handled by different synchronization mechanisms; then it builds a system called Persia (short for parallel recommendation training system with hybrid acceleration) to support this hybridTraining algorithm."
        },
        "publicationTypes": [
            "JournalArticle",
            "Book",
            "Conference"
        ],
        "publicationDate": "2021-11-10",
        "journal": {
            "name": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
        },
        "citationStyles": {
            "bibtex": "@Article{Lian2021PersiaAO,\n author = {Xiangru Lian and Binhang Yuan and Xuefeng Zhu and Yulong Wang and Yongjun He and Honghuan Wu and Lei Sun and H. Lyu and Chengjun Liu and Xing Dong and Yiqiao Liao and Mingnan Luo and Congfei Zhang and Jingru Xie and Haonan Li and Lei Chen and Renjie Huang and Jianying Lin and Chengchun Shu and Xue-Bo Qiu and Zhishan Liu and Dongying Kong and Lei Yuan and Hai-bo Yu and Sen Yang and Ce Zhang and Ji Liu},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},\n title = {Persia: An Open, Hybrid System Scaling Deep Learning-based Recommenders up to 100 Trillion Parameters},\n year = {2021}\n}\n"
        }
    },
    "692_vall-e_x": {
        "paperId": "c40ec51ddd4145402bd95eeb3ce6977778d87881",
        "externalIds": {
            "ArXiv": "2303.03926",
            "DBLP": "journals/corr/abs-2303-03926",
            "DOI": "10.48550/arXiv.2303.03926",
            "CorpusId": 257378493
        },
        "corpusId": 257378493,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/c40ec51ddd4145402bd95eeb3ce6977778d87881",
        "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
        "abstract": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 55,
        "citationCount": 50,
        "influentialCitationCount": 5,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.03926",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experimental results show that VALL-E X can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-03-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2303.03926"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2023SpeakFL,\n author = {Zi-Hua Zhang and Long Zhou and Chengyi Wang and Sanyuan Chen and Yu Wu and Shujie Liu and Zhuo Chen and Yanqing Liu and Huaming Wang and Jinyu Li and Lei He and Sheng Zhao and Furu Wei},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling},\n volume = {abs/2303.03926},\n year = {2023}\n}\n"
        }
    },
    "693_glove_(32b)": {
        "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
        "externalIds": {
            "DBLP": "conf/emnlp/PenningtonSM14",
            "ACL": "D14-1162",
            "MAG": "2250539671",
            "DOI": "10.3115/v1/D14-1162",
            "CorpusId": 1957433
        },
        "corpusId": 1957433,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5",
        "title": "GloVe: Global Vectors for Word Representation",
        "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2014,
        "referenceCount": 32,
        "citationCount": 28882,
        "influentialCitationCount": 3738,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods and produces a vector space with meaningful substructure."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-10-01",
        "journal": {
            "pages": "1532-1543"
        },
        "citationStyles": {
            "bibtex": "@Article{Pennington2014GloVeGV,\n author = {Jeffrey Pennington and R. Socher and Christopher D. Manning},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1532-1543},\n title = {GloVe: Global Vectors for Word Representation},\n year = {2014}\n}\n"
        }
    },
    "694_egru_(wt2)": {
        "paperId": "30977307de111a3374f8afcf662e134d860e976f",
        "externalIds": {
            "ArXiv": "2206.06178",
            "DBLP": "conf/iclr/SubramoneyNS0K23",
            "CorpusId": 257482853
        },
        "corpusId": 257482853,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/30977307de111a3374f8afcf662e134d860e976f",
        "title": "Efficient recurrent architectures through activity sparsity and sparse back-propagation through time",
        "abstract": "Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at https://github.com/KhaleelKhan/EvNN/.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 28,
        "citationCount": 8,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network, which means the model achieves efficiency without compromising task performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-06-13",
        "journal": null,
        "citationStyles": {
            "bibtex": "@Article{Subramoney2022EfficientRA,\n author = {Anand Subramoney and Khaleelulla Khan Nazeer and Mark Sch\u00f6ne and C. Mayr and D. Kappel},\n booktitle = {International Conference on Learning Representations},\n title = {Efficient recurrent architectures through activity sparsity and sparse back-propagation through time},\n year = {2022}\n}\n"
        }
    },
    "695_prototypical_networks": {
        "paperId": "c269858a7bb34e8350f2442ccf37797856ae9bca",
        "externalIds": {
            "DBLP": "conf/nips/SnellSZ17",
            "ArXiv": "1703.05175",
            "MAG": "2950537964",
            "CorpusId": 309759
        },
        "corpusId": 309759,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/c269858a7bb34e8350f2442ccf37797856ae9bca",
        "title": "Prototypical Networks for Few-shot Learning",
        "abstract": "We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "referenceCount": 38,
        "citationCount": 6287,
        "influentialCitationCount": 1471,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes Prototypical Networks for few-shot classification, and provides an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-03-15",
        "journal": {
            "pages": "4077-4087"
        },
        "citationStyles": {
            "bibtex": "@Article{Snell2017PrototypicalNF,\n author = {Jake Snell and Kevin Swersky and R. Zemel},\n booktitle = {Neural Information Processing Systems},\n pages = {4077-4087},\n title = {Prototypical Networks for Few-shot Learning},\n year = {2017}\n}\n"
        }
    },
    "696_yolov2": {
        "paperId": "7d39d69b23424446f0400ef603b2e3e22d0309d6",
        "externalIds": {
            "ArXiv": "1612.08242",
            "MAG": "2951433694",
            "DBLP": "journals/corr/RedmonF16",
            "DOI": "10.1109/CVPR.2017.690",
            "CorpusId": 786357
        },
        "corpusId": 786357,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6",
        "title": "YOLO9000: Better, Faster, Stronger",
        "abstract": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 20,
        "citationCount": 12957,
        "influentialCitationCount": 1381,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.08242",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories, is introduced and a method to jointly train on object detection and classification is proposed, both novel and drawn from prior work."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-12-25",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "6517-6525"
        },
        "citationStyles": {
            "bibtex": "@Article{Redmon2016YOLO9000BF,\n author = {Joseph Redmon and Ali Farhadi},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {6517-6525},\n title = {YOLO9000: Better, Faster, Stronger},\n year = {2016}\n}\n"
        }
    },
    "697_td-gammon": {
        "paperId": "646ff15fbd38f8c4e2b099ad09e4570179709c73",
        "externalIds": {
            "MAG": "2103626435",
            "DBLP": "conf/nips/Tesauro91",
            "DOI": "10.1007/BF00992697",
            "CorpusId": 996637
        },
        "corpusId": 996637,
        "publicationVenue": {
            "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
            "name": "Machine-mediated learning",
            "type": "journal",
            "alternate_names": [
                "Mach learn",
                "Machine Learning",
                "Mach Learn"
            ],
            "issn": "0732-6718",
            "alternate_issns": [
                "0885-6125"
            ],
            "url": "http://www.springer.com/computer/artificial/journal/10994",
            "alternate_urls": [
                "https://link.springer.com/journal/10994",
                "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/646ff15fbd38f8c4e2b099ad09e4570179709c73",
        "title": "Practical issues in temporal difference learning",
        "abstract": null,
        "venue": "Machine-mediated learning",
        "year": 1992,
        "referenceCount": 32,
        "citationCount": 696,
        "influentialCitationCount": 45,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/BF00992697.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Psychology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which surpasses comparable networks trained on a massive human expert data set."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1992-05-01",
        "journal": {
            "name": "Machine Learning",
            "pages": "257-277",
            "volume": "8"
        },
        "citationStyles": {
            "bibtex": "@Article{Tesauro1992PracticalII,\n author = {G. Tesauro},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {257-277},\n title = {Practical issues in temporal difference learning},\n volume = {8},\n year = {1992}\n}\n"
        }
    },
    "698_adclicknet": {
        "paperId": "daf9ed5dc6c6bad5367d7fd8561527da30e9b8dd",
        "externalIds": {
            "DBLP": "conf/kdd/HePJXLXSAHBC14",
            "MAG": "2076618162",
            "DOI": "10.1145/2648584.2648589",
            "CorpusId": 2999385
        },
        "corpusId": 2999385,
        "publicationVenue": {
            "id": "e1ab4e9b-5586-4645-bb70-e53e42367ffe",
            "name": "International Workshop on Data Mining for Online Advertising",
            "type": "conference",
            "alternate_names": [
                "ADKDD",
                "Int Workshop Data Min Online Advert"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/daf9ed5dc6c6bad5367d7fd8561527da30e9b8dd",
        "title": "Practical Lessons from Predicting Clicks on Ads at Facebook",
        "abstract": "Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central to most online advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3%, an improvement with significant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surprisingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic regression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with.",
        "venue": "International Workshop on Data Mining for Online Advertising",
        "year": 2014,
        "referenceCount": 12,
        "citationCount": 812,
        "influentialCitationCount": 62,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Business",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper introduces a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3%, an improvement with significant impact to the overall system performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-08-24",
        "journal": {
            "pages": "5:1-5:9"
        },
        "citationStyles": {
            "bibtex": "@Article{He2014PracticalLF,\n author = {Xinran He and Junfeng Pan and Ou Jin and Tianbing Xu and Bo Liu and Tao Xu and Yanxin Shi and Antoine Atallah and Ralf Herbrich and Stuart Bowers and J. Q. Candela},\n booktitle = {International Workshop on Data Mining for Online Advertising},\n pages = {5:1-5:9},\n title = {Practical Lessons from Predicting Clicks on Ads at Facebook},\n year = {2014}\n}\n"
        }
    },
    "702_nettalk_(transcription)": {
        "paperId": "de996c32045df6f7b404dda2a753b6a9becf3c08",
        "externalIds": {
            "MAG": "1580142630",
            "DBLP": "journals/compsys/SejnowskiR87",
            "CorpusId": 12926318
        },
        "corpusId": 12926318,
        "publicationVenue": {
            "id": "7f30f248-48d1-479a-a293-a82c1124630c",
            "name": "Complex Systems",
            "type": "journal",
            "alternate_names": [
                "Complex Syst"
            ],
            "issn": "0891-2513",
            "url": "https://www.complex-systems.com/",
            "alternate_urls": [
                "http://www.complex-systems.com/archives.html",
                "http://www.complex-systems.com/index.html",
                "http://www.complex-systems.com/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/de996c32045df6f7b404dda2a753b6a9becf3c08",
        "title": "Parallel Networks that Learn to Pronounce English Text",
        "abstract": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations.",
        "venue": "Complex Systems",
        "year": 1987,
        "referenceCount": 93,
        "citationCount": 1887,
        "influentialCitationCount": 76,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "H hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units, which suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "Complex Syst.",
            "volume": "1"
        },
        "citationStyles": {
            "bibtex": "@Article{Sejnowski1987ParallelNT,\n author = {T. Sejnowski and Charles R. Rosenberg},\n booktitle = {Complex Systems},\n journal = {Complex Syst.},\n title = {Parallel Networks that Learn to Pronounce English Text},\n volume = {1},\n year = {1987}\n}\n"
        }
    },
    "703_listen,_attend_and_spell": {
        "paperId": "3056add22b20e3361c38c0472d294a79d4031cb4",
        "externalIds": {
            "DBLP": "conf/icassp/ChanJLV16",
            "MAG": "2327501763",
            "ArXiv": "1508.01211",
            "DOI": "10.1109/ICASSP.2016.7472621",
            "CorpusId": 18165915
        },
        "corpusId": 18165915,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/3056add22b20e3361c38c0472d294a79d4031cb4",
        "title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",
        "abstract": "We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1% without a dictionary or an external language model and 10.3% with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0% on the same set.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2015,
        "referenceCount": 54,
        "citationCount": 2047,
        "influentialCitationCount": 165,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers is presented."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-08-05",
        "journal": {
            "name": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "pages": "4960-4964"
        },
        "citationStyles": {
            "bibtex": "@Article{Chan2015ListenAA,\n author = {William Chan and N. Jaitly and Quoc V. Le and O. Vinyals},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {4960-4964},\n title = {Listen, attend and spell: A neural network for large vocabulary conversational speech recognition},\n year = {2015}\n}\n"
        }
    },
    "705_mms-1b": {
        "paperId": "0f416c637a5a78435e6b12ebf1ce891224de0edc",
        "externalIds": {
            "ArXiv": "2305.13516",
            "DBLP": "journals/corr/abs-2305-13516",
            "DOI": "10.48550/arXiv.2305.13516",
            "CorpusId": 258841617
        },
        "corpusId": 258841617,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0f416c637a5a78435e6b12ebf1ce891224de0edc",
        "title": "Scaling Speech Technology to 1, 000+ Languages",
        "abstract": "Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 108,
        "citationCount": 88,
        "influentialCitationCount": 8,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.13516",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experiments show that the multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-05-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2305.13516"
        },
        "citationStyles": {
            "bibtex": "@Article{Pratap2023ScalingST,\n author = {Vineel Pratap and Andros Tjandra and Bowen Shi and Paden Tomasello and Arun Babu and Sayani Kundu and A. Elkahky and Zhaoheng Ni and Apoorv Vyas and Maryam Fazel-Zarandi and Alexei Baevski and Yossi Adi and Xiaohui Zhang and Wei-Ning Hsu and Alexis Conneau and Michael Auli},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Scaling Speech Technology to 1, 000+ Languages},\n volume = {abs/2305.13516},\n year = {2023}\n}\n"
        }
    },
    "706_codex": {
        "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "externalIds": {
            "DBLP": "journals/corr/abs-2107-03374",
            "ArXiv": "2107.03374",
            "CorpusId": 235755472
        },
        "corpusId": 235755472,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code",
        "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 127,
        "citationCount": 2262,
        "influentialCitationCount": 480,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that repeated sampling from the GPT language model is a surprisingly effective strategy for producing working solutions to difficult prompts, and the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics are discussed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-07-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2107.03374"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2021EvaluatingLL,\n author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and S. Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and F. Such and D. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and S. Balaji and Shantanu Jain and A. Carr and J. Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and M. Knight and Miles Brundage and Mira Murati and Katie Mayer and P. Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and I. Sutskever and Wojciech Zaremba},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Evaluating Large Language Models Trained on Code},\n volume = {abs/2107.03374},\n year = {2021}\n}\n"
        }
    },
    "707_memformer_(4_encoder_+_16_decoder)": {
        "paperId": "67ee20536c30a225b86902af2f091e28e5e19b40",
        "externalIds": {
            "ArXiv": "2010.06891",
            "DBLP": "conf/ijcnlp/WuLQGGY22",
            "CorpusId": 248157404
        },
        "corpusId": 248157404,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/67ee20536c30a225b86902af2f091e28e5e19b40",
        "title": "Memformer: A Memory-Augmented Transformer for Sequence Modeling",
        "abstract": "Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.",
        "venue": "AACL/IJCNLP",
        "year": 2020,
        "referenceCount": 22,
        "citationCount": 23,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information, and proposes a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back- Propagation through time with a significantly reduced memory requirement."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-10-14",
        "journal": {
            "pages": "308-318"
        },
        "citationStyles": {
            "bibtex": "@Article{Wu2020MemformerAM,\n author = {Qingyang Wu and Zhenzhong Lan and Kun Qian and Jing Gu and A. Geramifard and Zhou Yu},\n booktitle = {AACL/IJCNLP},\n pages = {308-318},\n title = {Memformer: A Memory-Augmented Transformer for Sequence Modeling},\n year = {2020}\n}\n"
        }
    },
    "711_fully_convolutional_networks": {
        "paperId": "6fc6803df5f9ae505cae5b2f178ade4062c768d0",
        "externalIds": {
            "DBLP": "journals/corr/LongSD14",
            "ArXiv": "1605.06211",
            "MAG": "2952632681",
            "DOI": "10.1109/CVPR.2015.7298965",
            "CorpusId": 1629541
        },
        "corpusId": 1629541,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/6fc6803df5f9ae505cae5b2f178ade4062c768d0",
        "title": "Fully convolutional networks for semantic segmentation",
        "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "referenceCount": 69,
        "citationCount": 32989,
        "influentialCitationCount": 4174,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1411.4038",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-11-14",
        "journal": {
            "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "3431-3440"
        },
        "citationStyles": {
            "bibtex": "@Article{Shelhamer2014FullyCN,\n author = {Evan Shelhamer and Jonathan Long and Trevor Darrell},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {3431-3440},\n title = {Fully convolutional networks for semantic segmentation},\n year = {2014}\n}\n"
        }
    },
    "712_nmm(lstm+rnn)": {
        "paperId": "31eb323786d7b7379b53b8d747eece17b12b1aac",
        "externalIds": {
            "DBLP": "journals/corr/abs-1708-06989",
            "ArXiv": "1708.06989",
            "MAG": "2721415098",
            "DOI": "10.1109/ICASSP.2017.7953250",
            "CorpusId": 20496081
        },
        "corpusId": 20496081,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/31eb323786d7b7379b53b8d747eece17b12b1aac",
        "title": "A Neural Network approach for mixing language models",
        "abstract": "The performance of Neural Network (NN)-based language models is steadily improving due to the emergence of new architectures, which are able to learn different natural language characteristics. This paper presents a novel framework, which shows that a significant improvement can be achieved by combining different existing heterogeneous models in a single architecture. This is done through 1) a feature layer, which separately learns different NN-based models and 2) a mixture layer, which merges the resulting model features. In doing so, this architecture benefits from the learning capabilities of each model with no noticeable increase in the number of model parameters or the training time. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2017,
        "referenceCount": 20,
        "citationCount": 7,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1708.06989",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel framework is presented, which shows that a significant improvement can be achieved by combining different existing heterogeneous models in a single architecture through 1) a feature layer, which separately learns different NN-based models and 2) a mixture layer,Which merges the resulting model features."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-03-01",
        "journal": {
            "name": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "pages": "5710-5714"
        },
        "citationStyles": {
            "bibtex": "@Article{Oualil2017ANN,\n author = {Youssef Oualil and D. Klakow},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {5710-5714},\n title = {A Neural Network approach for mixing language models},\n year = {2017}\n}\n"
        }
    },
    "713_multiscale_deformable_part_model": {
        "paperId": "860a9d55d87663ca88e74b3ca357396cd51733d0",
        "externalIds": {
            "MAG": "2120419212",
            "DBLP": "conf/cvpr/FelzenszwalbMR08",
            "DOI": "10.1109/CVPR.2008.4587597",
            "CorpusId": 14327585
        },
        "corpusId": 14327585,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/860a9d55d87663ca88e74b3ca357396cd51733d0",
        "title": "A discriminatively trained, multiscale, deformable part model",
        "abstract": "This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose.",
        "venue": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
        "year": 2008,
        "referenceCount": 28,
        "citationCount": 2877,
        "influentialCitationCount": 308,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://people.csail.mit.edu/torralba/courses/6.870/papers/FelzenszwalbMcAllesterRamanan.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A discriminatively trained, multiscale, deformable part model for object detection, which achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge and outperforms the best results in the 2007 challenge in ten out of twenty categories."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2008-06-23",
        "journal": {
            "name": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
            "pages": "1-8"
        },
        "citationStyles": {
            "bibtex": "@Article{Felzenszwalb2008ADT,\n author = {Pedro F. Felzenszwalb and David A. McAllester and Deva Ramanan},\n booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2008 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1-8},\n title = {A discriminatively trained, multiscale, deformable part model},\n year = {2008}\n}\n"
        }
    },
    "714_rnn-speedup": {
        "paperId": "07ca885cb5cc4328895bfaec9ab752d5801b14cd",
        "externalIds": {
            "DBLP": "conf/icassp/MikolovKBCK11",
            "MAG": "2171928131",
            "DOI": "10.1109/ICASSP.2011.5947611",
            "CorpusId": 14850173
        },
        "corpusId": 14850173,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/07ca885cb5cc4328895bfaec9ab752d5801b14cd",
        "title": "Extensions of recurrent neural network language model",
        "abstract": "We present several modifications of the original recurrent neural network language model (RNN LM).While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2011,
        "referenceCount": 19,
        "citationCount": 1542,
        "influentialCitationCount": 111,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Several modifications of the original recurrent neural network language model are presented, showing approaches that lead to more than 15 times speedup for both training and testing phases and possibilities how to reduce the amount of parameters in the model."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2011-05-22",
        "journal": {
            "name": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "pages": "5528-5531"
        },
        "citationStyles": {
            "bibtex": "@Article{Mikolov2011ExtensionsOR,\n author = {Tomas Mikolov and Stefan Kombrink and L. Burget and J. \u010cernock\u00fd and S. Khudanpur},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {5528-5531},\n title = {Extensions of recurrent neural network language model},\n year = {2011}\n}\n"
        }
    },
    "715_fairseq-dense_13b": {
        "paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170",
        "externalIds": {
            "ArXiv": "2112.10684",
            "DBLP": "conf/emnlp/ArtetxeBGMOSLDI22",
            "DOI": "10.18653/v1/2022.emnlp-main.804",
            "CorpusId": 245334345
        },
        "corpusId": 245334345,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/fb01415a0decfa3f3d6339930e95028ae1ff4170",
        "title": "Efficient Large Scale Language Modeling with Mixtures of Experts",
        "abstract": "Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using $\\sim$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "referenceCount": 92,
        "citationCount": 120,
        "influentialCitationCount": 18,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.emnlp-main.804.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-12-20",
        "journal": {
            "pages": "11699-11732"
        },
        "citationStyles": {
            "bibtex": "@Article{Artetxe2021EfficientLS,\n author = {Mikel Artetxe and Shruti Bhosale and Naman Goyal and Todor Mihaylov and Myle Ott and Sam Shleifer and Xi Victoria Lin and Jingfei Du and Srini Iyer and Ramakanth Pasunuru and Giridhar Anantharaman and Xian Li and Shuohui Chen and H. Ak\u0131n and Mandeep Baines and Louis Martin and Xing Zhou and Punit Singh Koura and Brian O'Horo and Jeff Wang and Luke Zettlemoyer and Mona T. Diab and Zornitsa Kozareva and Ves Stoyanov},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {11699-11732},\n title = {Efficient Large Scale Language Modeling with Mixtures of Experts},\n year = {2021}\n}\n"
        }
    },
    "716_metalm": {
        "paperId": "a8fd9c1625011741f74401ff9bdc1c584e25c86d",
        "externalIds": {
            "ArXiv": "2206.06336",
            "DBLP": "journals/corr/abs-2206-06336",
            "DOI": "10.48550/arXiv.2206.06336",
            "CorpusId": 249626024
        },
        "corpusId": 249626024,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/a8fd9c1625011741f74401ff9bdc1c584e25c86d",
        "title": "Language Models are General-Purpose Interfaces",
        "abstract": "Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 132,
        "citationCount": 68,
        "influentialCitationCount": 5,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2206.06336",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes to use language models as a general-purpose interface to various foundation models to jointly pretrain the interface and the modular encoders, and subsume the advantages and capabilities from both causal and non-causal modeling."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-06-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2206.06336"
        },
        "citationStyles": {
            "bibtex": "@Article{Hao2022LanguageMA,\n author = {Y. Hao and Haoyu Song and Li Dong and Shaohan Huang and Zewen Chi and Wenhui Wang and Shuming Ma and Furu Wei},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Language Models are General-Purpose Interfaces},\n volume = {abs/2206.06336},\n year = {2022}\n}\n"
        }
    },
    "717_rq-transformer_(3.8b_params_imagenet_dataset)_copy": {
        "paperId": "8fbc2d349d3d0945efa5e92fd3713734ce63d19e",
        "externalIds": {
            "ArXiv": "2203.01941",
            "DBLP": "journals/corr/abs-2203-01941",
            "DOI": "10.1109/CVPR52688.2022.01123",
            "CorpusId": 247244535
        },
        "corpusId": 247244535,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/8fbc2d349d3d0945efa5e92fd3713734ce63d19e",
        "title": "Autoregressive Image Generation using Residual Quantization",
        "abstract": "For autoregressive (AR) modeling of high-resolution images, vector quantization (VQ) represents an image as a sequence of discrete codes. A short sequence length is important for an AR model to reduce its computational costs to consider long-range interactions of codes. However, we postulate that previous VQ cannot shorten the code sequence and generate high-fidelity images together in terms of the rate-distortion trade-off. In this study, we propose the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images. Given a fixed codebook size, RQ-VAE can precisely approximate a feature map of an image and represent the image as a stacked map of discrete codes. Then, RQ-Transformer learns to predict the quantized feature vector at the next position by predicting the next stack of codes. Thanks to the precise approximation of RQ-VAE, we can represent a $256\\times 256$ image as $8\\times 8$ resolution of the feature map, and RQ-Transformer can efficiently reduce the computational costs. Consequently, our framework out-performs the existing AR models on various benchmarks of unconditional and conditional image generation. Our approach also has a significantly faster sampling speed than previous AR models to generate high-quality images.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "referenceCount": 53,
        "citationCount": 99,
        "influentialCitationCount": 18,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2203.01941",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This study proposes the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images and out-performs the existing AR models on various benchmarks of unconditional and conditional image generation."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-03-03",
        "journal": {
            "name": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "11513-11522"
        },
        "citationStyles": {
            "bibtex": "@Article{Lee2022AutoregressiveIG,\n author = {Doyup Lee and Chiheon Kim and Saehoon Kim and Minsu Cho and Wook-Shin Han},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {11513-11522},\n title = {Autoregressive Image Generation using Residual Quantization},\n year = {2022}\n}\n"
        }
    },
    "718_vit-g_14": {
        "paperId": "2a805d0e1b067444a554c5169d189fa1f649f411",
        "externalIds": {
            "ArXiv": "2106.04560",
            "DBLP": "journals/corr/abs-2106-04560",
            "DOI": "10.1109/CVPR52688.2022.01179",
            "CorpusId": 235367962
        },
        "corpusId": 235367962,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/2a805d0e1b067444a554c5169d189fa1f649f411",
        "title": "Scaling Vision Transformers",
        "abstract": "Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "referenceCount": 54,
        "citationCount": 695,
        "influentialCitationCount": 70,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2106.04560",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A ViT model with two billion parameters is successfully trained, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy and performs well for few-shot transfer."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-06-08",
        "journal": {
            "name": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "1204-1213"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhai2021ScalingVT,\n author = {Xiaohua Zhai and Alexander Kolesnikov and N. Houlsby and Lucas Beyer},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1204-1213},\n title = {Scaling Vision Transformers},\n year = {2021}\n}\n"
        }
    },
    "720_self_organizing_system": {
        "paperId": "616b9f5b957de2249ed1ae433b9be1bf1d45cdef",
        "externalIds": {
            "DBLP": "conf/aieeire/ClarkF55",
            "MAG": "2062231579",
            "DOI": "10.1145/1455292.1455309",
            "CorpusId": 16523575
        },
        "corpusId": 16523575,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/616b9f5b957de2249ed1ae433b9be1bf1d45cdef",
        "title": "Generalization of pattern recognition in a self-organizing system",
        "abstract": "A self-organizing system reported upon earlier is briefly described. Two further experiments to determine its properties have been carried out. The first demonstrates that self-organization still takes place even if the input patterns are subjected to considerable random variation. The second experiment indicates that, after organization with the usual fixed patterns, the system classifies other input patterns statistically according to a simple preponderance criterion. Significance of this result as a generalization in pattern recognition is discussed. Some remarks are made on methods of simulation of such systems and their relation to computer design.",
        "venue": "AFIPS '55 (Western)",
        "year": 1955,
        "referenceCount": 1,
        "citationCount": 40,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.1145/1455292.1455309",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The first experiment demonstrates that self-organization still takes place even if the input patterns are subjected to considerable random variation, and the second indicates that the system classifies other input patterns statistically according to a simple preponderance criterion."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1955-03-01",
        "journal": {
            "pages": "86-91"
        },
        "citationStyles": {
            "bibtex": "@Article{Clark1955GeneralizationOP,\n author = {W. Clark and B. G. Farley},\n booktitle = {AFIPS '55 (Western)},\n pages = {86-91},\n title = {Generalization of pattern recognition in a self-organizing system},\n year = {1955}\n}\n"
        }
    },
    "723_alphazero": {
        "paperId": "38fb1902c6a2ab4f767d4532b28a92473ea737aa",
        "externalIds": {
            "DBLP": "journals/corr/abs-1712-01815",
            "ArXiv": "1712.01815",
            "MAG": "2772709170",
            "CorpusId": 33081038
        },
        "corpusId": 33081038,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/38fb1902c6a2ab4f767d4532b28a92473ea737aa",
        "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
        "abstract": "The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",
        "venue": "arXiv.org",
        "year": 2017,
        "referenceCount": 48,
        "citationCount": 1402,
        "influentialCitationCount": 124,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper generalises the approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains, and convincingly defeated a world-champion program in each case."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-12-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1712.01815"
        },
        "citationStyles": {
            "bibtex": "@Article{Silver2017MasteringCA,\n author = {David Silver and T. Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and A. Guez and Marc Lanctot and L. Sifre and D. Kumaran and T. Graepel and T. Lillicrap and K. Simonyan and D. Hassabis},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},\n volume = {abs/1712.01815},\n year = {2017}\n}\n"
        }
    },
    "725_rubik's_cube_adr_robot": {
        "paperId": "320b227027030fc291de2896fc3c6da49d7614be",
        "externalIds": {
            "DBLP": "journals/corr/abs-1910-07113",
            "MAG": "2981030070",
            "ArXiv": "1910.07113",
            "CorpusId": 204734323
        },
        "corpusId": 204734323,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/320b227027030fc291de2896fc3c6da49d7614be",
        "title": "Solving Rubik's Cube with a Robot Hand",
        "abstract": "We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: this https URL",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 127,
        "citationCount": 966,
        "influentialCitationCount": 48,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot, made possible by a novel algorithm, which is called automatic domain randomization (ADR), and a robot platform built for machine learning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-10-16",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1910.07113"
        },
        "citationStyles": {
            "bibtex": "@Article{OpenAI2019SolvingRC,\n author = {OpenAI and Ilge Akkaya and Marcin Andrychowicz and Maciek Chociej and Mateusz Litwin and Bob McGrew and Arthur Petron and Alex Paino and Matthias Plappert and Glenn Powell and Raphael Ribas and Jonas Schneider and N. Tezak and Jerry Tworek and P. Welinder and Lilian Weng and Qiming Yuan and Wojciech Zaremba and Lei M. Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Solving Rubik's Cube with a Robot Hand},\n volume = {abs/1910.07113},\n year = {2019}\n}\n"
        }
    },
    "726_bigssl": {
        "paperId": "6fe21b01d2202defb8fcd75c40f306a88bd385dc",
        "externalIds": {
            "DBLP": "journals/jstsp/ZhangPHQGSJXHWZ22",
            "ArXiv": "2109.13226",
            "DOI": "10.1109/JSTSP.2022.3182537",
            "CorpusId": 237941095
        },
        "corpusId": 237941095,
        "publicationVenue": {
            "id": "e93ebb7d-cfa6-4361-8051-3c6dff3eed1f",
            "name": "IEEE Journal on Selected Topics in Signal Processing",
            "type": "journal",
            "alternate_names": [
                "IEEE J Sel Top Signal Process",
                "IEEE Journal of Selected Topics in Signal Processing"
            ],
            "issn": "1932-4553",
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=4200690"
        },
        "url": "https://www.semanticscholar.org/paper/6fe21b01d2202defb8fcd75c40f306a88bd385dc",
        "title": "BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition",
        "abstract": "We summarize the results of a host of efforts using giant automatic speech recognition (ASR) models pre-trained using large, diverse unlabeled datasets containing approximately a million hours of audio. We find that the combination of pre-training, self-training and scaling up model size greatly increases data efficiency, even for extremely large tasks with tens of thousands of hours of labeled data. In particular, on an ASR task with 34 k hours of labeled data, by fine-tuning an 8 billion parameter pre-trained Conformer model we can match state-of-the-art (SoTA) performance with only 3% of the training data and significantly improve SoTA with the full training set. We also report on the universal benefits gained from using big pre-trained and self-trained models for a large set of downstream tasks that cover a wide range of speech domains and span multiple orders of magnitudes of dataset sizes, including obtaining SoTA performance on many public benchmarks. In addition, we utilize the learned representation of pre-trained networks to achieve SoTA results on non-ASR tasks.",
        "venue": "IEEE Journal on Selected Topics in Signal Processing",
        "year": 2021,
        "referenceCount": 105,
        "citationCount": 123,
        "influentialCitationCount": 9,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2109.13226",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Engineering",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that the combination of pre-training, self-training and scaling up model size greatly increases data efficiency, even for extremely large tasks with tens of thousands of hours of labeled data."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-09-27",
        "journal": {
            "name": "IEEE Journal of Selected Topics in Signal Processing",
            "pages": "1519-1532",
            "volume": "16"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2021BigSSLET,\n author = {Yu Zhang and Daniel S. Park and Wei Han and James Qin and Anmol Gulati and Joel Shor and A. Jansen and Yuanzhong Xu and Yanping Huang and Shibo Wang and Zongwei Zhou and Bo Li and Min Ma and William Chan and Jiahui Yu and Yongqiang Wang and Liangliang Cao and K. Sim and B. Ramabhadran and Tara N. Sainath and Franccoise Beaufays and Zhifeng Chen and Quoc V. Le and Chung-Cheng Chiu and Ruoming Pang and Yonghui Wu},\n booktitle = {IEEE Journal on Selected Topics in Signal Processing},\n journal = {IEEE Journal of Selected Topics in Signal Processing},\n pages = {1519-1532},\n title = {BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition},\n volume = {16},\n year = {2021}\n}\n"
        }
    },
    "727_web_mining_+_decision_tree_recommender": {
        "paperId": "4a8a8ca8dfb28a7ee9689776a1117e317011f167",
        "externalIds": {
            "DBLP": "journals/eswa/ChoKK02",
            "MAG": "2026091747",
            "DOI": "10.1016/S0957-4174(02)00052-0",
            "CorpusId": 206110440
        },
        "corpusId": 206110440,
        "publicationVenue": {
            "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
            "name": "Expert systems with applications",
            "type": "journal",
            "alternate_names": [
                "Expert syst appl",
                "Expert Systems With Applications",
                "Expert Syst Appl"
            ],
            "issn": "0957-4174",
            "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
            "alternate_urls": [
                "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                "http://www.sciencedirect.com/science/journal/09574174"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/4a8a8ca8dfb28a7ee9689776a1117e317011f167",
        "title": "A personalized recommender system based on web usage mining and decision tree induction",
        "abstract": null,
        "venue": "Expert systems with applications",
        "year": 2002,
        "referenceCount": 36,
        "citationCount": 482,
        "influentialCitationCount": 15,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Business",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A personalized recommendation methodology is suggested by which to get further effectiveness and quality of recommendations when applied to an Internet shopping mall, based on a variety of data mining techniques."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2002-10-01",
        "journal": {
            "name": "Expert Syst. Appl.",
            "pages": "329-342",
            "volume": "23"
        },
        "citationStyles": {
            "bibtex": "@Article{Cho2002APR,\n author = {Y. Cho and Jae Kyeong Kim and S. Kim},\n booktitle = {Expert systems with applications},\n journal = {Expert Syst. Appl.},\n pages = {329-342},\n title = {A personalized recommender system based on web usage mining and decision tree induction},\n volume = {23},\n year = {2002}\n}\n"
        }
    },
    "728_gpu_implementation_of_neural_networks": {
        "paperId": "80664dab16a1f18ce1998e38a03f080c5e98363a",
        "externalIds": {
            "MAG": "2209146526",
            "DBLP": "journals/pr/OhJ04",
            "DOI": "10.1016/j.patcog.2004.01.013",
            "CorpusId": 12600784
        },
        "corpusId": 12600784,
        "publicationVenue": {
            "id": "266f640f-003e-453b-ab76-57e4053252f8",
            "name": "Pattern Recognition",
            "type": "journal",
            "alternate_names": [
                "Pattern Recognit"
            ],
            "issn": "0031-3203",
            "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
            "alternate_urls": [
                "https://www.journals.elsevier.com/pattern-recognition",
                "http://www.sciencedirect.com/science/journal/00313203"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/80664dab16a1f18ce1998e38a03f080c5e98363a",
        "title": "GPU implementation of neural networks",
        "abstract": null,
        "venue": "Pattern Recognition",
        "year": 2004,
        "referenceCount": 2,
        "citationCount": 400,
        "influentialCitationCount": 21,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Preliminary results produced a 20-fold performance enhancement using an ATI RADEON 9700 PRO board, and further research areas include benchmarking the performance with various hardware and GPU-aware learning algorithms."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2004-06-01",
        "journal": {
            "name": "Pattern Recognit.",
            "pages": "1311-1314",
            "volume": "37"
        },
        "citationStyles": {
            "bibtex": "@Article{Oh2004GPUIO,\n author = {Kyoungsu Oh and K. Jung},\n booktitle = {Pattern Recognition},\n journal = {Pattern Recognit.},\n pages = {1311-1314},\n title = {GPU implementation of neural networks},\n volume = {37},\n year = {2004}\n}\n"
        }
    },
    "729_relu_(norb)": {
        "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
        "externalIds": {
            "MAG": "1665214252",
            "DBLP": "conf/icml/NairH10",
            "CorpusId": 15539264
        },
        "corpusId": 15539264,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f",
        "title": "Rectified Linear Units Improve Restricted Boltzmann Machines",
        "abstract": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.",
        "venue": "International Conference on Machine Learning",
        "year": 2010,
        "referenceCount": 21,
        "citationCount": 16014,
        "influentialCitationCount": 1075,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Restricted Boltzmann machines were developed using binary stochastic hidden units that learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2010-06-21",
        "journal": {
            "pages": "807-814"
        },
        "citationStyles": {
            "bibtex": "@Article{Nair2010RectifiedLU,\n author = {Vinod Nair and Geoffrey E. Hinton},\n booktitle = {International Conference on Machine Learning},\n pages = {807-814},\n title = {Rectified Linear Units Improve Restricted Boltzmann Machines},\n year = {2010}\n}\n"
        }
    },
    "730_palm_(540b)": {
        "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "externalIds": {
            "ArXiv": "2204.02311",
            "DBLP": "journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23",
            "CorpusId": 247951931
        },
        "corpusId": 247951931,
        "publicationVenue": {
            "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
            "name": "Journal of machine learning research",
            "type": "journal",
            "alternate_names": [
                "Journal of Machine Learning Research",
                "J mach learn res",
                "J Mach Learn Res"
            ],
            "issn": "1532-4435",
            "alternate_issns": [
                "1533-7928"
            ],
            "url": "http://www.ai.mit.edu/projects/jmlr/",
            "alternate_urls": [
                "http://jmlr.csail.mit.edu/",
                "http://www.jmlr.org/",
                "http://portal.acm.org/affiliated/jmlr"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways",
        "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
        "venue": "Journal of machine learning research",
        "year": 2022,
        "referenceCount": 173,
        "citationCount": 3313,
        "influentialCitationCount": 267,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A 540-billion parameter, densely activated, Transformer language model, which is called PaLM achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-04-05",
        "journal": {
            "name": "J. Mach. Learn. Res.",
            "pages": "240:1-240:113",
            "volume": "24"
        },
        "citationStyles": {
            "bibtex": "@Article{Chowdhery2022PaLMSL,\n author = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and B. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and M. Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and H. Michalewski and Xavier Garc\u00eda and Vedant Misra and Kevin Robinson and L. Fedus and Denny Zhou and Daphne Ippolito and D. Luan and Hyeontaek Lim and Barret Zoph and A. Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and R. Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark D\u00edaz and Orhan Firat and Michele Catasta and Jason Wei and K. Meier-Hellstern and D. Eck and J. Dean and Slav Petrov and Noah Fiedel},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {240:1-240:113},\n title = {PaLM: Scaling Language Modeling with Pathways},\n volume = {24},\n year = {2022}\n}\n"
        }
    },
    "731_transformer-xl_define_(141m)": {
        "paperId": "841d43cf4015042a4ee45745c5b6f2c59c184da5",
        "externalIds": {
            "DBLP": "conf/iclr/MehtaKRH20",
            "MAG": "2990215755",
            "ArXiv": "1911.12385",
            "CorpusId": 208513914
        },
        "corpusId": 208513914,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/841d43cf4015042a4ee45745c5b6f2c59c184da5",
        "title": "DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling",
        "abstract": "For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, we describe a new method, DeFINE, for learning deep token representations efficiently. Our architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of Transformer-XL by half with minimal impact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4 points with a 17% reduction in parameters, achieving comparable performance to state-of-the-art methods with fewer parameters. For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 46,
        "citationCount": 20,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new method is described, DeFINE, for learning deep token representations efficiently, which uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-11-27",
        "journal": {
            "name": "arXiv: Computation and Language",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Mehta2019DeFINEDF,\n author = {Sachin Mehta and Rik Koncel-Kedziorski and Mohammad Rastegari and Hannaneh Hajishirzi},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Computation and Language},\n title = {DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling},\n year = {2019}\n}\n"
        }
    },
    "732_pangu-weather": {
        "paperId": "9904557ddf52402170da288cf3a88a4f28847f0d",
        "externalIds": {
            "DBLP": "journals/nature/BiXZCG023",
            "PubMedCentral": "10356604",
            "DOI": "10.1038/s41586-023-06185-3",
            "CorpusId": 259355870,
            "PubMed": "37407823"
        },
        "corpusId": 259355870,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/9904557ddf52402170da288cf3a88a4f28847f0d",
        "title": "Accurate medium-range global weather forecasting with 3D neural networks",
        "abstract": null,
        "venue": "Nature",
        "year": 2023,
        "referenceCount": 48,
        "citationCount": 81,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.nature.com/articles/s41586-023-06185-3.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Environmental Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-07-05",
        "journal": {
            "name": "Nature",
            "pages": "533 - 538",
            "volume": "619"
        },
        "citationStyles": {
            "bibtex": "@Article{Bi2023AccurateMG,\n author = {Kaifeng Bi and Lingxi Xie and Hengheng Zhang and Xin Chen and Xiaotao Gu and Q. Tian},\n booktitle = {Nature},\n journal = {Nature},\n pages = {533 - 538},\n title = {Accurate medium-range global weather forecasting with 3D neural networks},\n volume = {619},\n year = {2023}\n}\n"
        }
    },
    "733_cpc_v2": {
        "paperId": "1cae417456711c4da184f5efcd1b7464a7a0661a",
        "externalIds": {
            "MAG": "2944828972",
            "ArXiv": "1905.09272",
            "DBLP": "conf/icml/Henaff20",
            "CorpusId": 162168848
        },
        "corpusId": 162168848,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/1cae417456711c4da184f5efcd1b7464a7a0661a",
        "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding",
        "abstract": "Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "referenceCount": 105,
        "citationCount": 1242,
        "influentialCitationCount": 67,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations which make the variability in natural signals more predictable, and produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-05-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1905.09272"
        },
        "citationStyles": {
            "bibtex": "@Article{H\u00e9naff2019DataEfficientIR,\n author = {Olivier J. H\u00e9naff and A. Srinivas and J. Fauw and Ali Razavi and Carl Doersch and S. Eslami and A\u00e4ron van den Oord},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Data-Efficient Image Recognition with Contrastive Predictive Coding},\n volume = {abs/1905.09272},\n year = {2019}\n}\n"
        }
    },
    "734_japanese_dialog_transformers": {
        "paperId": "6c930b416cf68a93ba6181872022b44a99da9648",
        "externalIds": {
            "DBLP": "conf/slt/SugiyamaMANCNM22",
            "ArXiv": "2109.05217",
            "DOI": "10.1109/SLT54892.2023.10022973",
            "CorpusId": 237490447
        },
        "corpusId": 237490447,
        "publicationVenue": {
            "id": "d8dfb5ba-9312-410c-a361-8ad05f945939",
            "name": "Spoken Language Technology Workshop",
            "type": "conference",
            "alternate_names": [
                "SLT",
                "Spok Lang Technol Workshop"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/6c930b416cf68a93ba6181872022b44a99da9648",
        "title": "Empirical Analysis of Training Strategies of Transformer-Based Japanese Chit-Chat Systems",
        "abstract": "In recent years, several high-performance conversational systems have been proposed based on the Transformer encoder-decoder model. Although previous studies analyzed the effects of the model parameters and the decoding method on subjective dialogue evaluations with overall metrics, it is not analyzed enough how the differences of fine-tuning datasets affect the user's detailed impressions. In addition, the Transformer-based approach has mostly been verified for English, not for such languages as Japanese that have large inter-language distances. In this study, we developed large-scale Transformer-based Japanese dialogue models and Japanese chit-chat datasets and examined their effectiveness. We analyzed the relationships between users' multifaceted impressions and fine-tuning datasets.",
        "venue": "Spoken Language Technology Workshop",
        "year": 2021,
        "referenceCount": 47,
        "citationCount": 31,
        "influentialCitationCount": 8,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2109.05217",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This study developed large-scale Transformer-based Japanese dialogue models and Japanese chit-chat datasets and examined their effectiveness, and analyzed the relationships between users' multifaceted impressions and fine-tuning datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-09-11",
        "journal": {
            "name": "2022 IEEE Spoken Language Technology Workshop (SLT)",
            "pages": "685-691"
        },
        "citationStyles": {
            "bibtex": "@Article{Sugiyama2021EmpiricalAO,\n author = {Hiroaki Sugiyama and M. Mizukami and Tsunehiro Arimoto and Hiromi Narimatsu and Yuya Chiba and Hideharu Nakajima and Toyomi Meguro},\n booktitle = {Spoken Language Technology Workshop},\n journal = {2022 IEEE Spoken Language Technology Workshop (SLT)},\n pages = {685-691},\n title = {Empirical Analysis of Training Strategies of Transformer-Based Japanese Chit-Chat Systems},\n year = {2021}\n}\n"
        }
    },
    "736_fold2seq": {
        "paperId": "4dadbd18267b25e787678d5e945e77f41e250bc9",
        "externalIds": {
            "DBLP": "journals/corr/abs-2106-13058",
            "ArXiv": "2106.13058",
            "MAG": "3125817081",
            "CorpusId": 235624058,
            "PubMed": "34423306"
        },
        "corpusId": 235624058,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/4dadbd18267b25e787678d5e945e77f41e250bc9",
        "title": "Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design",
        "abstract": "Designing novel protein sequences for a desired 3D topological fold is a fundamental yet nontrivial task in protein engineering. Challenges exist due to the complex sequence-fold relationship, as well as the difficulties to capture the diversity of the sequences (therefore structures and functions) within a fold. To overcome these challenges, we propose Fold2Seq, a novel transformer-based generative framework for designing protein sequences conditioned on a specific target fold. To model the complex sequence-structure relationship, Fold2Seq jointly learns a sequence embedding using a transformer and a fold embedding from the density of secondary structural elements in 3D voxels. On test sets with single, high-resolution and complete structure inputs for individual folds, our experiments demonstrate improved or comparable performance of Fold2Seq in terms of speed, coverage, and reliability for sequence design, when compared to existing state-of-the-art methods that include data-driven deep generative models and physics-based RosettaDesign. The unique advantages of fold-based Fold2Seq, in comparison to a structure-based deep model and RosettaDesign, become more evident on three additional real-world challenges originating from low-quality, incomplete, or ambiguous input structures. Source code and data are available at https://github.com/IBM/fold2seq.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 50,
        "citationCount": 41,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Biology",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Fold2Seq is proposed, a novel transformer-based generative framework for designing protein sequences conditioned on a specific target fold that jointly learns a sequence embedding using a transformer and a fold embedding from the density of secondary structural elements in 3D voxels."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-05-04",
        "journal": {
            "name": "Proceedings of machine learning research",
            "pages": "\n          1261-1271\n        ",
            "volume": "139"
        },
        "citationStyles": {
            "bibtex": "@Article{Cao2021Fold2SeqAJ,\n author = {Yue Cao and Payel Das and Vijil Chenthamarakshan and Pin-Yu Chen and Igor Melnyk and Yang Shen},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of machine learning research},\n pages = {\n          1261-1271\n        },\n title = {Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design},\n volume = {139},\n year = {2021}\n}\n"
        }
    },
    "739_coda": {
        "paperId": "8b3ab63b477767c461e5ad208d106ab4df8bb0ec",
        "externalIds": {
            "DBLP": "journals/corr/abs-2105-14850",
            "ACL": "2021.acl-long.45",
            "ArXiv": "2105.14850",
            "DOI": "10.18653/v1/2021.acl-long.45",
            "CorpusId": 235253995
        },
        "corpusId": 235253995,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/8b3ab63b477767c461e5ad208d106ab4df8bb0ec",
        "title": "Cascaded Head-colliding Attention",
        "abstract": "Transformers have advanced the field of natural language processing (NLP) on a variety of important tasks. At the cornerstone of the Transformer architecture is the multi-head attention (MHA) mechanism which models pairwise interactions between the elements of the sequence. Despite its massive success, the current framework ignores interactions among different heads, leading to the problem that many of the heads are redundant in practice, which greatly wastes the capacity of the model. To improve parameter efficiency, we re-formulate the MHA as a latent variable model from a probabilistic perspective. We present cascaded head-colliding attention (CODA) which explicitly models the interactions between attention heads through a hierarchical variational distribution. We conduct extensive experiments and demonstrate that CODA outperforms the transformer baseline, by 0.6 perplexity on Wikitext-103 in language modeling, and by 0.6 BLEU on WMT14 EN-DE in machine translation, due to its improvements on the parameter efficiency.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 50,
        "citationCount": 2,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.acl-long.45.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "CODA is presented which explicitly models the interactions between attention heads through a hierarchical variational distribution and outperforms the transformer baseline, due to its improvements on the parameter efficiency."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-05-31",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2105.14850"
        },
        "citationStyles": {
            "bibtex": "@Article{Zheng2021CascadedHA,\n author = {Lin Zheng and Zhiyong Wu and Lingpeng Kong},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Cascaded Head-colliding Attention},\n volume = {abs/2105.14850},\n year = {2021}\n}\n"
        }
    },
    "740_hyperneat": {
        "paperId": "cf6c64b87459a3164ad54128fa085328c401c09f",
        "externalIds": {
            "DBLP": "journals/tciaig/HausknechtLMS14",
            "MAG": "2099397840",
            "DOI": "10.1109/TCIAIG.2013.2294713",
            "CorpusId": 11352605
        },
        "corpusId": 11352605,
        "publicationVenue": {
            "id": "c56495e2-32b0-440c-b5e2-839c615cfbe5",
            "name": "IEEE Transactions on Computational Intelligence and AI in Games",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Comput Intell AI Game"
            ],
            "issn": "1943-068X",
            "alternate_issns": [
                "1943-0698"
            ],
            "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=4804728",
            "alternate_urls": [
                "http://ieee-cis.org/pubs/tciaig/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/cf6c64b87459a3164ad54128fa085328c401c09f",
        "title": "A Neuroevolution Approach to General Atari Game Playing",
        "abstract": "This paper addresses the challenge of learning to play many different video games with little domain-specific knowledge. Specifically, it introduces a neuroevolution approach to general Atari 2600 game playing. Four neuroevolution algorithms were paired with three different state representations and evaluated on a set of 61 Atari games. The neuroevolution agents represent different points along the spectrum of algorithmic sophistication - including weight evolution on topologically fixed neural networks (conventional neuroevolution), covariance matrix adaptation evolution strategy (CMA-ES), neuroevolution of augmenting topologies (NEAT), and indirect network encoding (HyperNEAT). State representations include an object representation of the game screen, the raw pixels of the game screen, and seeded noise (a comparative baseline). Results indicate that direct-encoding methods work best on compact state representations while indirect-encoding methods (i.e., HyperNEAT) allow scaling to higher dimensional representations (i.e., the raw game screen). Previous approaches based on temporal-difference (TD) learning had trouble dealing with the large state spaces and sparse reward gradients often found in Atari games. Neuroevolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games. These results suggest that neuroevolution is a promising approach to general video game playing (GVGP).",
        "venue": "IEEE Transactions on Computational Intelligence and AI in Games",
        "year": 2014,
        "referenceCount": 53,
        "citationCount": 187,
        "influentialCitationCount": 11,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.cs.utexas.edu/~pstone/Papers/bib2html-links/TCIAIG13-mhauskn.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Results suggest that neuroevolution is a promising approach to general video game playing (GVGP) and achieved state-of-the-art results, even surpassing human high scores on three games."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-03-05",
        "journal": {
            "name": "IEEE Transactions on Computational Intelligence and AI in Games",
            "pages": "355-366",
            "volume": "6"
        },
        "citationStyles": {
            "bibtex": "@Article{Hausknecht2014ANA,\n author = {Matthew J. Hausknecht and J. Lehman and R. Miikkulainen and P. Stone},\n booktitle = {IEEE Transactions on Computational Intelligence and AI in Games},\n journal = {IEEE Transactions on Computational Intelligence and AI in Games},\n pages = {355-366},\n title = {A Neuroevolution Approach to General Atari Game Playing},\n volume = {6},\n year = {2014}\n}\n"
        }
    },
    "741_gpt-neox-20b": {
        "paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
        "externalIds": {
            "ACL": "2022.bigscience-1.9",
            "DBLP": "journals/corr/abs-2204-06745",
            "ArXiv": "2204.06745",
            "DOI": "10.48550/arXiv.2204.06745",
            "CorpusId": 248177957
        },
        "corpusId": 248177957,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
        "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
        "abstract": "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20B\u2019s architecture and training, and evaluate its performance. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.",
        "venue": "BIGSCIENCE",
        "year": 2022,
        "referenceCount": 142,
        "citationCount": 470,
        "influentialCitationCount": 47,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2204.06745",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "GPT-NeoX-20B is introduced, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-04-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2204.06745"
        },
        "citationStyles": {
            "bibtex": "@Article{Black2022GPTNeoX20BAO,\n author = {Sid Black and Stella Biderman and Eric Hallahan and Quentin G. Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and M. Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and J. Tow and Benqi Wang and Samuel Weinbach},\n booktitle = {BIGSCIENCE},\n journal = {ArXiv},\n title = {GPT-NeoX-20B: An Open-Source Autoregressive Language Model},\n volume = {abs/2204.06745},\n year = {2022}\n}\n"
        }
    },
    "742_u-palm_(540b)": {
        "paperId": "1bb6d5761903c7ac978188ae36e2648905e95dc5",
        "externalIds": {
            "ArXiv": "2210.11399",
            "DBLP": "journals/corr/abs-2210-11399",
            "DOI": "10.48550/arXiv.2210.11399",
            "CorpusId": 253018395
        },
        "corpusId": 253018395,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/1bb6d5761903c7ac978188ae36e2648905e95dc5",
        "title": "Transcending Scaling Laws with 0.1% Extra Compute",
        "abstract": "Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving $\\sim$4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2022,
        "referenceCount": 54,
        "citationCount": 45,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2210.11399",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "U-PaLM is introduced, a new set of models at 8B, 62B, and 540B scale which is able to substantially improve the scaling properties of large language models on downstream metrics and leads to 'emergent abilities' on challenging BIG-Bench tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-10-20",
        "journal": {
            "pages": "1471-1486"
        },
        "citationStyles": {
            "bibtex": "@Article{Tay2022TranscendingSL,\n author = {Yi Tay and Jason Wei and Hyung Won Chung and Vinh Q. Tran and David R. So and Siamak Shakeri and Xavier Garc\u00eda and H. Zheng and J. Rao and Aakanksha Chowdhery and Denny Zhou and Donald Metzler and Slav Petrov and N. Houlsby and Quoc V. Le and Mostafa Dehghani},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1471-1486},\n title = {Transcending Scaling Laws with 0.1% Extra Compute},\n year = {2022}\n}\n"
        }
    },
    "743_amdim": {
        "paperId": "9b09d296059909490096e34e9df2d95314787ad5",
        "externalIds": {
            "MAG": "2948012107",
            "ArXiv": "1906.00910",
            "DBLP": "journals/corr/abs-1906-00910",
            "CorpusId": 173990164
        },
        "corpusId": 173990164,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9b09d296059909490096e34e9df2d95314787ad5",
        "title": "Learning Representations by Maximizing Mutual Information Across Views",
        "abstract": "We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -- e.g., presence of certain objects or occurrence of certain events. \nFollowing our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: this https URL.",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "referenceCount": 48,
        "citationCount": 1228,
        "influentialCitationCount": 97,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work develops a model which learns image representations that significantly outperform prior methods on the tasks the authors consider, and extends this model to use mixture-based representations, where segmentation behaviour emerges as a natural side-effect."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-06-03",
        "journal": {
            "pages": "15509-15519"
        },
        "citationStyles": {
            "bibtex": "@Article{Bachman2019LearningRB,\n author = {Philip Bachman and R. Devon Hjelm and William Buchwalter},\n booktitle = {Neural Information Processing Systems},\n pages = {15509-15519},\n title = {Learning Representations by Maximizing Mutual Information Across Views},\n year = {2019}\n}\n"
        }
    },
    "744_iterative_bootstrapping_wsd": {
        "paperId": "944cba683d10d8c1a902e05cd68e32a9f47b372e",
        "externalIds": {
            "MAG": "2101210369",
            "ACL": "P95-1026",
            "DBLP": "conf/acl/Yarowsky95",
            "DOI": "10.3115/981658.981684",
            "CorpusId": 1487550
        },
        "corpusId": 1487550,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/944cba683d10d8c1a902e05cd68e32a9f47b372e",
        "title": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods",
        "abstract": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 1995,
        "referenceCount": 27,
        "citationCount": 2762,
        "influentialCitationCount": 181,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://dl.acm.org/ft_gateway.cfm?id=981684&type=pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "1995-06-26",
        "journal": {
            "pages": "189-196"
        },
        "citationStyles": {
            "bibtex": "@Article{Yarowsky1995UnsupervisedWS,\n author = {David Yarowsky},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {189-196},\n title = {Unsupervised Word Sense Disambiguation Rivaling Supervised Methods},\n year = {1995}\n}\n"
        }
    },
    "745_luke": {
        "paperId": "eedf2748a9a1ba2779cde95fd8bad9c2260d5317",
        "externalIds": {
            "ArXiv": "2010.01057",
            "DBLP": "journals/corr/abs-2010-01057",
            "MAG": "3090325631",
            "ACL": "2020.emnlp-main.523",
            "DOI": "10.18653/v1/2020.emnlp-main.523",
            "CorpusId": 222124841
        },
        "corpusId": 222124841,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/eedf2748a9a1ba2779cde95fd8bad9c2260d5317",
        "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
        "abstract": "Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at this https URL.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2020,
        "referenceCount": 46,
        "citationCount": 514,
        "influentialCitationCount": 85,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.emnlp-main.523.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "New pretrained contextualized representations of words and entities based on the bidirectional transformer, and an entity-aware self-attention mechanism that considers the types of tokens (words or entities) when computing attention scores are proposed."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-10-02",
        "journal": {
            "pages": "6442-6454"
        },
        "citationStyles": {
            "bibtex": "@Article{Yamada2020LUKEDC,\n author = {Ikuya Yamada and Akari Asai and Hiroyuki Shindo and Hideaki Takeda and Yuji Matsumoto},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {6442-6454},\n title = {LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention},\n year = {2020}\n}\n"
        }
    },
    "746_llama_guard": {
        "paperId": "8b28792f8405b737229afb92c99c579b86d8aa98",
        "externalIds": {
            "DBLP": "journals/corr/abs-2312-06674",
            "ArXiv": "2312.06674",
            "DOI": "10.48550/arXiv.2312.06674",
            "CorpusId": 266174345
        },
        "corpusId": 266174345,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/8b28792f8405b737229afb92c99c579b86d8aa98",
        "title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
        "abstract": "We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 26,
        "citationCount": 21,
        "influentialCitationCount": 5,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Llama Guard is introduced, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases and demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-12-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2312.06674"
        },
        "citationStyles": {
            "bibtex": "@Article{Inan2023LlamaGL,\n author = {Hakan Inan and K. Upasani and Jianfeng Chi and Rashi Rungta and Krithika Iyer and Yuning Mao and Michael Tontchev and Qing Hu and Brian Fuller and Davide Testuggine and Madian Khabsa},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations},\n volume = {abs/2312.06674},\n year = {2023}\n}\n"
        }
    },
    "748_albert": {
        "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
        "externalIds": {
            "MAG": "2996428491",
            "DBLP": "journals/corr/abs-1909-11942",
            "ArXiv": "1909.11942",
            "CorpusId": 202888986
        },
        "corpusId": 202888986,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7a064df1aeada7e69e5173f7d4c8606f4470365b",
        "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
        "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 72,
        "citationCount": 5133,
        "influentialCitationCount": 851,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT, and uses a self-supervised loss that focuses on modeling inter-sentence coherence."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-09-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1909.11942"
        },
        "citationStyles": {
            "bibtex": "@Article{Lan2019ALBERTAL,\n author = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\n volume = {abs/1909.11942},\n year = {2019}\n}\n"
        }
    },
    "749_codefusion_(python)": {
        "paperId": "1bfe2a9a40a5f34c5c6b99c182d37a6e93f95aa9",
        "externalIds": {
            "DBLP": "conf/emnlp/SinghCG0NV23",
            "ArXiv": "2310.17680",
            "DOI": "10.48550/arXiv.2310.17680",
            "CorpusId": 264555385
        },
        "corpusId": 264555385,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/1bfe2a9a40a5f34c5c6b99c182d37a6e93f95aa9",
        "title": "CodeFusion: A Pre-trained Diffusion Model for Code Generation",
        "abstract": "Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2023,
        "referenceCount": 36,
        "citationCount": 4,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces CodeFusion, a pre-trained diffusion code generation model that addresses this limitation of auto-regressive models by iteratively denoising a complete program conditioned on the encoded natural language."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-10-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2310.17680"
        },
        "citationStyles": {
            "bibtex": "@Article{Singh2023CodeFusionAP,\n author = {Mukul Singh and J. Cambronero and Sumit Gulwani and Vu Le and Carina Negreanu and Gust Verbruggen},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {CodeFusion: A Pre-trained Diffusion Model for Code Generation},\n volume = {abs/2310.17680},\n year = {2023}\n}\n"
        }
    },
    "750_convolutional_pose_machines": {
        "paperId": "864e7db59f2ccfec1ee9f6eba79566ac7b0634df",
        "externalIds": {
            "ArXiv": "1602.00134",
            "MAG": "2951987817",
            "DBLP": "journals/corr/WeiRKS16",
            "DOI": "10.1109/CVPR.2016.511",
            "CorpusId": 163946
        },
        "corpusId": 163946,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/864e7db59f2ccfec1ee9f6eba79566ac7b0634df",
        "title": "Convolutional Pose Machines",
        "abstract": "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 47,
        "citationCount": 2532,
        "influentialCitationCount": 302,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1602.00134",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work designs a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference in structured prediction tasks such as articulated pose estimation."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-01-30",
        "journal": {
            "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "4724-4732"
        },
        "citationStyles": {
            "bibtex": "@Article{Wei2016ConvolutionalPM,\n author = {S. Wei and V. Ramakrishna and T. Kanade and Yaser Sheikh},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4724-4732},\n title = {Convolutional Pose Machines},\n year = {2016}\n}\n"
        }
    },
    "751_hso": {
        "paperId": "b7e94220176d9d329ee064fd4359229bf92ef360",
        "externalIds": {
            "DBLP": "conf/emnlp/YoshidaG21",
            "ArXiv": "2112.08653",
            "DOI": "10.18653/v1/2021.findings-emnlp.346",
            "CorpusId": 237568129
        },
        "corpusId": 237568129,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/b7e94220176d9d329ee064fd4359229bf92ef360",
        "title": "Reconsidering the Past: Optimizing Hidden States in Language Models",
        "abstract": "We present Hidden-State Optimization (HSO), a gradient-based method for improving the performance of transformer language models at inference time. Similar to dynamic evaluation (Krause et al., 2018), HSO computes the gradient of the log-probability the language model assigns to an evaluation text, but uses it to update the cached hidden states rather than the model parameters. We test HSO with pretrained Transformer-XL and GPT-2 language models, finding improvement on the WikiText103 and PG-19 datasets in terms of perplexity, especially when evaluating a model outside of its training distribution. We also demonstrate downstream applicability by showing gains in the recently developed prompt-based few-shot evaluation setting, again with no extra parameters or training data.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "referenceCount": 29,
        "citationCount": 2,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.findings-emnlp.346.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Hidden-State Optimization is presented, a gradient-based method for improving the performance of transformer language models at inference time that computes the gradient of the log-probability the language model assigns to an evaluation text but uses it to update the cached hidden states rather than the model parameters."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-12-16",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2112.08653"
        },
        "citationStyles": {
            "bibtex": "@Article{Yoshida2021ReconsideringTP,\n author = {Davis Yoshida and Kevin Gimpel},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Reconsidering the Past: Optimizing Hidden States in Language Models},\n volume = {abs/2112.08653},\n year = {2021}\n}\n"
        }
    },
    "752_chinese_-_english_translation": {
        "paperId": "fc1d981dd051063ae586a56b05390fe3ea82f040",
        "externalIds": {
            "DBLP": "journals/corr/abs-1803-05567",
            "MAG": "2794365787",
            "ArXiv": "1803.05567",
            "CorpusId": 3912183
        },
        "corpusId": 3912183,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/fc1d981dd051063ae586a56b05390fe3ea82f040",
        "title": "Achieving Human Parity on Automatic Chinese to English News Translation",
        "abstract": "Machine translation has made rapid advances in recent years. Millions of people are using it today in online translation systems and mobile applications in order to communicate across language barriers. The question naturally arises whether such systems can approach or achieve parity with human translations. In this paper, we first address the problem of how to define and accurately measure human parity in translation. We then describe Microsoft's machine translation system and measure the quality of its translations on the widely used WMT 2017 news translation task from Chinese to English. We find that our latest neural machine translation system has reached a new state-of-the-art, and that the translation quality is at human parity when compared to professional human translations. We also find that it significantly exceeds the quality of crowd-sourced non-professional translations.",
        "venue": "arXiv.org",
        "year": 2018,
        "referenceCount": 65,
        "citationCount": 566,
        "influentialCitationCount": 23,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that Microsoft's latest neural machine translation system has reached a new state-of-the-art, and that the translation quality is at human parity when compared to professional human translations."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-03-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1803.05567"
        },
        "citationStyles": {
            "bibtex": "@Article{Hassan2018AchievingHP,\n author = {Hany Hassan and Anthony Aue and Chang Chen and Vishal Chowdhary and Jonathan Clark and C. Federmann and Xuedong Huang and Marcin Junczys-Dowmunt and W. Lewis and Mu Li and Shujie Liu and Tie-Yan Liu and Renqian Luo and Arul Menezes and Tao Qin and F. Seide and Xu Tan and Fei Tian and Lijun Wu and Shuangzhi Wu and Yingce Xia and Dongdong Zhang and Zhirui Zhang and Ming Zhou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Achieving Human Parity on Automatic Chinese to English News Translation},\n volume = {abs/1803.05567},\n year = {2018}\n}\n"
        }
    },
    "753_awd-lstm+wt+cache+iog_(wt2)": {
        "paperId": "3e31036be7a9017fe9473ce0b435427ed2cd88b5",
        "externalIds": {
            "MAG": "2962813775",
            "DBLP": "conf/ijcnlp/TakaseSN17",
            "ArXiv": "1709.08907",
            "ACL": "I17-2008",
            "CorpusId": 7479561
        },
        "corpusId": 7479561,
        "publicationVenue": {
            "id": "e783305c-5d8a-44b9-b7a2-449d474a85b2",
            "name": "International Joint Conference on Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "IJCNLP",
                "Int Jt Conf Nat Lang Process"
            ],
            "url": "https://www.aclweb.org/portal/ijcnlp"
        },
        "url": "https://www.semanticscholar.org/paper/3e31036be7a9017fe9473ce0b435427ed2cd88b5",
        "title": "Input-to-Output Gate to Improve RNN Language Models",
        "abstract": "This paper proposes a reinforcing method that refines the output layers of existing Recurrent Neural Network (RNN) language models. We refer to our proposed method as Input-to-Output Gate (IOG). IOG has an extremely simple structure, and thus, can be easily combined with any RNN language models. Our experiments on the Penn Treebank and WikiText-2 datasets demonstrate that IOG consistently boosts the performance of several different types of current topline RNN language models.",
        "venue": "International Joint Conference on Natural Language Processing",
        "year": 2017,
        "referenceCount": 20,
        "citationCount": 4,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a reinforcing method that refines the output layers of existing Recurrent Neural Network (RNN) language models as Input-to-Output Gate (IOG), which has an extremely simple structure and thus, can be easily combined with any RNN language models."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-09-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1709.08907"
        },
        "citationStyles": {
            "bibtex": "@Article{Takase2017InputtoOutputGT,\n author = {Sho Takase and Jun Suzuki and M. Nagata},\n booktitle = {International Joint Conference on Natural Language Processing},\n journal = {ArXiv},\n title = {Input-to-Output Gate to Improve RNN Language Models},\n volume = {abs/1709.08907},\n year = {2017}\n}\n"
        }
    },
    "755_greedy_layer-wise_dnn_training": {
        "paperId": "355d44f53428b1ac4fb2ab468d593c720640e5bd",
        "externalIds": {
            "MAG": "2540556213",
            "DBLP": "conf/nips/BengioLPL06",
            "DOI": "10.7551/mitpress/7503.003.0024",
            "CorpusId": 14201947
        },
        "corpusId": 14201947,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/355d44f53428b1ac4fb2ab468d593c720640e5bd",
        "title": "Greedy Layer-Wise Training of Deep Networks",
        "abstract": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.",
        "venue": "Neural Information Processing Systems",
        "year": 2006,
        "referenceCount": 18,
        "citationCount": 5083,
        "influentialCitationCount": 314,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "These experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2006-12-04",
        "journal": {
            "pages": "153-160"
        },
        "citationStyles": {
            "bibtex": "@Article{Bengio2006GreedyLT,\n author = {Yoshua Bengio and Pascal Lamblin and D. Popovici and H. Larochelle},\n booktitle = {Neural Information Processing Systems},\n pages = {153-160},\n title = {Greedy Layer-Wise Training of Deep Networks},\n year = {2006}\n}\n"
        }
    },
    "756_diffdock": {
        "paperId": "6f0d0b897d0e0963204719b80a8af43ca0d79d90",
        "externalIds": {
            "DBLP": "conf/iclr/CorsoSJBJ23",
            "ArXiv": "2210.01776",
            "DOI": "10.48550/arXiv.2210.01776",
            "CorpusId": 252693198
        },
        "corpusId": 252693198,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/6f0d0b897d0e0963204719b80a8af43ca0d79d90",
        "title": "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking",
        "abstract": "Predicting the binding structure of a small molecule ligand to a protein -- a task known as molecular docking -- is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4%), DiffDock maintains significantly higher precision (21.7%). Finally, DiffDock has fast inference times and provides confidence estimates with high selective accuracy.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 53,
        "citationCount": 193,
        "influentialCitationCount": 21,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.01776",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Biology",
            "Physics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Physics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "DiffDock is developed, a diffusion generative model over the non-Euclidean manifold of ligand poses, which maintains significantly higher precision than previous methods and has fast inference times and provides confidence estimates with high selective accuracy."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.01776"
        },
        "citationStyles": {
            "bibtex": "@Article{Corso2022DiffDockDS,\n author = {Gabriele Corso and Hannes St\u00e4rk and Bowen Jing and R. Barzilay and T. Jaakkola},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking},\n volume = {abs/2210.01776},\n year = {2022}\n}\n"
        }
    },
    "757_dark": {
        "paperId": "1dc9e3d2128ca9c11cd401a884d72183beb6a846",
        "externalIds": {
            "DOI": "10.1101/2022.01.27.478087",
            "CorpusId": 246410025
        },
        "corpusId": 246410025,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/1dc9e3d2128ca9c11cd401a884d72183beb6a846",
        "title": "Design in the DARK: Learning Deep Generative Models for De Novo Protein Design",
        "abstract": "The design of novel protein sequences is providing paths towards the development of novel therapeutics and materials. At the forefront is the challenging field of de novo protein design, which looks to design protein sequences unlike those found in nature using general design methodologies. In this work, we develop a tool for de novo design, based on a deep generative sequence model, that rapidly samples novel protein sequences with diverse and ordered structures. To build this tool we develop a framework, called DARK, that trains the underlying generative model on an iteratively expanding set of synthetic sequences. The resulting model generalizes where models trained on natural sequences struggle and greatly improves on the efficiency of comparable sampling-based approaches. We further show how it can generate high quality candidates for de novo design problems and aid in the development of further novel design methods, in all, providing another step, amongst others, towards truly automated and intelligent protein design.",
        "venue": "bioRxiv",
        "year": 2022,
        "referenceCount": 82,
        "citationCount": 25,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2022/01/28/2022.01.27.478087.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work develops a tool for de novo design, based on a deep generative sequence model, that rapidly samples novel protein sequences with diverse and ordered structures, and develops a framework that trains the underlying generative model on an iteratively expanding set of synthetic sequences."
        },
        "publicationTypes": null,
        "publicationDate": "2022-01-28",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Moffat2022DesignIT,\n author = {Lewis Moffat and Shaun M. Kandathil and David T. Jones},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {Design in the DARK: Learning Deep Generative Models for De Novo Protein Design},\n year = {2022}\n}\n"
        }
    },
    "758_system_11": {
        "paperId": "a3885c13438132b516e5ffc8b640d20b4e41a7a4",
        "externalIds": {
            "MAG": "2566703758",
            "DBLP": "conf/cvpr/RowleyBK96",
            "DOI": "10.1109/CVPR.1996.517075",
            "CorpusId": 40120983
        },
        "corpusId": 40120983,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/a3885c13438132b516e5ffc8b640d20b4e41a7a4",
        "title": "Neural network-based face detection",
        "abstract": "We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates.",
        "venue": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
        "year": 1996,
        "referenceCount": 65,
        "citationCount": 4388,
        "influentialCitationCount": 274,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A neural network-based face detection system that arbitrates between multiple networks to improve performance over a single network using a bootstrap algorithm, which eliminates the difficult task of manually selecting non-face training examples."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "1996-06-18",
        "journal": {
            "name": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "pages": "203-208"
        },
        "citationStyles": {
            "bibtex": "@Article{Rowley1996NeuralNF,\n author = {H. Rowley and S. Baluja and T. Kanade},\n booktitle = {Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n journal = {Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n pages = {203-208},\n title = {Neural network-based face detection},\n year = {1996}\n}\n"
        }
    },
    "759_minerva_(540b)": {
        "paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "externalIds": {
            "ArXiv": "2206.14858",
            "DBLP": "journals/corr/abs-2206-14858",
            "DOI": "10.48550/arXiv.2206.14858",
            "CorpusId": 250144408
        },
        "corpusId": 250144408,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models",
        "abstract": "Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "referenceCount": 70,
        "citationCount": 389,
        "influentialCitationCount": 32,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2206.14858",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-06-29",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2206.14858"
        },
        "citationStyles": {
            "bibtex": "@Article{Lewkowycz2022SolvingQR,\n author = {Aitor Lewkowycz and Anders Andreassen and David Dohan and Ethan Dyer and H. Michalewski and V. Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Solving Quantitative Reasoning Problems with Language Models},\n volume = {abs/2206.14858},\n year = {2022}\n}\n"
        }
    },
    "761_danet": {
        "paperId": "ad655c25e052fa4eeed53421344aca6f239c4c9d",
        "externalIds": {
            "MAG": "2892219791",
            "DBLP": "journals/corr/abs-1809-02983",
            "ArXiv": "1809.02983",
            "DOI": "10.1109/CVPR.2019.00326",
            "CorpusId": 52180375
        },
        "corpusId": 52180375,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/ad655c25e052fa4eeed53421344aca6f239c4c9d",
        "title": "Dual Attention Network for Scene Segmentation",
        "abstract": "In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the self-attention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "referenceCount": 39,
        "citationCount": 4052,
        "influentialCitationCount": 449,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1809.02983",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "New state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset is achieved without using coarse data."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-09-09",
        "journal": {
            "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "3141-3149"
        },
        "citationStyles": {
            "bibtex": "@Article{Fu2018DualAN,\n author = {J. Fu and J. Liu and Haijie Tian and Zhiwei Fang and Hanqing Lu},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {3141-3149},\n title = {Dual Attention Network for Scene Segmentation},\n year = {2018}\n}\n"
        }
    },
    "762_tome": {
        "paperId": "7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7",
        "externalIds": {
            "DBLP": "conf/iclr/JongZFSC22",
            "ArXiv": "2110.06176",
            "CorpusId": 248545122
        },
        "corpusId": 248545122,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7",
        "title": "MENTION MEMORY : INCORPORATING TEXTUAL KNOWLEDGE INTO TRANSFORMERS THROUGH ENTITY MENTION ATTENTION",
        "abstract": "Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge with `mention memory', a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 27,
        "citationCount": 31,
        "influentialCitationCount": 2,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge that achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2110.06176"
        },
        "citationStyles": {
            "bibtex": "@Article{Jong2022MENTIONM,\n author = {Michiel de Jong and Yury Zemlyanskiy and Nicholas FitzGerald and Fei Sha and W. Cohen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {MENTION MEMORY : INCORPORATING TEXTUAL KNOWLEDGE INTO TRANSFORMERS THROUGH ENTITY MENTION ATTENTION},\n volume = {abs/2110.06176},\n year = {2022}\n}\n"
        }
    },
    "764_brsm_+_cache": {
        "paperId": "8ed2a15a0840daa04f9837912abcd555e4165eee",
        "externalIds": {
            "DBLP": "conf/annpr/GordonRA20",
            "MAG": "2994590149",
            "ArXiv": "1912.01116",
            "DOI": "10.1007/978-3-030-58309-5_4",
            "CorpusId": 208548740
        },
        "corpusId": 208548740,
        "publicationVenue": {
            "id": "a4e98dc5-311b-49f6-bd5a-adfee5865dd7",
            "name": "IAPR International Workshop on Artificial Neural Networks in Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "Artificial Neural Networks in Pattern Recognition",
                "IAPR Int Workshop Artif Neural Netw Pattern Recognit",
                "Artif Neural Netw Pattern Recognit",
                "ANNPR"
            ],
            "url": "http://www.wikicfp.com/cfp/program?id=179"
        },
        "url": "https://www.semanticscholar.org/paper/8ed2a15a0840daa04f9837912abcd555e4165eee",
        "title": "Long Distance Relationships without Time Travel: Boosting the Performance of a Sparse Predictive Autoencoder in Sequence Modeling",
        "abstract": null,
        "venue": "IAPR International Workshop on Artificial Neural Networks in Pattern Recognition",
        "year": 2019,
        "referenceCount": 21,
        "citationCount": 4,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1912.01116",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that it is possible to train high performance recurrent networks using information that is local in time, and thereby achieve a significantly reduced memory footprint, and provides encouraging evidence that strong results on challenging tasks such as language modelling may be possible using less memory intensive, biologically-plausible training regimes."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-12-02",
        "journal": {
            "pages": "52-64"
        },
        "citationStyles": {
            "bibtex": "@Article{Gordon2019LongDR,\n author = {J. Gordon and D. Rawlinson and Subutai Ahmad},\n booktitle = {IAPR International Workshop on Artificial Neural Networks in Pattern Recognition},\n pages = {52-64},\n title = {Long Distance Relationships without Time Travel: Boosting the Performance of a Sparse Predictive Autoencoder in Sequence Modeling},\n year = {2019}\n}\n"
        }
    },
    "766_lstm_(hebbian,_cache,_mbpa)": {
        "paperId": "69ac3b35887eb42e8fe554619fc7255e6e95a4cb",
        "externalIds": {
            "MAG": "2795285343",
            "ArXiv": "1803.10049",
            "DBLP": "conf/icml/RaeDDL18",
            "CorpusId": 4347492
        },
        "corpusId": 4347492,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/69ac3b35887eb42e8fe554619fc7255e6e95a4cb",
        "title": "Fast Parametric Learning with Activation Memorization",
        "abstract": "Neural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times. In applications where most class labels are rare, such as language modelling, this can become a performance bottleneck. One potential remedy is to augment the network with a fast-learning non-parametric model which stores recent activations and class labels into an external memory. We explore a simplified architecture where we treat a subset of the model parameters as fast memory stores. This can help retain information over longer time intervals than a traditional memory, and does not require additional space or compute. In the case of image classification, we display faster binding of novel classes on an Omniglot image curriculum task. We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) --- the latter achieving a state-of-the-art perplexity of 29.2.",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "referenceCount": 54,
        "citationCount": 46,
        "influentialCitationCount": 4,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work explores a simplified architecture where a subset of the model parameters are treated as fast memory stores, which can help retain information over longer time intervals than a traditional memory, and does not require additional space or compute."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-03-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1803.10049"
        },
        "citationStyles": {
            "bibtex": "@Article{Rae2018FastPL,\n author = {Jack W. Rae and Chris Dyer and P. Dayan and T. Lillicrap},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Fast Parametric Learning with Activation Memorization},\n volume = {abs/1803.10049},\n year = {2018}\n}\n"
        }
    },
    "767_\u03bb-wasp": {
        "paperId": "c2ecc66c0e5f976b0e0d95c64ed2d1e283a2625d",
        "externalIds": {
            "DBLP": "conf/acl/WongM07",
            "ACL": "P07-1121",
            "MAG": "2121465811",
            "CorpusId": 9337134
        },
        "corpusId": 9337134,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/c2ecc66c0e5f976b0e0d95c64ed2d1e283a2625d",
        "title": "Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus",
        "abstract": "This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with \ufffdoperators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the bestperforming system so far in a database query domain.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2007,
        "referenceCount": 19,
        "citationCount": 366,
        "influentialCitationCount": 29,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A semantic parser based on a synchronous context-free grammar augmented with \ufffdoperators is learned given a set of training sentences and their correct logical forms, and is shown to be the bestperforming system so far in a database query domain."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2007-12-01",
        "journal": {
            "name": "",
            "pages": "960-967",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Wong2007LearningSG,\n author = {Yuk Wah Wong and R. Mooney},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {960-967},\n title = {Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus},\n year = {2007}\n}\n"
        }
    },
    "768_gpipe_(amoeba)": {
        "paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38",
        "externalIds": {
            "MAG": "2991040477",
            "DBLP": "conf/nips/HuangCBFCCLNLWC19",
            "CorpusId": 53670168
        },
        "corpusId": 53670168,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/d79a26226393f687ddbc375e32055b40b8ad8d38",
        "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
        "abstract": "Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "referenceCount": 68,
        "citationCount": 1139,
        "influentialCitationCount": 141,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "GPipe is introduced, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers by pipelining different sub-sequences of layers on separate accelerators, resulting in almost linear speedup when a model is partitioned across multiple accelerators."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-11-16",
        "journal": {
            "pages": "103-112"
        },
        "citationStyles": {
            "bibtex": "@Article{Huang2018GPipeET,\n author = {Yanping Huang and Yonglong Cheng and Dehao Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Z. Chen},\n booktitle = {Neural Information Processing Systems},\n pages = {103-112},\n title = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},\n year = {2018}\n}\n"
        }
    },
    "769_gru_+_p-thsm_(pretrain_via_brown)_(wt103)": {
        "paperId": "d063c8c0ab49a7a2e226da90d1b1ff430965b1e7",
        "externalIds": {
            "DBLP": "conf/ijcai/JiangRGSX17",
            "MAG": "2742102274",
            "DOI": "10.24963/ijcai.2017/271",
            "CorpusId": 29587600
        },
        "corpusId": 29587600,
        "publicationVenue": {
            "id": "67f7f831-711a-43c8-8785-1e09005359b5",
            "name": "International Joint Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "Int Jt Conf Artif Intell",
                "IJCAI"
            ],
            "url": "http://www.ijcai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/d063c8c0ab49a7a2e226da90d1b1ff430965b1e7",
        "title": "Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models",
        "abstract": "Recently, variants of neural networks for computational linguistics have been proposed and successfully applied to neural language modeling and neural machine translation. These neural models can leverage knowledge from massive corpora but they are extremely slow as they predict candidate words from a large vocabulary during training and inference. As an alternative to gradient approximation and softmax with class decomposition, we explore the tree-based hierarchical softmax method and reform its architecture, making it compatible with modern GPUs and introducing a compact tree-based loss function. When combined with several word hierarchical clustering algorithms, improved performance is achieved in language modelling task with intrinsic evaluation criterions on PTB, WikiText-2 and WikiText-103 datasets.",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2017,
        "referenceCount": 37,
        "citationCount": 6,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2017/0271.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The tree-based hierarchical softmax method is explored and its architecture is reformed, making it compatible with modern GPUs and introducing a compact tree- based loss function."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-08-19",
        "journal": {
            "pages": "1951-1957"
        },
        "citationStyles": {
            "bibtex": "@Article{Jiang2017ExplorationOT,\n author = {Nan Jiang and Wenge Rong and Min Gao and Yikang Shen and Z. Xiong},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {1951-1957},\n title = {Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models},\n year = {2017}\n}\n"
        }
    },
    "770_segment_anything_model": {
        "paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b",
        "externalIds": {
            "ArXiv": "2304.02643",
            "DBLP": "conf/iccv/KirillovMRMRGXW23",
            "DOI": "10.1109/ICCV51070.2023.00371",
            "CorpusId": 257952310
        },
        "corpusId": 257952310,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/7470a1702c8c86e6f28d32cfa315381150102f5b",
        "title": "Segment Anything",
        "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive \u2013 often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2023,
        "referenceCount": 148,
        "citationCount": 1688,
        "influentialCitationCount": 339,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Segment Anything Model (SAM) is introduced: a new task, model, and dataset for image segmentation, and its zero-shot performance is impressive \u2013 often competitive with or even superior to prior fully supervised results."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-04-05",
        "journal": {
            "name": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "pages": "3992-4003"
        },
        "citationStyles": {
            "bibtex": "@Article{Kirillov2023SegmentA,\n author = {A. Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and A. Berg and Wan-Yen Lo and Piotr Doll\u00e1r and Ross B. Girshick},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {3992-4003},\n title = {Segment Anything},\n year = {2023}\n}\n"
        }
    },
    "771_detr": {
        "paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
        "externalIds": {
            "MAG": "3096609285",
            "DBLP": "conf/eccv/CarionMSUKZ20",
            "ArXiv": "2005.12872",
            "DOI": "10.1007/978-3-030-58452-8_13",
            "CorpusId": 218889832
        },
        "corpusId": 218889832,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
        "title": "End-to-End Object Detection with Transformers",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2020,
        "referenceCount": 53,
        "citationCount": 7943,
        "influentialCitationCount": 1170,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2005.12872",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents a new method that views object detection as a direct set prediction problem, and demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-05-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2005.12872"
        },
        "citationStyles": {
            "bibtex": "@Article{Carion2020EndtoEndOD,\n author = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {End-to-End Object Detection with Transformers},\n volume = {abs/2005.12872},\n year = {2020}\n}\n"
        }
    },
    "772_dropout_(imagenet)": {
        "paperId": "0060745e006c5f14ec326904119dca19c6545e51",
        "externalIds": {
            "ArXiv": "1207.0580",
            "MAG": "1904365287",
            "DBLP": "journals/corr/abs-1207-0580",
            "CorpusId": 14832074
        },
        "corpusId": 14832074,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0060745e006c5f14ec326904119dca19c6545e51",
        "title": "Improving neural networks by preventing co-adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",
        "venue": "arXiv.org",
        "year": 2012,
        "referenceCount": 26,
        "citationCount": 7141,
        "influentialCitationCount": 641,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2012-07-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1207.0580"
        },
        "citationStyles": {
            "bibtex": "@Article{Hinton2012ImprovingNN,\n author = {Geoffrey E. Hinton and Nitish Srivastava and A. Krizhevsky and I. Sutskever and R. Salakhutdinov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Improving neural networks by preventing co-adaptation of feature detectors},\n volume = {abs/1207.0580},\n year = {2012}\n}\n"
        }
    },
    "773_lbl": {
        "paperId": "5b0d644f5c4b9880cbaf79932c0a4fa98996f068",
        "externalIds": {
            "MAG": "2120861206",
            "ArXiv": "1206.6426",
            "DBLP": "conf/icml/MnihT12",
            "CorpusId": 6633369
        },
        "corpusId": 6633369,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/5b0d644f5c4b9880cbaf79932c0a4fa98996f068",
        "title": "A fast and simple algorithm for training neural probabilistic language models",
        "abstract": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. \n \nWe propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. \n \nWe demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset.",
        "venue": "International Conference on Machine Learning",
        "year": 2012,
        "referenceCount": 25,
        "citationCount": 555,
        "influentialCitationCount": 70,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions and demonstrates the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2012-06-26",
        "journal": {
            "name": "",
            "pages": "419-426",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Mnih2012AFA,\n author = {A. Mnih and Y. Teh},\n booktitle = {International Conference on Machine Learning},\n pages = {419-426},\n title = {A fast and simple algorithm for training neural probabilistic language models},\n year = {2012}\n}\n"
        }
    },
    "774_ms-cnn": {
        "paperId": "d6473533e89e5f946a6ff3ad07c6a74ee9b47672",
        "externalIds": {
            "DBLP": "journals/corr/CaiFFV16",
            "ArXiv": "1607.07155",
            "MAG": "2490270993",
            "DOI": "10.1007/978-3-319-46493-0_22",
            "CorpusId": 9232270
        },
        "corpusId": 9232270,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/d6473533e89e5f946a6ff3ad07c6a74ee9b47672",
        "title": "A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "referenceCount": 42,
        "citationCount": 1405,
        "influentialCitationCount": 153,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46493-0_22.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi- scale object detection, which is learned end-to-end, by optimizing a multi-task loss."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-07-25",
        "journal": {
            "pages": "354-370"
        },
        "citationStyles": {
            "bibtex": "@Article{Cai2016AUM,\n author = {Zhaowei Cai and Quanfu Fan and R. Feris and N. Vasconcelos},\n booktitle = {European Conference on Computer Vision},\n pages = {354-370},\n title = {A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection},\n year = {2016}\n}\n"
        }
    },
    "775_inception-resnet-v2": {
        "paperId": "b5c26ab8767d046cb6e32d959fdf726aee89bb62",
        "externalIds": {
            "DBLP": "conf/aaai/SzegedyIVA17",
            "MAG": "2952505871",
            "ArXiv": "1602.07261",
            "DOI": "10.1609/aaai.v31i1.11231",
            "CorpusId": 1023605
        },
        "corpusId": 1023605,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/b5c26ab8767d046cb6e32d959fdf726aee89bb62",
        "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
        "abstract": "\n \n Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.\n \n",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "referenceCount": 23,
        "citationCount": 12076,
        "influentialCitationCount": 1138,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11231/11090",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly is given and several new streamlined architectures for both residual and non-residual Inception Networks are presented."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-02-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1602.07261"
        },
        "citationStyles": {
            "bibtex": "@Article{Szegedy2016Inceptionv4IA,\n author = {Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alexander A. Alemi},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},\n volume = {abs/1602.07261},\n year = {2016}\n}\n"
        }
    },
    "776_vit-g_14_(lit)": {
        "paperId": "197d5867a45a2988f4dd159063cdfbfe90164962",
        "externalIds": {
            "DBLP": "journals/corr/abs-2111-07991",
            "ArXiv": "2111.07991",
            "DOI": "10.1109/CVPR52688.2022.01759",
            "CorpusId": 244117175
        },
        "corpusId": 244117175,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/197d5867a45a2988f4dd159063cdfbfe90164962",
        "title": "LiT: Zero-Shot Transfer with Locked-image text Tuning",
        "abstract": "This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text mod-els while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image mod-els with unlocked text models work best. We call this in-stance of contrastive-tuning \u201cLocked-image Tuning\u201d (LiT), which just teaches a text model to read out good repre-sentations from a pre-trained image model for new tasks. A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsu-pervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT model achieves 84.5% zero-shot trans-fer accuracy on the ImageNet test set, and 81.1% on the challenging out-of-distribution ObjectNet test set.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "referenceCount": 73,
        "citationCount": 331,
        "influentialCitationCount": 43,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2111.07991",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "In the empirical study, it is found that locked pre-trained image mod-els with unlocked text models work best, and the proposed LiT model achieves 84.5% zero-shot trans-fer accuracy on the ImageNet test set, and 81.1% on the challenging out-of-distribution Object net test set."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-11-15",
        "journal": {
            "name": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "18102-18112"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhai2021LiTZT,\n author = {Xiaohua Zhai and Xiao Wang and Basil Mustafa and A. Steiner and Daniel Keysers and Alexander Kolesnikov and Lucas Beyer},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {18102-18112},\n title = {LiT: Zero-Shot Transfer with Locked-image text Tuning},\n year = {2021}\n}\n"
        }
    },
    "777_wd+lr+m": {
        "paperId": "d5201a097dcd7681a46ae507c063c59f901a846e",
        "externalIds": {
            "ArXiv": "2110.10461",
            "DBLP": "journals/corr/abs-2110-10461",
            "CorpusId": 239049858
        },
        "corpusId": 239049858,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/d5201a097dcd7681a46ae507c063c59f901a846e",
        "title": "Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation",
        "abstract": "Machine learning training methods depend plentifully and intricately on hyperparameters, motivating automated strategies for their optimisation. Many existing algorithms restart training for each new hyperparameter choice, at considerable computational cost. Some hypergradient-based one-pass methods exist, but these either cannot be applied to arbitrary optimiser hyperparameters (such as learning rates and momenta) or take several times longer to train than their base models. We extend these existing methods to develop an approximate hypergradient-based hyperparameter optimiser which is applicable to any continuous hyperparameter appearing in a differentiable model weight update, yet requires only one training episode, with no restarts. We also provide a motivating argument for convergence to the true hypergradient, and perform tractable gradient-based optimisation of independent learning rates for each model parameter. Our method performs competitively from varied random hyperparameter initialisations on several UCI datasets and Fashion-MNIST (using a one-layer MLP), Penn Treebank (using an LSTM) and CIFAR-10 (using a ResNet-18), in time only 2-3x greater than vanilla training.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 70,
        "citationCount": 9,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work extends existing methods to develop an approximate hypergradient-based hyperparameter optimiser which is applicable to any continuoushyperparameter appearing in a differentiable model weight update, yet requires only one training episode, with no restarts."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2110.10461"
        },
        "citationStyles": {
            "bibtex": "@Article{Clarke2021ScalableOO,\n author = {Ross M. Clarke and E. T. Oldewage and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation},\n volume = {abs/2110.10461},\n year = {2021}\n}\n"
        }
    },
    "778_moe-1.1t": {
        "paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170",
        "externalIds": {
            "ArXiv": "2112.10684",
            "DBLP": "conf/emnlp/ArtetxeBGMOSLDI22",
            "DOI": "10.18653/v1/2022.emnlp-main.804",
            "CorpusId": 245334345
        },
        "corpusId": 245334345,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/fb01415a0decfa3f3d6339930e95028ae1ff4170",
        "title": "Efficient Large Scale Language Modeling with Mixtures of Experts",
        "abstract": "Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using $\\sim$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "referenceCount": 92,
        "citationCount": 120,
        "influentialCitationCount": 18,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.emnlp-main.804.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-12-20",
        "journal": {
            "pages": "11699-11732"
        },
        "citationStyles": {
            "bibtex": "@Article{Artetxe2021EfficientLS,\n author = {Mikel Artetxe and Shruti Bhosale and Naman Goyal and Todor Mihaylov and Myle Ott and Sam Shleifer and Xi Victoria Lin and Jingfei Du and Srini Iyer and Ramakanth Pasunuru and Giridhar Anantharaman and Xian Li and Shuohui Chen and H. Ak\u0131n and Mandeep Baines and Louis Martin and Xing Zhou and Punit Singh Koura and Brian O'Horo and Jeff Wang and Luke Zettlemoyer and Mona T. Diab and Zornitsa Kozareva and Ves Stoyanov},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {11699-11732},\n title = {Efficient Large Scale Language Modeling with Mixtures of Experts},\n year = {2021}\n}\n"
        }
    },
    "779_awd-lstm-drill_+_dynamic_evaluation\u2020_(wt2)": {
        "paperId": "8f0856a0920ae7e86749ab99a8d126038b98d764",
        "externalIds": {
            "DBLP": "conf/icml/PappasH19",
            "MAG": "2951639640",
            "ArXiv": "1905.05513",
            "CorpusId": 153312576
        },
        "corpusId": 153312576,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/8f0856a0920ae7e86749ab99a8d126038b98d764",
        "title": "Deep Residual Output Layers for Neural Language Generation",
        "abstract": "Many tasks, including language generation, benefit from learning the structure of the output space, particularly when the space of output labels is large and the data is sparse. State-of-the-art neural language models indirectly capture the output space structure in their classifier weights since they lack parameter sharing across output labels. Learning shared output label mappings helps, but existing methods have limited expressivity and are prone to overfitting. In this paper, we investigate the usefulness of more powerful shared mappings for output labels, and propose a deep residual output mapping with dropout between layers to better capture the structure of the output space and avoid overfitting. Evaluations on three language generation tasks show that our output label mapping can match or improve state-of-the-art recurrent and self-attention architectures, and suggest that the classifier does not necessarily need to be high-rank to better model natural language if it is better at capturing the structure of the output space.",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "referenceCount": 57,
        "citationCount": 6,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Evaluations on three language generation tasks show that the output label mapping can match or improve state-of-the-art recurrent and self-attention architectures, and suggest that the classifier does not necessarily need to be high-rank to better model natural language if it is better at capturing the structure of the output space."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-05-14",
        "journal": {
            "pages": "5000-5011"
        },
        "citationStyles": {
            "bibtex": "@Article{Pappas2019DeepRO,\n author = {Nikolaos Pappas and James Henderson},\n booktitle = {International Conference on Machine Learning},\n pages = {5000-5011},\n title = {Deep Residual Output Layers for Neural Language Generation},\n year = {2019}\n}\n"
        }
    },
    "781_squeezenet": {
        "paperId": "592d2e65489f23ebd993dbdc0c84eda9ac8aadbe",
        "externalIds": {
            "ArXiv": "1602.07360",
            "DBLP": "journals/corr/IandolaMAHDK16",
            "MAG": "2279098554",
            "CorpusId": 14136028
        },
        "corpusId": 14136028,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/592d2e65489f23ebd993dbdc0c84eda9ac8aadbe",
        "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). \nThe SqueezeNet architecture is available for download here: this https URL",
        "venue": "arXiv.org",
        "year": 2016,
        "referenceCount": 52,
        "citationCount": 6424,
        "influentialCitationCount": 754,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a small DNN architecture called SqueezeNet, which achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters and is able to compress to less than 0.5MB (510x smaller than AlexNet)."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-02-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1602.07360"
        },
        "citationStyles": {
            "bibtex": "@Article{Iandola2016SqueezeNetAA,\n author = {F. Iandola and Matthew W. Moskewicz and Khalid Ashraf and Song Han and W. Dally and K. Keutzer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size},\n volume = {abs/1602.07360},\n year = {2016}\n}\n"
        }
    },
    "782_big-g_137b": {
        "paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881",
        "externalIds": {
            "DBLP": "journals/corr/abs-2206-04615",
            "ArXiv": "2206.04615",
            "CorpusId": 263625818
        },
        "corpusId": 263625818,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881",
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
        "abstract": "Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit\"breakthrough\"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 0,
        "citationCount": 805,
        "influentialCitationCount": 67,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Evaluation of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters finds that model performance and calibration both improve with scale, but are poor in absolute terms."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-06-09",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2206.04615"
        },
        "citationStyles": {
            "bibtex": "@Article{Srivastava2022BeyondTI,\n author = {Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adri\u00e0 Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and A. Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmuller and Andrew M. Dai and Andrew La and Andrew Kyle Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and A. Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakacs and B. R. Roberts and B. S. Loe and Barret Zoph and Bartlomiej Bojanowski and Batuhan Ozyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and B. Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and C'esar Ferri Ram'irez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Daniel H Garrette and Dan Hendrycks and D. Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and D. Gonz'alez and Danielle R. Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and D. Gilboa and David Dohan and D. Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and E. D. Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and E. Donoway and Ellie Pavlick and E. Rodol\u00e0 and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and E. Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and F. Siar and Fernando Mart'inez-Plumed and Francesca Happ'e and Fran\u00e7ois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germ\u00e1n Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Xinyue Wang and Gonzalo Jaimovitch-L'opez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and H. Bogar and Henry Shevlin and Hinrich Schutze and H. Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and John Kernion and Jacob Hilton and Jaehoon Lee and J. Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Koco'n and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Narain Sohl-Dickstein and Jason Phang and Jason Wei and J. Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Oluwadara Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Jane W Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jorg Frohberg and Jos Rozen and J. Hern\u00e1ndez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and K. Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and K. Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and K. Mathewson and Kristen Chiafullo and Ksenia Shkaruta and K. Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Luca Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Col'on and Luke Metz and Lutfi Kerem cSenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ram'irez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and M. Schubert and Medina Baitemirova and Melody Arnaud and M. McElrath and Michael Yee and Michael Cohen and Michael Gu and M. Ivanitskiy and Michael Starritt and M. Strube and Michal Swkedrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Monica Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and T. MukundVarma and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and N. Keskar and Niveditha Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and P. Doshi and Pascale Fung and P. Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and P. Eckersley and Phu Mon Htut and Pi-Bei Hwang and P. Milkowski and P. Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphael Milliere and Rhythm Garg and Richard Barnes and R. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan Le Bras and Rosanne Liu and Rowan Jacobs and Rui Zhang and R. Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi S. Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and S. Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima Debnath and Siamak Shakeri and Simon Thormeyer and S. Melzi and Siva Reddy and S. Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and S. Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T Piantadosi and Stuart M. Shieber and Summer Misherghi and S. Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsunori Hashimoto and Te-Lin Wu and T. Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and T. Kornev and T. Tunduny and Tobias Gerstenberg and T. Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and V. Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and W. Fedus and W. Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yu Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},\n volume = {abs/2206.04615},\n year = {2022}\n}\n"
        }
    },
    "783_fairseq_+_uid:_variance": {
        "paperId": "06994c2810c2720b302153fc73f8c4459f05fda7",
        "externalIds": {
            "ArXiv": "2105.07144",
            "DBLP": "conf/acl/WeiMC20",
            "ACL": "2021.acl-long.404",
            "DOI": "10.18653/v1/2021.acl-long.404",
            "CorpusId": 234741913
        },
        "corpusId": 234741913,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/06994c2810c2720b302153fc73f8c4459f05fda7",
        "title": "A Cognitive Regularizer for Language Modeling",
        "abstract": "The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling. Specifically, we augment the canonical MLE objective for training language models with a regularizer that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 51,
        "citationCount": 16,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.acl-long.404.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work augments the canonical MLE objective for training language models with a regularizer that encodes uniform information density, and finds that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-05-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2105.07144"
        },
        "citationStyles": {
            "bibtex": "@Article{Wei2021ACR,\n author = {Jason Wei and Clara Meister and Ryan Cotterell},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {A Cognitive Regularizer for Language Modeling},\n volume = {abs/2105.07144},\n year = {2021}\n}\n"
        }
    },
    "784_youtube_video_recommendation_system": {
        "paperId": "e7d53f538f5239739d1f943c81d17e4a167c65c6",
        "externalIds": {
            "MAG": "2040367556",
            "DBLP": "conf/recsys/DavidsonLLNVGGHLLS10",
            "DOI": "10.1145/1864708.1864770",
            "CorpusId": 1145979
        },
        "corpusId": 1145979,
        "publicationVenue": {
            "id": "61275a16-1e0d-479f-ac4e-f295310761f0",
            "name": "ACM Conference on Recommender Systems",
            "type": "conference",
            "alternate_names": [
                "Conf Recomm Syst",
                "RecSys",
                "ACM Conf Recomm Syst",
                "Conference on Recommender Systems"
            ],
            "url": "http://recsys.acm.org/"
        },
        "url": "https://www.semanticscholar.org/paper/e7d53f538f5239739d1f943c81d17e4a167c65c6",
        "title": "The YouTube video recommendation system",
        "abstract": "We discuss the video recommendation system in use at YouTube, the world's most popular online video community. The system recommends personalized sets of videos to users based on their activity on the site. We discuss some of the unique challenges that the system faces and how we address them. In addition, we provide details on the experimentation and evaluation framework used to test and tune new algorithms. We also present some of the findings from these experiments.",
        "venue": "ACM Conference on Recommender Systems",
        "year": 2010,
        "referenceCount": 6,
        "citationCount": 1103,
        "influentialCitationCount": 59,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.inf.unibz.it/~ricci/ISR/papers/p293-davidson.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The video recommendation system in use at YouTube, the world's most popular online video community, is discussed, with details on the experimentation and evaluation framework used to test and tune new algorithms."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2010-09-26",
        "journal": {
            "pages": "293-296"
        },
        "citationStyles": {
            "bibtex": "@Article{Davidson2010TheYV,\n author = {James Davidson and Benjamin Liebald and Junning Liu and Palash Nandy and T. Vleet and U. Gargi and Sujoy Gupta and Yu He and Mike Lambert and Blake Livingston and D. Sampath},\n booktitle = {ACM Conference on Recommender Systems},\n pages = {293-296},\n title = {The YouTube video recommendation system},\n year = {2010}\n}\n"
        }
    },
    "785_grover-mega": {
        "paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "externalIds": {
            "MAG": "2971008823",
            "DBLP": "conf/nips/ZellersHRBFRC19",
            "ArXiv": "1905.12616",
            "CorpusId": 168169824
        },
        "corpusId": 168169824,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "title": "Defending Against Neural Fake News",
        "abstract": "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "referenceCount": 73,
        "citationCount": 771,
        "influentialCitationCount": 114,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A model for controllable text generation called Grover, found that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data, and the best defense against Grover turns out to be Grover itself, with 92% accuracy."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-05-29",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1905.12616"
        },
        "citationStyles": {
            "bibtex": "@Article{Zellers2019DefendingAN,\n author = {Rowan Zellers and Ari Holtzman and Hannah Rashkin and Yonatan Bisk and Ali Farhadi and Franziska Roesner and Yejin Choi},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Defending Against Neural Fake News},\n volume = {abs/1905.12616},\n year = {2019}\n}\n"
        }
    },
    "786_temporal_convolutional_attention-based_network(tcan)_(wt2)": {
        "paperId": "d4e16ed9a80cef2db8c4668344e7479d33d5c58e",
        "externalIds": {
            "MAG": "3007057000",
            "DBLP": "journals/corr/abs-2002-12530",
            "ArXiv": "2002.12530",
            "CorpusId": 211572518
        },
        "corpusId": 211572518,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/d4e16ed9a80cef2db8c4668344e7479d33d5c58e",
        "title": "Temporal Convolutional Attention-based Network For Sequence Modeling",
        "abstract": "With the development of feed-forward models, the default model for sequence modeling has gradually evolved to replace recurrent networks. Many powerful feed-forward models based on convolutional networks and attention mechanism were proposed and show more potential to handle sequence modeling tasks. We wonder that is there an architecture that can not only achieve an approximate substitution of recurrent network, but also absorb the advantages of feed-forward models. So we propose an exploratory architecture referred to Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism. TCAN includes two parts, one is Temporal Attention (TA) which captures relevant features inside the sequence, the other is Enhanced Residual (ER) which extracts shallow layer's important information and transfers to deep layers. We improve the state-of-the-art results of bpc/perplexity to 26.92 on word-level PTB, 1.043 on character-level PTB, and 6.66 on WikiText-2.",
        "venue": "arXiv.org",
        "year": 2020,
        "referenceCount": 22,
        "citationCount": 29,
        "influentialCitationCount": 2,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes an exploratory architecture referred to Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism and improves the state-of-the-art results of bpc/perplexity."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-02-28",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2002.12530"
        },
        "citationStyles": {
            "bibtex": "@Article{Hao2020TemporalCA,\n author = {Hongyan Hao and Yan Wang and Yudi Xia and Jian Zhao and S. Furao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Temporal Convolutional Attention-based Network For Sequence Modeling},\n volume = {abs/2002.12530},\n year = {2020}\n}\n"
        }
    },
    "787_hopfield_network": {
        "paperId": "98b4d4e24aab57ab4e1124ff8106909050645cfa",
        "externalIds": {
            "MAG": "2189586318",
            "DOI": "10.1073/PNAS.79.8.2554",
            "CorpusId": 784288,
            "PubMed": "6953413"
        },
        "corpusId": 784288,
        "publicationVenue": {
            "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
            "name": "Proceedings of the National Academy of Sciences of the United States of America",
            "type": "journal",
            "alternate_names": [
                "PNAS",
                "PNAS online",
                "Proceedings of the National Academy of Sciences of the United States of America.",
                "Proc National Acad Sci",
                "Proceedings of the National Academy of Sciences",
                "Proc National Acad Sci u s Am"
            ],
            "issn": "0027-8424",
            "alternate_issns": [
                "1091-6490"
            ],
            "url": "https://www.jstor.org/journal/procnatiacadscie",
            "alternate_urls": [
                "http://www.pnas.org/",
                "https://www.pnas.org/",
                "http://www.jstor.org/journals/00278424.html",
                "www.pnas.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/98b4d4e24aab57ab4e1124ff8106909050645cfa",
        "title": "Neural networks and physical systems with emergent collective computational abilities.",
        "abstract": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.",
        "venue": "Proceedings of the National Academy of Sciences of the United States of America",
        "year": 1982,
        "referenceCount": 27,
        "citationCount": 17951,
        "influentialCitationCount": 890,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://europepmc.org/articles/pmc346238?pdf=render",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Physics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A model of a system having a large number of simple equivalent components, based on aspects of neurobiology but readily adapted to integrated circuits, produces a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1982-04-01",
        "journal": {
            "name": "Proceedings of the National Academy of Sciences of the United States of America",
            "pages": "\n          2554-8\n        ",
            "volume": "79 8"
        },
        "citationStyles": {
            "bibtex": "@Article{Hopfield1982NeuralNA,\n author = {J. Hopfield},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences of the United States of America},\n pages = {\n          2554-8\n        },\n title = {Neural networks and physical systems with emergent collective computational abilities.},\n volume = {79 8},\n year = {1982}\n}\n"
        }
    },
    "788_rl_mapping_instructions_(troubleshooting)": {
        "paperId": "cc1648c91ffda21bbe6e5f08f69c683588fc384c",
        "externalIds": {
            "MAG": "2122223050",
            "DBLP": "conf/acl/BranavanCZB09",
            "ACL": "P09-1010",
            "DOI": "10.3115/1687878.1687892",
            "CorpusId": 5249151
        },
        "corpusId": 5249151,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/cc1648c91ffda21bbe6e5f08f69c683588fc384c",
        "title": "Reinforcement Learning for Mapping Instructions to Actions",
        "abstract": "In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains --- Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2009,
        "referenceCount": 22,
        "citationCount": 308,
        "influentialCitationCount": 18,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.5555/1687878.1687892",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a reinforcement learning approach for mapping natural language instructions to sequences of executable actions, and uses a policy gradient algorithm to estimate the parameters of a log-linear model for action selection."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "publicationDate": "2009-08-02",
        "journal": {
            "pages": "82-90"
        },
        "citationStyles": {
            "bibtex": "@Article{Branavan2009ReinforcementLF,\n author = {S. Branavan and Harr Chen and Luke Zettlemoyer and R. Barzilay},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {82-90},\n title = {Reinforcement Learning for Mapping Instructions to Actions},\n year = {2009}\n}\n"
        }
    },
    "789_neural_architecture_search_with_base_8_and_shared_embeddings": {
        "paperId": "67d968c7450878190e45ac7886746de867bf673d",
        "externalIds": {
            "MAG": "2952431534",
            "ArXiv": "1611.01578",
            "DBLP": "journals/corr/ZophL16",
            "CorpusId": 12713052
        },
        "corpusId": 12713052,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/67d968c7450878190e45ac7886746de867bf673d",
        "title": "Neural Architecture Search with Reinforcement Learning",
        "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "referenceCount": 73,
        "citationCount": 4734,
        "influentialCitationCount": 577,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper uses a recurrent network to generate the model descriptions of neural networks and trains this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-11-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1611.01578"
        },
        "citationStyles": {
            "bibtex": "@Article{Zoph2016NeuralAS,\n author = {Barret Zoph and Quoc V. Le},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Neural Architecture Search with Reinforcement Learning},\n volume = {abs/1611.01578},\n year = {2016}\n}\n"
        }
    },
    "790_hanabi_4_player": {
        "paperId": "6a505dbfb89cf05344457bf85b2e8307af5c4ad0",
        "externalIds": {
            "DBLP": "journals/corr/abs-1902-00506",
            "ArXiv": "1902.00506",
            "MAG": "2913781869",
            "DOI": "10.1016/j.artint.2019.103216",
            "CorpusId": 59553476
        },
        "corpusId": 59553476,
        "publicationVenue": {
            "id": "96018464-22dc-4b5c-a172-c2f4a30ce131",
            "name": "Artificial Intelligence",
            "type": "journal",
            "alternate_names": [
                "Artif Intell"
            ],
            "issn": "0004-3702",
            "alternate_issns": [
                "2633-1403",
                "2710-1673",
                "2710-1681"
            ],
            "url": "http://www.elsevier.com/locate/artint",
            "alternate_urls": [
                "http://www.sciencedirect.com/science/journal/00043702",
                "https://www.journals.elsevier.com/artificial-intelligence"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/6a505dbfb89cf05344457bf85b2e8307af5c4ad0",
        "title": "The Hanabi Challenge: A New Frontier for AI Research",
        "abstract": null,
        "venue": "Artificial Intelligence",
        "year": 2019,
        "referenceCount": 91,
        "citationCount": 275,
        "influentialCitationCount": 33,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is argued that Hanabi elevates reasoning about the beliefs and intentions of other agents to the foreground and developing novel techniques for such theory of mind reasoning will not only be crucial for success in Hanabi, but also in broader collaborative efforts, especially those with human partners."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-02-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1902.00506"
        },
        "citationStyles": {
            "bibtex": "@Article{Bard2019TheHC,\n author = {Nolan Bard and Jakob N. Foerster and A. Chandar and Neil Burch and Marc Lanctot and H. F. Song and Emilio Parisotto and Vincent Dumoulin and Subhodeep Moitra and Edward Hughes and Iain Dunning and Shibl Mourad and H. Larochelle and Marc G. Bellemare and Michael H. Bowling},\n booktitle = {Artificial Intelligence},\n journal = {ArXiv},\n title = {The Hanabi Challenge: A New Frontier for AI Research},\n volume = {abs/1902.00506},\n year = {2019}\n}\n"
        }
    },
    "791_pnas-net": {
        "paperId": "5f79398057bf0bbda9ff50067bc1f2950c2a2266",
        "externalIds": {
            "ArXiv": "1712.00559",
            "MAG": "2949714964",
            "DBLP": "journals/corr/abs-1712-00559",
            "DOI": "10.1007/978-3-030-01246-5_2",
            "CorpusId": 40430109
        },
        "corpusId": 40430109,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/5f79398057bf0bbda9ff50067bc1f2950c2a2266",
        "title": "Progressive Neural Architecture Search",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2017,
        "referenceCount": 53,
        "citationCount": 1799,
        "influentialCitationCount": 283,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1712.00559",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms using a sequential model-based optimization (SMBO) strategy."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-12-02",
        "journal": {
            "pages": "19-35"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2017ProgressiveNA,\n author = {Chenxi Liu and Barret Zoph and Jonathon Shlens and Wei Hua and Li-Jia Li and Li Fei-Fei and A. Yuille and Jonathan Huang and K. Murphy},\n booktitle = {European Conference on Computer Vision},\n pages = {19-35},\n title = {Progressive Neural Architecture Search},\n year = {2017}\n}\n"
        }
    },
    "793_gpt-2_(fine-tuned_with_hydra)": {
        "paperId": "36e696827d2d8d2a07bea711b3e1fda9ee1c426f",
        "externalIds": {
            "ArXiv": "2110.08633",
            "DBLP": "journals/corr/abs-2110-08633",
            "CorpusId": 239017028
        },
        "corpusId": 239017028,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/36e696827d2d8d2a07bea711b3e1fda9ee1c426f",
        "title": "Hydra: A System for Large Multi-Model Deep Learning",
        "abstract": "Scaling up model depth and size is now a common approach to raise accuracy in many deep learning (DL) applications, as evidenced by the widespread success of multi-billion or even trillion parameter models in natural language processing (NLP) research. Despite success in DL research and at major technology companies, broader practical adoption of such large models among domain scientists and businesses is still bottlenecked by GPU memory limits, high training costs, and low GPU availability, even on public clouds. Model selection needs further compound these resource challenges: users often need to compare dozens of models with different hyper-parameters or neural architectures to suit their specific task and dataset. In this paper, we present Hydra, a system designed to tackle such challenges by enabling out-of-the-box scaling for multi-large-model DL workloads on even commodity GPUs in a resource-efficient manner. Hydra is the first approach to holistically optimize the execution of multi-model workloads for large DL models. We do this by adapting prior\"model-parallel\"execution schemes to work with scalable parameter offloading across the memory hierarchy and further hybridizing this approach with task-parallel job scheduling techniques. Hydra decouples scalability of model parameters from parallelism of execution, thus enabling DL users to train even a 6-billion parameter model on a single commodity GPU. It also fully exploits the speedup potential of task parallelism in multi-GPU setups, yielding near-linear strong scaling and making rigorous model selection perhaps more practical for such models. We evaluate end-to-end performance by fine-tuning GPT-2 for language modeling. We find that Hydra offers between 50% and 100% higher training throughput than even the best settings of state-of-the-art industrial frameworks such as DeepSpeed and GPipe for multi-large-model training.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 63,
        "citationCount": 4,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Hydra is the first approach to holistically optimize the execution of multi-model workloads for large DL models by adapting prior\"model-parallel\"execution schemes to work with scalable parameter offloading across the memory hierarchy and further hybridizing this approach with task-parallel job scheduling techniques."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-16",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2110.08633"
        },
        "citationStyles": {
            "bibtex": "@Article{Nagrecha2021HydraAS,\n author = {Kabir Nagrecha and Arun Kumar},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Hydra: A System for Large Multi-Model Deep Learning},\n volume = {abs/2110.08633},\n year = {2021}\n}\n"
        }
    },
    "794_binarized_neural_network_(mnist)": {
        "paperId": "123ae35aa7d6838c817072032ce5615bb891652d",
        "externalIds": {
            "DBLP": "journals/corr/CourbariauxB16",
            "MAG": "2260663238",
            "CorpusId": 6564560
        },
        "corpusId": 6564560,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/123ae35aa7d6838c817072032ce5615bb891652d",
        "title": "BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1",
        "abstract": "We introduce BinaryNet, a method which trains DNNs with binary weights and activations when computing parameters\u2019 gradient. We show that it is possible to train a Multi Layer Perceptron (MLP) on MNIST and ConvNets on CIFAR-10 and SVHN with BinaryNet and achieve nearly state-of-the-art results. At run-time, BinaryNet drastically reduces memory usage and replaces most multiplications by 1-bit exclusive-not-or (XNOR) operations, which might have a big impact on both general-purpose and dedicated Deep Learning hardware. We wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST MLP 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classi\ufb01cation accuracy. The code for BinaryNet is available.",
        "venue": "arXiv.org",
        "year": 2016,
        "referenceCount": 43,
        "citationCount": 654,
        "influentialCitationCount": 78,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "BinaryNet, a method which trains DNNs with binary weights and activations when computing parameters\u2019 gradient is introduced, which drastically reduces memory usage and replaces most multiplications by 1-bit exclusive-not-or (XNOR) operations, which might have a big impact on both general-purpose and dedicated Deep Learning hardware."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-02-09",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1602.02830"
        },
        "citationStyles": {
            "bibtex": "@Article{Courbariaux2016BinaryNetTD,\n author = {Matthieu Courbariaux and Yoshua Bengio},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1},\n volume = {abs/1602.02830},\n year = {2016}\n}\n"
        }
    },
    "796_feedforward_nn": {
        "paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649",
        "externalIds": {
            "DBLP": "journals/jmlr/GlorotB10",
            "MAG": "2606251538",
            "CorpusId": 5575601
        },
        "corpusId": 5575601,
        "publicationVenue": {
            "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
            "name": "International Conference on Artificial Intelligence and Statistics",
            "type": "conference",
            "alternate_names": [
                "AISTATS",
                "Int Conf Artif Intell Stat"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649",
        "title": "Understanding the difficulty of training deep feedforward neural networks",
        "abstract": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a \u201cbetter\u201d basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "year": 2010,
        "referenceCount": 21,
        "citationCount": 15771,
        "influentialCitationCount": 794,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future."
        },
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "publicationDate": "2010-03-31",
        "journal": {
            "pages": "249-256"
        },
        "citationStyles": {
            "bibtex": "@Article{Glorot2010UnderstandingTD,\n author = {Xavier Glorot and Yoshua Bengio},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n pages = {249-256},\n title = {Understanding the difficulty of training deep feedforward neural networks},\n year = {2010}\n}\n"
        }
    },
    "797_progressivegan": {
        "paperId": "744fe47157477235032f7bb3777800f9f2f45e52",
        "externalIds": {
            "MAG": "2766527293",
            "DBLP": "conf/iclr/KarrasALL18",
            "ArXiv": "1710.10196",
            "CorpusId": 3568073
        },
        "corpusId": 3568073,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52",
        "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
        "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "referenceCount": 65,
        "citationCount": 6009,
        "influentialCitationCount": 1018,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new training methodology for generative adversarial networks is described, starting from a low resolution, and adding new layers that model increasingly fine details as training progresses, allowing for images of unprecedented quality."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-10-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1710.10196"
        },
        "citationStyles": {
            "bibtex": "@Article{Karras2017ProgressiveGO,\n author = {Tero Karras and Timo Aila and S. Laine and J. Lehtinen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},\n volume = {abs/1710.10196},\n year = {2017}\n}\n"
        }
    },
    "798_rita-xlarge": {
        "paperId": "6c790c1a4233f5201b052b149a3c6c76261aee68",
        "externalIds": {
            "DBLP": "journals/corr/abs-2205-05789",
            "ArXiv": "2205.05789",
            "DOI": "10.48550/arXiv.2205.05789",
            "CorpusId": 248722069
        },
        "corpusId": 248722069,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/6c790c1a4233f5201b052b149a3c6c76261aee68",
        "title": "RITA: a Study on Scaling Up Generative Protein Sequence Models",
        "abstract": "In this work we introduce RITA: a suite of autoregressive generative models for protein sequences, with up to 1.2 billion parameters, trained on over 280 million protein sequences belonging to the UniRef-100 database. Such generative models hold the promise of greatly accelerating protein design. We conduct the first systematic study of how capabilities evolve with model size for autoregressive transformers in the protein domain: we evaluate RITA models in next amino acid prediction, zero-shot fitness, and enzyme function prediction, showing benefits from increased scale. We release the RITA models openly, to the benefit of the research community.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 45,
        "citationCount": 47,
        "influentialCitationCount": 6,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2205.05789",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces RITA: a suite of autoregressive generative models for protein sequences, with up to 1.2 billion parameters, trained on over 280 million protein sequences belonging to the UniRef-100 database, and conducts the first systematic study of how capabilities evolve with model size for Autoregressive transformers in the protein domain."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-05-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2205.05789"
        },
        "citationStyles": {
            "bibtex": "@Article{Hesslow2022RITAAS,\n author = {Daniel Hesslow and Niccol\u00f3 Zanichelli and Pascal Notin and Iacopo Poli and D. Marks},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {RITA: a Study on Scaling Up Generative Protein Sequence Models},\n volume = {abs/2205.05789},\n year = {2022}\n}\n"
        }
    },
    "799_sparse_all-mlp": {
        "paperId": "3ff00d83a2dac26b5885f312e7ff8dedcdadf71c",
        "externalIds": {
            "ArXiv": "2203.06850",
            "DBLP": "journals/corr/abs-2203-06850",
            "DOI": "10.48550/arXiv.2203.06850",
            "CorpusId": 247447253
        },
        "corpusId": 247447253,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/3ff00d83a2dac26b5885f312e7ff8dedcdadf71c",
        "title": "Efficient Language Modeling with Sparse all-MLP",
        "abstract": "All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2$\\times$ improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 40,
        "citationCount": 10,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2203.06850",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work analyzes the limitations of MLPs in expressiveness, and proposes sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions, which significantly increase model capacity and expressiveness while keeping the compute constant."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-03-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2203.06850"
        },
        "citationStyles": {
            "bibtex": "@Article{Yu2022EfficientLM,\n author = {Ping Yu and Mikel Artetxe and Myle Ott and Sam Shleifer and Hongyu Gong and Ves Stoyanov and Xian Li},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Efficient Language Modeling with Sparse all-MLP},\n volume = {abs/2203.06850},\n year = {2022}\n}\n"
        }
    },
    "800_omnivec": {
        "paperId": "2d47489190be65e601a3d35e6d348b6723bfdfc8",
        "externalIds": {
            "DBLP": "journals/corr/abs-2311-05709",
            "ArXiv": "2311.05709",
            "DOI": "10.48550/arXiv.2311.05709",
            "CorpusId": 265128674
        },
        "corpusId": 265128674,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/2d47489190be65e601a3d35e6d348b6723bfdfc8",
        "title": "OmniVec: Learning robust representations with cross modal sharing",
        "abstract": "Majority of research in learning based methods has been towards designing and training networks for specific tasks. However, many of the learning based tasks, across modalities, share commonalities and could be potentially tackled in a joint framework. We present an approach in such direction, to learn multiple tasks, in multiple modalities, with a unified architecture. The proposed network is composed of task specific encoders, a common trunk in the middle, followed by task specific prediction heads. We first pre-train it by self-supervised masked training, followed by sequential training for the different tasks. We train the network on all major modalities, e.g.\\ visual, audio, text and 3D, and report results on $22$ diverse and challenging public benchmarks. We demonstrate empirically that, using a joint network to train across modalities leads to meaningful information sharing and this allows us to achieve state-of-the-art results on most of the benchmarks. We also show generalization of the trained network on cross-modal tasks as well as unseen datasets and tasks.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 112,
        "citationCount": 5,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated empirically that, using a joint network to train across modalities leads to meaningful information sharing and this allows the proposed network to achieve state-of-the-art results on most of the benchmarks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-11-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2311.05709"
        },
        "citationStyles": {
            "bibtex": "@Article{Srivastava2023OmniVecLR,\n author = {Siddharth Srivastava and Gaurav Sharma},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {OmniVec: Learning robust representations with cross modal sharing},\n volume = {abs/2311.05709},\n year = {2023}\n}\n"
        }
    },
    "801_transformerxl_+_fwl": {
        "paperId": "37ba9c33025fb31f25436010e12c65a0bafc0e1f",
        "externalIds": {
            "ACL": "2022.emnlp-main.661",
            "ArXiv": "2212.02475",
            "DBLP": "journals/corr/abs-2212-02475",
            "DOI": "10.48550/arXiv.2212.02475",
            "CorpusId": 254247079
        },
        "corpusId": 254247079,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/37ba9c33025fb31f25436010e12c65a0bafc0e1f",
        "title": "Meta-Learning Fast Weight Language Models",
        "abstract": "Dynamic evaluation of language models (LMs) adapts model parameters at test time using gradient information from previous tokens and substantially improves LM performance. However, it requires over 3x more compute than standard inference. We present Fast Weight Layers (FWLs), a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention. A key improvement over dynamic evaluation is that FWLs can also be applied at training time, so the model learns to make good use of gradient updates. FWLs can easily be added on top of existing transformer models, require relatively little extra compute or memory to run, and significantly improve language modeling perplexity.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2022,
        "referenceCount": 24,
        "citationCount": 6,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2212.02475",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Fast Weight Layers are presented, a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention and can also be applied at training time, so the model learns to make good use of gradient updates."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-12-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2212.02475"
        },
        "citationStyles": {
            "bibtex": "@Article{Clark2022MetaLearningFW,\n author = {Kevin Clark and Kelvin Guu and Ming-Wei Chang and Panupong Pasupat and Geoffrey E. Hinton and Mohammad Norouzi},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Meta-Learning Fast Weight Language Models},\n volume = {abs/2212.02475},\n year = {2022}\n}\n"
        }
    },
    "802_omegaplm": {
        "paperId": "867d80c8779e1d301a5fc6e267e263f7e4c4c5c7",
        "externalIds": {
            "DOI": "10.1101/2022.07.21.500999",
            "CorpusId": 251042608
        },
        "corpusId": 251042608,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/867d80c8779e1d301a5fc6e267e263f7e4c4c5c7",
        "title": "High-resolution de novo structure prediction from primary sequence",
        "abstract": "Recent breakthroughs have used deep learning to exploit evolutionary information in multiple sequence alignments (MSAs) to accurately predict protein structures. However, MSAs of homologous proteins are not always available, such as with orphan proteins or fast-evolving proteins like antibodies, and a protein typically folds in a natural setting from its primary amino acid sequence into its three-dimensional structure, suggesting that evolutionary information and MSAs should not be necessary to predict a protein\u2019s folded form. Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures. OmegaFold enables accurate predictions on orphan proteins that do not belong to any functionally characterized protein family and antibodies that tend to have noisy MSAs due to fast evolution. Our study fills a much-encountered gap in structure prediction and brings us a step closer to understanding protein folding in nature.",
        "venue": "bioRxiv",
        "year": 2022,
        "referenceCount": 90,
        "citationCount": 195,
        "influentialCitationCount": 22,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2022/07/22/2022.07.21.500999.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "OmegaFold is introduced, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone, using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures."
        },
        "publicationTypes": null,
        "publicationDate": "2022-07-22",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Wu2022HighresolutionDN,\n author = {R. Wu and Fan Ding and Rui Wang and Rui Shen and Xiwen Zhang and Shitong Luo and Chenpeng Su and Zuofan Wu and Qi Xie and B. Berger and Jianzhu Ma and Jian Peng},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {High-resolution de novo structure prediction from primary sequence},\n year = {2022}\n}\n"
        }
    },
    "803_dncnn": {
        "paperId": "0c00a328fa7cd56ee60338c54e89bd48310db80b",
        "externalIds": {
            "DBLP": "journals/corr/ZhangZCM016",
            "MAG": "3099906833",
            "ArXiv": "1608.03981",
            "DOI": "10.1109/TIP.2017.2662206",
            "CorpusId": 996788,
            "PubMed": "28166495"
        },
        "corpusId": 996788,
        "publicationVenue": {
            "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
            "name": "IEEE Transactions on Image Processing",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Image Process"
            ],
            "issn": "1057-7149",
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
            "alternate_urls": [
                "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b",
        "title": "Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising",
        "abstract": "The discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks, such as Gaussian denoising, single image super-resolution, and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.",
        "venue": "IEEE Transactions on Image Processing",
        "year": 2016,
        "referenceCount": 51,
        "citationCount": 5680,
        "influentialCitationCount": 992,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1608.03981",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper investigates the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image Denoising, and uses residual learning and batch normalization to speed up the training process as well as boost theDenoising performance."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-08-13",
        "journal": {
            "name": "IEEE Transactions on Image Processing",
            "pages": "3142-3155",
            "volume": "26"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2016BeyondAG,\n author = {K. Zhang and W. Zuo and Yunjin Chen and Deyu Meng and Lei Zhang},\n booktitle = {IEEE Transactions on Image Processing},\n journal = {IEEE Transactions on Image Processing},\n pages = {3142-3155},\n title = {Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising},\n volume = {26},\n year = {2016}\n}\n"
        }
    },
    "804_named_entity_recognition_model": {
        "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
        "externalIds": {
            "MAG": "3037932933",
            "ArXiv": "1607.06450",
            "DBLP": "journals/corr/BaKH16",
            "CorpusId": 8236317
        },
        "corpusId": 8236317,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/97fb4e3d45bb098e27e0071448b6152217bd35a5",
        "title": "Layer Normalization",
        "abstract": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
        "venue": "arXiv.org",
        "year": 2016,
        "referenceCount": 33,
        "citationCount": 8036,
        "influentialCitationCount": 363,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-07-21",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1607.06450"
        },
        "citationStyles": {
            "bibtex": "@Article{Ba2016LayerN,\n author = {Jimmy Ba and J. Kiros and Geoffrey E. Hinton},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Layer Normalization},\n volume = {abs/1607.06450},\n year = {2016}\n}\n"
        }
    },
    "805_yolo": {
        "paperId": "f8e79ac0ea341056ef20f2616628b3e964764cfd",
        "externalIds": {
            "MAG": "2950099460",
            "DBLP": "conf/cvpr/RedmonDGF16",
            "ArXiv": "1506.02640",
            "DOI": "10.1109/CVPR.2016.91",
            "CorpusId": 206594738
        },
        "corpusId": 206594738,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd",
        "title": "You Only Look Once: Unified, Real-Time Object Detection",
        "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "referenceCount": 40,
        "citationCount": 28107,
        "influentialCitationCount": 2729,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1506.02640",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background, and outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-06-08",
        "journal": {
            "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "779-788"
        },
        "citationStyles": {
            "bibtex": "@Article{Redmon2015YouOL,\n author = {Joseph Redmon and S. Divvala and Ross B. Girshick and Ali Farhadi},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {779-788},\n title = {You Only Look Once: Unified, Real-Time Object Detection},\n year = {2015}\n}\n"
        }
    },
    "807_awd-lstm_-_3-layer_lstm_(tied)_+_continuous_cache_pointer_(wt2)": {
        "paperId": "58c6f890a1ae372958b7decf56132fe258152722",
        "externalIds": {
            "DBLP": "conf/iclr/MerityKS18",
            "ArXiv": "1708.02182",
            "MAG": "2952022146",
            "CorpusId": 212756
        },
        "corpusId": 212756,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/58c6f890a1ae372958b7decf56132fe258152722",
        "title": "Regularizing and Optimizing LSTM Language Models",
        "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "referenceCount": 47,
        "citationCount": 1015,
        "influentialCitationCount": 194,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization and introduces NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-08-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1708.02182"
        },
        "citationStyles": {
            "bibtex": "@Article{Merity2017RegularizingAO,\n author = {Stephen Merity and N. Keskar and R. Socher},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Regularizing and Optimizing LSTM Language Models},\n volume = {abs/1708.02182},\n year = {2017}\n}\n"
        }
    },
    "808_maxout_networks": {
        "paperId": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
        "externalIds": {
            "MAG": "3037950864",
            "DBLP": "conf/icml/GoodfellowWMCB13",
            "ArXiv": "1302.4389",
            "CorpusId": 10600578
        },
        "corpusId": 10600578,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/b7b915d508987b73b61eccd2b237e7ed099a2d29",
        "title": "Maxout Networks",
        "abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.",
        "venue": "International Conference on Machine Learning",
        "year": 2013,
        "referenceCount": 26,
        "citationCount": 2089,
        "influentialCitationCount": 228,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simple new model called maxout is defined designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2013-02-18",
        "journal": {
            "pages": "1319-1327"
        },
        "citationStyles": {
            "bibtex": "@Article{Goodfellow2013MaxoutN,\n author = {I. Goodfellow and David Warde-Farley and Mehdi Mirza and Aaron C. Courville and Yoshua Bengio},\n booktitle = {International Conference on Machine Learning},\n pages = {1319-1327},\n title = {Maxout Networks},\n year = {2013}\n}\n"
        }
    },
    "810_bloomz-176b": {
        "paperId": "4972b88f8f324a4fa18e921f62a9857af2b5fc7b",
        "externalIds": {
            "ACL": "2023.acl-long.891",
            "DBLP": "conf/acl/MuennighoffWSRB23",
            "DOI": "10.18653/v1/2023.acl-long.891",
            "CorpusId": 253264914
        },
        "corpusId": 253264914,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/4972b88f8f324a4fa18e921f62a9857af2b5fc7b",
        "title": "Crosslingual Generalization through Multitask Finetuning",
        "abstract": "Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/ bigscience-workshop/xmtf.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2023,
        "referenceCount": 0,
        "citationCount": 240,
        "influentialCitationCount": 25,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.891.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Surprisingly, models are capable of zero-shot generalization to tasks in languages they have never intentionally seen and it is conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "15991-16111"
        },
        "citationStyles": {
            "bibtex": "@Article{Muennighoff2023CrosslingualGT,\n author = {Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng-Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir R. Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {15991-16111},\n title = {Crosslingual Generalization through Multitask Finetuning},\n year = {2023}\n}\n"
        }
    },
    "813_segatron-xl_large,_m=384_+_hcp": {
        "paperId": "609e1c196fced582caf9113aa6a003b64d3cdcd6",
        "externalIds": {
            "DBLP": "conf/acl/BaiWS022",
            "ACL": "2022.acl-long.96",
            "ArXiv": "2203.10692",
            "DOI": "10.48550/arXiv.2203.10692",
            "CorpusId": 247593920
        },
        "corpusId": 247593920,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/609e1c196fced582caf9113aa6a003b64d3cdcd6",
        "title": "Better Language Model with Hypernym Class Prediction",
        "abstract": "Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2022,
        "referenceCount": 39,
        "citationCount": 6,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.10692",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This study hypothesizes that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words and train large neural LMs by gradually annealing from predicting the class to token prediction during training."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-03-21",
        "journal": {
            "pages": "1352-1362"
        },
        "citationStyles": {
            "bibtex": "@Article{Bai2022BetterLM,\n author = {Richard He Bai and Tong Wang and Alessandro Sordoni and Peng Shi},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {1352-1362},\n title = {Better Language Model with Hypernym Class Prediction},\n year = {2022}\n}\n"
        }
    },
    "814_hiero": {
        "paperId": "ad3d2f463916784d0c14a19936c1544309a0a440",
        "externalIds": {
            "ACL": "P05-1033",
            "DBLP": "conf/acl/Chiang05",
            "MAG": "2152263452",
            "DOI": "10.3115/1219840.1219873",
            "CorpusId": 384994
        },
        "corpusId": 384994,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/ad3d2f463916784d0c14a19936c1544309a0a440",
        "title": "A Hierarchical Phrase-Based Model for Statistical Machine Translation",
        "abstract": "We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2005,
        "referenceCount": 28,
        "citationCount": 1297,
        "influentialCitationCount": 202,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1219840.1219873",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information, which can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2005-06-25",
        "journal": {
            "pages": "263-270"
        },
        "citationStyles": {
            "bibtex": "@Article{Chiang2005AHP,\n author = {David Chiang},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {263-270},\n title = {A Hierarchical Phrase-Based Model for Statistical Machine Translation},\n year = {2005}\n}\n"
        }
    },
    "815_rnn-wer": {
        "paperId": "0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f",
        "externalIds": {
            "MAG": "3111804057",
            "DBLP": "conf/icml/GravesJ14",
            "CorpusId": 1166498
        },
        "corpusId": 1166498,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f",
        "title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks",
        "abstract": "This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.",
        "venue": "International Conference on Machine Learning",
        "year": 2014,
        "referenceCount": 23,
        "citationCount": 2069,
        "influentialCitationCount": 151,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation is presented, based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-06-21",
        "journal": {
            "pages": "1764-1772"
        },
        "citationStyles": {
            "bibtex": "@Article{Graves2014TowardsES,\n author = {Alex Graves and N. Jaitly},\n booktitle = {International Conference on Machine Learning},\n pages = {1764-1772},\n title = {Towards End-To-End Speech Recognition with Recurrent Neural Networks},\n year = {2014}\n}\n"
        }
    },
    "816_consert": {
        "paperId": "077c713bccd9d2c7fde68d4cbde06ab0f07a6855",
        "externalIds": {
            "ArXiv": "2105.11741",
            "DBLP": "journals/corr/abs-2105-11741",
            "ACL": "2021.acl-long.393",
            "DOI": "10.18653/v1/2021.acl-long.393",
            "CorpusId": 235187266
        },
        "corpusId": 235187266,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/077c713bccd9d2c7fde68d4cbde06ab0f07a6855",
        "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer",
        "abstract": "Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 39,
        "citationCount": 393,
        "influentialCitationCount": 48,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.acl-long.393.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "ConSERT is presented, a Contrastive Framework for Self-Supervised SEntence Representation Transfer that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way and achieves new state-of-the-art performance on STS tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-05-25",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2105.11741"
        },
        "citationStyles": {
            "bibtex": "@Article{Yan2021ConSERTAC,\n author = {Yuanmeng Yan and Rumei Li and Sirui Wang and Fuzheng Zhang and Wei Wu and Weiran Xu},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer},\n volume = {abs/2105.11741},\n year = {2021}\n}\n"
        }
    },
    "817_mcdnn_(mnist)": {
        "paperId": "398c296d0cc7f9d180f84969f8937e6d3a413796",
        "externalIds": {
            "MAG": "2141125852",
            "DBLP": "conf/cvpr/CiresanMS12",
            "ArXiv": "1202.2745",
            "DOI": "10.1109/CVPR.2012.6248110",
            "CorpusId": 2161592
        },
        "corpusId": 2161592,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/398c296d0cc7f9d180f84969f8937e6d3a413796",
        "title": "Multi-column deep neural networks for image classification",
        "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.",
        "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition",
        "year": 2012,
        "referenceCount": 44,
        "citationCount": 3785,
        "influentialCitationCount": 189,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1202.2745",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "On the very competitive MNIST handwriting benchmark, this method is the first to achieve near-human performance and improves the state-of-the-art on a plethora of common image classification benchmarks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2012-02-13",
        "journal": {
            "name": "2012 IEEE Conference on Computer Vision and Pattern Recognition",
            "pages": "3642-3649"
        },
        "citationStyles": {
            "bibtex": "@Article{Ciresan2012MulticolumnDN,\n author = {D. Ciresan and U. Meier and J. Schmidhuber},\n booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2012 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {3642-3649},\n title = {Multi-column deep neural networks for image classification},\n year = {2012}\n}\n"
        }
    },
    "818_network_in_network": {
        "paperId": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
        "externalIds": {
            "MAG": "1799366690",
            "DBLP": "journals/corr/LinCY13",
            "ArXiv": "1312.4400",
            "CorpusId": 16636683
        },
        "corpusId": 16636683,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
        "title": "Network In Network",
        "abstract": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "referenceCount": 20,
        "citationCount": 5775,
        "influentialCitationCount": 558,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "With enhanced local modeling via the micro network, the proposed deep network structure NIN is able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2013-12-16",
        "journal": {
            "name": "CoRR",
            "volume": "abs/1312.4400"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2013NetworkIN,\n author = {Min Lin and Qiang Chen and Shuicheng Yan},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Network In Network},\n volume = {abs/1312.4400},\n year = {2013}\n}\n"
        }
    },
    "819_audiolm": {
        "paperId": "8c870bef01a4fbb20f60722ffc2f6bee3870b18b",
        "externalIds": {
            "DBLP": "journals/corr/abs-2209-03143",
            "ArXiv": "2209.03143",
            "DOI": "10.1109/TASLP.2023.3288409",
            "CorpusId": 252111134
        },
        "corpusId": 252111134,
        "publicationVenue": {
            "id": "309e00f7-4bbd-461f-ab37-a90cd14ef21d",
            "name": "IEEE/ACM Transactions on Audio Speech and Language Processing",
            "alternate_names": [
                "IEEE/ACM Trans Audio Speech Lang Process"
            ],
            "issn": "2329-9290",
            "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=6570655",
            "alternate_urls": [
                "https://signalprocessingsociety.org/publications-resources/ieeeacm-transactions-audio-speech-and-language-processing/ieeeacm"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/8c870bef01a4fbb20f60722ffc2f6bee3870b18b",
        "title": "AudioLM: A Language Modeling Approach to Audio Generation",
        "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
        "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "year": 2022,
        "referenceCount": 68,
        "citationCount": 230,
        "influentialCitationCount": 19,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2209.03143",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work uses the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-09-07",
        "journal": {
            "name": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
            "pages": "2523-2533",
            "volume": "31"
        },
        "citationStyles": {
            "bibtex": "@Article{Borsos2022AudioLMAL,\n author = {Zal\u00e1n Borsos and Rapha\u00ebl Marinier and Damien Vincent and E. Kharitonov and O. Pietquin and Matthew Sharifi and Dominik Roblek and O. Teboul and David Grangier and M. Tagliasacchi and Neil Zeghidour},\n booktitle = {IEEE/ACM Transactions on Audio Speech and Language Processing},\n journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},\n pages = {2523-2533},\n title = {AudioLM: A Language Modeling Approach to Audio Generation},\n volume = {31},\n year = {2022}\n}\n"
        }
    },
    "820_n\u00fcwa": {
        "paperId": "97bee918b08c244eb2e54d41e8ea6da00a3e5dbf",
        "externalIds": {
            "DBLP": "journals/corr/abs-2111-12417",
            "ArXiv": "2111.12417",
            "DOI": "10.1007/978-3-031-19787-1_41",
            "CorpusId": 244527261
        },
        "corpusId": 244527261,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/97bee918b08c244eb2e54d41e8ea6da00a3e5dbf",
        "title": "N\u00dcWA: Visual Synthesis Pre-training for Neural visUal World creAtion",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2021,
        "referenceCount": 52,
        "citationCount": 199,
        "influentialCitationCount": 27,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2111.12417",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Compared to several strong baselines, N\\\"UWA achieves state-of-the-art results on text-to-image generation, text- to-video generation, video prediction, etc, and it also shows surprisingly good zero-shot capabilities onText-guided image and video manipulation tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-11-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2111.12417"
        },
        "citationStyles": {
            "bibtex": "@Article{Wu2021N\u00dcWAVS,\n author = {Chenfei Wu and Jian Liang and Lei Ji and Fan Yang and Yuejian Fang and Daxin Jiang and Nan Duan},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {N\u00dcWA: Visual Synthesis Pre-training for Neural visUal World creAtion},\n volume = {abs/2111.12417},\n year = {2021}\n}\n"
        }
    },
    "821_qrnn": {
        "paperId": "060400d658b8d271bcc6072416c177ecc8c2f827",
        "externalIds": {
            "CorpusId": 219962598
        },
        "corpusId": 219962598,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/060400d658b8d271bcc6072416c177ecc8c2f827",
        "title": "Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours Extended",
        "abstract": "Word-level language modeling (WLM) is one the foundational tasks of unsupervised natural language processing. Most modern architectures for WLM use several LSTM layers, followed by a softmax layer. Even with larger batch sizes and a multi-GPU setup, training of these networks on large-vocabulary corpora is slow due to increased computation involving the softmax and the high cost of recurrence computation. We propose a model architecture and training strategy that enables us to achieve state-of-the-art performance on the WikiText-103 data set using a single GPU while being substantially faster than an NVIDIA cuDNN LSTM-based model by utilizing the Quasi-Recurrent Neural Network (QRNN), an adaptive softmax with weight tying, and longer sequences within batches.",
        "venue": "",
        "year": 2018,
        "referenceCount": 17,
        "citationCount": 3,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": null,
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a model architecture and training strategy that enables us to achieve state-of-the-art performance on the WikiText-103 data set using a single GPU while being substantially faster than an NVIDIA cuDNN LSTM-based model by utilizing the Quasi-Recurrent Neural Network (QRNN), an adaptive softmax with weight tying, and longer sequences within batches."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Merity2018ScalableLM,\n author = {Stephen Merity and N. Keskar and James Bradbury and R. Socher},\n title = {Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours Extended},\n year = {2018}\n}\n"
        }
    },
    "822_gpu_dbns": {
        "paperId": "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1",
        "externalIds": {
            "MAG": "2120432001",
            "DBLP": "conf/icml/RainaMN09",
            "DOI": "10.1145/1553374.1553486",
            "CorpusId": 392458
        },
        "corpusId": 392458,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1",
        "title": "Large-scale deep unsupervised learning using graphics processors",
        "abstract": "The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples.\n In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.",
        "venue": "International Conference on Machine Learning",
        "year": 2009,
        "referenceCount": 36,
        "citationCount": 690,
        "influentialCitationCount": 23,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.robotics.stanford.edu/~ang/papers/icml09-LargeScaleUnsupervisedDeepLearningGPU.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is argued that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2009-06-14",
        "journal": {
            "pages": "873-880"
        },
        "citationStyles": {
            "bibtex": "@Article{Raina2009LargescaleDU,\n author = {Rajat Raina and Anand Madhavan and A. Ng},\n booktitle = {International Conference on Machine Learning},\n pages = {873-880},\n title = {Large-scale deep unsupervised learning using graphics processors},\n year = {2009}\n}\n"
        }
    },
    "823_byt5-xxl": {
        "paperId": "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7",
        "externalIds": {
            "DBLP": "journals/tacl/XueBCANKRR22",
            "ArXiv": "2105.13626",
            "ACL": "2022.tacl-1.17",
            "DOI": "10.1162/tacl_a_00461",
            "CorpusId": 235248316
        },
        "corpusId": 235248316,
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7",
        "title": "ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models",
        "abstract": "Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: They can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Because byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.1",
        "venue": "Transactions of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 68,
        "citationCount": 271,
        "influentialCitationCount": 42,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00461/2004058/tacl_a_00461.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-05-28",
        "journal": {
            "name": "Transactions of the Association for Computational Linguistics",
            "pages": "291-306",
            "volume": "10"
        },
        "citationStyles": {
            "bibtex": "@Article{Xue2021ByT5TA,\n author = {Linting Xue and Aditya Barua and Noah Constant and Rami Al-Rfou and Sharan Narang and Mihir Kale and Adam Roberts and Colin Raffel},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {291-306},\n title = {ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models},\n volume = {10},\n year = {2021}\n}\n"
        }
    },
    "824_nettalk_(dictionary)": {
        "paperId": "de996c32045df6f7b404dda2a753b6a9becf3c08",
        "externalIds": {
            "MAG": "1580142630",
            "DBLP": "journals/compsys/SejnowskiR87",
            "CorpusId": 12926318
        },
        "corpusId": 12926318,
        "publicationVenue": {
            "id": "7f30f248-48d1-479a-a293-a82c1124630c",
            "name": "Complex Systems",
            "type": "journal",
            "alternate_names": [
                "Complex Syst"
            ],
            "issn": "0891-2513",
            "url": "https://www.complex-systems.com/",
            "alternate_urls": [
                "http://www.complex-systems.com/archives.html",
                "http://www.complex-systems.com/index.html",
                "http://www.complex-systems.com/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/de996c32045df6f7b404dda2a753b6a9becf3c08",
        "title": "Parallel Networks that Learn to Pronounce English Text",
        "abstract": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations.",
        "venue": "Complex Systems",
        "year": 1987,
        "referenceCount": 93,
        "citationCount": 1887,
        "influentialCitationCount": 76,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "H hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units, which suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "Complex Syst.",
            "volume": "1"
        },
        "citationStyles": {
            "bibtex": "@Article{Sejnowski1987ParallelNT,\n author = {T. Sejnowski and Charles R. Rosenberg},\n booktitle = {Complex Systems},\n journal = {Complex Syst.},\n title = {Parallel Networks that Learn to Pronounce English Text},\n volume = {1},\n year = {1987}\n}\n"
        }
    },
    "825_h-lstm+wg+rcp+rcg+wp": {
        "paperId": "d8764fa376864346f0fef6c3fb1b34c98d009e3c",
        "externalIds": {
            "DBLP": "journals/corr/abs-1901-10997",
            "ArXiv": "1901.10997",
            "MAG": "2913686213",
            "CorpusId": 59413861
        },
        "corpusId": 59413861,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/d8764fa376864346f0fef6c3fb1b34c98d009e3c",
        "title": "Hardware-Guided Symbiotic Training for Compact, Accurate, yet Execution-Efficient LSTM",
        "abstract": "Many long short-term memory (LSTM) applications need fast yet compact models. Neural network compression approaches, such as the grow-and-prune paradigm, have proved to be promising for cutting down network complexity by skipping insignificant weights. However, current compression strategies are mostly hardware-agnostic and network complexity reduction does not always translate into execution efficiency. In this work, we propose a hardware-guided symbiotic training methodology for compact, accurate, yet execution-efficient inference models. It is based on our observation that hardware may introduce substantial non-monotonic behavior, which we call the latency hysteresis effect, when evaluating network size vs. inference latency. This observation raises question about the mainstream smaller-dimension-is-better compression strategy, which often leads to a sub-optimal model architecture. By leveraging the hardware-impacted hysteresis effect and sparsity, we are able to achieve the symbiosis of model compactness and accuracy with execution efficiency, thus reducing LSTM latency while increasing its accuracy. We have evaluated our algorithms on language modeling and speech recognition applications. Relative to the traditional stacked LSTM architecture obtained for the Penn Treebank dataset, we reduce the number of parameters by 18.0x (30.5x) and measured run-time latency by up to 2.4x (5.2x) on Nvidia GPUs (Intel Xeon CPUs) without any accuracy degradation. For the DeepSpeech2 architecture obtained for the AN4 dataset, we reduce the number of parameters by 7.0x (19.4x), word error rate from 12.9% to 9.9% (10.4%), and measured run-time latency by up to 1.7x (2.4x) on Nvidia GPUs (Intel Xeon CPUs). Thus, our method yields compact, accurate, yet execution-efficient inference models.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 45,
        "citationCount": 8,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a hardware-guided symbiotic training methodology for compact, accurate, yet execution-efficient inference models based on the observation that hardware may introduce substantial non-monotonic behavior, which it is called the latency hysteresis effect, when evaluating network size vs. inference latency."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-01-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1901.10997"
        },
        "citationStyles": {
            "bibtex": "@Article{Yin2019HardwareGuidedST,\n author = {Hongxu Yin and Guoyang Chen and Yingmin Li and Shuai Che and Weifeng Zhang and N. Jha},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Hardware-Guided Symbiotic Training for Compact, Accurate, yet Execution-Efficient LSTM},\n volume = {abs/1901.10997},\n year = {2019}\n}\n"
        }
    },
    "826_tranception": {
        "paperId": "832aa77be74d7d3a8b33b9abc0f48b8b7babac12",
        "externalIds": {
            "ArXiv": "2205.13760",
            "DBLP": "journals/corr/abs-2205-13760",
            "DOI": "10.48550/arXiv.2205.13760",
            "CorpusId": 249151868
        },
        "corpusId": 249151868,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/832aa77be74d7d3a8b33b9abc0f48b8b7babac12",
        "title": "Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval",
        "abstract": "The ability to accurately model the fitness landscape of protein sequences is critical to a wide range of applications, from quantifying the effects of human variants on disease likelihood, to predicting immune-escape mutations in viruses and designing novel biotherapeutic proteins. Deep generative models of protein sequences trained on multiple sequence alignments have been the most successful approaches so far to address these tasks. The performance of these methods is however contingent on the availability of sufficiently deep and diverse alignments for reliable training. Their potential scope is thus limited by the fact many protein families are hard, if not impossible, to align. Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap. We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches. To enable more rigorous model testing across a broader range of protein families, we develop ProteinGym -- an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "referenceCount": 139,
        "citationCount": 100,
        "influentialCitationCount": 18,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2205.13760",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Tranception is introduced, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance and an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-05-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2205.13760"
        },
        "citationStyles": {
            "bibtex": "@Article{Notin2022TranceptionPF,\n author = {Pascal Notin and M. Dias and J. Frazer and Javier Marchena-Hurtado and Aidan N. Gomez and D. Marks and Y. Gal},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval},\n volume = {abs/2205.13760},\n year = {2022}\n}\n"
        }
    },
    "827_3d_city_reconstruction": {
        "paperId": "9922e9f7c87b859df2e8af355f03744d33023f0e",
        "externalIds": {
            "DBLP": "journals/cacm/AgarwalFSSCSS11",
            "MAG": "2536680313",
            "DOI": "10.1145/2001269.2001293",
            "CorpusId": 7448214
        },
        "corpusId": 7448214,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/9922e9f7c87b859df2e8af355f03744d33023f0e",
        "title": "Building Rome in a day",
        "abstract": "We present a system that can match and reconstruct 3D scenes from extremely large collections of photographs such as those found by searching for a given city (e.g., Rome) on Internet photo sharing sites. Our system uses a collection of novel parallel distributed matching and reconstruction algorithms, designed to maximize parallelism at each stage in the pipeline and minimize serialization bottlenecks. It is designed to scale gracefully with both the size of the problem and the amount of available computation. We have experimented with a variety of alternative algorithms at each stage of the pipeline and report on which ones work best in a parallel computing environment. Our experimental results demonstrate that it is now possible to reconstruct cities consisting of 150K images in less than a day on a cluster with 500 compute cores.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2009,
        "referenceCount": 37,
        "citationCount": 2137,
        "influentialCitationCount": 113,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "History"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "History",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A system that can match and reconstruct 3D scenes from extremely large collections of photographs such as those found by searching for a given city on Internet photo sharing sites and is designed to scale gracefully with both the size of the problem and the amount of available computation."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2009-09-01",
        "journal": {
            "name": "2009 IEEE 12th International Conference on Computer Vision",
            "pages": "72-79"
        },
        "citationStyles": {
            "bibtex": "@Article{Agarwal2009BuildingRI,\n author = {Sameer Agarwal and Yasutaka Furukawa and Noah Snavely and Ian Simon and B. Curless and S. Seitz and R. Szeliski},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2009 IEEE 12th International Conference on Computer Vision},\n pages = {72-79},\n title = {Building Rome in a day},\n year = {2009}\n}\n"
        }
    },
    "828_esm2-15b": {
        "paperId": "c49a0912595a1cc70aab63524f64ed08c92194a8",
        "externalIds": {
            "DOI": "10.1126/science.ade2574",
            "CorpusId": 253259177,
            "PubMed": "36927031"
        },
        "corpusId": 253259177,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/c49a0912595a1cc70aab63524f64ed08c92194a8",
        "title": "Evolutionary-scale prediction of atomic level protein structure with a language model",
        "abstract": "Artificial intelligence has the potential to open insight into the structure of proteins at the scale of evolution. It has only recently been possible to extend protein structure prediction to two hundred million cataloged proteins. Characterizing the structures of the exponentially growing billions of protein sequences revealed by large scale gene sequencing experiments would necessitate a break-through in the speed of folding. Here we show that direct inference of structure from primary sequence using a large language model enables an order of magnitude speed-up in high resolution structure prediction. Leveraging the insight that language models learn evolutionary patterns across millions of sequences, we train models up to 15B parameters, the largest language model of proteins to date. As the language models are scaled they learn information that enables prediction of the three-dimensional structure of a protein at the resolution of individual atoms. This results in prediction that is up to 60x faster than state-of-the-art while maintaining resolution and accuracy. Building on this, we present the ESM Metage-nomic Atlas. This is the first large-scale structural characterization of metagenomic proteins, with more than 617 million structures. The atlas reveals more than 225 million high confidence predictions, including millions whose structures are novel in comparison with experimentally determined structures, giving an unprecedented view into the vast breadth and diversity of the structures of some of the least understood proteins on earth.",
        "venue": "bioRxiv",
        "year": 2022,
        "referenceCount": 85,
        "citationCount": 727,
        "influentialCitationCount": 97,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Medicine",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that direct inference of structure from primary sequence using a large language model enables an order of magnitude speed-up in high resolution structure prediction, which results in prediction that is up to 60x faster than state-of-the-art while maintaining resolution and accuracy."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-12-21",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2022EvolutionaryscalePO,\n author = {Zeming Lin and Halil Akin and Roshan Rao and B. Hie and Zhongkai Zhu and Wenting Lu and Nikita Smetanin and Robert Verkuil and Ori Kabeli and Y. Shmueli and Allan dos Santos Costa and Maryam Fazel-Zarandi and Tom Sercu and Salvatore Candido and Alexander Rives},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {Evolutionary-scale prediction of atomic level protein structure with a language model},\n year = {2022}\n}\n"
        }
    },
    "829_s4": {
        "paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "externalIds": {
            "DBLP": "conf/iclr/GuGR22",
            "ArXiv": "2111.00396",
            "CorpusId": 240354066
        },
        "corpusId": 240354066,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
        "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "referenceCount": 61,
        "citationCount": 372,
        "influentialCitationCount": 70,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Structured State Space sequence model (S4) is proposed based on a new parameterization for the SSM, and it is shown that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-31",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2111.00396"
        },
        "citationStyles": {
            "bibtex": "@Article{Gu2021EfficientlyML,\n author = {Albert Gu and Karan Goel and Christopher R'e},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Efficiently Modeling Long Sequences with Structured State Spaces},\n volume = {abs/2111.00396},\n year = {2021}\n}\n"
        }
    },
    "830_moco": {
        "paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc",
        "externalIds": {
            "ArXiv": "1911.05722",
            "DBLP": "conf/cvpr/He0WXG20",
            "MAG": "2987283559",
            "DOI": "10.1109/cvpr42600.2020.00975",
            "CorpusId": 207930212
        },
        "corpusId": 207930212,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/add2f205338d70e10ce5e686df4a690e2851bdfc",
        "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
        "abstract": "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "referenceCount": 65,
        "citationCount": 8675,
        "influentialCitationCount": 1610,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1911.05722",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-11-13",
        "journal": {
            "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "9726-9735"
        },
        "citationStyles": {
            "bibtex": "@Article{He2019MomentumCF,\n author = {Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross B. Girshick},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {9726-9735},\n title = {Momentum Contrast for Unsupervised Visual Representation Learning},\n year = {2019}\n}\n"
        }
    },
    "831_time-delay_neural_networks": {
        "paperId": "cd62c9976534a6a2096a38244f6cbb03635a127e",
        "externalIds": {
            "MAG": "2117671523",
            "DBLP": "journals/tsp/WaibelHHSL89",
            "DOI": "10.1109/29.21701",
            "CorpusId": 9563026
        },
        "corpusId": 9563026,
        "publicationVenue": {
            "id": "d186589f-48a7-41f1-b5ff-31139cf8ac78",
            "name": "IEEE Transactions on Acoustics Speech and Signal Processing",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Acoust Speech Signal Process",
                "IEEE Transactions on Acoustics, Speech, and Signal Processing"
            ],
            "issn": "0096-3518",
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=29"
        },
        "url": "https://www.semanticscholar.org/paper/cd62c9976534a6a2096a38244f6cbb03635a127e",
        "title": "Phoneme recognition using time-delay neural networks",
        "abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >",
        "venue": "IEEE Transactions on Acoustics Speech and Signal Processing",
        "year": 1989,
        "referenceCount": 55,
        "citationCount": 2961,
        "influentialCitationCount": 174,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1989-03-01",
        "journal": {
            "name": "IEEE Trans. Acoust. Speech Signal Process.",
            "pages": "328-339",
            "volume": "37"
        },
        "citationStyles": {
            "bibtex": "@Article{Waibel1989PhonemeRU,\n author = {A. Waibel and Toshiyuki Hanazawa and Geoffrey E. Hinton and K. Shikano and Kevin J. Lang},\n booktitle = {IEEE Transactions on Acoustics Speech and Signal Processing},\n journal = {IEEE Trans. Acoust. Speech Signal Process.},\n pages = {328-339},\n title = {Phoneme recognition using time-delay neural networks},\n volume = {37},\n year = {1989}\n}\n"
        }
    },
    "832_hyenadna": {
        "paperId": "bfd2b76998a0521c12903ef5ced517adf70ad2ba",
        "externalIds": {
            "DBLP": "conf/nips/NguyenPFTWBMPRB23",
            "ArXiv": "2306.15794",
            "DOI": "10.48550/arXiv.2306.15794",
            "CorpusId": 259274952,
            "PubMed": "37426456"
        },
        "corpusId": 259274952,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/bfd2b76998a0521c12903ef5ced517adf70ad2ba",
        "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
        "abstract": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyenas new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level, an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics for simple adaptation to novel tasks without updating pretrained model weights. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 17 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on all 8 datasets on average by +9 accuracy points.",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "referenceCount": 59,
        "citationCount": 46,
        "influentialCitationCount": 8,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.15794",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Biology",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level, an up to 500x increase over previous dense attention-based models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-06-27",
        "journal": {
            "name": "ArXiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Nguyen2023HyenaDNALG,\n author = {Eric D Nguyen and Michael Poli and Marjan Faizi and A. Thomas and C. Birch-sykes and Michael Wornow and Aman Patel and Clayton M. Rabideau and Stefano Massaroli and Y. Bengio and Stefano Ermon and S. Baccus and Christopher R\u00e9},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution},\n year = {2023}\n}\n"
        }
    },
    "833_cube-space_autoencoder": {
        "paperId": "46eaf6a0d377a970e0a143cdaa015049fb4785d2",
        "externalIds": {
            "MAG": "3035259124",
            "ArXiv": "2004.12850",
            "DBLP": "journals/corr/abs-2004-12850",
            "DOI": "10.24963/ijcai.2020/371",
            "CorpusId": 263092329
        },
        "corpusId": 263092329,
        "publicationVenue": {
            "id": "67f7f831-711a-43c8-8785-1e09005359b5",
            "name": "International Joint Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "Int Jt Conf Artif Intell",
                "IJCAI"
            ],
            "url": "http://www.ijcai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/46eaf6a0d377a970e0a143cdaa015049fb4785d2",
        "title": "Learning Neural-Symbolic Descriptive Planning Models via Cube-Space Priors: The Voyage Home (to STRIPS)",
        "abstract": "We achieved a new milestone in the difficult task of enabling agents to learn about their environment autonomously. Our neuro-symbolic architecture is trained end-to-end to produce a succinct and effective discrete state transition model from images alone. Our target representation (the Planning Domain Definition Language) is already in a form that off-the-shelf solvers can consume, and opens the door to the rich array of modern heuristic search capabilities. We demonstrate how the sophisticated innate prior we place on the learning process significantly reduces the complexity of the learned representation, and reveals a connection to the graph-theoretic notion of ``cube-like graphs'', thus opening the door to a deeper understanding of the ideal properties for learned symbolic representations. We show that the powerful domain-independent heuristics allow our system to solve visual 15-Puzzle instances which are beyond the reach of blind search, without resorting to the Reinforcement Learning approach that requires a huge amount of training on the domain-dependent reward information.",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2020,
        "referenceCount": 63,
        "citationCount": 44,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2020/0371.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work demonstrates how the sophisticated innate prior placed on the learning process significantly reduces the complexity of the learned representation, and reveals a connection to the graph-theoretic notion of ``cube-like graphs'', thus opening the door to a deeper understanding of the ideal properties for learned symbolic representations."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-04-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2004.12850"
        },
        "citationStyles": {
            "bibtex": "@Article{Asai2020LearningND,\n author = {Masataro Asai and Christian Muise},\n booktitle = {International Joint Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Learning Neural-Symbolic Descriptive Planning Models via Cube-Space Priors: The Voyage Home (to STRIPS)},\n volume = {abs/2004.12850},\n year = {2020}\n}\n"
        }
    },
    "836_2-layer_skip-lstm_+_dropout_tuning_(wt2)": {
        "paperId": "717892acc8767a028218b5053ebe57a4f59685d1",
        "externalIds": {
            "ArXiv": "1805.09208",
            "MAG": "2804845563",
            "DBLP": "journals/corr/abs-1805-09208",
            "CorpusId": 43922508
        },
        "corpusId": 43922508,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/717892acc8767a028218b5053ebe57a4f59685d1",
        "title": "Pushing the bounds of dropout",
        "abstract": "We show that dropout training is best understood as performing MAP estimation concurrently for a family of conditional models whose objectives are themselves lower bounded by the original dropout objective. This discovery allows us to pick any model from this family after training, which leads to a substantial improvement on regularisation-heavy language modelling. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds than the fully stochastic dropout objective. We argue that since the deterministic subvariant's bound is equal to its objective, and the highest amongst these models, the predominant view of it as a good approximation to MC averaging is misleading. Rather, deterministic dropout is the best available approximation to the true objective.",
        "venue": "arXiv.org",
        "year": 2018,
        "referenceCount": 29,
        "citationCount": 13,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that dropout training is best understood as performing MAP estimation concurrently for a family of conditional models whose objectives are themselves lower bounded by the original dropout objective, which allows us to pick any model from this family after training, which leads to a substantial improvement on regularisation-heavy language modelling."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-05-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1805.09208"
        },
        "citationStyles": {
            "bibtex": "@Article{Melis2018PushingTB,\n author = {G\u00e1bor Melis and C. Blundell and Tom\u00e1s Kocisk\u00fd and K. Hermann and Chris Dyer and Phil Blunsom},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Pushing the bounds of dropout},\n volume = {abs/1805.09208},\n year = {2018}\n}\n"
        }
    },
    "837_diabetes": {
        "paperId": "c37f0aa36c6c4ea2509de480f666d69377b6be61",
        "externalIds": {
            "MAG": "1962606224",
            "DBLP": "conf/aime/AndreassenHBOC91",
            "DOI": "10.1007/978-3-642-48650-0_19",
            "CorpusId": 31117823
        },
        "corpusId": 31117823,
        "publicationVenue": {
            "id": "4e1eea23-99a8-4d19-a986-00ec1a3f1718",
            "name": "Conference on Artificial Intelligence in Medicine in Europe",
            "type": "conference",
            "alternate_names": [
                "Artif Intell Med Eur",
                "AIME",
                "Artificial Intelligence in Medicine in Europe",
                "Conf Artif Intell Med Eur"
            ],
            "url": "http://www.wikicfp.com/cfp/program?id=110"
        },
        "url": "https://www.semanticscholar.org/paper/c37f0aa36c6c4ea2509de480f666d69377b6be61",
        "title": "A Model-Based Approach to Insulin Adjustment",
        "abstract": null,
        "venue": "Conference on Artificial Intelligence in Medicine in Europe",
        "year": 1991,
        "referenceCount": 18,
        "citationCount": 106,
        "influentialCitationCount": 10,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A differential equation model of carbohydrate metabolism was implemented in the form of a causal probabilistic network that permitted explicit represen-tations of the uncertainties associated with model based predictions of 24-hour blood glucose profiles."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "239-248"
        },
        "citationStyles": {
            "bibtex": "@Article{Andreassen1991AMA,\n author = {S. Andreassen and R. Hovorka and J. Benn and K. Olesen and E. Carson},\n booktitle = {Conference on Artificial Intelligence in Medicine in Europe},\n pages = {239-248},\n title = {A Model-Based Approach to Insulin Adjustment},\n year = {1991}\n}\n"
        }
    },
    "838_go-explore": {
        "paperId": "616ac6772508e72084360158078058dfa8bda7e7",
        "externalIds": {
            "MAG": "3018036994",
            "DBLP": "journals/nature/EcoffetHLSC21",
            "ArXiv": "2004.12919",
            "DOI": "10.1038/s41586-020-03157-9",
            "CorpusId": 216552951,
            "PubMed": "33627813"
        },
        "corpusId": 216552951,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/616ac6772508e72084360158078058dfa8bda7e7",
        "title": "First return, then explore",
        "abstract": null,
        "venue": "Nature",
        "year": 2020,
        "referenceCount": 107,
        "citationCount": 263,
        "influentialCitationCount": 30,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2004.12919",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Go-Explore, a family of algorithms that explicitly remembers promising states and returns to them as a basis for further exploration solves all as-yet-unsolved Atari games and out-performs previous algorithms on Montezuma\u2019s Revenge and Pitfall."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-04-27",
        "journal": {
            "name": "Nature",
            "pages": "580 - 586",
            "volume": "590"
        },
        "citationStyles": {
            "bibtex": "@Article{Ecoffet2020FirstRT,\n author = {Adrien Ecoffet and Joost Huizinga and J. Lehman and Kenneth O. Stanley and J. Clune},\n booktitle = {Nature},\n journal = {Nature},\n pages = {580 - 586},\n title = {First return, then explore},\n volume = {590},\n year = {2020}\n}\n"
        }
    },
    "839_integer_transformer": {
        "paperId": "503503ec4395ab0e36d4f0a190772f7785649319",
        "externalIds": {
            "MAG": "3086017621",
            "ArXiv": "2009.08034",
            "DBLP": "journals/corr/abs-2009-08034",
            "DOI": "10.24963/ijcai.2020/520",
            "CorpusId": 220480965
        },
        "corpusId": 220480965,
        "publicationVenue": {
            "id": "67f7f831-711a-43c8-8785-1e09005359b5",
            "name": "International Joint Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "Int Jt Conf Artif Intell",
                "IJCAI"
            ],
            "url": "http://www.ijcai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/503503ec4395ab0e36d4f0a190772f7785649319",
        "title": "Towards Fully 8-bit Integer Inference for the Transformer Model",
        "abstract": "8-bit integer inference, as a promising direction in reducing both the latency and storage of deep neural networks, has made great progress recently. On the other hand, previous systems still rely on 32-bit floating point for certain functions in complex models (e.g., Softmax in Transformer), and make heavy use of quantization and de-quantization. In this work, we show that after a principled modification on the Transformer architecture, dubbed Integer Transformer, an (almost) fully 8-bit integer inference algorithm Scale Propagation could be derived. De-quantization is adopted when necessary, which makes the network more efficient. Our experiments on WMT16 En<->Ro, WMT14 En<->De and En->Fr translation tasks as well as the WikiText-103 language modelling task show that the fully 8-bit Transformer system achieves comparable performance with the floating point baseline but requires nearly 4x less memory footprint.",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2020,
        "referenceCount": 21,
        "citationCount": 50,
        "influentialCitationCount": 6,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2020/0520.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that after a principled modification on the Transformer architecture, dubbed Integer Transformer, an (almost) fully 8-bit integer inference algorithm Scale Propagation could be derived and achieves comparable performance with the floating point baseline but requires nearly 4x less memory footprint."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-07-01",
        "journal": {
            "pages": "3759-3765"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2020TowardsF8,\n author = {Ye Lin and Yanyang Li and Tengbo Liu and Tong Xiao and Tongran Liu and Jingbo Zhu},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {3759-3765},\n title = {Towards Fully 8-bit Integer Inference for the Transformer Model},\n year = {2020}\n}\n"
        }
    },
    "840_lmsi-palm": {
        "paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd",
        "externalIds": {
            "DBLP": "conf/emnlp/0001GHW00023",
            "ArXiv": "2210.11610",
            "DOI": "10.48550/arXiv.2210.11610",
            "CorpusId": 253080328
        },
        "corpusId": 253080328,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/3fa70115248377c3d1517c9f978791a296fbc1dd",
        "title": "Large Language Models Can Self-Improve",
        "abstract": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate\"high-confidence\"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2022,
        "referenceCount": 54,
        "citationCount": 226,
        "influentialCitationCount": 20,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.11610",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work uses a pre-trained LLM to generate \"high-confidence\"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs to improve the general reasoning ability."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-10-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.11610"
        },
        "citationStyles": {
            "bibtex": "@Article{Huang2022LargeLM,\n author = {Jiaxin Huang and S. Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Large Language Models Can Self-Improve},\n volume = {abs/2210.11610},\n year = {2022}\n}\n"
        }
    },
    "841_blstm_for_handwriting_(2)": {
        "paperId": "a0aca3246845016bd8dc996944476f3dd5a5ba56",
        "externalIds": {
            "CorpusId": 5756363
        },
        "corpusId": 5756363,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/a0aca3246845016bd8dc996944476f3dd5a5ba56",
        "title": "Unconstrained Online Handwriting Recognition with Recurrent Neural Networks",
        "abstract": "In online handwriting recognition the trajectory of the pen is recorded during writing. Although the trajectory provides a compact and complete representation of the written output, it is hard to transcribe directly, because each letter is spread over many pen locations. Most recognition systems therefore employ sophisticated preprocessing techniques to put the inputs into a more localised form. However these techniques require considerable human effort, and are specific to particular languages and alphabets. This paper describes a system capable of directly transcribing raw online handwriting data. The system consists of an advanced recurrent neural network with an output layer designed for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained online database, we record excellent results using either raw or preprocessed data, well outperforming a state-of-the-art HMM based system in both cases.",
        "venue": "",
        "year": 2007,
        "referenceCount": 16,
        "citationCount": 98,
        "influentialCitationCount": 7,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": null,
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A system capable of directly transcribing raw online handwriting data is described, consisting of an advanced recurrent neural network with an output layer designed for sequence labelling, combined with a probabilistic language model."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Graves2007UnconstrainedOH,\n author = {Alex Graves and M. Liwicki},\n title = {Unconstrained Online Handwriting Recognition with Recurrent Neural Networks},\n year = {2007}\n}\n"
        }
    },
    "842_guanaco-65b": {
        "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
        "externalIds": {
            "DBLP": "journals/corr/abs-2305-14314",
            "ArXiv": "2305.14314",
            "DOI": "10.48550/arXiv.2305.14314",
            "CorpusId": 258841328
        },
        "corpusId": 258841328,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/32ac52069e562d4f900afee70bdca63f53461481",
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
        "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "referenceCount": 73,
        "citationCount": 534,
        "influentialCitationCount": 75,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14314",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA, and current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-05-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2305.14314"
        },
        "citationStyles": {
            "bibtex": "@Article{Dettmers2023QLoRAEF,\n author = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {QLoRA: Efficient Finetuning of Quantized LLMs},\n volume = {abs/2305.14314},\n year = {2023}\n}\n"
        }
    },
    "843_hra": {
        "paperId": "3c63f8b8263cd6cc4c8c7429d46bb656accddc49",
        "externalIds": {
            "MAG": "2624731731",
            "DBLP": "journals/corr/SeijenFRLBT17",
            "ArXiv": "1706.04208",
            "CorpusId": 2294751
        },
        "corpusId": 2294751,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/3c63f8b8263cd6cc4c8c7429d46bb656accddc49",
        "title": "Hybrid Reward Architecture for Reinforcement Learning",
        "abstract": "One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the corresponding value function can be approximated more easily by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "referenceCount": 32,
        "citationCount": 222,
        "influentialCitationCount": 22,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new method is proposed, called Hybrid Reward Architecture (HRA), which takes as input a decomposed reward function and learns a separate value function for each component reward function, enabling more effective learning."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-06-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1706.04208"
        },
        "citationStyles": {
            "bibtex": "@Article{Seijen2017HybridRA,\n author = {H. V. Seijen and Mehdi Fatemi and R. Laroche and Joshua Romoff and Tavian Barnes and Jeffrey Tsang},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Hybrid Reward Architecture for Reinforcement Learning},\n volume = {abs/1706.04208},\n year = {2017}\n}\n"
        }
    },
    "844_vgg16": {
        "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
        "externalIds": {
            "MAG": "2949429431",
            "ArXiv": "1409.1556",
            "DBLP": "journals/corr/SimonyanZ14a",
            "CorpusId": 14124313
        },
        "corpusId": 14124313,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108",
        "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "referenceCount": 43,
        "citationCount": 86491,
        "influentialCitationCount": 13402,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-09-04",
        "journal": {
            "name": "CoRR",
            "volume": "abs/1409.1556"
        },
        "citationStyles": {
            "bibtex": "@Article{Simonyan2014VeryDC,\n author = {K. Simonyan and Andrew Zisserman},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},\n volume = {abs/1409.1556},\n year = {2014}\n}\n"
        }
    },
    "845_advantage_learning": {
        "paperId": "f88a6f6fd6611543220482e6b3a5f379b7bf5049",
        "externalIds": {
            "DBLP": "conf/aaai/BellemareOGTM16",
            "MAG": "2962847657",
            "ArXiv": "1512.04860",
            "DOI": "10.1609/aaai.v30i1.10303",
            "CorpusId": 1907310
        },
        "corpusId": 1907310,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/f88a6f6fd6611543220482e6b3a5f379b7bf5049",
        "title": "Increasing the Action Gap: New Operators for Reinforcement Learning",
        "abstract": "\n \n This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.\n \n",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "referenceCount": 35,
        "citationCount": 142,
        "influentialCitationCount": 20,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10303/10162",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An operator for tabular representations is described, the consistent Bellman operator, which incorporates a notion of local policy consistency, which leads to an increase in the action gap at each state; increasing this gap mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-12-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1512.04860"
        },
        "citationStyles": {
            "bibtex": "@Article{Bellemare2015IncreasingTA,\n author = {Marc G. Bellemare and Georg Ostrovski and A. Guez and P. Thomas and R. Munos},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Increasing the Action Gap: New Operators for Reinforcement Learning},\n volume = {abs/1512.04860},\n year = {2015}\n}\n"
        }
    },
    "847_cascaded_lnet-anet": {
        "paperId": "6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
        "externalIds": {
            "MAG": "1834627138",
            "ArXiv": "1411.7766",
            "DBLP": "journals/corr/LiuLWT14",
            "DOI": "10.1109/ICCV.2015.425",
            "CorpusId": 459456
        },
        "corpusId": 459456,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
        "title": "Deep Learning Face Attributes in the Wild",
        "abstract": "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2014,
        "referenceCount": 44,
        "citationCount": 6822,
        "influentialCitationCount": 1592,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1411.7766",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel deep learning framework for attribute prediction in the wild that cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-11-27",
        "journal": {
            "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
            "pages": "3730-3738"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2014DeepLF,\n author = {Ziwei Liu and Ping Luo and Xiaogang Wang and Xiaoou Tang},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {3730-3738},\n title = {Deep Learning Face Attributes in the Wild},\n year = {2014}\n}\n"
        }
    },
    "848_primer": {
        "paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2",
        "externalIds": {
            "ArXiv": "2109.08668",
            "DBLP": "conf/nips/SoMLDSL21",
            "CorpusId": 237563187
        },
        "corpusId": 237563187,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/4a8964ea0de47010fb458021b68fa3ef5c4b77b2",
        "title": "Primer: Searching for Efficient Transformers for Language Modeling",
        "abstract": "Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 60,
        "citationCount": 106,
        "influentialCitationCount": 11,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work identifies an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling, and proves empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-09-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2109.08668"
        },
        "citationStyles": {
            "bibtex": "@Article{So2021PrimerSF,\n author = {David R. So and Wojciech Ma'nke and Hanxiao Liu and Zihang Dai and Noam M. Shazeer and Quoc V. Le},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Primer: Searching for Efficient Transformers for Language Modeling},\n volume = {abs/2109.08668},\n year = {2021}\n}\n"
        }
    },
    "849_cognitron": {
        "paperId": "03b4a233b19cf202ba9117d501a82a48ef3ed6e9",
        "externalIds": {
            "MAG": "2010315761",
            "DOI": "10.1007/BF00342633",
            "CorpusId": 28586460,
            "PubMed": "1203338"
        },
        "corpusId": 28586460,
        "publicationVenue": {
            "id": "57cada26-a03e-494e-929e-a71ac35f2ad0",
            "name": "Biological cybernetics",
            "type": "journal",
            "alternate_names": [
                "Biological cybern",
                "Biological Cybern",
                "Biological Cybernetics"
            ],
            "issn": "0340-1200",
            "url": "http://link.springer.com/journal/422",
            "alternate_urls": [
                "https://link.springer.com/journal/volumesAndIssues/422"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/03b4a233b19cf202ba9117d501a82a48ef3ed6e9",
        "title": "Cognitron: A self-organizing multilayered neural network",
        "abstract": null,
        "venue": "Biological cybernetics",
        "year": 1975,
        "referenceCount": 14,
        "citationCount": 481,
        "influentialCitationCount": 22,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new hypothesis for the organization of synapses between neurons is proposed: \u201cThe synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y\u201d, and a new algorithm with which a multilayered neural network is effectively organized can be deduced."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1975-09-01",
        "journal": {
            "name": "Biological Cybernetics",
            "pages": "121-136",
            "volume": "20"
        },
        "citationStyles": {
            "bibtex": "@Article{Fukushima1975CognitronAS,\n author = {K. Fukushima},\n booktitle = {Biological cybernetics},\n journal = {Biological Cybernetics},\n pages = {121-136},\n title = {Cognitron: A self-organizing multilayered neural network},\n volume = {20},\n year = {1975}\n}\n"
        }
    },
    "851_goat": {
        "paperId": "e75fb417b54a6eae589ff382874de09d7f58a3de",
        "externalIds": {
            "DBLP": "journals/corr/abs-2107-12808",
            "ArXiv": "2107.12808",
            "CorpusId": 236447390
        },
        "corpusId": 236447390,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/e75fb417b54a6eae589ff382874de09d7f58a3de",
        "title": "Open-Ended Learning Leads to Generally Capable Agents",
        "abstract": "In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 109,
        "citationCount": 138,
        "influentialCitationCount": 14,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work defines a universe of tasks within an environment domain and demonstrates the ability to train agents that are generally capable across this vast space and beyond, and shows that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2107.12808"
        },
        "citationStyles": {
            "bibtex": "@Article{Team2021OpenEndedLL,\n author = {Open-Ended Learning Team and Adam Stooke and Anuj Mahajan and C. Barros and Charlie Deck and Jakob Bauer and Jakub Sygnowski and Maja Trebacz and Max Jaderberg and Micha\u00ebl Mathieu and Nathan McAleese and N. Bradley-Schmieg and Nathaniel Wong and Nicolas Porcel and Roberta Raileanu and Steph Hughes-Fitt and Valentin Dalibard and Wojciech M. Czarnecki},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Open-Ended Learning Leads to Generally Capable Agents},\n volume = {abs/2107.12808},\n year = {2021}\n}\n"
        }
    },
    "852_t5-3b": {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "externalIds": {
            "DBLP": "journals/jmlr/RaffelSRLNMZLL20",
            "MAG": "2981852735",
            "ArXiv": "1910.10683",
            "CorpusId": 204838007
        },
        "corpusId": 204838007,
        "publicationVenue": {
            "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
            "name": "Journal of machine learning research",
            "type": "journal",
            "alternate_names": [
                "Journal of Machine Learning Research",
                "J mach learn res",
                "J Mach Learn Res"
            ],
            "issn": "1532-4435",
            "alternate_issns": [
                "1533-7928"
            ],
            "url": "http://www.ai.mit.edu/projects/jmlr/",
            "alternate_urls": [
                "http://jmlr.csail.mit.edu/",
                "http://www.jmlr.org/",
                "http://portal.acm.org/affiliated/jmlr"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
        "venue": "Journal of machine learning research",
        "year": 2019,
        "referenceCount": 134,
        "citationCount": 12390,
        "influentialCitationCount": 1789,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-10-23",
        "journal": {
            "name": "J. Mach. Learn. Res.",
            "pages": "140:1-140:67",
            "volume": "21"
        },
        "citationStyles": {
            "bibtex": "@Article{Raffel2019ExploringTL,\n author = {Colin Raffel and Noam M. Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {140:1-140:67},\n title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n volume = {21},\n year = {2019}\n}\n"
        }
    },
    "853_cpm-2": {
        "paperId": "00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d",
        "externalIds": {
            "DBLP": "journals/aiopen/ZhangGHCXSYQGKC21",
            "ArXiv": "2106.10715",
            "DOI": "10.1016/j.aiopen.2021.12.003",
            "CorpusId": 235490263
        },
        "corpusId": 235490263,
        "publicationVenue": {
            "id": "6c35576a-a87d-4dc1-a576-780572d8d0e6",
            "name": "AI Open",
            "type": "journal",
            "issn": "2666-6510",
            "url": "https://www.keaipublishing.com/en/journals/ai-open/"
        },
        "url": "https://www.semanticscholar.org/paper/00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d",
        "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models",
        "abstract": null,
        "venue": "AI Open",
        "year": 2021,
        "referenceCount": 44,
        "citationCount": 67,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A suite of cost-effective techniques for the use of PLMs to deal with the efficiency issues of pre-training, fine-tuning, and inference and validate the efficiency of InfMoE when conducting inference of large-scale models having tens of billions of parameters on a single GPU."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-06-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2106.10715"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2021CPM2LC,\n author = {Zhengyan Zhang and Yuxian Gu and Xu Han and Shengqi Chen and Chaojun Xiao and Zhenbo Sun and Yuan Yao and Fanchao Qi and Jian Guan and Pei Ke and Yanzheng Cai and Guoyang Zeng and Zhixing Tan and Zhiyuan Liu and Minlie Huang and Wentao Han and Yang Liu and Xiaoyan Zhu and Maosong Sun},\n booktitle = {AI Open},\n journal = {ArXiv},\n title = {CPM-2: Large-scale Cost-effective Pre-trained Language Models},\n volume = {abs/2106.10715},\n year = {2021}\n}\n"
        }
    },
    "854_pluribus": {
        "paperId": "2ee463bba9d4db6aec0eab17e54431a6dc80bf17",
        "externalIds": {
            "MAG": "2960876848",
            "DOI": "10.1126/science.aay2400",
            "CorpusId": 195892791,
            "PubMed": "31296650"
        },
        "corpusId": 195892791,
        "publicationVenue": {
            "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
            "name": "Science",
            "type": "journal",
            "issn": "0193-4511",
            "alternate_issns": [
                "0036-8075"
            ],
            "url": "https://www.jstor.org/journal/science",
            "alternate_urls": [
                "https://www.sciencemag.org/",
                "http://www.sciencemag.org/",
                "http://www.jstor.org/journals/00368075.html",
                "http://www.sciencemag.org/archive/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/2ee463bba9d4db6aec0eab17e54431a6dc80bf17",
        "title": "Superhuman AI for multiplayer poker",
        "abstract": "AI now masters six-player poker Computer programs have shown superiority over humans in two-player games such as chess, Go, and heads-up, no-limit Texas hold'em poker. However, poker games usually include six players\u2014a much trickier challenge for artificial intelligence than the two-player variant. Brown and Sandholm developed a program, dubbed Pluribus, that learned how to play six-player no-limit Texas hold'em by playing against five copies of itself (see the Perspective by Blair and Saffidine). When pitted against five elite professional poker players, or with five copies of Pluribus playing against one professional, the computer performed significantly better than humans over the course of 10,000 hands of poker. Science, this issue p. 885; see also p. 864 An AI dubbed Pluribus performs significantly better than human professionals in six-player no-limit Texas hold\u2019em poker. In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold\u2019em poker, the most popular form of poker played by humans.",
        "venue": "Science",
        "year": 2019,
        "referenceCount": 31,
        "citationCount": 564,
        "influentialCitationCount": 34,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.science.org/cms/asset/910714a7-ee2a-486e-9970-42fb893b08d9/pap.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Psychology",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Psychology",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Pluribus, an AI that is stronger than top human professionals in six-player no-limit Texas hold\u2019em poker, the most popular form of poker played by humans, is presented."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-07-11",
        "journal": {
            "name": "Science",
            "pages": "885 - 890",
            "volume": "365"
        },
        "citationStyles": {
            "bibtex": "@Article{Brown2019SuperhumanAF,\n author = {Noam Brown and T. Sandholm},\n booktitle = {Science},\n journal = {Science},\n pages = {885 - 890},\n title = {Superhuman AI for multiplayer poker},\n volume = {365},\n year = {2019}\n}\n"
        }
    },
    "856_resnext-101_billion-scale": {
        "paperId": "88ee291cf1f57fd0f4914a80b986a08a90d887f1",
        "externalIds": {
            "ArXiv": "1905.00546",
            "DBLP": "journals/corr/abs-1905-00546",
            "MAG": "2943152387",
            "CorpusId": 143422950
        },
        "corpusId": 143422950,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/88ee291cf1f57fd0f4914a80b986a08a90d887f1",
        "title": "Billion-scale semi-supervised learning for image classification",
        "abstract": "This paper presents a study of semi-supervised learning with large convolutional networks. We propose a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images (up to 1 billion). Our main goal is to improve the performance for a given target architecture, like ResNet-50 or ResNext. We provide an extensive analysis of the success factors of our approach, which leads us to formulate some recommendations to produce high-accuracy models for image classification with semi-supervised learning. As a result, our approach brings important gains to standard architectures for image, video and fine-grained classification. For instance, by leveraging one billion unlabelled images, our learned vanilla ResNet-50 achieves 81.2% top-1 accuracy on the ImageNet benchmark.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 44,
        "citationCount": 403,
        "influentialCitationCount": 23,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images to improve the performance for a given target architecture, like ResNet-50 or ResNext."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-05-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1905.00546"
        },
        "citationStyles": {
            "bibtex": "@Article{Yalniz2019BillionscaleSL,\n author = {I. Z. Yalniz and H. J\u00e9gou and Kan Chen and Manohar Paluri and D. Mahajan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Billion-scale semi-supervised learning for image classification},\n volume = {abs/1905.00546},\n year = {2019}\n}\n"
        }
    },
    "858_mmlstm": {
        "paperId": "e20401e864fdd459c737cc950acfbce53b0d3492",
        "externalIds": {
            "MAG": "2993654181",
            "DBLP": "journals/tnn/ShuangLGLS20",
            "DOI": "10.1109/TNNLS.2019.2947563",
            "CorpusId": 209330225,
            "PubMed": "31825875"
        },
        "corpusId": 209330225,
        "publicationVenue": {
            "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
            "name": "IEEE Transactions on Neural Networks and Learning Systems",
            "alternate_names": [
                "IEEE Trans Neural Netw Learn Syst"
            ],
            "issn": "2162-237X",
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
            "alternate_urls": [
                "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e20401e864fdd459c737cc950acfbce53b0d3492",
        "title": "Major\u2013Minor Long Short-Term Memory for Word-Level Language Model",
        "abstract": "Language model (LM) plays an important role in natural language processing (NLP) systems, such as machine translation, speech recognition, learning token embeddings, natural language generation, and text classification. Recently, the multilayer long short-term memory (LSTM) models have been demonstrated to achieve promising performance on word-level language modeling. For each LSTM layer, larger hidden size usually means more diverse semantic features, which enables the LM to perform better. However, we have observed that when a certain LSTM layer reaches a sufficiently large scale, the promotion of overall effect will slow down, as its hidden size increases. In this article, we analyze that an important factor leading to this phenomenon is the high correlation between the newly extended hidden states and the original hidden states, which hinders diverse feature expression of the LSTM. As a result, when the scale is large enough, simply lengthening the LSTM hidden states will cost tremendous extra parameters but has little effect. We propose a simple yet effective improvement on each LSTM layer consisting of a large-scale Major LSTM and a small-scale Minor LSTM to break the high correlation between the two parts of hidden states, which we call Major\u2013Minor LSTMs (MMLSTMs). In experiments, we demonstrate the LM with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) data sets and outperforms the baseline by 3.3 points in perplexity on WikiText-103 data set without increasing model parameter counts.",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "year": 2019,
        "referenceCount": 59,
        "citationCount": 14,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The LM with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank and WikiText-2 data sets and outperforms the baseline by 3.3 points in perplexity onWikiText-103 data set without increasing model parameter counts."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-12-05",
        "journal": {
            "name": "IEEE Transactions on Neural Networks and Learning Systems",
            "pages": "3932-3946",
            "volume": "31"
        },
        "citationStyles": {
            "bibtex": "@Article{Shuang2019MajorMinorLS,\n author = {Kai Shuang and Rui Li and Mengyu Gu and Jonathan Loo and Sen Su},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {3932-3946},\n title = {Major\u2013Minor Long Short-Term Memory for Word-Level Language Model},\n volume = {31},\n year = {2019}\n}\n"
        }
    },
    "859_ferret_(13b)": {
        "paperId": "458111ac5a0f73bb35a2acf55298268be25ccfa2",
        "externalIds": {
            "ArXiv": "2310.07704",
            "DBLP": "journals/corr/abs-2310-07704",
            "DOI": "10.48550/arXiv.2310.07704",
            "CorpusId": 263834718
        },
        "corpusId": 263834718,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/458111ac5a0f73bb35a2acf55298268be25ccfa2",
        "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
        "abstract": "We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination. Code and data will be available at https://github.com/apple/ml-ferret",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 75,
        "citationCount": 50,
        "influentialCitationCount": 10,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.07704",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The introduction of Ferret, a new Multimodal Large Language Model capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions, and a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-10-12",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2310.07704"
        },
        "citationStyles": {
            "bibtex": "@Article{You2023FerretRA,\n author = {Haoxuan You and Haotian Zhang and Zhe Gan and Xianzhi Du and Bowen Zhang and Zirui Wang and Liangliang Cao and Shih-Fu Chang and Yinfei Yang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Ferret: Refer and Ground Anything Anywhere at Any Granularity},\n volume = {abs/2310.07704},\n year = {2023}\n}\n"
        }
    },
    "860_limoe": {
        "paperId": "499d3bb3acbc10730dd6582bd9b8f646bf22ccd5",
        "externalIds": {
            "ArXiv": "2206.02770",
            "DBLP": "conf/nips/MustafaRPJH22",
            "DOI": "10.48550/arXiv.2206.02770",
            "CorpusId": 249394802
        },
        "corpusId": 249394802,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/499d3bb3acbc10730dd6582bd9b8f646bf22ccd5",
        "title": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts",
        "abstract": "Large sparsely-activated models have obtained excellent performance in multiple domains. However, such models are typically trained on a single modality at a time. We present the Language-Image MoE, LIMoE, a sparse mixture of experts model capable of multimodal learning. LIMoE accepts both images and text simultaneously, while being trained using a contrastive loss. MoEs are a natural fit for a multimodal backbone, since expert layers can learn an appropriate partitioning of modalities. However, new challenges arise; in particular, training stability and balanced expert utilization, for which we propose an entropy-based regularization scheme. Across multiple scales, we demonstrate remarkable performance improvement over dense models of equivalent computational cost. LIMoE-L/16 trained comparably to CLIP-L/14 achieves 78.6% zero-shot ImageNet accuracy (vs. 76.2%), and when further scaled to H/14 (with additional data) it achieves 84.1%, comparable to state-of-the-art methods which use larger custom per-modality backbones and pre-training schemes. We analyse the quantitative and qualitative behavior of LIMoE, and demonstrate phenomena such as differing treatment of the modalities and the organic emergence of modality-specific experts.",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "referenceCount": 53,
        "citationCount": 82,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2206.02770",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents the Language-Image MoE, LIMoE, a sparse mixture of experts model capable of multimodal learning, and proposes an entropy-based regularization scheme for its training stability and balanced expert utilization."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-06-06",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2206.02770"
        },
        "citationStyles": {
            "bibtex": "@Article{Mustafa2022MultimodalCL,\n author = {Basil Mustafa and C. Riquelme and J. Puigcerver and Rodolphe Jenatton and N. Houlsby},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts},\n volume = {abs/2206.02770},\n year = {2022}\n}\n"
        }
    },
    "861_bigchaos_2008": {
        "paperId": "f0b554683b425a0aad3720c8b0bd122eaa3c9b35",
        "externalIds": {
            "MAG": "65731240",
            "CorpusId": 16317324
        },
        "corpusId": 16317324,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/f0b554683b425a0aad3720c8b0bd122eaa3c9b35",
        "title": "The BigChaos Solution to the Netflix Prize 2008",
        "abstract": "The team \u201cBellKor in BigChaos\u201d is a combined team of team BellKor and BigChaos. The solution with a RMSE of 0.8616 is created by a linear blend of the results from both teams. In the following paper we describe the results of BigChaos. 1 Preface During the last 2 years of research we tried a variety of different collaborative filtering algorithms. In the following we describe all methods which turned out to be useful for the Netflix Prize competition. In sections 4 to 18 we describe all algorithms used to produce our predictions, listed in section 20. We minimized the RMSE of each individual predictor on the probe set. Whenever possible, we therefore take a simple automatic parameter tuner described in section 3 to tune the meta parameters. Details are reported in the section describing the individual algorithm, and for each predictor, the meta parameters used are reported in section 20. If an algorithm has been trained on the residuals of another, we note that explicitly; others are based on raw ratings. Exact details can again be found in the predictor listing in section 20. Here we note for every predictor, if it was based on the residuals of another algorithm. 2 Notation Throughout the document we keep the following notation. A user is denoted with u and item/movie with i. The rating given by user u to item i is denoted by rui. Predictions for the rating rui are denoted with rui. N(i) stands for the set of users who voted the item i, and N(u) is the set of items voted by the user u. N(u, i) is the set of the K most similar items to item i which were rated by user u and is therefore a subset of N(u). The learning rate is called \u03b7 and the regularization \u03bb. Constants like \u03b1, \u03b2, \u03b3, \u03b4 \u03b8, \u03b8, ... can have different meanings for every algorithm. 3 Parameter Tuner In several algorithms like in Section 5, there is a need for tuning parameters, in order to minimize the RMSE of a particular predictor. We use two algorithms to automatically tune them. The target is to adjust N parameters pi, to minimize the RMSE on the probe set with the error of a given set of parameters being denoted as E(p1, p2, ..., pN ). In general, finding good parameters is a mixture of good manual setting and some automatic fine tuning. Both methods described below, are sensitive to the initialization values of pi. They are not guaranteed to find a global optimum, and can easily get stuck in local optima. So starting with a random initialization of the parameters does not guarantee to get the best solution. To ensure that all our results are reproducible, we report all found parameter values for each individual predictor in section 20. Note that the reported values are these, which we found and used. For most algorithms there may be a much better parameterization.",
        "venue": "",
        "year": 2008,
        "referenceCount": 5,
        "citationCount": 33,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The team \u201cBellKor in Bigchaos\u201d is a combined team of team BellKor and BigChaos, and the solution with a RMSE of 0.8616 is created by a linear blend of the results from both teams."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{T\u00f6scher2008TheBS,\n author = {Andreas T\u00f6scher},\n title = {The BigChaos Solution to the Netflix Prize 2008},\n year = {2008}\n}\n"
        }
    },
    "862_polycoder": {
        "paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e",
        "externalIds": {
            "DBLP": "conf/pldi/Xu0NH22",
            "ArXiv": "2202.13169",
            "DOI": "10.1145/3520312.3534862",
            "CorpusId": 247158549
        },
        "corpusId": 247158549,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/b32a6f6ef7dd775e0f876b4713ceccebc56e651e",
        "title": "A systematic evaluation of large language models of code",
        "abstract": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169.",
        "venue": "MAPS@PLDI",
        "year": 2022,
        "referenceCount": 37,
        "citationCount": 287,
        "influentialCitationCount": 27,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.1145/3520312.3534862",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work finds that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling, and identifies an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code."
        },
        "publicationTypes": [
            "JournalArticle",
            "Book"
        ],
        "publicationDate": "2022-02-26",
        "journal": {
            "name": "Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming"
        },
        "citationStyles": {
            "bibtex": "@Article{Xu2022ASE,\n author = {Frank F. Xu and Uri Alon and Graham Neubig and V. Hellendoorn},\n booktitle = {MAPS@PLDI},\n journal = {Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},\n title = {A systematic evaluation of large language models of code},\n year = {2022}\n}\n"
        }
    },
    "863_wizardcoder-15.5b": {
        "paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805",
        "externalIds": {
            "ArXiv": "2306.08568",
            "DBLP": "journals/corr/abs-2306-08568",
            "DOI": "10.48550/arXiv.2306.08568",
            "CorpusId": 259164815
        },
        "corpusId": 259164815,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/454c8fef2957aa2fb13eb2c7a454393a2ee83805",
        "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
        "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 37,
        "citationCount": 164,
        "influentialCitationCount": 34,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.08568",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "WizardCoder is introduced, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code, and surpasses all other open-source Code LLM by a substantial margin."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-06-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2306.08568"
        },
        "citationStyles": {
            "bibtex": "@Article{Luo2023WizardCoderEC,\n author = {Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {WizardCoder: Empowering Code Large Language Models with Evol-Instruct},\n volume = {abs/2306.08568},\n year = {2023}\n}\n"
        }
    },
    "864_ftw": {
        "paperId": "ea64dbd29ab90d8944bcc6a05680aeaacc86fc0a",
        "externalIds": {
            "MAG": "2947862328",
            "ArXiv": "1807.01281",
            "DBLP": "journals/corr/abs-1807-01281",
            "DOI": "10.1126/science.aau6249",
            "CorpusId": 49561741,
            "PubMed": "31147514"
        },
        "corpusId": 49561741,
        "publicationVenue": {
            "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
            "name": "Science",
            "type": "journal",
            "issn": "0193-4511",
            "alternate_issns": [
                "0036-8075"
            ],
            "url": "https://www.jstor.org/journal/science",
            "alternate_urls": [
                "https://www.sciencemag.org/",
                "http://www.sciencemag.org/",
                "http://www.jstor.org/journals/00368075.html",
                "http://www.sciencemag.org/archive/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/ea64dbd29ab90d8944bcc6a05680aeaacc86fc0a",
        "title": "Human-level performance in 3D multiplayer games with population-based reinforcement learning",
        "abstract": "Artificial teamwork Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans. Science, this issue p. 859 Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode. Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.",
        "venue": "Science",
        "year": 2018,
        "referenceCount": 99,
        "citationCount": 607,
        "influentialCitationCount": 25,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://science.sciencemag.org/content/sci/364/6443/859.full.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A tournament-style evaluation is used to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-07-03",
        "journal": {
            "name": "Science",
            "pages": "859 - 865",
            "volume": "364"
        },
        "citationStyles": {
            "bibtex": "@Article{Jaderberg2018HumanlevelPI,\n author = {Max Jaderberg and Wojciech M. Czarnecki and Iain Dunning and Luke Marris and Guy Lever and Antonio Garc\u00eda Casta\u00f1eda and Charlie Beattie and Neil C. Rabinowitz and Ari S. Morcos and Avraham Ruderman and Nicolas Sonnerat and Tim Green and Louise Deason and Joel Z. Leibo and David Silver and D. Hassabis and K. Kavukcuoglu and T. Graepel},\n booktitle = {Science},\n journal = {Science},\n pages = {859 - 865},\n title = {Human-level performance in 3D multiplayer games with population-based reinforcement learning},\n volume = {364},\n year = {2018}\n}\n"
        }
    },
    "865_code_llama_70b": {
        "paperId": "0b0debb710366cdff461938c80763eace1651af6",
        "externalIds": {
            "DBLP": "journals/corr/abs-2308-12950",
            "ArXiv": "2308.12950",
            "DOI": "10.48550/arXiv.2308.12950",
            "CorpusId": 261100919
        },
        "corpusId": 261100919,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0b0debb710366cdff461938c80763eace1651af6",
        "title": "Code Llama: Open Foundation Models for Code",
        "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 92,
        "citationCount": 389,
        "influentialCitationCount": 62,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2308.12950",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-08-24",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2308.12950"
        },
        "citationStyles": {
            "bibtex": "@Article{Rozi\u00e8re2023CodeLO,\n author = {Baptiste Rozi\u00e8re and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Tan and Yossi Adi and Jingyu Liu and Tal Remez and J. Rapin and Artyom Kozhevnikov and I. Evtimov and Joanna Bitton and Manish P Bhatt and Cristian Cant\u00f3n Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre D'efossez and Jade Copet and F. Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Code Llama: Open Foundation Models for Code},\n volume = {abs/2308.12950},\n year = {2023}\n}\n"
        }
    },
    "866_s-norm": {
        "paperId": "3c78c6df5eb1695b6a399e346dde880af27d1016",
        "externalIds": {
            "MAG": "2765390718",
            "ACL": "P18-1078",
            "ArXiv": "1710.10723",
            "DBLP": "conf/acl/GardnerC18",
            "DOI": "10.18653/v1/P18-1078",
            "CorpusId": 223637
        },
        "corpusId": 223637,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/3c78c6df5eb1695b6a399e346dde880af27d1016",
        "title": "Simple and Effective Multi-Paragraph Reading Comprehension",
        "abstract": "We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2017,
        "referenceCount": 37,
        "citationCount": 420,
        "influentialCitationCount": 66,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-1078.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs, which involves sampling multiple paragraphs from each document, and using an objective function that requires themodel to produce globally correct output."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-10-29",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1710.10723"
        },
        "citationStyles": {
            "bibtex": "@Article{Clark2017SimpleAE,\n author = {Christopher Clark and Matt Gardner},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Simple and Effective Multi-Paragraph Reading Comprehension},\n volume = {abs/1710.10723},\n year = {2017}\n}\n"
        }
    },
    "867_alexnet": {
        "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
        "externalIds": {
            "MAG": "2618530766",
            "DBLP": "conf/nips/KrizhevskySH12",
            "DOI": "10.1145/3065386",
            "CorpusId": 195908774
        },
        "corpusId": 195908774,
        "publicationVenue": {
            "id": "4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
            "name": "Communications of the ACM",
            "type": "journal",
            "alternate_names": [
                "Commun ACM",
                "Communications of The ACM"
            ],
            "issn": "0001-0782",
            "url": "http://www.acm.org/pubs/cacm/",
            "alternate_urls": [
                "http://portal.acm.org/cacm",
                "http://www.acm.org/pubs/contents/journals/cacm/",
                "https://cacm.acm.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff",
        "title": "ImageNet classification with deep convolutional neural networks",
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "venue": "Communications of the ACM",
        "year": 2012,
        "referenceCount": 44,
        "citationCount": 106144,
        "influentialCitationCount": 12532,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://dl.acm.org/ft_gateway.cfm?id=3065386&type=pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2012-12-03",
        "journal": {
            "name": "Communications of the ACM",
            "pages": "84 - 90",
            "volume": "60"
        },
        "citationStyles": {
            "bibtex": "@Article{Krizhevsky2012ImageNetCW,\n author = {A. Krizhevsky and I. Sutskever and Geoffrey E. Hinton},\n booktitle = {Communications of the ACM},\n journal = {Communications of the ACM},\n pages = {84 - 90},\n title = {ImageNet classification with deep convolutional neural networks},\n volume = {60},\n year = {2012}\n}\n"
        }
    },
    "868_lstm+grab": {
        "paperId": "8796aa65cfaa6be32786ba16d2bcba01df12b661",
        "externalIds": {
            "DBLP": "journals/corr/abs-2205-10733",
            "ArXiv": "2205.10733",
            "DOI": "10.48550/arXiv.2205.10733",
            "CorpusId": 248986720
        },
        "corpusId": 248986720,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/8796aa65cfaa6be32786ba16d2bcba01df12b661",
        "title": "GraB: Finding Provably Better Data Permutations than Random Reshuffling",
        "abstract": "Random reshuffling, which randomly permutes the dataset each epoch, is widely adopted in model training because it yields faster convergence than with-replacement sampling. Recent studies indicate greedily chosen data orderings can further speed up convergence empirically, at the cost of using more computation and memory. However, greedy ordering lacks theoretical justification and has limited utility due to its non-trivial memory and computation overhead. In this paper, we first formulate an example-ordering framework named herding and answer affirmatively that SGD with herding converges at the rate $O(T^{-2/3})$ on smooth, non-convex objectives, faster than the $O(n^{1/3}T^{-2/3})$ obtained by random reshuffling, where $n$ denotes the number of data points and $T$ denotes the total number of iterations. To reduce the memory overhead, we leverage discrepancy minimization theory to propose an online Gradient Balancing algorithm (GraB) that enjoys the same rate as herding, while reducing the memory usage from $O(nd)$ to just $O(d)$ and computation from $O(n^2)$ to $O(n)$, where $d$ denotes the model dimension. We show empirically on applications including MNIST, CIFAR10, WikiText and GLUE that GraB can outperform random reshuffling in terms of both training and validation performance, and even outperform state-of-the-art greedy ordering while reducing memory usage over $100\\times$.",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "referenceCount": 42,
        "citationCount": 11,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2205.10733",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown empirically on applications including MNIST, CIFAR10, WikiText and GLUE that GraB can outperform random reshuffling in terms of both training and validation performance, and even outperform state-of-the-art greedy ordering while reducing memory usage over 100 times."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-05-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2205.10733"
        },
        "citationStyles": {
            "bibtex": "@Article{Lu2022GraBFP,\n author = {Yucheng Lu and Wentao Guo and Christopher De Sa},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {GraB: Finding Provably Better Data Permutations than Random Reshuffling},\n volume = {abs/2205.10733},\n year = {2022}\n}\n"
        }
    },
    "869_yuan_1.0": {
        "paperId": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79",
        "externalIds": {
            "DBLP": "journals/corr/abs-2110-04725",
            "ArXiv": "2110.04725",
            "CorpusId": 238582964
        },
        "corpusId": 238582964,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0ab41d455d676542b37ca1499bb19ea6a5d1cf79",
        "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning",
        "abstract": "Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 28,
        "citationCount": 38,
        "influentialCitationCount": 8,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a method that incorporates large-scale distributed training performance into model architecture design and achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2110.04725"
        },
        "citationStyles": {
            "bibtex": "@Article{Wu2021Yuan1L,\n author = {Shaohua Wu and Xudong Zhao and Tong Yu and Rongguo Zhang and C. Shen and Hongli Liu and Feng Li and Hong Zhu and Jiangang Luo and Liang Xu and Xuanwei Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning},\n volume = {abs/2110.04725},\n year = {2021}\n}\n"
        }
    },
    "870_alphago_zero": {
        "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
        "externalIds": {
            "MAG": "2766447205",
            "DBLP": "journals/nature/SilverSSAHGHBLB17",
            "DOI": "10.1038/nature24270",
            "CorpusId": 205261034,
            "PubMed": "29052630"
        },
        "corpusId": 205261034,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/c27db32efa8137cbf654902f8f728f338e55cd1c",
        "title": "Mastering the game of Go without human knowledge",
        "abstract": null,
        "venue": "Nature",
        "year": 2017,
        "referenceCount": 68,
        "citationCount": 8044,
        "influentialCitationCount": 354,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://discovery.ucl.ac.uk/10045895/1/agz_unformatted_nature.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An algorithm based solely on reinforcement learning is introduced, without human data, guidance or domain knowledge beyond game rules, that achieves superhuman performance, winning 100\u20130 against the previously published, champion-defeating AlphaGo."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-10-19",
        "journal": {
            "name": "Nature",
            "pages": "354-359",
            "volume": "550"
        },
        "citationStyles": {
            "bibtex": "@Article{Silver2017MasteringTG,\n author = {David Silver and Julian Schrittwieser and K. Simonyan and Ioannis Antonoglou and Aja Huang and A. Guez and T. Hubert and Lucas baker and Matthew Lai and A. Bolton and Yutian Chen and T. Lillicrap and Fan Hui and L. Sifre and George van den Driessche and T. Graepel and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {354-359},\n title = {Mastering the game of Go without human knowledge},\n volume = {550},\n year = {2017}\n}\n"
        }
    },
    "871_seq2seq_lstm": {
        "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
        "externalIds": {
            "MAG": "2130942839",
            "DBLP": "conf/nips/SutskeverVL14",
            "ArXiv": "1409.3215",
            "CorpusId": 7961699
        },
        "corpusId": 7961699,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d",
        "title": "Sequence to Sequence Learning with Neural Networks",
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "referenceCount": 41,
        "citationCount": 18510,
        "influentialCitationCount": 1348,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure, and finds that reversing the order of the words in all source sentences improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-09-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1409.3215"
        },
        "citationStyles": {
            "bibtex": "@Article{Sutskever2014SequenceTS,\n author = {I. Sutskever and O. Vinyals and Quoc V. Le},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Sequence to Sequence Learning with Neural Networks},\n volume = {abs/1409.3215},\n year = {2014}\n}\n"
        }
    },
    "872_drlim": {
        "paperId": "46f30e94dd3d5902141c5fbe58d0bc9189545c76",
        "externalIds": {
            "DBLP": "conf/cvpr/HadsellCL06",
            "MAG": "2138621090",
            "DOI": "10.1109/CVPR.2006.100",
            "CorpusId": 8281592
        },
        "corpusId": 8281592,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/46f30e94dd3d5902141c5fbe58d0bc9189545c76",
        "title": "Dimensionality Reduction by Learning an Invariant Mapping",
        "abstract": "Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar\" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2006,
        "referenceCount": 19,
        "citationCount": 4475,
        "influentialCitationCount": 262,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2006-06-17",
        "journal": {
            "name": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
            "pages": "1735-1742",
            "volume": "2"
        },
        "citationStyles": {
            "bibtex": "@Article{Hadsell2006DimensionalityRB,\n author = {R. Hadsell and S. Chopra and Yann LeCun},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},\n pages = {1735-1742},\n title = {Dimensionality Reduction by Learning an Invariant Mapping},\n volume = {2},\n year = {2006}\n}\n"
        }
    },
    "873_xtrimopglm_-100b": {
        "paperId": "c064c79e3026f81e5043cd5b0f4264b4d43336e6",
        "externalIds": {
            "DBLP": "journals/corr/abs-2401-06199",
            "ArXiv": "2401.06199",
            "DOI": "10.1101/2023.07.05.547496",
            "CorpusId": 259502990
        },
        "corpusId": 259502990,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/c064c79e3026f81e5043cd5b0f4264b4d43336e6",
        "title": "xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein",
        "abstract": "Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to an advanced 3D structural prediction model that surpasses existing language model-based tools. 2) xTrimoPGLM not only can generate de novo protein sequences following the principles of natural ones, but also can perform programmable generation after supervised fine-tuning (SFT) on curated sequences. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences, contributing to the evolving landscape of foundation models in protein science.",
        "venue": "bioRxiv",
        "year": 2024,
        "referenceCount": 110,
        "citationCount": 26,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2023/07/14/2023.07.05.547496.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Biology",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a unified protein language model, xTrimoPGLM, to address protein understanding and generation tasks simultaneously through an innovative pre-training framework, leading to an advanced 3D structural prediction model that surpasses existing language model-based tools."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2024-01-11",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2024xTrimoPGLMU1,\n author = {Bo Chen and Xingyi Cheng and Yangli-ao Geng and Shengyin Li and Xin Zeng and Bo Wang and Jing Gong and Chiming Liu and Aohan Zeng and Yuxiao Dong and Jie Tang and Leo T. Song},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein},\n year = {2024}\n}\n"
        }
    },
    "877_hybrid_h3-2.7b": {
        "paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "externalIds": {
            "DBLP": "conf/iclr/FuDSTRR23",
            "ArXiv": "2212.14052",
            "DOI": "10.48550/arXiv.2212.14052",
            "CorpusId": 255340454
        },
        "corpusId": 255340454,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/5a77b508302771fc083bf24e0bcda8553c9b5421",
        "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
        "abstract": "State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 65,
        "citationCount": 88,
        "influentialCitationCount": 13,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2212.14052",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-12-28",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2212.14052"
        },
        "citationStyles": {
            "bibtex": "@Article{Dao2022HungryHH,\n author = {Tri Dao and Daniel Y. Fu and Khaled Kamal Saab and A. Thomas and A. Rudra and Christopher R\u00e9},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},\n volume = {abs/2212.14052},\n year = {2022}\n}\n"
        }
    },
    "878_symmetric_residual_encoder-decoder_net": {
        "paperId": "18168aea48a22f6fe2fe407c0ff70083cba225a7",
        "externalIds": {
            "DBLP": "conf/nips/MaoSY16",
            "MAG": "2950670068",
            "CorpusId": 10987457
        },
        "corpusId": 10987457,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/18168aea48a22f6fe2fe407c0ff70083cba225a7",
        "title": "Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections",
        "abstract": "In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. De-convolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and de-convolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, The skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to de-convolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than all previously reported state-of-the-art methods.",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "referenceCount": 34,
        "citationCount": 1412,
        "influentialCitationCount": 113,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes to symmetrically link convolutional and de-convolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum, making training deep networks easier and achieving restoration performance gains consequently."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-03-30",
        "journal": {
            "pages": "2802-2810"
        },
        "citationStyles": {
            "bibtex": "@Article{Mao2016ImageRU,\n author = {Xiao-Jiao Mao and Chunhua Shen and Yubin Yang},\n booktitle = {Neural Information Processing Systems},\n pages = {2802-2810},\n title = {Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections},\n year = {2016}\n}\n"
        }
    },
    "879_diplodocus": {
        "paperId": "7842368bf1afde87deb63333871760ae848f01f9",
        "externalIds": {
            "DBLP": "journals/corr/abs-2210-05492",
            "ArXiv": "2210.05492",
            "DOI": "10.48550/arXiv.2210.05492",
            "CorpusId": 252815905
        },
        "corpusId": 252815905,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7842368bf1afde87deb63333871760ae848f01f9",
        "title": "Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning",
        "abstract": "No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitation-learned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 37,
        "citationCount": 20,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.05492",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces a planning algorithm that regularizes a reward-maximizing policy toward a human imitation-learned policy and extends it into a self-play reinforcement learning algorithm called RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.05492"
        },
        "citationStyles": {
            "bibtex": "@Article{Bakhtin2022MasteringTG,\n author = {A. Bakhtin and David J. Wu and Adam Lerer and Jonathan Gray and Athul Paul Jacob and Gabriele Farina and Alexander H. Miller and Noam Brown},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning},\n volume = {abs/2210.05492},\n year = {2022}\n}\n"
        }
    },
    "880_senet_(imagenet)": {
        "paperId": "fb37561499573109fc2cebb6a7b08f44917267dd",
        "externalIds": {
            "MAG": "2963420686",
            "DBLP": "journals/corr/abs-1709-01507",
            "ArXiv": "1709.01507",
            "DOI": "10.1109/CVPR.2018.00745",
            "CorpusId": 140309863
        },
        "corpusId": 140309863,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/fb37561499573109fc2cebb6a7b08f44917267dd",
        "title": "Squeeze-and-Excitation Networks",
        "abstract": "Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ~25% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.",
        "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
        "year": 2017,
        "referenceCount": 85,
        "citationCount": 19656,
        "influentialCitationCount": 1651,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1709.01507",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a novel architectural unit, which is term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels and finds that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-09-05",
        "journal": {
            "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "pages": "7132-7141"
        },
        "citationStyles": {
            "bibtex": "@Article{Hu2017SqueezeandExcitationN,\n author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and E. Wu},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {7132-7141},\n title = {Squeeze-and-Excitation Networks},\n year = {2017}\n}\n"
        }
    },
    "882_moe": {
        "paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586",
        "externalIds": {
            "DBLP": "journals/corr/ShazeerMMDLHD17",
            "MAG": "2952339051",
            "ArXiv": "1701.06538",
            "CorpusId": 12462234
        },
        "corpusId": 12462234,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/510e26733aaff585d65701b9f1be7ca9d5afc586",
        "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
        "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "referenceCount": 45,
        "citationCount": 1483,
        "influentialCitationCount": 179,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks, and applies the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-01-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1701.06538"
        },
        "citationStyles": {
            "bibtex": "@Article{Shazeer2017OutrageouslyLN,\n author = {Noam M. Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc V. Le and Geoffrey E. Hinton and J. Dean},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},\n volume = {abs/1701.06538},\n year = {2017}\n}\n"
        }
    },
    "883_skywork-13b": {
        "paperId": "71c9d2b995c19d43c519eb5ca9504ac790490398",
        "externalIds": {
            "DBLP": "journals/corr/abs-2310-19341",
            "ArXiv": "2310.19341",
            "DOI": "10.48550/arXiv.2310.19341",
            "CorpusId": 264802115
        },
        "corpusId": 264802115,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/71c9d2b995c19d43c519eb5ca9504ac790490398",
        "title": "Skywork: A More Open Bilingual Foundation Model",
        "abstract": "In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves \\emph{state of the art} performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 56,
        "citationCount": 26,
        "influentialCitationCount": 3,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date, and introduces a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-10-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2310.19341"
        },
        "citationStyles": {
            "bibtex": "@Article{Wei2023SkyworkAM,\n author = {Tianwen Wei and Liang Zhao and Lichang Zhang and Bo Zhu and Lijie Wang and Haihua Yang and Biye Li and Cheng Cheng and Weiwei L\u00fc and Rui Hu and Chenxia Li and Liu Yang and Xilin Luo and X. Wu and Lunan Liu and Wenjun Cheng and Peng Cheng and Jianhao Zhang and Xiaoyu Zhang and Lei Lin and Xiaokun Wang and Yutuan Ma and Chuanhai Dong and Yanqi Sun and Yifu Chen and Yongyi Peng and Xiaojuan Liang and Shuicheng Yan and Han Fang and Yahui Zhou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Skywork: A More Open Bilingual Foundation Model},\n volume = {abs/2310.19341},\n year = {2023}\n}\n"
        }
    },
    "885_data2vec_(language)": {
        "paperId": "8f2bca9d684005675e294b33c26481e36f528cdb",
        "externalIds": {
            "ArXiv": "2202.03555",
            "DBLP": "conf/icml/BaevskiHXBGA22",
            "CorpusId": 246652264
        },
        "corpusId": 246652264,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/8f2bca9d684005675e294b33c26481e36f528cdb",
        "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
        "abstract": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "referenceCount": 86,
        "citationCount": 523,
        "influentialCitationCount": 93,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Data2vec is a framework that uses the same learning method for either speech, NLP or computer vision to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-02-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2202.03555"
        },
        "citationStyles": {
            "bibtex": "@Article{Baevski2022data2vecAG,\n author = {Alexei Baevski and Wei-Ning Hsu and Qiantong Xu and Arun Babu and Jiatao Gu and Michael Auli},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language},\n volume = {abs/2202.03555},\n year = {2022}\n}\n"
        }
    },
    "887_mid-level_features": {
        "paperId": "498efaa51f5eda731dc6199c3547b9465717fa68",
        "externalIds": {
            "DBLP": "conf/cvpr/BoureauBLP10",
            "MAG": "2090042335",
            "DOI": "10.1109/CVPR.2010.5539963",
            "CorpusId": 90113
        },
        "corpusId": 90113,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/498efaa51f5eda731dc6199c3547b9465717fa68",
        "title": "Learning mid-level features for recognition",
        "abstract": "Many successful models for scene or object recognition transform low-level descriptors (such as Gabor filter responses, or SIFT descriptors) into richer representations of intermediate complexity. This process can often be broken down into two steps: (1) a coding step, which performs a pointwise transformation of the descriptors into a representation better adapted to the task, and (2) a pooling step, which summarizes the coded features over larger neighborhoods. Several combinations of coding and pooling schemes have been proposed in the literature. The goal of this paper is threefold. We seek to establish the relative importance of each step of mid-level feature extraction through a comprehensive cross evaluation of several types of coding modules (hard and soft vector quantization, sparse coding) and pooling schemes (by taking the average, or the maximum), which obtains state-of-the-art performance or better on several recognition benchmarks. We show how to improve the best performing coding scheme by learning a supervised discriminative dictionary for sparse coding. We provide theoretical and empirical insight into the remarkable performance of max pooling. By teasing apart components shared by modern mid-level feature extractors, our approach aims to facilitate the design of better recognition architectures.",
        "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
        "year": 2010,
        "referenceCount": 32,
        "citationCount": 1135,
        "influentialCitationCount": 99,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work seeks to establish the relative importance of each step of mid-level feature extraction through a comprehensive cross evaluation of several types of coding modules and pooling schemes and shows how to improve the best performing coding scheme by learning a supervised discriminative dictionary for sparse coding."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2010-06-13",
        "journal": {
            "name": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "pages": "2559-2566"
        },
        "citationStyles": {
            "bibtex": "@Article{Boureau2010LearningMF,\n author = {Y-Lan Boureau and F. Bach and Yann LeCun and J. Ponce},\n booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n pages = {2559-2566},\n title = {Learning mid-level features for recognition},\n year = {2010}\n}\n"
        }
    },
    "889_pangu-\u03c3": {
        "paperId": "362cbfd0d05e139cd6cf049754098a6e1520b910",
        "externalIds": {
            "ArXiv": "2303.10845",
            "DBLP": "journals/corr/abs-2303-10845",
            "DOI": "10.48550/arXiv.2303.10845",
            "CorpusId": 257666647
        },
        "corpusId": 257666647,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/362cbfd0d05e139cd6cf049754098a6e1520b910",
        "title": "PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing",
        "abstract": "The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\\Sigma}. With parameter inherent from PanGu-{\\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 73,
        "citationCount": 35,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.10845",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The experimental findings show that PanGu-{\\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks and demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-03-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2303.10845"
        },
        "citationStyles": {
            "bibtex": "@Article{Ren2023PanGu\u03a3TT,\n author = {Xiaozhe Ren and Pingyi Zhou and Xinfan Meng and Xinjing Huang and Yadao Wang and Weichao Wang and Pengfei Li and Xiaoda Zhang and A. V. Podolskiy and G. Arshinov and A. Bout and Irina Piontkovskaya and Jiansheng Wei and Xin Jiang and Teng Su and Qun Liu and Jun Yao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing},\n volume = {abs/2303.10845},\n year = {2023}\n}\n"
        }
    },
    "890_rns-rnn": {
        "paperId": "1a22e6406e67c1737ec073cc646f60cb78631a4c",
        "externalIds": {
            "DBLP": "conf/iclr/DuSell022",
            "ArXiv": "2109.01982",
            "CorpusId": 237420771
        },
        "corpusId": 237420771,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/1a22e6406e67c1737ec073cc646f60cb78631a4c",
        "title": "Learning Hierarchical Structures with Differentiable Nondeterministic Stacks",
        "abstract": "Learning hierarchical structures in sequential data -- from simple algorithmic patterns to natural language -- in a reliable, generalizable way remains a challenging problem for neural language models. Past work has shown that recurrent neural networks (RNNs) struggle to generalize on held-out algorithmic or syntactic patterns without supervision or some inductive bias. To remedy this, many papers have explored augmenting RNNs with various differentiable stacks, by analogy with finite automata and pushdown automata (PDAs). In this paper, we improve the performance of our recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure that simulates a nondeterministic PDA, with two important changes. First, the model now assigns unnormalized positive weights instead of probabilities to stack actions, and we provide an analysis of why this improves training. Second, the model can directly observe the state of the underlying PDA. Our model achieves lower cross-entropy than all previous stack RNNs on five context-free language modeling tasks (within 0.05 nats of the information-theoretic lower bound), including a task on which the NS-RNN previously failed to outperform a deterministic stack RNN baseline. Finally, we propose a restricted version of the NS-RNN that incrementally processes infinitely long sequences, and we present language modeling results on the Penn Treebank.",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "referenceCount": 33,
        "citationCount": 9,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper improves the performance of the recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure that simulates a nondeterministic PDA, with two important changes."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-09-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2109.01982"
        },
        "citationStyles": {
            "bibtex": "@Article{DuSell2021LearningHS,\n author = {Brian DuSell and David Chiang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning Hierarchical Structures with Differentiable Nondeterministic Stacks},\n volume = {abs/2109.01982},\n year = {2021}\n}\n"
        }
    },
    "891_atlas": {
        "paperId": "ad5970584754cc7a1d91c95ab84a1e210258183a",
        "externalIds": {
            "DBLP": "conf/emnlp/KhashabiMKSTCH20",
            "MAG": "3099655892",
            "ArXiv": "2005.00700",
            "ACL": "2020.findings-emnlp.171",
            "DOI": "10.18653/v1/2020.findings-emnlp.171",
            "CorpusId": 218487109
        },
        "corpusId": 218487109,
        "publicationVenue": {
            "id": "479d5605-51be-4346-b1d6-4334084504df",
            "name": "Findings",
            "type": "journal",
            "issn": "2652-8800",
            "url": "https://findingspress.org/"
        },
        "url": "https://www.semanticscholar.org/paper/ad5970584754cc7a1d91c95ab84a1e210258183a",
        "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System",
        "abstract": "Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.",
        "venue": "Findings",
        "year": 2020,
        "referenceCount": 58,
        "citationCount": 574,
        "influentialCitationCount": 115,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2005.00700",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work uses the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats, and results in a new state of the art on 10 factoid and commonsense question answering datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-05-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2005.00700"
        },
        "citationStyles": {
            "bibtex": "@Article{Khashabi2020UnifiedQACF,\n author = {Daniel Khashabi and Sewon Min and Tushar Khot and Ashish Sabharwal and Oyvind Tafjord and Peter Clark and Hannaneh Hajishirzi},\n booktitle = {Findings},\n journal = {ArXiv},\n title = {UnifiedQA: Crossing Format Boundaries With a Single QA System},\n volume = {abs/2005.00700},\n year = {2020}\n}\n"
        }
    },
    "892_mnasnet-a3": {
        "paperId": "693c97ecedb0a84539b7162c95e89fa3cd84ca73",
        "externalIds": {
            "ArXiv": "1807.11626",
            "DBLP": "conf/cvpr/TanCPVSHL19",
            "MAG": "2886953980",
            "DOI": "10.1109/CVPR.2019.00293",
            "CorpusId": 51891697
        },
        "corpusId": 51891697,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/693c97ecedb0a84539b7162c95e89fa3cd84ca73",
        "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
        "abstract": "Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8\u00d7 faster than MobileNetV2 with 0.5% higher accuracy and 2.3\u00d7 faster than NASNet with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "referenceCount": 43,
        "citationCount": 2519,
        "influentialCitationCount": 342,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1807.11626",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-07-31",
        "journal": {
            "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "2815-2823"
        },
        "citationStyles": {
            "bibtex": "@Article{Tan2018MnasNetPN,\n author = {Mingxing Tan and Bo Chen and Ruoming Pang and Vijay Vasudevan and Quoc V. Le},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2815-2823},\n title = {MnasNet: Platform-Aware Neural Architecture Search for Mobile},\n year = {2018}\n}\n"
        }
    },
    "893_lamda": {
        "paperId": "b3848d32f7294ec708627897833c4097eb4d8778",
        "externalIds": {
            "DBLP": "journals/corr/abs-2201-08239",
            "ArXiv": "2201.08239",
            "CorpusId": 246063428
        },
        "corpusId": 246063428,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/b3848d32f7294ec708627897833c4097eb4d8778",
        "title": "LaMDA: Language Models for Dialog Applications",
        "abstract": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 120,
        "citationCount": 1042,
        "influentialCitationCount": 81,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-01-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2201.08239"
        },
        "citationStyles": {
            "bibtex": "@Article{Thoppilan2022LaMDALM,\n author = {R. Thoppilan and Daniel De Freitas and Jamie Hall and Noam M. Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and Yaguang Li and Hongrae Lee and H. Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and M. Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Yanqi Zhou and Chung-Ching Chang and I. Krivokon and W. Rusch and Marc Pickett and K. Meier-Hellstern and M. Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and J. S\u00f8raker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark D\u00edaz and B. Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and V. Kuzmina and Joseph Fenton and Aaron Cohen and R. Bernstein and R. Kurzweil and Blaise Aguera-Arcas and Claire Cui and M. Croak and E. Chi and Quoc Le},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {LaMDA: Language Models for Dialog Applications},\n volume = {abs/2201.08239},\n year = {2022}\n}\n"
        }
    },
    "896_galactica": {
        "paperId": "7d645a3fd276918374fd9483fd675c28e46506d1",
        "externalIds": {
            "ArXiv": "2211.09085",
            "DBLP": "journals/corr/abs-2211-09085",
            "DOI": "10.48550/arXiv.2211.09085",
            "CorpusId": 253553203
        },
        "corpusId": 253553203,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1",
        "title": "Galactica: A Large Language Model for Science",
        "abstract": "Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 107,
        "citationCount": 381,
        "influentialCitationCount": 52,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2211.09085",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Physics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Galactica is introduced: a large language model that can store, combine and reason about scientific knowledge, and sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-11-16",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2211.09085"
        },
        "citationStyles": {
            "bibtex": "@Article{Taylor2022GalacticaAL,\n author = {Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and A. Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Galactica: A Large Language Model for Science},\n volume = {abs/2211.09085},\n year = {2022}\n}\n"
        }
    },
    "897_svd_in_recommender_systems": {
        "paperId": "c3eddc356634d8c0d6db385567deffe1cf7cb3f6",
        "externalIds": {
            "MAG": "1832221731",
            "DOI": "10.21236/ada439541",
            "CorpusId": 17065558
        },
        "corpusId": 17065558,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/c3eddc356634d8c0d6db385567deffe1cf7cb3f6",
        "title": "Application of Dimensionality Reduction in Recommender System - A Case Study",
        "abstract": "Abstract : We investigate the use of dimensionality reduction to improve performance for a new class of data analysis software called \"recommender systems\" Recommender systems apply knowledge discovery techniques to the problem of making product recommendations during a live customer interaction. These systems are achieving widespread success in E-commerce nowadays, especially with the advent of the Internet. The tremendous growth of customers and products poses three key challenges for recommender systems in the E-commerce domain. These are: producing high quality recommendations, performing many recommendations per second for millions of customers and products, and achieving high coverage in the face of data sparsity. One successful recommender system technology is collaborative filtering, which works by matching customer preferences to other customers in making recommendations. Collaborative filtering has been shown to produce high quality recommendations, but the performance degrades with the number of customers and products. New recommender system technologies are needed that can quickly produce high quality recommendations, even for very largescale problems. This paper presents two different experiments where we have explored one technology called Singular Value Decomposition (SVD) to reduce the dimensionality of recommender system databases. Each experiment compares the quality of a recommender system using SVD with the quality of a recommender system using collaborative filtering. The first experiment compares the effectiveness of the two recommender systems at predicting consumer preferences based on a database of explicit ratings of products. The second experiment compares the effectiveness of the two recommender systems at producing Top-N lists based on a real-life customer purchase database from an E-Commerce site. Our experience suggests that SVD has the potential to meet many of the challenges of recommender systems, under certain conditions.",
        "venue": "",
        "year": 2000,
        "referenceCount": 33,
        "citationCount": 1529,
        "influentialCitationCount": 120,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://robotics.stanford.edu/~ronnyk/WEBKDD2000/papers/sarwar.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents two different experiments where one technology called Singular Value Decomposition (SVD) is explored to reduce the dimensionality of recommender system databases and suggests that SVD has the potential to meet many of the challenges ofRecommender systems, under certain conditions."
        },
        "publicationTypes": null,
        "publicationDate": "2000-07-14",
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Sarwar2000ApplicationOD,\n author = {B. Sarwar and G. Karypis and J. Konstan and J. Riedl},\n title = {Application of Dimensionality Reduction in Recommender System - A Case Study},\n year = {2000}\n}\n"
        }
    },
    "898_rl_mapping_instructions_(games)": {
        "paperId": "cc1648c91ffda21bbe6e5f08f69c683588fc384c",
        "externalIds": {
            "MAG": "2122223050",
            "DBLP": "conf/acl/BranavanCZB09",
            "ACL": "P09-1010",
            "DOI": "10.3115/1687878.1687892",
            "CorpusId": 5249151
        },
        "corpusId": 5249151,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/cc1648c91ffda21bbe6e5f08f69c683588fc384c",
        "title": "Reinforcement Learning for Mapping Instructions to Actions",
        "abstract": "In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains --- Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2009,
        "referenceCount": 22,
        "citationCount": 308,
        "influentialCitationCount": 18,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.5555/1687878.1687892",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a reinforcement learning approach for mapping natural language instructions to sequences of executable actions, and uses a policy gradient algorithm to estimate the parameters of a log-linear model for action selection."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "publicationDate": "2009-08-02",
        "journal": {
            "pages": "82-90"
        },
        "citationStyles": {
            "bibtex": "@Article{Branavan2009ReinforcementLF,\n author = {S. Branavan and Harr Chen and Luke Zettlemoyer and R. Barzilay},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {82-90},\n title = {Reinforcement Learning for Mapping Instructions to Actions},\n year = {2009}\n}\n"
        }
    },
    "899_nlm": {
        "paperId": "0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d",
        "externalIds": {
            "ArXiv": "2109.04212",
            "ACL": "2021.emnlp-main.461",
            "DBLP": "conf/emnlp/HeNB21",
            "DOI": "10.18653/v1/2021.emnlp-main.461",
            "CorpusId": 237452184
        },
        "corpusId": 237452184,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d",
        "title": "Efficient Nearest Neighbor Language Models",
        "abstract": "Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "referenceCount": 35,
        "citationCount": 68,
        "influentialCitationCount": 9,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.461.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The recently proposed k-nearest neighbors language model is taken as an example, and methods to improve its efficiency along various dimensions are explored, able to achieve up to a 6x speed-up in inference speed while retaining comparable performance."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-09-09",
        "journal": {
            "pages": "5703-5714"
        },
        "citationStyles": {
            "bibtex": "@Article{He2021EfficientNN,\n author = {Junxian He and Graham Neubig and Taylor Berg-Kirkpatrick},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {5703-5714},\n title = {Efficient Nearest Neighbor Language Models},\n year = {2021}\n}\n"
        }
    },
    "901_par_transformer_large": {
        "paperId": "81a1037757bc4b794c4bc483939f76526ca8d813",
        "externalIds": {
            "DBLP": "journals/corr/abs-2009-04534",
            "ArXiv": "2009.04534",
            "MAG": "3084302239",
            "CorpusId": 221586466
        },
        "corpusId": 221586466,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/81a1037757bc4b794c4bc483939f76526ca8d813",
        "title": "Pay Attention when Required",
        "abstract": "Transformer-based models consist of interleaved feed-forward blocks - that capture content meaning, and relatively more expensive self-attention blocks - that capture context meaning. In this paper, we explored trade-offs and ordering of the blocks to improve upon the current Transformer architecture and proposed PAR Transformer. It needs 35% lower compute time than Transformer-XL achieved by replacing ~63% of the self-attention blocks with feed-forward blocks, and retains the perplexity on WikiText-103 language modelling benchmark. We further validated our results on text8 and enwiki8 datasets, as well as on the BERT model.",
        "venue": "arXiv.org",
        "year": 2020,
        "referenceCount": 38,
        "citationCount": 8,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper explored trade-offs and ordering of the blocks to improve upon the current Transformer architecture and proposed PAR Transformer, which needs 35% lower compute time than Transformer-XL achieved by replacing ~63% of the self-attention blocks with feed-forward blocks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-09-09",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2009.04534"
        },
        "citationStyles": {
            "bibtex": "@Article{Mandava2020PayAW,\n author = {Swetha Mandava and Szymon Migacz and A. Fit-Florea},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Pay Attention when Required},\n volume = {abs/2009.04534},\n year = {2020}\n}\n"
        }
    },
    "902_sparse_wide_gpt-3_small": {
        "paperId": "c62321199fb07ce317921ff8832e9da1cee778af",
        "externalIds": {
            "ArXiv": "2303.11525",
            "CorpusId": 257636503
        },
        "corpusId": 257636503,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/c62321199fb07ce317921ff8832e9da1cee778af",
        "title": "Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency",
        "abstract": "Recent research has focused on weight sparsity in neural network training to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t training FLOPs). However, sparse weight training often sacrifices accuracy, requiring extended training schedules to attain the accuracy of dense models. In contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses sparsity to improve accuracy while maintaining dense model FLOPs. Using a single hyperparameter (i.e., sparsity level), Sparse-IFTs efficiently replace dense layers, expanding the search space for optimal sparse masks. In addition, dynamic sparse training with Sparse-IFT models effectively navigates this larger sparse mask-weight space, which is evidenced by a spectral analysis using Ramanujan graph properties. Our study reveals a robust correlation among mask topology, weights, and final performance. Notably, without adjusting hyperparameters, replacing dense layers with Sparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18 on ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a simple-to-use set of sparse transformations. Code is available at: https://github.com/CerebrasResearch/Sparse-IFT.",
        "venue": "",
        "year": 2023,
        "referenceCount": 100,
        "citationCount": 2,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a simple-to-use set of sparse transformations through a simple-to-use set of sparse transformations."
        },
        "publicationTypes": null,
        "publicationDate": "2023-03-21",
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Thangarasa2023SparseIFTSI,\n author = {Vithursan Thangarasa and S. Saxena and Abhay Gupta and Sean Lie},\n title = {Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency},\n year = {2023}\n}\n"
        }
    },
    "903_optimized_single-layer_net": {
        "paperId": "be9a17321537d9289875fe475b71f4821457b435",
        "externalIds": {
            "MAG": "2809446226",
            "DBLP": "journals/jmlr/CoatesNL11",
            "CorpusId": 308212
        },
        "corpusId": 308212,
        "publicationVenue": {
            "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
            "name": "International Conference on Artificial Intelligence and Statistics",
            "type": "conference",
            "alternate_names": [
                "AISTATS",
                "Int Conf Artif Intell Stat"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/be9a17321537d9289875fe475b71f4821457b435",
        "title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning",
        "abstract": "A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Specifically, we will apply several othe-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR, NORB, and STL datasets using only singlelayer networks. We then present a detailed analysis of the eect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (\u201cstride\u201d) between extracted features, and the eect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance\u2014so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyperparameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively).",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "year": 2011,
        "referenceCount": 36,
        "citationCount": 3405,
        "influentialCitationCount": 671,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance\u2014so critical, in fact, that when these parameters are pushed to their limits, they achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2011-12-01",
        "journal": {
            "pages": "215-223"
        },
        "citationStyles": {
            "bibtex": "@Article{Coates2011AnAO,\n author = {Adam Coates and A. Ng and Honglak Lee},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n pages = {215-223},\n title = {An Analysis of Single-Layer Networks in Unsupervised Feature Learning},\n year = {2011}\n}\n"
        }
    },
    "905_vit-huge_14": {
        "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
        "externalIds": {
            "MAG": "3094502228",
            "DBLP": "journals/corr/abs-2010-11929",
            "ArXiv": "2010.11929",
            "CorpusId": 225039882
        },
        "corpusId": 225039882,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "referenceCount": 65,
        "citationCount": 16918,
        "influentialCitationCount": 2876,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-10-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2010.11929"
        },
        "citationStyles": {
            "bibtex": "@Article{Dosovitskiy2020AnII,\n author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and G. Heigold and S. Gelly and Jakob Uszkoreit and N. Houlsby},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n volume = {abs/2010.11929},\n year = {2020}\n}\n"
        }
    },
    "906_lrcn": {
        "paperId": "f01fc808592ea7c473a69a6e7484040a435f36d9",
        "externalIds": {
            "MAG": "2508429489",
            "DBLP": "journals/pami/DonahueHRVGSD17",
            "ArXiv": "1411.4389",
            "DOI": "10.1109/CVPR.2015.7298878",
            "CorpusId": 5736847,
            "PubMed": "27608449"
        },
        "corpusId": 5736847,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9",
        "title": "Long-term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \u201ctemporally deep\u201d, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \u201cdoubly deep\u201d in that they can be compositional in spatial and temporal \u201clayers\u201d. Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "referenceCount": 90,
        "citationCount": 5654,
        "influentialCitationCount": 548,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and shows such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-11-17",
        "journal": {
            "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "2625-2634"
        },
        "citationStyles": {
            "bibtex": "@Article{Donahue2014LongtermRC,\n author = {Jeff Donahue and Lisa Anne Hendricks and Marcus Rohrbach and Subhashini Venugopalan and S. Guadarrama and Kate Saenko and Trevor Darrell},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2625-2634},\n title = {Long-term recurrent convolutional networks for visual recognition and description},\n year = {2014}\n}\n"
        }
    },
    "908_2-layer-lstm+deep-gradient-compression": {
        "paperId": "92495abbac86394cb759bec15a763dbf49a8e590",
        "externalIds": {
            "MAG": "2951832676",
            "DBLP": "conf/iclr/LinHM0D18",
            "ArXiv": "1712.01887",
            "CorpusId": 38796293
        },
        "corpusId": 38796293,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/92495abbac86394cb759bec15a763dbf49a8e590",
        "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training",
        "abstract": "Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "referenceCount": 41,
        "citationCount": 1149,
        "influentialCitationCount": 141,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper finds 99.9% of the gradient exchange in distributed SGD is redundant, and proposes Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth, which enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributedTraining on mobile."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-12-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1712.01887"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2017DeepGC,\n author = {Yujun Lin and Song Han and Huizi Mao and Yu Wang and W. Dally},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n volume = {abs/1712.01887},\n year = {2017}\n}\n"
        }
    },
    "909_dlrm-2021": {
        "paperId": "bfc4b683d3221b631c8aed9861cb87cdb3ad78e4",
        "externalIds": {
            "DBLP": "journals/corr/abs-2104-05158",
            "CorpusId": 233210406
        },
        "corpusId": 233210406,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/bfc4b683d3221b631c8aed9861cb87cdb3ad78e4",
        "title": "High-performance, Distributed Training of Large-scale Deep Learning Recommendation Models",
        "abstract": ".",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 64,
        "citationCount": 37,
        "influentialCitationCount": 9,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2104.05158"
        },
        "citationStyles": {
            "bibtex": "@Article{Mudigere2021HighperformanceDT,\n author = {Dheevatsa Mudigere and Y. Hao and Jianyu Huang and Andrew Tulloch and Srinivas Sridharan and Xing Liu and Mustafa Ozdal and Jade Nie and Jongsoo Park and Liangchen Luo and J. Yang and Leon Gao and Dmytro Ivchenko and Aarti Basant and Yuxi Hu and Jiyan Yang and E. K. Ardestani and Xiaodong Wang and Rakesh Komuravelli and Ching-Hsiang Chu and Serhat Yilmaz and Huayu Li and Jiyuan Qian and Zhuobo Feng and Yi-An Ma and Junjie Yang and Ellie Wen and Hong Li and Lin Yang and Chonglin Sun and Whitney Zhao and Dimitry Melts and Krishnaveni Dhulipala and Krishna Kishore and Tyler N. Graf and Assaf Eisenman and Kiran Kumar Matam and Adi Gangidi and Guoqiang Jerry Chen and M. Krishnan and A. Nayak and Krishnakumar Nair and Bharath Muthiah and Mahmoud khorashadi and P. Bhattacharya and Petr Lapukhov and M. Naumov and Lin Qiao and M. Smelyanskiy and Bill Jia and Vijay Rao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {High-performance, Distributed Training of Large-scale Deep Learning Recommendation Models},\n volume = {abs/2104.05158},\n year = {2021}\n}\n"
        }
    },
    "910_crf-rnn": {
        "paperId": "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a",
        "externalIds": {
            "ArXiv": "1502.03240",
            "MAG": "3100332847",
            "DBLP": "journals/corr/ZhengJRVSDHT15",
            "DOI": "10.1109/ICCV.2015.179",
            "CorpusId": 1318262
        },
        "corpusId": 1318262,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a",
        "title": "Conditional Random Fields as Recurrent Neural Networks",
        "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "referenceCount": 71,
        "citationCount": 2450,
        "influentialCitationCount": 311,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1502.03240",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling is introduced, and top results are obtained on the challenging Pascal VOC 2012 segmentation benchmark."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-02-11",
        "journal": {
            "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
            "pages": "1529-1537"
        },
        "citationStyles": {
            "bibtex": "@Article{Zheng2015ConditionalRF,\n author = {Shuai Zheng and Sadeep Jayasumana and Bernardino Romera-Paredes and Vibhav Vineet and Zhizhong Su and Dalong Du and Chang Huang and Philip H. S. Torr},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {1529-1537},\n title = {Conditional Random Fields as Recurrent Neural Networks},\n year = {2015}\n}\n"
        }
    },
    "911_cloob": {
        "paperId": "eb95b02edfaeb28f528b5ee8b705388bb9a933be",
        "externalIds": {
            "DBLP": "conf/nips/FurstRLTTRKKKBH22",
            "ArXiv": "2110.11316",
            "CorpusId": 239050171
        },
        "corpusId": 239050171,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/eb95b02edfaeb28f528b5ee8b705388bb9a933be",
        "title": "CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP",
        "abstract": "CLIP yielded impressive results on zero-shot transfer learning tasks and is considered as a foundation model like BERT or GPT3. CLIP vision models that have a rich representation are pre-trained using the InfoNCE objective and natural language supervision before they are fine-tuned on particular tasks. Though CLIP excels at zero-shot transfer learning, it suffers from an explaining away problem, that is, it focuses on one or few features, while neglecting other relevant features. This problem is caused by insufficiently extracting the covariance structure in the original multi-modal data. We suggest to use modern Hopfield networks to tackle the problem of explaining away. Their retrieved embeddings have an enriched covariance structure derived from co-occurrences of features in the stored embeddings. However, modern Hopfield networks increase the saturation effect of the InfoNCE objective which hampers learning. We propose to use the InfoLOOB objective to mitigate this saturation effect. We introduce the novel\"Contrastive Leave One Out Boost\"(CLOOB), which uses modern Hopfield networks for covariance enrichment together with the InfoLOOB objective. In experiments we compare CLOOB to CLIP after pre-training on the Conceptual Captions and the YFCC dataset with respect to their zero-shot transfer learning performance on other datasets. CLOOB consistently outperforms CLIP at zero-shot transfer learning across all considered architectures and datasets.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 130,
        "citationCount": 68,
        "influentialCitationCount": 11,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces the novel\"Contrastive Leave One Out Boost\"(CLOOB), which uses modern Hopfield networks for covariance enrichment together with the InfoLOOB objective to mitigate this saturation effect of the InfoNCE objective."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-21",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2110.11316"
        },
        "citationStyles": {
            "bibtex": "@Article{Furst2021CLOOBMH,\n author = {Andreas Furst and Elisabeth Rumetshofer and Viet-Hung Tran and Hubert Ramsauer and Fei Tang and Johannes Lehner and David P. Kreil and Michael Kopp and G. Klambauer and Angela Bitto-Nemling and Sepp Hochreiter},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP},\n volume = {abs/2110.11316},\n year = {2021}\n}\n"
        }
    },
    "914_rsm": {
        "paperId": "df1cde8e53b2655551a6c699fc34c940ebd985ac",
        "externalIds": {
            "DBLP": "conf/ijcnn/RawlinsonAK21",
            "MAG": "2947369026",
            "ArXiv": "1905.11589",
            "DOI": "10.1109/IJCNN52387.2021.9533760",
            "CorpusId": 167217474
        },
        "corpusId": 167217474,
        "publicationVenue": {
            "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
            "name": "IEEE International Joint Conference on Neural Network",
            "type": "conference",
            "alternate_names": [
                "IJCNN",
                "IEEE Int Jt Conf Neural Netw",
                "Int Jt Conf Neural Netw",
                "International Joint Conference on Neural Network"
            ],
            "url": "http://www.wikicfp.com/cfp/program?id=1573"
        },
        "url": "https://www.semanticscholar.org/paper/df1cde8e53b2655551a6c699fc34c940ebd985ac",
        "title": "Learning distant cause and effect using only local and immediate credit assignment",
        "abstract": "We present a recurrent neural network memory that uses sparse coding to create a combinatoric encoding of sequential inputs. The network is trained using only local and immediate credit assignment. Despite this constraint, results are comparable to networks trained using deep backpropagation or BackProp Through Time (BPTT). With several examples, we show that the network can associate distant cause and effect in a discrete stochastic process, predict partially-observable higherorder sequences, and learn to generate many time-steps of video simulations. Typical memory consumption is 10-30x less than conventional RNNs, such as LSTM, trained by BPTT. One limitation of the memory is generalization to unseen input sequences. We additionally explore this limitation by measuring next-word prediction perplexity on the Penn Treebank dataset.",
        "venue": "IEEE International Joint Conference on Neural Network",
        "year": 2019,
        "referenceCount": 46,
        "citationCount": 3,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1905.11589",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A recurrent neural network memory that uses sparse coding to create a combinatoric encoding of sequential inputs that can associate distant causes and effects in a discrete stochastic process, predict partially-observable higher-order sequences, and enable a DQN agent to navigate a maze by giving it memory."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-05-28",
        "journal": {
            "name": "2021 International Joint Conference on Neural Networks (IJCNN)",
            "pages": "1-9"
        },
        "citationStyles": {
            "bibtex": "@Article{Rawlinson2019LearningDC,\n author = {D. Rawlinson and Abdelrahman Ahmed and Gideon Kowadlo},\n booktitle = {IEEE International Joint Conference on Neural Network},\n journal = {2021 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-9},\n title = {Learning distant cause and effect using only local and immediate credit assignment},\n year = {2019}\n}\n"
        }
    },
    "915_gans": {
        "paperId": "6296aa7cab06eaf058f7291040b320b5a83c0091",
        "externalIds": {
            "DBLP": "conf/icccnt/Krichen23",
            "ArXiv": "2203.00667",
            "DOI": "10.1109/ICCCNT56998.2023.10306417",
            "CorpusId": 1033682
        },
        "corpusId": 1033682,
        "publicationVenue": {
            "id": "47c47088-7640-441b-85ba-375fe8f5c3a4",
            "name": "International Conference on Computing Communication and Networking Technologies",
            "type": "conference",
            "alternate_names": [
                "Int Conf Comput Commun Netw Technol",
                "International Conference on Computing, Communication and Networking Technologies",
                "ICCCNT"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/6296aa7cab06eaf058f7291040b320b5a83c0091",
        "title": "Generative Adversarial Networks",
        "abstract": "Generative Adversarial Networks (GANs) are a type of deep learning techniques that have shown remarkable success in generating realistic images, videos, and other types of data. This paper provides a comprehensive guide to GANs, covering their architecture, loss functions, training methods, applications, evaluation metrics, challenges, and future directions. We begin with an introduction to GANs and their historical development, followed by a review of the background and related work. We then provide a detailed overview of the GAN architecture, including the generator and discriminator networks, and discuss the key design choices and variations. Next, we review the loss functions utilized in GANs, including the original minimax objective, as well as more recent approaches s.a. Wasserstein distance and gradient penalty. We then delve into the training of GANs, discussing common techniques s.a. alternating optimization, minibatch discrimination, and spectral normalization. We also provide a survey of the various applications of GANs across domains. In addition, we review the evaluation metrics utilized to assess the diversity and quality of GAN-produced data. Furthermore, we discuss the challenges and open issues in GANs, including mode collapse, training instability, and ethical considerations. Finally, we provide a glimpse into the future directions of GAN research, including improving scalability, developing new architectures, incorporating domain knowledge, and exploring new applications. Overall, this paper serves as a comprehensive guide to GANs, providing both theoretical and practical insights for researchers and practitioners in the field.",
        "venue": "International Conference on Computing Communication and Networking Technologies",
        "year": 2022,
        "referenceCount": 112,
        "citationCount": 26695,
        "influentialCitationCount": 3262,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A comprehensive guide to GANs, covering their architecture, loss functions, training methods, applications, evaluation metrics, challenges, and future directions is provided."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "publicationDate": "2022-02-03",
        "journal": {
            "name": "2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)",
            "pages": "1-7"
        },
        "citationStyles": {
            "bibtex": "@Article{Goodfellow2022GenerativeAN,\n author = {I. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron C. Courville and Yoshua Bengio},\n booktitle = {International Conference on Computing Communication and Networking Technologies},\n journal = {2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)},\n pages = {1-7},\n title = {Generative Adversarial Networks},\n year = {2022}\n}\n"
        }
    },
    "916_dropout_(cifar)": {
        "paperId": "0060745e006c5f14ec326904119dca19c6545e51",
        "externalIds": {
            "ArXiv": "1207.0580",
            "MAG": "1904365287",
            "DBLP": "journals/corr/abs-1207-0580",
            "CorpusId": 14832074
        },
        "corpusId": 14832074,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0060745e006c5f14ec326904119dca19c6545e51",
        "title": "Improving neural networks by preventing co-adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",
        "venue": "arXiv.org",
        "year": 2012,
        "referenceCount": 26,
        "citationCount": 7141,
        "influentialCitationCount": 641,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2012-07-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1207.0580"
        },
        "citationStyles": {
            "bibtex": "@Article{Hinton2012ImprovingNN,\n author = {Geoffrey E. Hinton and Nitish Srivastava and A. Krizhevsky and I. Sutskever and R. Salakhutdinov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Improving neural networks by preventing co-adaptation of feature detectors},\n volume = {abs/1207.0580},\n year = {2012}\n}\n"
        }
    },
    "917_pali": {
        "paperId": "28630034bb29760df01ab033b743e30b37f336ae",
        "externalIds": {
            "DBLP": "journals/corr/abs-2209-06794",
            "ArXiv": "2209.06794",
            "DOI": "10.48550/arXiv.2209.06794",
            "CorpusId": 252222320
        },
        "corpusId": 252222320,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/28630034bb29760df01ab033b743e30b37f336ae",
        "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
        "abstract": "Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 112,
        "citationCount": 356,
        "influentialCitationCount": 61,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2209.06794",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The PaLI (Pathways Language and Image model), a model that achieves state-of-the-art in multiple vision and language tasks, while retaining a simple, modular, and scalable design."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-09-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2209.06794"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2022PaLIAJ,\n author = {Xi Chen and Xiao Wang and Soravit Changpinyo and A. Piergiovanni and Piotr Padlewski and Daniel M. Salz and Sebastian Goodman and Adam Grycner and Basil Mustafa and Lucas Beyer and Alexander Kolesnikov and J. Puigcerver and Nan Ding and Keran Rong and Hassan Akbari and Gaurav Mishra and Linting Xue and Ashish V. Thapliyal and James Bradbury and Weicheng Kuo and Mojtaba Seyedhosseini and Chao Jia and Burcu Karagol Ayan and C. Riquelme and A. Steiner and A. Angelova and Xiaohua Zhai and N. Houlsby and Radu Soricut},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {PaLI: A Jointly-Scaled Multilingual Language-Image Model},\n volume = {abs/2209.06794},\n year = {2022}\n}\n"
        }
    },
    "918_resnext-50": {
        "paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c",
        "externalIds": {
            "MAG": "2953328958",
            "DBLP": "conf/cvpr/XieGDTH17",
            "ArXiv": "1611.05431",
            "DOI": "10.1109/CVPR.2017.634",
            "CorpusId": 8485068
        },
        "corpusId": 8485068,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/f6e0856b4a9199fa968ac00da612a9407b5cb85c",
        "title": "Aggregated Residual Transformations for Deep Neural Networks",
        "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 48,
        "citationCount": 8681,
        "influentialCitationCount": 1209,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.05431",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "On the ImageNet-1K dataset, it is empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy and is more effective than going deeper or wider when the authors increase the capacity."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-11-16",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "5987-5995"
        },
        "citationStyles": {
            "bibtex": "@Article{Xie2016AggregatedRT,\n author = {Saining Xie and Ross B. Girshick and Piotr Doll\u00e1r and Z. Tu and Kaiming He},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5987-5995},\n title = {Aggregated Residual Transformations for Deep Neural Networks},\n year = {2016}\n}\n"
        }
    },
    "919_zymctrl": {
        "paperId": "7ee13fdab3b4911928f041696debbe72bb2fe255",
        "externalIds": {
            "CorpusId": 266350157
        },
        "corpusId": 266350157,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/7ee13fdab3b4911928f041696debbe72bb2fe255",
        "title": "ZymCTRL: a conditional language model for the controllable generation of artificial enzymes",
        "abstract": "The design of custom-tailored proteins has the potential to provide novel and groundbreaking solutions in many fields, including molecular medicine or environmental sciences. Among protein classes, enzymes are particularly attractive because their complex active sites can accelerate chemical transformations by several orders of magnitude. Since enzymes are biodegradable nanoscopic materials, they hold an unmatched promise as sustainable, large-scale industrial catalysts. Motivated by the enormous success of language models in designing novel yet nature-like proteins, we hypothesised that an enzyme-specific language model could provide new opportunities to design purpose-built artificial enzymes. Here, we describe ZymCTRL, a conditional language model trained on the BRENDA database of enzymes, which generates enzymes of a specific enzymatic class upon a user prompt. ZymCTRL generates artificial enzymes distant from natural ones while their intended functionality matches predictions from orthogonal methods. We release the model to the community.",
        "venue": "",
        "year": null,
        "referenceCount": 43,
        "citationCount": 7,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": null,
        "s2FieldsOfStudy": [
            {
                "category": "Chemistry",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "ZymCTRL is described, a conditional language model trained on the BRENDA database of enzymes, which generates enzymes of a specific enzymatic class upon a user prompt, which generates artificial enzymes distant from natural ones while their intended functionality matches predictions from orthogonal methods."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": null,
        "citationStyles": {
            "bibtex": "@Misc{None,\n author = {Geraldene Munsamy and Sebastian Lindner and Philipp Lorenz and Noelia Ferruz},\n title = {ZymCTRL: a conditional language model for the controllable generation of artificial enzymes}\n}\n"
        }
    },
    "920_opt-175b": {
        "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "externalIds": {
            "DBLP": "journals/corr/abs-2205-01068",
            "ArXiv": "2205.01068",
            "CorpusId": 248496292
        },
        "corpusId": 248496292,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/13a0d8bb38f739990c8cd65a44061c6534f17221",
        "title": "OPT: Open Pre-trained Transformer Language Models",
        "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 120,
        "citationCount": 1868,
        "influentialCitationCount": 233,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, is presented, which is aimed to fully and responsibly share with interested researchers."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-05-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2205.01068"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2022OPTOP,\n author = {Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona T. Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {OPT: Open Pre-trained Transformer Language Models},\n volume = {abs/2205.01068},\n year = {2022}\n}\n"
        }
    },
    "921_madlad-400_10b": {
        "paperId": "420daaec9d0e3bbf3c4d945e0c63ca5aa217373b",
        "externalIds": {
            "ArXiv": "2309.04662",
            "DBLP": "journals/corr/abs-2309-04662",
            "DOI": "10.48550/arXiv.2309.04662",
            "CorpusId": 261682406
        },
        "corpusId": 261682406,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/420daaec9d0e3bbf3c4d945e0c63ca5aa217373b",
        "title": "MADLAD-400: A Multilingual And Document-Level Large Audited Dataset",
        "abstract": "We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models available to the research community.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 0,
        "citationCount": 13,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.04662",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages, and trains and releases a 10.7B-parameter multilingual machine translation model, which is competitive with models that are significantly larger, and reports the results on different domains."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-09-09",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2309.04662"
        },
        "citationStyles": {
            "bibtex": "@Article{Kudugunta2023MADLAD400AM,\n author = {Sneha Kudugunta and Isaac Caswell and Biao Zhang and Xavier Garc\u00eda and Christopher A. Choquette-Choo and Katherine Lee and Derrick Xin and Aditya Kusupati and Romi Stella and Ankur Bapna and Orhan Firat},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {MADLAD-400: A Multilingual And Document-Level Large Audited Dataset},\n volume = {abs/2309.04662},\n year = {2023}\n}\n"
        }
    },
    "922_grown_to_prune_two-layer_stacked_lstm": {
        "paperId": "910c8790bc7eb7bb0c66fa470f243c1cf2626119",
        "externalIds": {
            "DBLP": "conf/iclr/YuanSM21",
            "ArXiv": "2007.15353",
            "MAG": "3046041674",
            "CorpusId": 220871147
        },
        "corpusId": 220871147,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/910c8790bc7eb7bb0c66fa470f243c1cf2626119",
        "title": "Growing Efficient Deep Networks by Structured Continuous Sparsification",
        "abstract": "We develop an approach to training deep networks while dynamically adjusting their architecture, driven by a principled combination of accuracy and sparsity objectives. Unlike conventional pruning approaches, our method adopts a gradual continuous relaxation of discrete network structure optimization and then samples sparse subnetworks, enabling efficient deep networks to be trained in a growing and pruning manner. Extensive experiments across CIFAR-10, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional models for image classification and semantic segmentation, and recurrent models for language modeling, show that our training scheme yields efficient networks that are smaller and more accurate than those produced by competing pruning methods.",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "referenceCount": 75,
        "citationCount": 34,
        "influentialCitationCount": 6,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work develops an approach to training deep networks while dynamically adjusting their architecture, driven by a principled combination of accuracy and sparsity objectives, that yields efficient networks that are smaller and more accurate than those produced by competing pruning methods."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-07-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2007.15353"
        },
        "citationStyles": {
            "bibtex": "@Article{Yuan2020GrowingED,\n author = {Xin Yuan and Pedro H. P. Savarese and M. Maire},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Growing Efficient Deep Networks by Structured Continuous Sparsification},\n volume = {abs/2007.15353},\n year = {2020}\n}\n"
        }
    },
    "923_relu_(lfw)": {
        "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
        "externalIds": {
            "MAG": "1665214252",
            "DBLP": "conf/icml/NairH10",
            "CorpusId": 15539264
        },
        "corpusId": 15539264,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f",
        "title": "Rectified Linear Units Improve Restricted Boltzmann Machines",
        "abstract": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.",
        "venue": "International Conference on Machine Learning",
        "year": 2010,
        "referenceCount": 21,
        "citationCount": 16014,
        "influentialCitationCount": 1075,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Restricted Boltzmann machines were developed using binary stochastic hidden units that learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2010-06-21",
        "journal": {
            "pages": "807-814"
        },
        "citationStyles": {
            "bibtex": "@Article{Nair2010RectifiedLU,\n author = {Vinod Nair and Geoffrey E. Hinton},\n booktitle = {International Conference on Machine Learning},\n pages = {807-814},\n title = {Rectified Linear Units Improve Restricted Boltzmann Machines},\n year = {2010}\n}\n"
        }
    },
    "925_ntm": {
        "paperId": "c3823aacea60bc1f2cabb9283144690a3d015db5",
        "externalIds": {
            "MAG": "2167839676",
            "DBLP": "journals/corr/GravesWD14",
            "ArXiv": "1410.5401",
            "CorpusId": 15299054
        },
        "corpusId": 15299054,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/c3823aacea60bc1f2cabb9283144690a3d015db5",
        "title": "Neural Turing Machines",
        "abstract": "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.",
        "venue": "arXiv.org",
        "year": 2014,
        "referenceCount": 42,
        "citationCount": 2138,
        "influentialCitationCount": 218,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-10-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1410.5401"
        },
        "citationStyles": {
            "bibtex": "@Article{Graves2014NeuralTM,\n author = {Alex Graves and Greg Wayne and Ivo Danihelka},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Neural Turing Machines},\n volume = {abs/1410.5401},\n year = {2014}\n}\n"
        }
    },
    "926_relational_memory_core": {
        "paperId": "fd4ae71916cf400bfd1490f275e91b154eb69160",
        "externalIds": {
            "MAG": "2951646478",
            "DBLP": "conf/nips/SantoroFRRCWWVP18",
            "ArXiv": "1806.01822",
            "CorpusId": 46940459
        },
        "corpusId": 46940459,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/fd4ae71916cf400bfd1490f275e91b154eb69160",
        "title": "Relational recurrent neural networks",
        "abstract": "Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a \\textit{Relational Memory Core} (RMC) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "referenceCount": 48,
        "citationCount": 198,
        "influentialCitationCount": 28,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new memory module -- a \\textit{Relational Memory Core} (RMC) -- is used which employs multi-head dot product attention to allow memories to interact and achieves state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-06-05",
        "journal": {
            "pages": "7310-7321"
        },
        "citationStyles": {
            "bibtex": "@Article{Santoro2018RelationalRN,\n author = {Adam Santoro and Ryan Faulkner and David Raposo and Jack W. Rae and Mike Chrzanowski and T. Weber and Daan Wierstra and O. Vinyals and Razvan Pascanu and T. Lillicrap},\n booktitle = {Neural Information Processing Systems},\n pages = {7310-7321},\n title = {Relational recurrent neural networks},\n year = {2018}\n}\n"
        }
    },
    "927_ankh_base": {
        "paperId": "1420b9ff9c7ecfc8c8b5fdce4517e6fc51cebf92",
        "externalIds": {
            "DBLP": "journals/corr/abs-2301-06568",
            "ArXiv": "2301.06568",
            "DOI": "10.1101/2023.01.16.524265",
            "CorpusId": 255941875
        },
        "corpusId": 255941875,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/1420b9ff9c7ecfc8c8b5fdce4517e6fc51cebf92",
        "title": "Ankh \u2625: Optimized Protein Language Model Unlocks General-Purpose Modelling",
        "abstract": "As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google\u2019s TPU-v4 surpassing the state-of-the-art performance with fewer parameters (<10% for pre-training, <7% for inference, and <30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.",
        "venue": "bioRxiv",
        "year": 2023,
        "referenceCount": 59,
        "citationCount": 28,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2023/01/18/2023.01.16.524265.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents Ankh, the first general-purpose PLM trained on Google\u2019s TPU-v4 surpassing the state-of-the-art performance with fewer parameters, and provides a representative range of structure and function benchmarks where Ankh excels."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-01-16",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Elnaggar2023AnkhO,\n author = {Ahmed Elnaggar and Hazem Essam and Wafaa Salah-Eldin and Walid Moustafa and Mohamed Elkerdawy and Charlotte Rochereau and B. Rost},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {Ankh \u2625: Optimized Protein Language Model Unlocks General-Purpose Modelling},\n year = {2023}\n}\n"
        }
    },
    "928_vit-g_(model_soup)": {
        "paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84",
        "externalIds": {
            "DBLP": "conf/icml/WortsmanIGRLMNF22",
            "ArXiv": "2203.05482",
            "DOI": "10.48550/arXiv.2203.05482",
            "CorpusId": 247362886
        },
        "corpusId": 247362886,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/54020e5fe48ebb250f27d744e20a63cac2988a84",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
        "abstract": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "referenceCount": 119,
        "citationCount": 403,
        "influentialCitationCount": 58,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.05482",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-03-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2203.05482"
        },
        "citationStyles": {
            "bibtex": "@Article{Wortsman2022ModelSA,\n author = {Mitchell Wortsman and Gabriel Ilharco and S. Gadre and R. Roelofs and Raphael Gontijo-Lopes and Ari S. Morcos and Hongseok Namkoong and Ali Farhadi and Y. Carmon and Simon Kornblith and Ludwig Schmidt},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},\n volume = {abs/2203.05482},\n year = {2022}\n}\n"
        }
    },
    "929_transformer-xl+adamp": {
        "paperId": "e11e8d81c68cc782f564ed78e595b66790719804",
        "externalIds": {
            "MAG": "3092043977",
            "DBLP": "conf/iclr/HeoCOHYKUH21",
            "CorpusId": 222295578
        },
        "corpusId": 222295578,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/e11e8d81c68cc782f564ed78e595b66790719804",
        "title": "AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights",
        "abstract": "Normalization techniques are a boon for modern deep learning. They let weights converge more quickly with often better generalization performances. It has been argued that the normalization-induced scale invariance among the weights provides an advantageous ground for gradient descent (GD) optimizers: the effective step sizes are automatically reduced over time, stabilizing the overall training procedure. It is often overlooked, however, that the additional introduction of momentum in GD optimizers results in a far more rapid reduction in effective step sizes for scale-invariant weights, a phenomenon that has not yet been studied and may have caused unwanted side effects in the current practice. This is a crucial issue because arguably the vast majority of modern deep neural networks consist of (1) momentum-based GD (e.g. SGD or Adam) and (2) scale-invariant parameters. In this paper, we verify that the widely-adopted combination of the two ingredients lead to the premature decay of effective step sizes and sub-optimal model performances. We propose a simple and effective remedy, SGDP and AdamP: get rid of the radial component, or the norm-increasing direction, at each optimizer step. Because of the scale invariance, this modification only alters the effective step sizes without changing the effective update directions, thus enjoying the original convergence properties of GD optimizers. Given the ubiquity of momentum GD and scale invariance in machine learning, we have evaluated our methods against the baselines on 13 benchmarks. They range from vision tasks like classification (e.g. ImageNet), retrieval (e.g. CUB and SOP), and detection (e.g. COCO) to language modelling (e.g. WikiText) and audio classification (e.g. DCASE) tasks. We verify that our solution brings about uniform gains in those benchmarks. Source code is available at this https URL.",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "referenceCount": 77,
        "citationCount": 97,
        "influentialCitationCount": 10,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a simple and effective remedy, SGDP and AdamP: get rid of the radial component, or the norm-increasing direction, at each optimizer step, which alters the effective step sizes without changing the effective update directions, thus enjoying the original convergence properties of GD optimizers."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-06-15",
        "journal": {
            "name": "arXiv: Learning",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Heo2020AdamPSD,\n author = {Byeongho Heo and Sanghyuk Chun and Seong Joon Oh and Dongyoon Han and Sangdoo Yun and Gyuwan Kim and Youngjung Uh and Jung-Woo Ha},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Learning},\n title = {AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights},\n year = {2020}\n}\n"
        }
    },
    "930_trimelmext_(247m)": {
        "paperId": "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1",
        "externalIds": {
            "DBLP": "journals/corr/abs-2205-12674",
            "ArXiv": "2205.12674",
            "ACL": "2022.emnlp-main.382",
            "DOI": "10.48550/arXiv.2205.12674",
            "CorpusId": 249062699
        },
        "corpusId": 249062699,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/da1d6445b6b64ce9eb4587ba8abbdc490f648ec1",
        "title": "Training Language Models with Memory Augmentation",
        "abstract": "Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories\u2014local, long-term, and external memory\u2014at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2022,
        "referenceCount": 61,
        "citationCount": 73,
        "influentialCitationCount": 9,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2205.12674",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents TRIME, a novel yet simple training approach designed for training LMs with memory augmentation that adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-05-25",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2205.12674"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhong2022TrainingLM,\n author = {Zexuan Zhong and Tao Lei and Danqi Chen},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Training Language Models with Memory Augmentation},\n volume = {abs/2205.12674},\n year = {2022}\n}\n"
        }
    },
    "932_rnnsearch-50*": {
        "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
        "externalIds": {
            "MAG": "2133564696",
            "ArXiv": "1409.0473",
            "DBLP": "journals/corr/BahdanauCB14",
            "CorpusId": 11212020
        },
        "corpusId": 11212020,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "referenceCount": 33,
        "citationCount": 24602,
        "influentialCitationCount": 2473,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and it is proposed to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-09-01",
        "journal": {
            "name": "CoRR",
            "volume": "abs/1409.0473"
        },
        "citationStyles": {
            "bibtex": "@Article{Bahdanau2014NeuralMT,\n author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Neural Machine Translation by Jointly Learning to Align and Translate},\n volume = {abs/1409.0473},\n year = {2014}\n}\n"
        }
    },
    "933_image-to-image_cgan": {
        "paperId": "8acbe90d5b852dadea7810345451a99608ee54c7",
        "externalIds": {
            "MAG": "2963073614",
            "DBLP": "conf/cvpr/IsolaZZE17",
            "ArXiv": "1611.07004",
            "DOI": "10.1109/CVPR.2017.632",
            "CorpusId": 6200260
        },
        "corpusId": 6200260,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/8acbe90d5b852dadea7810345451a99608ee54c7",
        "title": "Image-to-Image Translation with Conditional Adversarial Networks",
        "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 71,
        "citationCount": 16542,
        "influentialCitationCount": 2901,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.07004",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Conditional adversarial networks are investigated as a general-purpose solution to image-to-image translation problems and it is demonstrated that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-11-21",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "5967-5976"
        },
        "citationStyles": {
            "bibtex": "@Article{Isola2016ImagetoImageTW,\n author = {Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5967-5976},\n title = {Image-to-Image Translation with Conditional Adversarial Networks},\n year = {2016}\n}\n"
        }
    },
    "935_sequence-based_pattern_recognition": {
        "paperId": "48bb173e9586949d263086926d01d6e66cc177d5",
        "externalIds": {
            "MAG": "1978460546",
            "DBLP": "conf/aieeire/Selfridge55a",
            "DOI": "10.1145/1455292.1455310",
            "CorpusId": 8721044
        },
        "corpusId": 8721044,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/48bb173e9586949d263086926d01d6e66cc177d5",
        "title": "Pattern recognition and modern computers",
        "abstract": "We consider the process we call Pattern Recognition. By this we mean the extraction of the significant features of data from a background of irrelevant detail. What we are interested in is simulating this process on digital computers. We give examples on three levels of complexity corresponding to the subjects of the other three speakers here today. We examine in detail the problem on the second level, visual recognition of simple shapes.",
        "venue": "AFIPS '55 (Western)",
        "year": 1899,
        "referenceCount": 0,
        "citationCount": 160,
        "influentialCitationCount": 3,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work considers the process the authors call Pattern Recognition, the extraction of the significant features of data from a background of irrelevant detail, and examines in detail the problem on the second level, visual recognition of simple shapes."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1899-12-30",
        "journal": {
            "pages": "91-93"
        },
        "citationStyles": {
            "bibtex": "@Article{Selfridge1899PatternRA,\n author = {O. Selfridge},\n booktitle = {AFIPS '55 (Western)},\n pages = {91-93},\n title = {Pattern recognition and modern computers},\n year = {1899}\n}\n"
        }
    },
    "936_polyglot-ko-12.8b": {
        "paperId": "3fae5fb60ba9eeead5f1b2681ac6b09fe9cc4926",
        "externalIds": {
            "ArXiv": "2306.02254",
            "DBLP": "journals/corr/abs-2306-02254",
            "DOI": "10.48550/arXiv.2306.02254",
            "CorpusId": 259075964
        },
        "corpusId": 259075964,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/3fae5fb60ba9eeead5f1b2681ac6b09fe9cc4926",
        "title": "A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models",
        "abstract": "Polyglot is a pioneering project aimed at enhancing the non-English language performance of multilingual language models. Despite the availability of various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often resort to building monolingual models in their respective languages due to the dissatisfaction with the current multilingual models non-English language capabilities. Addressing this gap, we seek to develop advanced multilingual language models that offer improved performance in non-English languages. In this paper, we introduce the Polyglot Korean models, which represent a specific focus rather than being multilingual in nature. In collaboration with TUNiB, our team collected 1.2TB of Korean data meticulously curated for our research journey. We made a deliberate decision to prioritize the development of Korean models before venturing into multilingual models. This choice was motivated by multiple factors: firstly, the Korean models facilitated performance comparisons with existing multilingual models; and finally, they catered to the specific needs of Korean companies and researchers. This paper presents our work in developing the Polyglot Korean models, which propose some steps towards addressing the non-English language performance gap in multilingual language models.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 18,
        "citationCount": 10,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.02254",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The Polyglot Korean models are introduced, which represent a specific focus rather than being multilingual in nature, and propose some steps towards addressing the non-English language performance gap in multilingual language models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-06-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2306.02254"
        },
        "citationStyles": {
            "bibtex": "@Article{Ko2023ATR,\n author = {H. Ko and Kichang Yang and Minho Ryu and Taekyoon Choi and Seungmu Yang and Jiwung Hyun and Sung-Yong Park and Kyubyong Park},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models},\n volume = {abs/2306.02254},\n year = {2023}\n}\n"
        }
    },
    "937_spalm_+_relationlm": {
        "paperId": "4d1e6c441a0b6aa2b9ec500a5064dc307d797b0c",
        "externalIds": {
            "DBLP": "journals/tacl/LiuYB22",
            "ArXiv": "2201.09680",
            "ACL": "2022.tacl-1.32",
            "DOI": "10.1162/tacl_a_00476",
            "CorpusId": 246240585
        },
        "corpusId": 246240585,
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/4d1e6c441a0b6aa2b9ec500a5064dc307d797b0c",
        "title": "Relational Memory-Augmented Language Models",
        "abstract": "We present a memory-augmented approach to condition an autoregressive language model on a knowledge graph. We represent the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation. Experiments on WikiText-103, WMT19, and enwik8 English datasets demonstrate that our approach produces a better language model in terms of perplexity and bits per character. We also show that relational memory improves coherence, is complementary to token-based memory, and enables causal interventions. Our model provides a simple yet effective way to combine an autoregressive language model and a knowledge graph for more coherent and logical generation.",
        "venue": "Transactions of the Association for Computational Linguistics",
        "year": 2022,
        "referenceCount": 99,
        "citationCount": 23,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00476/2020721/tacl_a_00476.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A memory-augmented approach to condition an autoregressive language model on a knowledge graph that represents the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-01-24",
        "journal": {
            "name": "Transactions of the Association for Computational Linguistics",
            "pages": "555-572",
            "volume": "10"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2022RelationalML,\n author = {Qi Liu and Dani Yogatama and Phil Blunsom},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {555-572},\n title = {Relational Memory-Augmented Language Models},\n volume = {10},\n year = {2022}\n}\n"
        }
    },
    "938_semantic_taxonomy_induction": {
        "paperId": "93bb6228776eafa606965e21f229d548de1998eb",
        "externalIds": {
            "ACL": "P06-1101",
            "DBLP": "conf/acl/SnowJN06",
            "MAG": "2144108169",
            "DOI": "10.3115/1220175.1220276",
            "CorpusId": 14680675
        },
        "corpusId": 14680675,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/93bb6228776eafa606965e21f229d548de1998eb",
        "title": "Semantic Taxonomy Induction from Heterogenous Evidence",
        "abstract": "We propose a novel algorithm for inducing semantic taxonomies. Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns. By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa. We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1). We add 10,000 novel synsets to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classifiers. Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2006,
        "referenceCount": 23,
        "citationCount": 541,
        "influentialCitationCount": 53,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1220175.1220276",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a novel algorithm for inducing semantic taxonomies that flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2006-07-17",
        "journal": {
            "name": "",
            "pages": "801-808",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Snow2006SemanticTI,\n author = {R. Snow and Dan Jurafsky and A. Ng},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {801-808},\n title = {Semantic Taxonomy Induction from Heterogenous Evidence},\n year = {2006}\n}\n"
        }
    },
    "939_recursive_sentiment_autoencoder": {
        "paperId": "cfa2646776405d50533055ceb1b7f050e9014dcb",
        "externalIds": {
            "DBLP": "conf/emnlp/SocherPHNM11",
            "MAG": "71795751",
            "ACL": "D11-1014",
            "CorpusId": 3116311
        },
        "corpusId": 3116311,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/cfa2646776405d50533055ceb1b7f050e9014dcb",
        "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions",
        "abstract": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2011,
        "referenceCount": 41,
        "citationCount": 1316,
        "influentialCitationCount": 142,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions that outperform other state-of-the-art approaches on commonly used datasets, without using any pre-defined sentiment lexica or polarity shifting rules."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "publicationDate": "2011-07-27",
        "journal": {
            "pages": "151-161"
        },
        "citationStyles": {
            "bibtex": "@Article{Socher2011SemiSupervisedRA,\n author = {R. Socher and Jeffrey Pennington and E. Huang and A. Ng and Christopher D. Manning},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {151-161},\n title = {Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions},\n year = {2011}\n}\n"
        }
    },
    "940_progen2-xlarge": {
        "paperId": "26133033149afb4b45e5d0a4bd1dc712a236810e",
        "externalIds": {
            "DBLP": "journals/corr/abs-2206-13517",
            "ArXiv": "2206.13517",
            "DOI": "10.48550/arXiv.2206.13517",
            "CorpusId": 250089293,
            "PubMed": "37909046"
        },
        "corpusId": 250089293,
        "publicationVenue": {
            "id": "ebb34bf6-eb0a-4d70-81fc-53360c4c20b3",
            "name": "Cell Systems",
            "type": "journal",
            "alternate_names": [
                "Cell syst",
                "Cell systems",
                "Cell Syst"
            ],
            "issn": "2405-4712",
            "url": "https://www.journals.elsevier.com/cell-systems",
            "alternate_urls": [
                "https://www.sciencedirect.com/journal/cell-systems"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/26133033149afb4b45e5d0a4bd1dc712a236810e",
        "title": "ProGen2: Exploring the Boundaries of Protein Language Models",
        "abstract": "Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial-intelligence-driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional fine-tuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. Our models and code are open sourced for widespread adoption in protein engineering. A record of this paper's Transparent Peer Review process is included in the supplemental information.",
        "venue": "Cell Systems",
        "year": 2022,
        "referenceCount": 73,
        "citationCount": 107,
        "influentialCitationCount": 13,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2206.13517",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases are introduced."
        },
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "publicationDate": "2022-06-27",
        "journal": {
            "name": "Cell systems"
        },
        "citationStyles": {
            "bibtex": "@Article{Nijkamp2022ProGen2ET,\n author = {Erik Nijkamp and Jeffrey A. Ruffolo and Eli N. Weinstein and N. Naik and Ali Madani},\n booktitle = {Cell Systems},\n journal = {Cell systems},\n title = {ProGen2: Exploring the Boundaries of Protein Language Models},\n year = {2022}\n}\n"
        }
    },
    "942_t5-11b": {
        "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "externalIds": {
            "DBLP": "journals/jmlr/RaffelSRLNMZLL20",
            "MAG": "2981852735",
            "ArXiv": "1910.10683",
            "CorpusId": 204838007
        },
        "corpusId": 204838007,
        "publicationVenue": {
            "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
            "name": "Journal of machine learning research",
            "type": "journal",
            "alternate_names": [
                "Journal of Machine Learning Research",
                "J mach learn res",
                "J Mach Learn Res"
            ],
            "issn": "1532-4435",
            "alternate_issns": [
                "1533-7928"
            ],
            "url": "http://www.ai.mit.edu/projects/jmlr/",
            "alternate_urls": [
                "http://jmlr.csail.mit.edu/",
                "http://www.jmlr.org/",
                "http://portal.acm.org/affiliated/jmlr"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/6c4b76232bb72897685d19b3d264c6ee3005bc2b",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
        "venue": "Journal of machine learning research",
        "year": 2019,
        "referenceCount": 134,
        "citationCount": 12390,
        "influentialCitationCount": 1789,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-10-23",
        "journal": {
            "name": "J. Mach. Learn. Res.",
            "pages": "140:1-140:67",
            "volume": "21"
        },
        "citationStyles": {
            "bibtex": "@Article{Raffel2019ExploringTL,\n author = {Colin Raffel and Noam M. Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {140:1-140:67},\n title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n volume = {21},\n year = {2019}\n}\n"
        }
    },
    "944_fasttext": {
        "paperId": "892e53fe5cd39f037cb2a961499f42f3002595dd",
        "externalIds": {
            "MAG": "2468328197",
            "ArXiv": "1607.01759",
            "DBLP": "journals/corr/JoulinGBM16",
            "ACL": "E17-2068",
            "DOI": "10.18653/V1/E17-2068",
            "CorpusId": 1210515
        },
        "corpusId": 1210515,
        "publicationVenue": {
            "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
            "name": "Conference of the European Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Conf Eur Chapter Assoc Comput Linguistics",
                "EACL"
            ],
            "url": "https://www.aclweb.org/anthology/venues/eacl/"
        },
        "url": "https://www.semanticscholar.org/paper/892e53fe5cd39f037cb2a961499f42f3002595dd",
        "title": "Bag of Tricks for Efficient Text Classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.",
        "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
        "year": 2016,
        "referenceCount": 31,
        "citationCount": 4081,
        "influentialCitationCount": 416,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/E17-2068.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A simple and efficient baseline for text classification is explored that shows that the fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-07-06",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1607.01759"
        },
        "citationStyles": {
            "bibtex": "@Article{Joulin2016BagOT,\n author = {Armand Joulin and Edouard Grave and Piotr Bojanowski and Tomas Mikolov},\n booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Bag of Tricks for Efficient Text Classification},\n volume = {abs/1607.01759},\n year = {2016}\n}\n"
        }
    },
    "945_retinanet-r101": {
        "paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20",
        "externalIds": {
            "MAG": "2950100464",
            "DBLP": "journals/corr/abs-1708-02002",
            "DOI": "10.1109/ICCV.2017.324",
            "CorpusId": 47252984
        },
        "corpusId": 47252984,
        "publicationVenue": {
            "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
            "name": "IEEE International Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ICCV",
                "IEEE Int Conf Comput Vis",
                "ICCV Workshops",
                "ICCV Work"
            ],
            "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
        },
        "url": "https://www.semanticscholar.org/paper/79cfb51a51fc093f66aac8e858afe2e14d4a1f20",
        "title": "Focal Loss for Dense Object Detection",
        "abstract": "The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "referenceCount": 42,
        "citationCount": 18288,
        "influentialCitationCount": 2622,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1708.02002",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes to address the extreme foreground-background class imbalance encountered during training of dense detectors by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples, and develops a novel Focal Loss, which focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-08-07",
        "journal": {
            "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
            "pages": "2999-3007"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2017FocalLF,\n author = {Tsung-Yi Lin and Priya Goyal and Ross B. Girshick and Kaiming He and Piotr Doll\u00e1r},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {2999-3007},\n title = {Focal Loss for Dense Object Detection},\n year = {2017}\n}\n"
        }
    },
    "946_l_ul-seq": {
        "paperId": "53a77e8f73f2ca422d6e38fa9ecc490231ac044c",
        "externalIds": {
            "MAG": "2968297680",
            "DBLP": "conf/iclr/WelleckKRDCW20",
            "ArXiv": "1908.04319",
            "CorpusId": 199551982
        },
        "corpusId": 199551982,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/53a77e8f73f2ca422d6e38fa9ecc490231ac044c",
        "title": "Neural Text Generation with Unlikelihood Training",
        "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-$k$ and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 37,
        "citationCount": 442,
        "influentialCitationCount": 71,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution, thus providing a strong alternative to existing techniques."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-08-12",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1908.04319"
        },
        "citationStyles": {
            "bibtex": "@Article{Welleck2019NeuralTG,\n author = {S. Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and J. Weston},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Neural Text Generation with Unlikelihood Training},\n volume = {abs/1908.04319},\n year = {2019}\n}\n"
        }
    },
    "948_doc_+_finetune\u2217_+_partial_shuffle_(wt2)": {
        "paperId": "121e30c48546e671dc5e16c694c5e69b392cf8fb",
        "externalIds": {
            "ArXiv": "1903.04167",
            "DBLP": "journals/corr/abs-1903-04167",
            "MAG": "2922249085",
            "CorpusId": 73728677
        },
        "corpusId": 73728677,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/121e30c48546e671dc5e16c694c5e69b392cf8fb",
        "title": "Partially Shuffling the Training Data to Improve Language Models",
        "abstract": "Although SGD requires shuffling the training data between epochs, currently none of the word-level language modeling systems do this. Naively shuffling all sentences in the training data would not permit the model to learn inter-sentence dependencies. Here we present a method that partially shuffles the training data between epochs. This method makes each batch random, while keeping most sentence ordering intact. It achieves new state of the art results on word-level language modeling on both the Penn Treebank and WikiText-2 datasets.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 5,
        "citationCount": 5,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a method that partially shuffles the training data between epochs, which makes each batch random, while keeping most sentence ordering intact, and achieves new state of the art results on word-level language modeling on both the Penn Treebank and WikiText-2 datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-03-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1903.04167"
        },
        "citationStyles": {
            "bibtex": "@Article{Press2019PartiallyST,\n author = {Ofir Press},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Partially Shuffling the Training Data to Improve Language Models},\n volume = {abs/1903.04167},\n year = {2019}\n}\n"
        }
    },
    "949_transformer+recurrent_windows_of_context": {
        "paperId": "54bc3e055d05e44c010febc669e8dea394643efc",
        "externalIds": {
            "MAG": "3049463507",
            "ArXiv": "2008.07027",
            "DBLP": "journals/corr/abs-2008-07027",
            "CorpusId": 221140187
        },
        "corpusId": 221140187,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/54bc3e055d05e44c010febc669e8dea394643efc",
        "title": "Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size",
        "abstract": "Fine-tuning a pretrained transformer for a downstream task has become a standard method in NLP in the last few years. While the results from these models are impressive, applying them can be extremely computationally expensive, as is pretraining new models with the latest architectures. We present a novel method for applying pretrained transformer language models which lowers their memory requirement both at training and inference time. An additional benefit is that our method removes the fixed context size constraint that most transformer models have, allowing for more flexible use. When applied to the GPT-2 language model, we find that our method attains better perplexity than an unmodified GPT-2 model on the PG-19 and WikiText-103 corpora, for a given amount of computation or memory.",
        "venue": "arXiv.org",
        "year": 2020,
        "referenceCount": 28,
        "citationCount": 5,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel method for applying pretrained transformer language models which lowers their memory requirement both at training and inference time, and removes the fixed context size constraint that most transformer models have, allowing for more flexible use."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-08-16",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2008.07027"
        },
        "citationStyles": {
            "bibtex": "@Article{Yoshida2020AddingRT,\n author = {Davis Yoshida and Allyson Ettinger and Kevin Gimpel},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size},\n volume = {abs/2008.07027},\n year = {2020}\n}\n"
        }
    },
    "951_retrieval-quality-knn-lms": {
        "paperId": "e832d22ca901346f50e8367afb99bd2bf6e29421",
        "externalIds": {
            "DBLP": "journals/corr/abs-2210-15859",
            "ArXiv": "2210.15859",
            "DOI": "10.48550/arXiv.2210.15859",
            "CorpusId": 253258450
        },
        "corpusId": 253258450,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/e832d22ca901346f50e8367afb99bd2bf6e29421",
        "title": "You can't pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM",
        "abstract": "Retrieval-enhanced language models (LMs), which condition their predictions on text retrieved from large external datastores, have recently shown significant perplexity improvements compared to standard LMs. One such approach, the $k$NN-LM, interpolates any existing LM's predictions with the output of a $k$-nearest neighbors model and requires no additional training. In this paper, we explore the importance of lexical and semantic matching in the context of items retrieved by $k$NN-LM. We find two trends: (1) the presence of large overlapping $n$-grams between the datastore and evaluation set plays an important factor in strong performance, even when the datastore is derived from the training data; and (2) the $k$NN-LM is most beneficial when retrieved items have high semantic similarity with the query. Based on our analysis, we define a new formulation of the $k$NN-LM that uses retrieval quality to assign the interpolation coefficient. We empirically measure the effectiveness of our approach on two English language modeling datasets, Wikitext-103 and PG-19. Our re-formulation of the $k$NN-LM is beneficial in both cases, and leads to nearly 4% improvement in perplexity on the Wikitext-103 test set.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2022,
        "referenceCount": 44,
        "citationCount": 8,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2210.15859",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new formulation of the $k$NN-LM is defined that uses retrieval quality to assign the interpolation coefficient and is beneficial in both cases, and leads to nearly 4% improvement in perplexity on the Wikitext-103 test set."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-10-28",
        "journal": {
            "pages": "2997-3007"
        },
        "citationStyles": {
            "bibtex": "@Article{Drozdov2022YouCP,\n author = {Andrew Drozdov and Shufan Wang and Razieh Rahimi and A. McCallum and Hamed Zamani and Mohit Iyyer},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {2997-3007},\n title = {You can't pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM},\n year = {2022}\n}\n"
        }
    },
    "952_refinenet": {
        "paperId": "de4ee92cfad3734ca820d004bc9ee75fc9dcfbf4",
        "externalIds": {
            "DBLP": "conf/cvpr/LinMSR17",
            "MAG": "2951402970",
            "ArXiv": "1611.06612",
            "DOI": "10.1109/CVPR.2017.549",
            "CorpusId": 5696978
        },
        "corpusId": 5696978,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/de4ee92cfad3734ca820d004bc9ee75fc9dcfbf4",
        "title": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation",
        "abstract": "Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 48,
        "citationCount": 2542,
        "influentialCitationCount": 233,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.06612",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "RefineNet is presented, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections and introduces chained residual pooling, which captures rich background context in an efficient manner."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-11-20",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "5168-5177"
        },
        "citationStyles": {
            "bibtex": "@Article{Lin2016RefineNetMR,\n author = {Guosheng Lin and Anton Milan and Chunhua Shen and I. Reid},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5168-5177},\n title = {RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation},\n year = {2016}\n}\n"
        }
    },
    "953_bagualu": {
        "paperId": "408efcc963ab871558e5e145b54ad7ae2aeb11c7",
        "externalIds": {
            "DBLP": "conf/ppopp/MaHQCWSZWTZLFHG22",
            "DOI": "10.1145/3503221.3508417",
            "CorpusId": 247289671
        },
        "corpusId": 247289671,
        "publicationVenue": {
            "id": "11e94c04-6b12-41c9-963d-1a08cdbc306d",
            "name": "ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming",
            "type": "conference",
            "alternate_names": [
                "PPoPP",
                "ACM SIGPLAN Symp Princ Pract Parallel Program",
                "ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming",
                "ACM SIGPLAN Symp Princ  Pract Parallel Program"
            ],
            "url": "http://www.acm.org/sigplan/ppopp.htm"
        },
        "url": "https://www.semanticscholar.org/paper/408efcc963ab871558e5e145b54ad7ae2aeb11c7",
        "title": "BaGuaLu: targeting brain scale pretrained models with over 37 million cores",
        "abstract": "Large-scale pretrained AI models have shown state-of-the-art accuracy in a series of important applications. As the size of pretrained AI models grows dramatically each year in an effort to achieve higher accuracy, training such models requires massive computing and memory capabilities, which accelerates the convergence of AI and HPC. However, there are still gaps in deploying AI applications on HPC systems, which need application and system co-design based on specific hardware features. To this end, this paper proposes BaGuaLu1, the first work targeting training brain scale models on an entire exascale supercomputer, the New Generation Sunway Supercomputer. By combining hardware-specific intra-node optimization and hybrid parallel strategies, BaGuaLu enables decent performance and scalability on unprecedentedly large models. The evaluation shows that BaGuaLu can train 14.5-trillion-parameter models with a performance of over 1 EFLOPS using mixed-precision and has the capability to train 174-trillion-parameter models, which rivals the number of synapses in a human brain.",
        "venue": "ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming",
        "year": 2022,
        "referenceCount": 35,
        "citationCount": 26,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.1145/3503221.3508417",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "BaGuaLu is proposed, the first work targeting training brain scale models on an entire exascale supercomputer, the New Generation Sunway Supercomputer, by combining hardware-specific intra-node optimization and hybrid parallel strategies, which enables decent performance and scalability on unprecedentedly large models."
        },
        "publicationTypes": [
            "JournalArticle",
            "Book",
            "Conference"
        ],
        "publicationDate": "2022-03-28",
        "journal": {
            "name": "Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming"
        },
        "citationStyles": {
            "bibtex": "@Article{Ma2022BaGuaLuTB,\n author = {Zixuan Ma and Jiaao He and J. Qiu and Huan Cao and Yuanwei Wang and Zhenbo Sun and Liyan Zheng and Haojie Wang and Shizhi Tang and Tianyu Zheng and Junyang Lin and Guanyu Feng and Zeqiang Huang and Jie Gao and Aohan Zeng and Jianwei Zhang and Runxin Zhong and Tianhui Shi and Shangkun Liu and Weimin Zheng and Jie Tang and Hongxia Yang and Xin Liu and Jidong Zhai and Wenguang Chen},\n booktitle = {ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming},\n journal = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},\n title = {BaGuaLu: targeting brain scale pretrained models with over 37 million cores},\n year = {2022}\n}\n"
        }
    },
    "954_mono3d++": {
        "paperId": "4dbedeff6c076a9e4636de4e81e22ada514eb172",
        "externalIds": {
            "DBLP": "conf/aaai/HeS19",
            "ArXiv": "1901.03446",
            "MAG": "2951663167",
            "DOI": "10.1609/aaai.v33i01.33018409",
            "CorpusId": 57825785
        },
        "corpusId": 57825785,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/4dbedeff6c076a9e4636de4e81e22ada514eb172",
        "title": "Mono3D++: Monocular 3D Vehicle Detection with Two-Scale 3D Hypotheses and Task Priors",
        "abstract": "We present a method to infer 3D pose and shape of vehicles from a single image. To tackle this ill-posed problem, we optimize two-scale projection consistency between the generated 3D hypotheses and their 2D pseudo-measurements. Specifically, we use a morphable wireframe model to generate a fine-scaled representation of vehicle shape and pose. To reduce its sensitivity to 2D landmarks, we jointly model the 3D bounding box as a coarse representation which improves robustness. We also integrate three task priors, including unsupervised monocular depth, a ground plane constraint as well as vehicle shape priors, with forward projection errors into an overall energy function.",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "referenceCount": 38,
        "citationCount": 108,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4856/4729",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The method to infer 3D pose and shape of vehicles from a single image is presented, using a morphable wireframe model to generate a fine-scaled representation of vehicle shape and pose and jointly model the 3D bounding box as a coarse representation which improves robustness."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-01-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1901.03446"
        },
        "citationStyles": {
            "bibtex": "@Article{He2019Mono3DM3,\n author = {Tong He and Stefano Soatto},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Mono3D++: Monocular 3D Vehicle Detection with Two-Scale 3D Hypotheses and Task Priors},\n volume = {abs/1901.03446},\n year = {2019}\n}\n"
        }
    },
    "956_llama-7b": {
        "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "externalIds": {
            "ArXiv": "2302.13971",
            "DBLP": "journals/corr/abs-2302-13971",
            "DOI": "10.48550/arXiv.2302.13971",
            "CorpusId": 257219404
        },
        "corpusId": 257219404,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models",
        "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 80,
        "citationCount": 4186,
        "influentialCitationCount": 671,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.13971",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "LLaMA, a collection of foundation language models ranging from 7B to 65B parameters, is introduced and it is shown that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-02-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2302.13971"
        },
        "citationStyles": {
            "bibtex": "@Article{Touvron2023LLaMAOA,\n author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth\u00e9e Lacroix and Baptiste Rozi\u00e8re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {LLaMA: Open and Efficient Foundation Language Models},\n volume = {abs/2302.13971},\n year = {2023}\n}\n"
        }
    },
    "957_deconvolutional_network": {
        "paperId": "83dfe3980b875c4e5fe6f2cb1df131cc46d175c8",
        "externalIds": {
            "MAG": "2293078015",
            "DBLP": "conf/cvpr/ZeilerKTF10",
            "DOI": "10.1109/CVPR.2010.5539957",
            "CorpusId": 9893011
        },
        "corpusId": 9893011,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/83dfe3980b875c4e5fe6f2cb1df131cc46d175c8",
        "title": "Deconvolutional networks",
        "abstract": "Building robust low and mid-level image representations, beyond edge primitives, is a long-standing goal in vision. Many existing feature detectors spatially pool edge information which destroys cues such as edge intersections, parallelism and symmetry. We present a learning framework where features that capture these mid-level cues spontaneously emerge from image data. Our approach is based on the convolutional decomposition of images under a spar-sity constraint and is totally unsupervised. By building a hierarchy of such decompositions we can learn rich feature sets that are a robust image representation for both the analysis and synthesis of images.",
        "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
        "year": 2010,
        "referenceCount": 33,
        "citationCount": 1476,
        "influentialCitationCount": 73,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents a learning framework where features that capture these mid-level cues spontaneously emerge from image data, based on the convolutional decomposition of images under a spar-sity constraint and is totally unsupervised."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2010-06-01",
        "journal": {
            "name": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "pages": "2528-2535"
        },
        "citationStyles": {
            "bibtex": "@Article{Zeiler2010DeconvolutionalN,\n author = {Matthew D. Zeiler and Dilip Krishnan and Graham W. Taylor and R. Fergus},\n booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n pages = {2528-2535},\n title = {Deconvolutional networks},\n year = {2010}\n}\n"
        }
    },
    "959_smooct": {
        "paperId": "7b687599b4425aa959036071030e1212a3b359c7",
        "externalIds": {
            "MAG": "2778799024",
            "CorpusId": 1465907
        },
        "corpusId": 1465907,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/7b687599b4425aa959036071030e1212a3b359c7",
        "title": "Self-play Monte-Carlo tree search in computer poker",
        "abstract": "\u00a9 Copyright 2014. Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Self-play reinforcement learning has proved to be succ essful in many perfect information two-player games. However, research carrying over its theoretical guaranI ces and practical success to games of imperfect inform ation has been lacking. In this paper, we evaluate self- play Monte-Carlo Tree Search (MCTS) in limit Texas Hold'em and Kuhn poker. We introduce a variant of the established UCB algorithm and provide first empinc al results demonstrating its ability to find approximate Nash equilibria.",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2014,
        "referenceCount": 26,
        "citationCount": 15,
        "influentialCitationCount": 2,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper introduces a variant of the established UCB algorithm and provides first results demonstrating its ability to find approximate Nash equilibria in self- play Monte-Carlo Tree Search in limit Texas Hold'em and Kuhn poker."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Heinrich2014SelfplayMT,\n author = {Johannes Heinrich and David Silver},\n booktitle = {AAAI Conference on Artificial Intelligence},\n title = {Self-play Monte-Carlo tree search in computer poker},\n year = {2014}\n}\n"
        }
    },
    "960_mitosis": {
        "paperId": "6196fa503ecd1b0798e5fd3a48ea519fc3ff5765",
        "externalIds": {
            "MAG": "22040386",
            "DBLP": "conf/miccai/CiresanGGS13",
            "DOI": "10.1007/978-3-642-40763-5_51",
            "CorpusId": 15222519,
            "PubMed": "24579167"
        },
        "corpusId": 15222519,
        "publicationVenue": {
            "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
            "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "type": "conference",
            "alternate_names": [
                "Medical Image Computing and Computer-Assisted Intervention",
                "MICCAI",
                "Med Image Comput Comput Interv",
                "Int Conf Med Image Comput Comput Interv"
            ],
            "url": "http://www.miccai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/6196fa503ecd1b0798e5fd3a48ea519fc3ff5765",
        "title": "Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks",
        "abstract": null,
        "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
        "year": 2013,
        "referenceCount": 21,
        "citationCount": 1453,
        "influentialCitationCount": 68,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-40763-5_51.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work uses deep max-pooling convolutional neural networks to detect mitosis in breast histology images using as context a patch centered on the pixel to classify each pixel in the images."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2013-09-22",
        "journal": {
            "name": "Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "pages": "\n          411-8\n        ",
            "volume": "16 Pt 2"
        },
        "citationStyles": {
            "bibtex": "@Article{Ciresan2013MitosisDI,\n author = {D. Ciresan and A. Giusti and L. Gambardella and J. Schmidhuber},\n booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},\n journal = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},\n pages = {\n          411-8\n        },\n title = {Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks},\n volume = {16 Pt 2},\n year = {2013}\n}\n"
        }
    },
    "961_enas": {
        "paperId": "fe9b8aac9fa3bfd9724db5a881a578e471e612d7",
        "externalIds": {
            "ArXiv": "1802.03268",
            "DBLP": "conf/icml/PhamGZLD18",
            "MAG": "2785366763",
            "CorpusId": 3638969
        },
        "corpusId": 3638969,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/fe9b8aac9fa3bfd9724db5a881a578e471e612d7",
        "title": "Efficient Neural Architecture Search via Parameter Sharing",
        "abstract": "We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "referenceCount": 55,
        "citationCount": 2421,
        "influentialCitationCount": 472,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Efficient Neural Architecture Search is a fast and inexpensive approach for automatic model design that establishes a new state-of-the-art among all methods without post-training processing and delivers strong empirical performances using much fewer GPU-hours."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-02-09",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1802.03268"
        },
        "citationStyles": {
            "bibtex": "@Article{Pham2018EfficientNA,\n author = {Hieu Pham and M. Guan and Barret Zoph and Quoc V. Le and J. Dean},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Efficient Neural Architecture Search via Parameter Sharing},\n volume = {abs/1802.03268},\n year = {2018}\n}\n"
        }
    },
    "962_glm-130b": {
        "paperId": "1d26c947406173145a4665dd7ab255e03494ea28",
        "externalIds": {
            "DBLP": "journals/corr/abs-2210-02414",
            "ArXiv": "2210.02414",
            "DOI": "10.48550/arXiv.2210.02414",
            "CorpusId": 252715691
        },
        "corpusId": 252715691,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/1d26c947406173145a4665dd7ab255e03494ea28",
        "title": "GLM-130B: An Open Bilingual Pre-trained Model",
        "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 154,
        "citationCount": 561,
        "influentialCitationCount": 84,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.02414",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained is unveiled and a unique scaling property of GLM-130B is leveraged to reach INT4 quantization without post training, with almost no performance loss."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.02414"
        },
        "citationStyles": {
            "bibtex": "@Article{Zeng2022GLM130BAO,\n author = {Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and W. Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and P. Zhang and Yuxiao Dong and Jie Tang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {GLM-130B: An Open Bilingual Pre-trained Model},\n volume = {abs/2210.02414},\n year = {2022}\n}\n"
        }
    },
    "963_lstm_+_dynamic_eval": {
        "paperId": "fa9decd1395cc2f39e9921f870ebc8a8ec2bd08d",
        "externalIds": {
            "ArXiv": "1709.07432",
            "MAG": "2757047188",
            "DBLP": "journals/corr/abs-1709-07432",
            "CorpusId": 215827131
        },
        "corpusId": 215827131,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/fa9decd1395cc2f39e9921f870ebc8a8ec2bd08d",
        "title": "Dynamic Evaluation of Neural Sequence Models",
        "abstract": "We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "referenceCount": 53,
        "citationCount": 123,
        "influentialCitationCount": 19,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively and character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08bits/char respectively."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-09-21",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1709.07432"
        },
        "citationStyles": {
            "bibtex": "@Article{Krause2017DynamicEO,\n author = {Ben Krause and Emmanuel Kahembwe and Iain Murray and S. Renals},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Dynamic Evaluation of Neural Sequence Models},\n volume = {abs/1709.07432},\n year = {2017}\n}\n"
        }
    },
    "964_glm-10b": {
        "paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "externalIds": {
            "DBLP": "conf/acl/DuQLDQY022",
            "ArXiv": "2103.10360",
            "ACL": "2022.acl-long.26",
            "DOI": "10.18653/v1/2022.acl-long.26",
            "CorpusId": 247519241
        },
        "corpusId": 247519241,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
        "abstract": "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25\u00d7 parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 64,
        "citationCount": 552,
        "influentialCitationCount": 78,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.acl-long.26.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks, and achieves the best performance from a single pretrained model with 1.25\u00d7 parameters of BERT Large."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-03-18",
        "journal": {
            "pages": "320-335"
        },
        "citationStyles": {
            "bibtex": "@Article{Du2021GLMGL,\n author = {Zhengxiao Du and Yujie Qian and Xiao Liu and Ming Ding and J. Qiu and Zhilin Yang and Jie Tang},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {320-335},\n title = {GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n year = {2021}\n}\n"
        }
    },
    "965_afp+fpi_(ptb)": {
        "paperId": "2471f351ddfc59ee3b1870012b42abd9c5ddb476",
        "externalIds": {
            "DBLP": "journals/corr/abs-2106-02417",
            "ArXiv": "2106.02417",
            "CorpusId": 235352942
        },
        "corpusId": 235352942,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/2471f351ddfc59ee3b1870012b42abd9c5ddb476",
        "title": "Approximate Fixed-Points in Recurrent Neural Networks",
        "abstract": "Recurrent neural networks are widely used in speech and language processing. Due to dependency on the past, standard algorithms for training these models, such as back-propagation through time (BPTT), cannot be efficiently parallelised. Furthermore, applying these models to more complex structures than sequences requires inference time approximations, which introduce inconsistency between inference and training. This paper shows that recurrent neural networks can be reformulated as fixed-points of non-linear equation systems. These fixed-points can be computed using an iterative algorithm exactly and in as many iterations as the length of any given sequence. Each iteration of this algorithm adds one additional Markovian-like order of dependencies such that upon termination all dependencies modelled by the recurrent neural networks have been incorporated. Although exact fixed-points inherit the same parallelization and inconsistency issues, this paper shows that approximate fixed-points can be computed in parallel and used consistently in training and inference including tasks such as lattice rescoring. Experimental validation is performed in two tasks, Penn Tree Bank and WikiText-2, and shows that approximate fixed-points yield competitive prediction performance to recurrent neural networks trained using the BPTT algorithm.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 36,
        "citationCount": 2,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that approximate fixed-points can be computed in parallel and used consistently in training and inference including tasks such as lattice rescoring."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-06-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2106.02417"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2021ApproximateFI,\n author = {Zhengxiong Wang and A. Ragni},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Approximate Fixed-Points in Recurrent Neural Networks},\n volume = {abs/2106.02417},\n year = {2021}\n}\n"
        }
    },
    "967_mt0-13b": {
        "paperId": "4972b88f8f324a4fa18e921f62a9857af2b5fc7b",
        "externalIds": {
            "ACL": "2023.acl-long.891",
            "DBLP": "conf/acl/MuennighoffWSRB23",
            "DOI": "10.18653/v1/2023.acl-long.891",
            "CorpusId": 253264914
        },
        "corpusId": 253264914,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/4972b88f8f324a4fa18e921f62a9857af2b5fc7b",
        "title": "Crosslingual Generalization through Multitask Finetuning",
        "abstract": "Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/ bigscience-workshop/xmtf.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2023,
        "referenceCount": 0,
        "citationCount": 240,
        "influentialCitationCount": 25,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.acl-long.891.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Surprisingly, models are capable of zero-shot generalization to tasks in languages they have never intentionally seen and it is conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "15991-16111"
        },
        "citationStyles": {
            "bibtex": "@Article{Muennighoff2023CrosslingualGT,\n author = {Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng-Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir R. Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {15991-16111},\n title = {Crosslingual Generalization through Multitask Finetuning},\n year = {2023}\n}\n"
        }
    },
    "968_word-independent-srnn+kn5": {
        "paperId": "45e0a172e8986226c847a5ed0843c9c080eb442b",
        "externalIds": {
            "DBLP": "journals/corr/OualilGSK17",
            "MAG": "2963315049",
            "ArXiv": "1703.08068",
            "DOI": "10.21437/Interspeech.2016-422",
            "CorpusId": 5762940
        },
        "corpusId": 5762940,
        "publicationVenue": {
            "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
            "name": "Interspeech",
            "type": "conference",
            "alternate_names": [
                "Conf Int Speech Commun Assoc",
                "INTERSPEECH",
                "Conference of the International Speech Communication Association"
            ],
            "issn": "2308-457X",
            "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
            "alternate_urls": [
                "http://www.isca-speech.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/45e0a172e8986226c847a5ed0843c9c080eb442b",
        "title": "Sequential Recurrent Neural Networks for Language Modeling",
        "abstract": "Feedforward Neural Network (FNN)-based language models estimate the probability of the next word based on the history of the last N words, whereas Recurrent Neural Networks (RNN) perform the same task based only on the last word and some context information that cycles in the network. This paper presents a novel approach, which bridges the gap between these two categories of networks. In particular, we propose an architecture which takes advantage of the explicit, sequential enumeration of the word history in FNN structure while enhancing each word representation at the projection layer through recurrent context information that evolves in the network. The context integration is performed using an additional word-dependent weight matrix that is also learned during the training. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.",
        "venue": "Interspeech",
        "year": 2016,
        "referenceCount": 20,
        "citationCount": 5,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1703.08068",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An architecture is proposed which takes advantage of the explicit, sequential enumeration of the word history in FNN structure while enhancing each word representation at the projection layer through recurrent context information that evolves in the network."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-09-08",
        "journal": {
            "pages": "3509-3513"
        },
        "citationStyles": {
            "bibtex": "@Article{Oualil2016SequentialRN,\n author = {Youssef Oualil and Clayton Greenberg and Mittul Singh and D. Klakow},\n booktitle = {Interspeech},\n pages = {3509-3513},\n title = {Sequential Recurrent Neural Networks for Language Modeling},\n year = {2016}\n}\n"
        }
    },
    "969_ar-ldm": {
        "paperId": "33ef78737ba57ecc1ff98c22369a8e17ed90eb98",
        "externalIds": {
            "ArXiv": "2211.10950",
            "DBLP": "journals/corr/abs-2211-10950",
            "DOI": "10.48550/arXiv.2211.10950",
            "CorpusId": 253734226
        },
        "corpusId": 253734226,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/33ef78737ba57ecc1ff98c22369a8e17ed90eb98",
        "title": "Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models",
        "abstract": "Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 46,
        "citationCount": 20,
        "influentialCitationCount": 5,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2211.10950",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images that achieves SoTA FID scores on PororoSV, FlintstonesV, and the newly introduced challenging dataset VIST containing natural images."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-11-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2211.10950"
        },
        "citationStyles": {
            "bibtex": "@Article{Pan2022SynthesizingCS,\n author = {Xichen Pan and Pengda Qin and Yuhong Li and Hui Xue and Wenhu Chen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models},\n volume = {abs/2211.10950},\n year = {2022}\n}\n"
        }
    },
    "970_zoneout_+_variational_lstm_(wt2)": {
        "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
        "externalIds": {
            "ArXiv": "1609.07843",
            "MAG": "2525332836",
            "DBLP": "journals/corr/MerityXBS16",
            "CorpusId": 16299141
        },
        "corpusId": 16299141,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/efbd381493bb9636f489b965a2034d529cd56bcd",
        "title": "Pointer Sentinel Mixture Models",
        "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "referenceCount": 29,
        "citationCount": 1785,
        "influentialCitationCount": 333,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank while using far fewer parameters than a standard softmax LSTM and the freely available WikiText corpus is introduced."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-09-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1609.07843"
        },
        "citationStyles": {
            "bibtex": "@Article{Merity2016PointerSM,\n author = {Stephen Merity and Caiming Xiong and James Bradbury and R. Socher},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Pointer Sentinel Mixture Models},\n volume = {abs/1609.07843},\n year = {2016}\n}\n"
        }
    },
    "973_fingpt-13b": {
        "paperId": "bd09391fbd124dc0c0a6be5d0ab2eb5d9c43fbac",
        "externalIds": {
            "DBLP": "journals/corr/abs-2310-04793",
            "ArXiv": "2310.04793",
            "DOI": "10.48550/arXiv.2310.04793",
            "CorpusId": 263829590
        },
        "corpusId": 263829590,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/bd09391fbd124dc0c0a6be5d0ab2eb5d9c43fbac",
        "title": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets",
        "abstract": "In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs).",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 39,
        "citationCount": 4,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.04793",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Economics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Economics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Business",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts, is introduced, ensuring a seamless and transparent integration of open- source models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-10-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2310.04793"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2023FinGPTIT,\n author = {Neng Wang and Hongyang Yang and Chris Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets},\n volume = {abs/2310.04793},\n year = {2023}\n}\n"
        }
    },
    "975_dense-indrnn+dynamic_eval": {
        "paperId": "1781dd49c2e935dc441634d0df322a6174c6ec17",
        "externalIds": {
            "ArXiv": "1910.06251",
            "DBLP": "journals/corr/abs-1910-06251",
            "MAG": "2979434318",
            "CorpusId": 204512317
        },
        "corpusId": 204512317,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/1781dd49c2e935dc441634d0df322a6174c6ec17",
        "title": "Deep Independently Recurrent Neural Network (IndRNN)",
        "abstract": "Recurrent neural networks (RNNs) are known to be difficult to train due to the gradient vanishing and exploding problems and thus difficult to learn long-term patterns. Long short-term memory (LSTM) was developed to address these problems, but the use of hyperbolic tangent and the sigmoid activation functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep RNN is challenging. Moreover, training of LSTM is very compute-intensive as the recurrent connection using matrix product is conducted at every time step. To address these problems, this paper proposes a new type of RNNs with the recurrent connection formulated as Hadamard product, referred to as independently recurrent neural network (IndRNN), where neurons in the same layer are independent of each other and connected across layers. The gradient vanishing and exploding problems are solved in IndRNN by simply regulating the recurrent weights, and thus long-term dependencies can be learned. Moreover, an IndRNN can work with non-saturated activation functions such as ReLU and be still trained robustly. Different deeper IndRNN architectures, including the basic stacked IndRNN, residual IndRNN and densely connected IndRNN, have been investigated, all of which can be much deeper than the existing RNNs. Furthermore, IndRNN reduces the computation at each time step and can be over 10 times faster than the LSTM. The code is made publicly available at this https URL. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (the 21 layers residual IndRNN and deep densely connected IndRNN used in the experiment for example). Better performances have been achieved on various tasks with IndRNNs compared with the traditional RNN and LSTM.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 75,
        "citationCount": 41,
        "influentialCitationCount": 6,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experimental results have shown that the proposed IndRNN is able to process very long sequences, can be used to construct very deep networks and reduces the computation at each time step and can be over 10 times faster than the LSTM."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-10-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1910.06251"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2019DeepIR,\n author = {Shuai Li and Wanqing Li and Chris Cook and Yanbo Gao and Ce Zhu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Independently Recurrent Neural Network (IndRNN)},\n volume = {abs/1910.06251},\n year = {2019}\n}\n"
        }
    },
    "976_culturome": {
        "paperId": "71171f8d34d0eec5630a16fff239c978fe53c383",
        "externalIds": {
            "MAG": "2019096529",
            "DOI": "10.1126/science.1199644",
            "CorpusId": 40104730,
            "PubMed": "21163965"
        },
        "corpusId": 40104730,
        "publicationVenue": {
            "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
            "name": "Science",
            "type": "journal",
            "issn": "0193-4511",
            "alternate_issns": [
                "0036-8075"
            ],
            "url": "https://www.jstor.org/journal/science",
            "alternate_urls": [
                "https://www.sciencemag.org/",
                "http://www.sciencemag.org/",
                "http://www.jstor.org/journals/00368075.html",
                "http://www.sciencemag.org/archive/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/71171f8d34d0eec5630a16fff239c978fe53c383",
        "title": "Quantitative Analysis of Culture Using Millions of Digitized Books",
        "abstract": "Linguistic and cultural changes are revealed through the analyses of words appearing in books. We constructed a corpus of digitized texts containing about 4% of all books ever printed. Analysis of this corpus enables us to investigate cultural trends quantitatively. We survey the vast terrain of \u2018culturomics,\u2019 focusing on linguistic and cultural phenomena that were reflected in the English language between 1800 and 2000. We show how this approach can provide insights about fields as diverse as lexicography, the evolution of grammar, collective memory, the adoption of technology, the pursuit of fame, censorship, and historical epidemiology. Culturomics extends the boundaries of rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities.",
        "venue": "Science",
        "year": 2010,
        "referenceCount": 61,
        "citationCount": 2575,
        "influentialCitationCount": 149,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dash.harvard.edu/bitstream/1/8899722/1/MichelScience2011.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Medicine",
            "History",
            "Art"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "History",
                "source": "external"
            },
            {
                "category": "Art",
                "source": "external"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "History",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work surveys the vast terrain of \u2018culturomics,\u2019 focusing on linguistic and cultural phenomena that were reflected in the English language between 1800 and 2000, and shows how this approach can provide insights about fields as diverse as lexicography, the evolution of grammar, collective memory, the adoption of technology and the pursuit of fame."
        },
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "publicationDate": "2010-12-17",
        "journal": {
            "name": "Science",
            "pages": "176 - 182",
            "volume": "331"
        },
        "citationStyles": {
            "bibtex": "@Article{Aiden2010QuantitativeAO,\n author = {Erez Lieberman Aiden and Jean-Baptiste Michel},\n booktitle = {Science},\n journal = {Science},\n pages = {176 - 182},\n title = {Quantitative Analysis of Culture Using Millions of Digitized Books},\n volume = {331},\n year = {2010}\n}\n"
        }
    },
    "977_retrieval-augmented_generator": {
        "paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
        "externalIds": {
            "ArXiv": "2005.11401",
            "MAG": "3027879771",
            "DBLP": "conf/nips/LewisPPPKGKLYR020",
            "CorpusId": 218869575
        },
        "corpusId": 218869575,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
        "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "referenceCount": 66,
        "citationCount": 1515,
        "influentialCitationCount": 212,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation, and finds that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-05-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2005.11401"
        },
        "citationStyles": {
            "bibtex": "@Article{Lewis2020RetrievalAugmentedGF,\n author = {Patrick Lewis and Ethan Perez and Aleksandara Piktus and F. Petroni and V. Karpukhin and Naman Goyal and Heinrich Kuttler and M. Lewis and Wen-tau Yih and Tim Rockt\u00e4schel and Sebastian Riedel and Douwe Kiela},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},\n volume = {abs/2005.11401},\n year = {2020}\n}\n"
        }
    },
    "978_bellkor_2007": {
        "paperId": "f4ebb542c752a0dc423f94fd121e2edb8f6275ba",
        "externalIds": {
            "MAG": "2289525833",
            "CorpusId": 11877045
        },
        "corpusId": 11877045,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/f4ebb542c752a0dc423f94fd121e2edb8f6275ba",
        "title": "The BellKor solution to the Netflix Prize",
        "abstract": "Our final solution (RMSE=0.8712) consists of blending 107 individual results. Since many of these results are close variants, we first describe the main approaches behind them. Then, we will move to describing each individual result. The core components of the solution are published in our ICDM'2007 paper [1] (or, KDD-Cup'2007 paper [2]), and also in the earlier KDD'2007 paper [3]. We assume that the reader is familiar with these works and our terminology there. A movie-oriented k-NN approach was thoroughly described in our KDD-Cup'2007 paper [kNN]. We apply it as a post-processor for most other models. Interestingly, it was most effective when applied on residuals of RBMs [5], thereby driving the Quiz RMSE from 0.9093 to 0.8888. An earlier k-NN approach was described in the KDD'2007 paper ([3], Sec. 3) [Slow-kNN]. It appears that this earlier approach can achieve slightly more accurate results than the newer one, at the expense of a significant increase in running time. Consequently, we dropped the older approach, though some results involving it survive within the final blend. We also tried more na\u00efve k-NN models, where interpolation weights are based on pairwise similarities between movies (see [2], Sec. 2.2). Specifically, we based weights on corr 2 /(1-corr 2) [Corr-kNN], or on mse-10 [MSE-kNN]. Here, corr is the Pearson correlation coefficient between the two respective movies, and mse is the mean squared distance between two movies (see definition of s ij in Sec. 4.1 of [2]). We also tried taking the interpolation weights as the \"support-based similarities\", which will be defined shortly [Supp-kNN]. Other variants that we tried for computing the interpolation coefficients are: (1) using our KDD-Cup'2007 [2] method on a binary user-movie matrix, which replaces every rating with \" 1 \" , and sets non-rated user-movie pairs to \" 0 \" [Bin-kNN]. (2) Taking results of factorization, and regressing the factors associated with the target movie on the factors associated with its neighbors. Then, the resulting regression coefficients are used as interpolation weights [Fctr-kNN]. As explained in our papers, we also tried user-oriented k-NN approaches. Either in a profound way (see: [1], Sec. 4.3; [3], Sec. 5) [User-kNN], or by just taking weights as pairwise similarities among users [User-MSE-kNN], which is the user-oriented parallel of the aforementioned [MSE-kNN]. Prior to computing interpolation weights, one has to choose the set of neighbors. We find the most similar neighbors based on an appropriate similarity measure. In \u2026",
        "venue": "",
        "year": 2007,
        "referenceCount": 6,
        "citationCount": 234,
        "influentialCitationCount": 16,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The final solution (RMSE=0.8712) consists of blending 107 individual results, and the main approaches behind them are described, which can achieve slightly more accurate results than the newer one, at the expense of a significant increase in running time."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Bell2007TheBS,\n author = {Robert M. Bell and Y. Koren and C. Volinsky},\n title = {The BellKor solution to the Netflix Prize},\n year = {2007}\n}\n"
        }
    },
    "979_tacotron_2": {
        "paperId": "1a2599e467e855f845dcbf9282f8bdbd97b85708",
        "externalIds": {
            "MAG": "2964243274",
            "DBLP": "journals/corr/abs-1712-05884",
            "ArXiv": "1712.05884",
            "DOI": "10.1109/ICASSP.2018.8461368",
            "CorpusId": 206742911
        },
        "corpusId": 206742911,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/1a2599e467e855f845dcbf9282f8bdbd97b85708",
        "title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
        "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and $F_{0}$ features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2017,
        "referenceCount": 31,
        "citationCount": 2237,
        "influentialCitationCount": 359,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1712.05884",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Tacotron 2, a neural network architecture for speech synthesis directly from text that is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those Spectrograms is described."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-12-16",
        "journal": {
            "name": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "pages": "4779-4783"
        },
        "citationStyles": {
            "bibtex": "@Article{Shen2017NaturalTS,\n author = {Jonathan Shen and Ruoming Pang and Ron J. Weiss and M. Schuster and N. Jaitly and Zongheng Yang and Z. Chen and Yu Zhang and Yuxuan Wang and R. Skerry-Ryan and R. Saurous and Yannis Agiomyrgiannakis and Yonghui Wu},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {4779-4783},\n title = {Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions},\n year = {2017}\n}\n"
        }
    },
    "980_stacked_hourglass_network": {
        "paperId": "848938e6199bad08f1db6f3239b260cfa901e95f",
        "externalIds": {
            "ArXiv": "1603.06937",
            "DBLP": "journals/corr/NewellYD16",
            "MAG": "2950762923",
            "DOI": "10.1007/978-3-319-46484-8_29",
            "CorpusId": 13613792
        },
        "corpusId": 13613792,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/848938e6199bad08f1db6f3239b260cfa901e95f",
        "title": "Stacked Hourglass Networks for Human Pose Estimation",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "referenceCount": 50,
        "citationCount": 4478,
        "influentialCitationCount": 644,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1603.06937",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces a novel convolutional network architecture for the task of human pose estimation that is described as a \u201cstacked hourglass\u201d network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-03-22",
        "journal": {
            "pages": "483-499"
        },
        "citationStyles": {
            "bibtex": "@Article{Newell2016StackedHN,\n author = {Alejandro Newell and Kaiyu Yang and Jia Deng},\n booktitle = {European Conference on Computer Vision},\n pages = {483-499},\n title = {Stacked Hourglass Networks for Human Pose Estimation},\n year = {2016}\n}\n"
        }
    },
    "981_pointnet": {
        "paperId": "d997beefc0922d97202789d2ac307c55c2c52fba",
        "externalIds": {
            "MAG": "2950642167",
            "DBLP": "conf/cvpr/QiSMG17",
            "ArXiv": "1612.00593",
            "DOI": "10.1109/CVPR.2017.16",
            "CorpusId": 5115938
        },
        "corpusId": 5115938,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/d997beefc0922d97202789d2ac307c55c2c52fba",
        "title": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
        "abstract": "Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 34,
        "citationCount": 10784,
        "influentialCitationCount": 2627,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.00593",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper designs a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input and provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-12-02",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "77-85"
        },
        "citationStyles": {
            "bibtex": "@Article{Qi2016PointNetDL,\n author = {C. Qi and Hao Su and Kaichun Mo and L. Guibas},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {77-85},\n title = {PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},\n year = {2016}\n}\n"
        }
    },
    "982_santacoder": {
        "paperId": "1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a",
        "externalIds": {
            "ArXiv": "2301.03988",
            "DBLP": "journals/corr/abs-2301-03988",
            "DOI": "10.48550/arXiv.2301.03988",
            "CorpusId": 255570209
        },
        "corpusId": 255570209,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a",
        "title": "SantaCoder: don't reach for the stars!",
        "abstract": "The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 46,
        "citationCount": 85,
        "influentialCitationCount": 12,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2301.03988",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The current state of the Personally Identifiable Information (PII) redaction pipeline is outlined, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data are outlined."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-01-09",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2301.03988"
        },
        "citationStyles": {
            "bibtex": "@Article{Allal2023SantaCoderDR,\n author = {Loubna Ben Allal and Raymond Li and Denis Kocetkov and Chenghao Mou and Christopher Akiki and Carlos Mu\u00f1oz Ferrandis and Niklas Muennighoff and Mayank Mishra and A. Gu and Manan Dey and Logesh Kumar Umapathi and Carolyn Jane Anderson and Yangtian Zi and J. Poirier and Hailey Schoelkopf and S. Troshin and Dmitry Abulkhanov and M. Romero and M. Lappert and F. Toni and Bernardo Garc'ia del R'io and Qian Liu and Shamik Bose and Urvashi Bhattacharyya and Terry Yue Zhuo and I. Yu and Paulo Villegas and Marco Zocca and Sourab Mangrulkar and D. Lansky and Huu Nguyen and Danish Contractor and Luisa Villa and Jia Li and Dzmitry Bahdanau and Yacine Jernite and S. Hughes and Daniel Fried and Arjun Guha and H. D. Vries and Leandro von Werra},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {SantaCoder: don't reach for the stars!},\n volume = {abs/2301.03988},\n year = {2023}\n}\n"
        }
    },
    "983_jurassic-x": {
        "paperId": "1bcde55995a957b3e8a595d536b816cb8989cf1d",
        "externalIds": {
            "DBLP": "journals/corr/abs-2205-00445",
            "ArXiv": "2205.00445",
            "DOI": "10.48550/arXiv.2205.00445",
            "CorpusId": 248496374
        },
        "corpusId": 248496374,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/1bcde55995a957b3e8a595d536b816cb8989cf1d",
        "title": "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",
        "abstract": "Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced\"miracle\") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 28,
        "citationCount": 40,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2205.00445",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules is described, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced\"miracle\") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-05-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2205.00445"
        },
        "citationStyles": {
            "bibtex": "@Article{Karpas2022MRKLSA,\n author = {Ehud D. Karpas and Omri Abend and Yonatan Belinkov and Barak Lenz and Opher Lieber and Nir Ratner and Y. Shoham and Hofit Bata and Yoav Levine and Kevin Leyton-Brown and Dor Muhlgay and N. Rozen and Erez Schwartz and Gal Shachaf and Shai Shalev-Shwartz and A. Shashua and Moshe Tenenholtz},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning},\n volume = {abs/2205.00445},\n year = {2022}\n}\n"
        }
    },
    "984_zip_cnn": {
        "paperId": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
        "externalIds": {
            "MAG": "2147800946",
            "DBLP": "journals/neco/LeCunBDHHHJ89",
            "DOI": "10.1162/neco.1989.1.4.541",
            "CorpusId": 41312633
        },
        "corpusId": 41312633,
        "publicationVenue": {
            "id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
            "name": "Neural Computation",
            "type": "journal",
            "alternate_names": [
                "Neural Comput"
            ],
            "issn": "0899-7667",
            "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667",
            "alternate_urls": [
                "http://ieeexplore.ieee.org/servlet/opac?punumber=6720226",
                "http://www.mitpressjournals.org/loi/neco",
                "https://www.mitpressjournals.org/loi/neco"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
        "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
        "venue": "Neural Computation",
        "year": 1989,
        "referenceCount": 13,
        "citationCount": 9877,
        "influentialCitationCount": 679,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1989-12-01",
        "journal": {
            "name": "Neural Computation",
            "pages": "541-551",
            "volume": "1"
        },
        "citationStyles": {
            "bibtex": "@Article{LeCun1989BackpropagationAT,\n author = {Yann LeCun and B. Boser and J. Denker and D. Henderson and R. Howard and W. Hubbard and L. Jackel},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {541-551},\n title = {Backpropagation Applied to Handwritten Zip Code Recognition},\n volume = {1},\n year = {1989}\n}\n"
        }
    },
    "985_word2vec_(large)": {
        "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
        "externalIds": {
            "ArXiv": "1310.4546",
            "MAG": "2950133940",
            "DBLP": "conf/nips/MikolovSCCD13",
            "CorpusId": 16447573
        },
        "corpusId": 16447573,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "referenceCount": 24,
        "citationCount": 31045,
        "influentialCitationCount": 3959,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2013-10-16",
        "journal": {
            "pages": "3111-3119"
        },
        "citationStyles": {
            "bibtex": "@Article{Mikolov2013DistributedRO,\n author = {Tomas Mikolov and I. Sutskever and Kai Chen and G. Corrado and J. Dean},\n booktitle = {Neural Information Processing Systems},\n pages = {3111-3119},\n title = {Distributed Representations of Words and Phrases and their Compositionality},\n year = {2013}\n}\n"
        }
    },
    "987_learning_deep_architectures": {
        "paperId": "d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
        "externalIds": {
            "MAG": "2072128103",
            "DBLP": "journals/ftml/Bengio09",
            "DOI": "10.1561/2200000006",
            "CorpusId": 207178999
        },
        "corpusId": 207178999,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
        "title": "Learning Deep Architectures for AI",
        "abstract": "Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.",
        "venue": "Found. Trends Mach. Learn.",
        "year": 2007,
        "referenceCount": 250,
        "citationCount": 8243,
        "influentialCitationCount": 496,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.iro.umontreal.ca/~bengioy/papers/ftml.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer modelssuch as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks are discussed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "Found. Trends Mach. Learn.",
            "pages": "1-127",
            "volume": "2"
        },
        "citationStyles": {
            "bibtex": "@Article{Bengio2007LearningDA,\n author = {Yoshua Bengio},\n booktitle = {Found. Trends Mach. Learn.},\n journal = {Found. Trends Mach. Learn.},\n pages = {1-127},\n title = {Learning Deep Architectures for AI},\n volume = {2},\n year = {2007}\n}\n"
        }
    },
    "989_1-layer-lstm": {
        "paperId": "9730773198f56034be6fde9bd34ee4223cb094be",
        "externalIds": {
            "DBLP": "journals/corr/abs-2007-06389",
            "MAG": "3041044626",
            "ArXiv": "2007.06389",
            "CorpusId": 220496077
        },
        "corpusId": 220496077,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/9730773198f56034be6fde9bd34ee4223cb094be",
        "title": "Term Revealing: Furthering Quantization at Run Time on Quantized DNNs",
        "abstract": "We present a novel technique, called Term Revealing (TR), for furthering quantization at run time for improved performance of Deep Neural Networks (DNNs) already quantized with conventional quantization methods. TR operates on power-of-two terms in binary expressions of values. In computing a dot-product computation, TR dynamically selects a fixed number of largest terms to use from the values of the two vectors in the dot product. By exploiting normal-like weight and data distributions typically present in DNNs, TR has a minimal impact on DNN model performance (i.e., accuracy or perplexity). We use TR to facilitate tightly synchronized processor arrays, such as systolic arrays, for efficient parallel processing. We show an FPGA implementation that can use a small number of control bits to switch between conventional quantization and TR-enabled quantization with a negligible delay. To enhance TR efficiency further, we use a signed digit representation (SDR), as opposed to classic binary encoding with only nonnegative power-of-two terms. To perform conversion from binary to SDR, we develop an efficient encoding method called HESE (Hybrid Encoding for Signed Expressions) that can be performed in one pass looking at only two bits at a time. We evaluate TR with HESE encoded values on an MLP for MNIST, multiple CNNs for ImageNet, and an LSTM for Wikitext-2, and show significant reductions in inference computations (between 3-10x) compared to conventional quantization for the same level of model performance.",
        "venue": "arXiv.org",
        "year": 2020,
        "referenceCount": 48,
        "citationCount": 8,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An FPGA implementation that can use a small number of control bits to switch between conventional quantization and TR-enabled quantization with a negligible delay is shown, and an efficient encoding method called HESE (Hybrid Encoding for Signed Expressions) that can be performed in one pass looking at only two bits at a time is developed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-07-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2007.06389"
        },
        "citationStyles": {
            "bibtex": "@Article{Kung2020TermRF,\n author = {H. T. Kung and Bradley McDanel and S. Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Term Revealing: Furthering Quantization at Run Time on Quantized DNNs},\n volume = {abs/2007.06389},\n year = {2020}\n}\n"
        }
    },
    "990_opt-iml_(175b)": {
        "paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
        "externalIds": {
            "ArXiv": "2212.12017",
            "DBLP": "journals/corr/abs-2212-12017",
            "DOI": "10.48550/arXiv.2212.12017",
            "CorpusId": 255096269
        },
        "corpusId": 255096269,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/e965e93e76a9e6c4e4863d145b5c007b540d575d",
        "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
        "abstract": "Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 85,
        "citationCount": 171,
        "influentialCitationCount": 23,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2212.12017",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper describes the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes and releases OPT-IML at both scales, together with an evaluation framework to measure three types of model generalizations."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-12-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2212.12017"
        },
        "citationStyles": {
            "bibtex": "@Article{Iyer2022OPTIMLSL,\n author = {S. Iyer and Xi Victoria Lin and Ramakanth Pasunuru and Todor Mihaylov and Daniel Simig and Ping Yu and Kurt Shuster and Tianlu Wang and Qing Liu and Punit Singh Koura and Xian Li and Brian O'Horo and Gabriel Pereyra and Jeff Wang and Christopher Dewan and Asli Celikyilmaz and Luke Zettlemoyer and Veselin Stoyanov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},\n volume = {abs/2212.12017},\n year = {2022}\n}\n"
        }
    },
    "991_deepface": {
        "paperId": "9f2efadf66817f1b38f58b3f50c7c8f34c69d89a",
        "externalIds": {
            "MAG": "2145287260",
            "DBLP": "conf/cvpr/TaigmanYRW14",
            "DOI": "10.1109/CVPR.2014.220",
            "CorpusId": 2814088
        },
        "corpusId": 2814088,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/9f2efadf66817f1b38f58b3f50c7c8f34c69d89a",
        "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
        "abstract": "In modern face recognition, the conventional pipeline consists of four stages: detect => align => represent => classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27%, closely approaching human-level performance.",
        "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
        "year": 2014,
        "referenceCount": 33,
        "citationCount": 5790,
        "influentialCitationCount": 435,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work revisits both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-06-01",
        "journal": {
            "name": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
            "pages": "1701-1708"
        },
        "citationStyles": {
            "bibtex": "@Article{Taigman2014DeepFaceCT,\n author = {Yaniv Taigman and Ming Yang and Marc'Aurelio Ranzato and Lior Wolf},\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1701-1708},\n title = {DeepFace: Closing the Gap to Human-Level Performance in Face Verification},\n year = {2014}\n}\n"
        }
    },
    "992_nas+ess_(156m)": {
        "paperId": "1650c7f229389b73f1f112d3b3e96f72ff9a72bf",
        "externalIds": {
            "DBLP": "journals/corr/abs-2005-02593",
            "ArXiv": "2005.02593",
            "MAG": "3023460197",
            "ACL": "2020.acl-main.592",
            "DOI": "10.18653/v1/2020.acl-main.592",
            "CorpusId": 218516935
        },
        "corpusId": 218516935,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/1650c7f229389b73f1f112d3b3e96f72ff9a72bf",
        "title": "Learning Architectures from an Extended Search Space for Language Modeling",
        "abstract": "Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS). For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously. We implement our model in a differentiable architecture search system. For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB. Moreover, the learned architectures show good transferability to other systems. E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "referenceCount": 47,
        "citationCount": 9,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.592.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a general approach to learn both intra-cell and inter-cell architectures to extend the search space of NAS and implements it in a differentiable architecture search system."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-05-06",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2005.02593"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2020LearningAF,\n author = {Yinqiao Li and Chi Hu and Yuhao Zhang and Nuo Xu and Yufan Jiang and Tong Xiao and Jingbo Zhu and Tongran Liu and Changliang Li},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Learning Architectures from an Extended Search Space for Language Modeling},\n volume = {abs/2005.02593},\n year = {2020}\n}\n"
        }
    },
    "993_gpt-2-medium+pixelfly": {
        "paperId": "90b21dbad8969b74d704eed15a3d98722a88e464",
        "externalIds": {
            "DBLP": "conf/iclr/ChenDLY0RR22",
            "ArXiv": "2112.00029",
            "CorpusId": 244773609
        },
        "corpusId": 244773609,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/90b21dbad8969b74d704eed15a3d98722a88e464",
        "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models",
        "abstract": "Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "referenceCount": 108,
        "citationCount": 50,
        "influentialCitationCount": 11,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers and empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-11-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2112.00029"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2021PixelatedBS,\n author = {Beidi Chen and Tri Dao and Kaizhao Liang and Jiaming Yang and Zhao Song and A. Rudra and C. R\u00e9},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models},\n volume = {abs/2112.00029},\n year = {2021}\n}\n"
        }
    },
    "995_samuel_neural_checkers": {
        "paperId": "e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772",
        "externalIds": {
            "MAG": "208001290",
            "DBLP": "journals/ibmrd/Samuel00",
            "DOI": "10.1147/rd.33.0210",
            "CorpusId": 2126705
        },
        "corpusId": 2126705,
        "publicationVenue": {
            "id": "555db9fd-8025-4984-8082-971e1e6bdb24",
            "name": "IBM Journal of Research and Development",
            "type": "journal",
            "alternate_names": [
                "IBM J Res Dev",
                "Ibm J Res Dev",
                "Ibm Journal of Research and Development"
            ],
            "issn": "0018-8646",
            "alternate_issns": [
                "2151-8556"
            ],
            "url": "http://www.research.ibm.com/journal/index.html",
            "alternate_urls": [
                "http://ieeexplore.ieee.org/servlet/opac?punumber=5288520"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772",
        "title": "Some Studies in Machine Learning Using the Game of Checkers",
        "abstract": "Abstract A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method. Full use is made of the so-called \u201calpha-beta\u201d pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the program to look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program's playing ability has been greatly improved.tplay checker masters, the",
        "venue": "IBM Journal of Research and Development",
        "year": 1967,
        "referenceCount": 18,
        "citationCount": 4486,
        "influentialCitationCount": 120,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.cs.virginia.edu/~evans/greatworks/samuel1959.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method and to permit the program to look ahead to a much greater depth than it otherwise could do."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1967-11-01",
        "journal": {
            "name": "IBM J. Res. Dev.",
            "pages": "206-227",
            "volume": "44"
        },
        "citationStyles": {
            "bibtex": "@Article{Samuel1967SomeSI,\n author = {A. Samuel},\n booktitle = {IBM Journal of Research and Development},\n journal = {IBM J. Res. Dev.},\n pages = {206-227},\n title = {Some Studies in Machine Learning Using the Game of Checkers},\n volume = {44},\n year = {1967}\n}\n"
        }
    },
    "997_two-stream_convnets_for_action_recognition": {
        "paperId": "67dccc9a856b60bdc4d058d83657a089b8ad4486",
        "externalIds": {
            "MAG": "2952186347",
            "DBLP": "conf/nips/SimonyanZ14",
            "ArXiv": "1406.2199",
            "CorpusId": 11797475
        },
        "corpusId": 11797475,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/67dccc9a856b60bdc4d058d83657a089b8ad4486",
        "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
        "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. \n \nOur contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "referenceCount": 33,
        "citationCount": 6848,
        "influentialCitationCount": 994,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a two-stream ConvNet architecture which incorporates spatial and temporal networks and demonstrates that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-06-09",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1406.2199"
        },
        "citationStyles": {
            "bibtex": "@Article{Simonyan2014TwoStreamCN,\n author = {K. Simonyan and Andrew Zisserman},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Two-Stream Convolutional Networks for Action Recognition in Videos},\n volume = {abs/1406.2199},\n year = {2014}\n}\n"
        }
    },
    "998_sparseopt-175b": {
        "paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996",
        "externalIds": {
            "DBLP": "journals/corr/abs-2301-00774",
            "ArXiv": "2301.00774",
            "DOI": "10.48550/arXiv.2301.00774",
            "CorpusId": 255372747
        },
        "corpusId": 255372747,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/909ad57ce8caa6b390a65ae09db352d27d8f3996",
        "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
        "abstract": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "referenceCount": 56,
        "citationCount": 168,
        "influentialCitationCount": 29,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2301.00774",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-01-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2301.00774"
        },
        "citationStyles": {
            "bibtex": "@Article{Frantar2023SparseGPTML,\n author = {Elias Frantar and Dan Alistarh},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},\n volume = {abs/2301.00774},\n year = {2023}\n}\n"
        }
    },
    "999_muzero_vp9": {
        "paperId": "5a3fe4482c8d754b290cf90c0fc03362d02ee1a4",
        "externalIds": {
            "DBLP": "journals/corr/abs-2202-06626",
            "ArXiv": "2202.06626",
            "CorpusId": 246823228
        },
        "corpusId": 246823228,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/5a3fe4482c8d754b290cf90c0fc03362d02ee1a4",
        "title": "MuZero with Self-competition for Rate Control in VP9 Video Compression",
        "abstract": "Video streaming usage has seen a significant rise as entertainment, education, and business increasingly rely on online video. Optimizing video compression has the potential to increase access and quality of content to users, and reduce energy use and costs overall. In this paper, we present an application of the MuZero algorithm to the challenge of video compression. Specifically, we target the problem of learning a rate control policy to select the quantization parameters (QP) in the encoding process of libvpx, an open source VP9 video compression library widely used by popular video-on-demand (VOD) services. We treat this as a sequential decision making problem to maximize the video quality with an episodic constraint imposed by the target bitrate. Notably, we introduce a novel self-competition based reward mechanism to solve constrained RL with variable constraint satisfaction difficulty, which is challenging for existing constrained RL methods. We demonstrate that the MuZero-based rate control achieves an average 6.28% reduction in size of the compressed videos for the same delivered video quality level (measured as PSNR BD-rate) compared to libvpx's two-pass VBR rate control policy, while having better constraint satisfaction behavior.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 49,
        "citationCount": 31,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper targets the problem of learning a rate control policy to select the quantization parameters (QP) in the encoding process of libvpx, an open source VP9 video compression library widely used by popular video-on-demand (VOD) services."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-02-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2202.06626"
        },
        "citationStyles": {
            "bibtex": "@Article{Mandhane2022MuZeroWS,\n author = {Amol Mandhane and A. Zhernov and M. Rauh and Chenjie Gu and Miaosen Wang and Flora Xue and Wendy Shang and Derek Pang and Rene Claus and Ching-Han Chiang and Cheng Chen and Jingning Han and Angie Chen and D. Mankowitz and Jackson Broshear and Julian Schrittwieser and T. Hubert and O. Vinyals and Timothy A. Mann},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {MuZero with Self-competition for Rate Control in VP9 Video Compression},\n volume = {abs/2202.06626},\n year = {2022}\n}\n"
        }
    },
    "1001_gradient_boosting_machine": {
        "paperId": "1679beddda3a183714d380e944fe6bf586c083cd",
        "externalIds": {
            "MAG": "1678356000",
            "DOI": "10.1214/AOS/1013203451",
            "CorpusId": 39450643
        },
        "corpusId": 39450643,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/1679beddda3a183714d380e944fe6bf586c083cd",
        "title": "Greedy function approximation: A gradient boosting machine.",
        "abstract": "Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such TreeBoost models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.",
        "venue": "",
        "year": 2001,
        "referenceCount": 29,
        "citationCount": 19115,
        "influentialCitationCount": 1851,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion, and specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification."
        },
        "publicationTypes": null,
        "publicationDate": "2001-10-01",
        "journal": {
            "name": "Annals of Statistics",
            "pages": "1189-1232",
            "volume": "29"
        },
        "citationStyles": {
            "bibtex": "@Article{Friedman2001GreedyFA,\n author = {J. Friedman},\n journal = {Annals of Statistics},\n pages = {1189-1232},\n title = {Greedy function approximation: A gradient boosting machine.},\n volume = {29},\n year = {2001}\n}\n"
        }
    },
    "1003_spalm_+_knn": {
        "paperId": "46c585ee9abf76779ea4b863d2da4358efd0d1d3",
        "externalIds": {
            "DBLP": "journals/tacl/YogatamadK21",
            "ArXiv": "2102.02557",
            "DOI": "10.1162/tacl_a_00371",
            "CorpusId": 231717977
        },
        "corpusId": 231717977,
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/46c585ee9abf76779ea4b863d2da4358efd0d1d3",
        "title": "Adaptive Semiparametric Language Models",
        "abstract": "Abstract We present a language model that combines a large parametric neural network (i.e., a transformer) with a non-parametric episodic memory component in an integrated architecture. Our model uses extended short-term context by caching local hidden states\u2014similar to transformer-XL\u2014and global long-term memory by retrieving a set of nearest neighbor tokens at each timestep. We design a gating function to adaptively combine multiple information sources to make a prediction. This mechanism allows the model to use either local context, short-term memory, or long-term memory (or any combination of them) on an ad hoc basis depending on the context. Experiments on word-based and character-based language modeling datasets demonstrate the efficacy of our proposed method compared to strong baselines.",
        "venue": "Transactions of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 41,
        "citationCount": 78,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00371/1924150/tacl_a_00371.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A language model that combines a large parametric neural network with a non-parametric episodic memory component in an integrated architecture is presented and a gating function to adaptively combine multiple information sources to make a prediction is designed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-02-04",
        "journal": {
            "name": "Transactions of the Association for Computational Linguistics",
            "pages": "362-373",
            "volume": "9"
        },
        "citationStyles": {
            "bibtex": "@Article{Yogatama2021AdaptiveSL,\n author = {Dani Yogatama and Cyprien de Masson d'Autume and Lingpeng Kong},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {362-373},\n title = {Adaptive Semiparametric Language Models},\n volume = {9},\n year = {2021}\n}\n"
        }
    },
    "1004_lira": {
        "paperId": "f13eb69e37fcf67645ce39769d50a6a9d741ac2c",
        "externalIds": {
            "DBLP": "journals/ivc/KussulB04",
            "MAG": "1980698266",
            "DOI": "10.1016/j.imavis.2004.03.008",
            "CorpusId": 6752588
        },
        "corpusId": 6752588,
        "publicationVenue": {
            "id": "6cc36eeb-d056-42c4-a306-7bcb239cc442",
            "name": "Image and Vision Computing",
            "type": "journal",
            "alternate_names": [
                "Image Vis Comput"
            ],
            "issn": "0262-8856",
            "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525443/description#description",
            "alternate_urls": [
                "http://www.sciencedirect.com/science/journal/02628856",
                "https://www.journals.elsevier.com/image-and-vision-computing"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/f13eb69e37fcf67645ce39769d50a6a9d741ac2c",
        "title": "Improved method of handwritten digit recognition tested on MNIST database",
        "abstract": null,
        "venue": "Image and Vision Computing",
        "year": 2004,
        "referenceCount": 16,
        "citationCount": 177,
        "influentialCitationCount": 8,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel neural classifier LImited Receptive Area (LIRA) for the image recognition that contains three neuron layers: sensor, associative and output layers and shows sufficiently good results in task of the pin\u2013hole position estimation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2004-10-01",
        "journal": {
            "name": "Image Vis. Comput.",
            "pages": "971-981",
            "volume": "22"
        },
        "citationStyles": {
            "bibtex": "@Article{Kussul2004ImprovedMO,\n author = {E. Kussul and T. Baidyk},\n booktitle = {Image and Vision Computing},\n journal = {Image Vis. Comput.},\n pages = {971-981},\n title = {Improved method of handwritten digit recognition tested on MNIST database},\n volume = {22},\n year = {2004}\n}\n"
        }
    },
    "1005_agile_soccer_robot": {
        "paperId": "98d98c4cbcb0c6d413bc3bdb1a1052542ac636b2",
        "externalIds": {
            "ArXiv": "2304.13653",
            "DBLP": "journals/corr/abs-2304-13653",
            "DOI": "10.48550/arXiv.2304.13653",
            "CorpusId": 258331581
        },
        "corpusId": 258331581,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/98d98c4cbcb0c6d413bc3bdb1a1052542ac636b2",
        "title": "Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning",
        "abstract": "We investigate whether Deep Reinforcement Learning (Deep RL) is able to synthesize sophisticated and safe movement skills for a low-cost, miniature humanoid robot that can be composed into complex behavioral strategies in dynamic environments. We used Deep RL to train a humanoid robot with 20 actuated joints to play a simplified one-versus-one (1v1) soccer game. We first trained individual skills in isolation and then composed those skills end-to-end in a self-play setting. The resulting policy exhibits robust and dynamic movement skills such as rapid fall recovery, walking, turning, kicking and more; and transitions between them in a smooth, stable, and efficient manner - well beyond what is intuitively expected from the robot. The agents also developed a basic strategic understanding of the game, and learned, for instance, to anticipate ball movements and to block opponent shots. The full range of behaviors emerged from a small set of simple rewards. Our agents were trained in simulation and transferred to real robots zero-shot. We found that a combination of sufficiently high-frequency control, targeted dynamics randomization, and perturbations during training in simulation enabled good-quality transfer, despite significant unmodeled effects and variations across robot instances. Although the robots are inherently fragile, minor hardware modifications together with basic regularization of the behavior during training led the robots to learn safe and effective movements while still performing in a dynamic and agile way. Indeed, even though the agents were optimized for scoring, in experiments they walked 156% faster, took 63% less time to get up, and kicked 24% faster than a scripted baseline, while efficiently combining the skills to achieve the longer term objectives. Examples of the emergent behaviors and full 1v1 matches are available on the supplementary website.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 105,
        "citationCount": 27,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2304.13653",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that a combination of sufficiently high-frequency control, targeted dynamics randomization, and perturbations during training in simulation enabled good-quality transfer, despite significant unmodeled effects and variations across robot instances."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-04-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2304.13653"
        },
        "citationStyles": {
            "bibtex": "@Article{Haarnoja2023LearningAS,\n author = {Tuomas Haarnoja and Ben Moran and Guy Lever and Sandy H. Huang and Dhruva Tirumala and Markus Wulfmeier and Jan Humplik and S. Tunyasuvunakool and Noah Siegel and Roland Hafner and Michael Bloesch and Kristian Hartikainen and Arunkumar Byravan and Leonard Hasenclever and Yuval Tassa and Fereshteh Sadeghi and Nathan Batchelor and Federico Casarini and Stefano Saliceti and Charles Game and Neil Sreendra and Kushal Patel and Marlon Gwira and Andrea Huber and N. Hurley and F. Nori and R. Hadsell and N. Heess},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning},\n volume = {abs/2304.13653},\n year = {2023}\n}\n"
        }
    },
    "1006_stella": {
        "paperId": "be1558b852a05bf626e4736d4e12721707065913",
        "externalIds": {
            "MAG": "2887196021",
            "DOI": "10.1016/S1474-6670(17)69682-4",
            "CorpusId": 70346544
        },
        "corpusId": 70346544,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/be1558b852a05bf626e4736d4e12721707065913",
        "title": "STELLA: A scheme for a learning machine",
        "abstract": null,
        "venue": "",
        "year": 1963,
        "referenceCount": 1,
        "citationCount": 24,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A scheme for a learning machine, which is being constructed in the form of a mechanical tortoise which takes its name from its laboratory origin, in which the machine explores the possibilities of its future actions with a view to modifying its performance."
        },
        "publicationTypes": null,
        "publicationDate": "1963-06-01",
        "journal": {
            "name": "IFAC Proceedings Volumes",
            "pages": "497-502",
            "volume": "1"
        },
        "citationStyles": {
            "bibtex": "@Article{Andreae1963STELLAAS,\n author = {J. Andreae},\n journal = {IFAC Proceedings Volumes},\n pages = {497-502},\n title = {STELLA: A scheme for a learning machine},\n volume = {1},\n year = {1963}\n}\n"
        }
    },
    "1007_pyramidnet": {
        "paperId": "5bdf07c9897ca70788fff61dec56178a2bd0c29c",
        "externalIds": {
            "DBLP": "journals/corr/HanKK16",
            "MAG": "2531425418",
            "ArXiv": "1610.02915",
            "DOI": "10.1109/CVPR.2017.668",
            "CorpusId": 5398883
        },
        "corpusId": 5398883,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/5bdf07c9897ca70788fff61dec56178a2bd0c29c",
        "title": "Deep Pyramidal Residual Networks",
        "abstract": "Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes. This also applies to residual networks and is very closely related to their performance. In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible. This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks. Code is available at https://github.com/jhkim89/PyramidNet.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 37,
        "citationCount": 616,
        "influentialCitationCount": 94,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1610.02915",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This research gradually increases the feature map dimension at all units to involve as many locations as possible in the network architecture and proposes a novel residual unit capable of further improving the classification accuracy with the new network architecture."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-10-10",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "6307-6315"
        },
        "citationStyles": {
            "bibtex": "@Article{Han2016DeepPR,\n author = {Dongyoon Han and Jiwhan Kim and Junmo Kim},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {6307-6315},\n title = {Deep Pyramidal Residual Networks},\n year = {2016}\n}\n"
        }
    },
    "1008_cicero": {
        "paperId": "e89ed6bb1864558e3889f5f2fb8931643c633479",
        "externalIds": {
            "DOI": "10.1126/science.ade9097",
            "CorpusId": 253759631,
            "PubMed": "36413172"
        },
        "corpusId": 253759631,
        "publicationVenue": {
            "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
            "name": "Science",
            "type": "journal",
            "issn": "0193-4511",
            "alternate_issns": [
                "0036-8075"
            ],
            "url": "https://www.jstor.org/journal/science",
            "alternate_urls": [
                "https://www.sciencemag.org/",
                "http://www.sciencemag.org/",
                "http://www.jstor.org/journals/00368075.html",
                "http://www.sciencemag.org/archive/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e89ed6bb1864558e3889f5f2fb8931643c633479",
        "title": "Human-level play in the game of Diplomacy by combining language models with strategic reasoning",
        "abstract": "Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players\u2019 beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game. Description AI masters Diplomacy The game Diplomacy has been a major challenge for artificial intelligence (AI). Unlike other competitive games that AI has recently mastered, such as chess, Go, and poker, Diplomacy cannot be solved purely through self-play; it requires the development of an agent to understand other players\u2019 motivations and perspectives and to use natural language to negotiate complex shared plans. The Meta Fundamental AI Research Diplomacy Team (FAIR) et al. developed an agent that is able to play the full natural language form of the game and demonstrates performance well above the human average in an online Diplomacy league. The present work has far-reaching implications for the development of cooperative AI and language models for communication with people, even when interactions involve a mixture of aligned and competing interests. \u2014YS Artificial intelligence demonstrates human-level performance in the strategic board game Diplomacy.",
        "venue": "Science",
        "year": 2022,
        "referenceCount": 62,
        "citationCount": 174,
        "influentialCitationCount": 9,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Political Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players, is introduced."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-11-22",
        "journal": {
            "name": "Science",
            "pages": "1067 - 1074",
            "volume": "378"
        },
        "citationStyles": {
            "bibtex": "@Article{Bakhtin2022HumanlevelPI,\n author = {A. Bakhtin and Noam Brown and Emily Dinan and Gabriele Farina and Colin Flaherty and Daniel Fried and Andrew Goff and Jonathan Gray and Hengyuan Hu and Athul Paul Jacob and Mojtaba Komeili and Karthik Konath and Minae Kwon and Adam Lerer and Mike Lewis and Alexander H. Miller and S. Mitts and Adithya Renduchintala and Stephen Roller and Dirk Rowe and Weiyan Shi and Joe Spisak and Alexander Wei and David J. Wu and Hugh Zhang and Markus Zijlstra},\n booktitle = {Science},\n journal = {Science},\n pages = {1067 - 1074},\n title = {Human-level play in the game of Diplomacy by combining language models with strategic reasoning},\n volume = {378},\n year = {2022}\n}\n"
        }
    },
    "1009_frage-awd-lstm-memoryaug-neuralcache_(wt2)": {
        "paperId": "5d56fd4ff34a8a64ef13b1465007ebdbeb22957a",
        "externalIds": {
            "DBLP": "conf/interspeech/LiPK20",
            "MAG": "3091208084",
            "ArXiv": "2009.13774",
            "DOI": "10.21437/interspeech.2020-3020",
            "CorpusId": 221996154
        },
        "corpusId": 221996154,
        "publicationVenue": {
            "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
            "name": "Interspeech",
            "type": "conference",
            "alternate_names": [
                "Conf Int Speech Commun Assoc",
                "INTERSPEECH",
                "Conference of the International Speech Communication Association"
            ],
            "issn": "2308-457X",
            "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
            "alternate_urls": [
                "http://www.isca-speech.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/5d56fd4ff34a8a64ef13b1465007ebdbeb22957a",
        "title": "Neural Language Modeling with Implicit Cache Pointers",
        "abstract": "A cache-inspired approach is proposed for neural language models (LMs) to improve long-range dependency and better predict rare words from long contexts. This approach is a simpler alternative to attention-based pointer mechanism that enables neural LMs to reproduce words from recent history. Without using attention and mixture structure, the method only involves appending extra tokens that represent words in history to the output layer of a neural LM and modifying training supervisions accordingly. A memory-augmentation unit is introduced to learn words that are particularly likely to repeat. We experiment with both recurrent neural network- and Transformer-based LMs. Perplexity evaluation on Penn Treebank and WikiText-2 shows the proposed model outperforms both LSTM and LSTM with attention-based pointer mechanism and is more effective on rare words. N-best rescoring experiments on Switchboard indicate that it benefits both very rare and frequent words. However, it is challenging for the proposed model as well as two other models with attention-based pointer mechanism to obtain good overall WER reductions.",
        "venue": "Interspeech",
        "year": 2020,
        "referenceCount": 30,
        "citationCount": 2,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2009.13774",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A cache-inspired approach is proposed for neural language models (LMs) to improve long-range dependency and better predict rare words from long contexts and is more effective on rare words."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-09-29",
        "journal": {
            "pages": "3625-3629"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2020NeuralLM,\n author = {Ke Li and Daniel Povey and S. Khudanpur},\n booktitle = {Interspeech},\n pages = {3625-3629},\n title = {Neural Language Modeling with Implicit Cache Pointers},\n year = {2020}\n}\n"
        }
    },
    "1011_ankh_large": {
        "paperId": "1420b9ff9c7ecfc8c8b5fdce4517e6fc51cebf92",
        "externalIds": {
            "DBLP": "journals/corr/abs-2301-06568",
            "ArXiv": "2301.06568",
            "DOI": "10.1101/2023.01.16.524265",
            "CorpusId": 255941875
        },
        "corpusId": 255941875,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/1420b9ff9c7ecfc8c8b5fdce4517e6fc51cebf92",
        "title": "Ankh \u2625: Optimized Protein Language Model Unlocks General-Purpose Modelling",
        "abstract": "As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google\u2019s TPU-v4 surpassing the state-of-the-art performance with fewer parameters (<10% for pre-training, <7% for inference, and <30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.",
        "venue": "bioRxiv",
        "year": 2023,
        "referenceCount": 59,
        "citationCount": 28,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2023/01/18/2023.01.16.524265.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents Ankh, the first general-purpose PLM trained on Google\u2019s TPU-v4 surpassing the state-of-the-art performance with fewer parameters, and provides a representative range of structure and function benchmarks where Ankh excels."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-01-16",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Elnaggar2023AnkhO,\n author = {Ahmed Elnaggar and Hazem Essam and Wafaa Salah-Eldin and Walid Moustafa and Mohamed Elkerdawy and Charlotte Rochereau and B. Rost},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {Ankh \u2625: Optimized Protein Language Model Unlocks General-Purpose Modelling},\n year = {2023}\n}\n"
        }
    },
    "1012_rotation": {
        "paperId": "aab368284210c1bb917ec2d31b84588e3d2d7eb4",
        "externalIds": {
            "ArXiv": "1803.07728",
            "DBLP": "journals/corr/abs-1803-07728",
            "MAG": "2962742544",
            "CorpusId": 4009713
        },
        "corpusId": 4009713,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/aab368284210c1bb917ec2d31b84588e3d2d7eb4",
        "title": "Unsupervised Representation Learning by Predicting Image Rotations",
        "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: this https URL .",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 35,
        "citationCount": 2804,
        "influentialCitationCount": 288,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input, and demonstrates both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-02-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1803.07728"
        },
        "citationStyles": {
            "bibtex": "@Article{Gidaris2018UnsupervisedRL,\n author = {Spyros Gidaris and Praveer Singh and N. Komodakis},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Unsupervised Representation Learning by Predicting Image Rotations},\n volume = {abs/1803.07728},\n year = {2018}\n}\n"
        }
    },
    "1013_wave2vec_2.0_large": {
        "paperId": "49a049dc85e2380dde80501a984878341dd8efdf",
        "externalIds": {
            "ArXiv": "2006.11477",
            "DBLP": "conf/nips/BaevskiZMA20",
            "MAG": "3036601975",
            "CorpusId": 219966759
        },
        "corpusId": 219966759,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/49a049dc85e2380dde80501a984878341dd8efdf",
        "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
        "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "referenceCount": 62,
        "citationCount": 3337,
        "influentialCitationCount": 907,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-06-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2006.11477"
        },
        "citationStyles": {
            "bibtex": "@Article{Baevski2020wav2vec2A,\n author = {Alexei Baevski and Henry Zhou and Abdel-rahman Mohamed and Michael Auli},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},\n volume = {abs/2006.11477},\n year = {2020}\n}\n"
        }
    },
    "1014_xception": {
        "paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d",
        "externalIds": {
            "DBLP": "conf/cvpr/Chollet17",
            "ArXiv": "1610.02357",
            "MAG": "2531409750",
            "DOI": "10.1109/CVPR.2017.195",
            "CorpusId": 2375110
        },
        "corpusId": 2375110,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/5b6ec746d309b165f9f9def873a2375b6fb40f3d",
        "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "referenceCount": 27,
        "citationCount": 11407,
        "influentialCitationCount": 1574,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1610.02357",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions, and shows that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset, and significantly outperforms it on a larger image classification dataset."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-10-07",
        "journal": {
            "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "1800-1807"
        },
        "citationStyles": {
            "bibtex": "@Article{Chollet2016XceptionDL,\n author = {Fran\u00e7ois Chollet},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1800-1807},\n title = {Xception: Deep Learning with Depthwise Separable Convolutions},\n year = {2016}\n}\n"
        }
    },
    "1015_lda": {
        "paperId": "4574d77fff19e093782178595a8988a7f3aa1969",
        "externalIds": {
            "CorpusId": 3177797
        },
        "corpusId": 3177797,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/4574d77fff19e093782178595a8988a7f3aa1969",
        "title": "Latent Dirichlet Allocation",
        "abstract": "Latent Dirichlet allocation(LDA) is a generative topic model to find latent topics in a text corpus. It can be trained via collapsed Gibbs sampling. In this project, we train LDA models on two datasets, Classic400 and BBCSport dataset. We discuss possible ways to evaluate goodness-of-fit and to detect overfitting problem of LDA model, and we use these criteria to choose proper hyperparameters, observe convergence, and evaluate the models, the criteria we use include perplexity, VI-distance, visualization of clustering results, and highest-probability words.",
        "venue": "",
        "year": 2009,
        "referenceCount": 6,
        "citationCount": 34124,
        "influentialCitationCount": 6065,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": null,
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Blei2009LatentDA,\n author = {D. Blei and A. Ng and Michael I. Jordan},\n title = {Latent Dirichlet Allocation},\n year = {2009}\n}\n"
        }
    },
    "1016_e-spa": {
        "paperId": "b8c236dc5963dac36b0d8e419beb5876e3a18f96",
        "externalIds": {
            "DBLP": "journals/corr/abs-2302-10322",
            "ArXiv": "2302.10322",
            "DOI": "10.48550/arXiv.2302.10322",
            "CorpusId": 257050560
        },
        "corpusId": 257050560,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/b8c236dc5963dac36b0d8e419beb5876e3a18f96",
        "title": "Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation",
        "abstract": "Skip connections and normalisation layers form two standard architectural components that are ubiquitous for the training of Deep Neural Networks (DNNs), but whose precise roles are poorly understood. Recent approaches such as Deep Kernel Shaping have made progress towards reducing our reliance on them, using insights from wide NN kernel theory to improve signal propagation in vanilla DNNs (which we define as networks without skips or normalisation). However, these approaches are incompatible with the self-attention layers present in transformers, whose kernels are intrinsically more complicated to analyse and control. And so the question remains: is it possible to train deep vanilla transformers? We answer this question in the affirmative by designing several approaches that use combinations of parameter initialisations, bias matrices and location-dependent rescaling to achieve faithful signal propagation in vanilla transformers. Our methods address various intricacies specific to signal propagation in transformers, including the interaction with positional encoding and causal masking. In experiments on WikiText-103 and C4, our approaches enable deep transformers without normalisation to train at speeds matching their standard counterparts, and deep vanilla transformers to reach the same performance as standard ones after about 5 times more iterations.",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "referenceCount": 71,
        "citationCount": 13,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.10322",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Several approaches are designed that use combinations of parameter initialisations, bias matrices and location-dependent rescaling to achieve faithful signal propagation in vanilla transformers, including the interaction with positional encoding and causal masking."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-02-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2302.10322"
        },
        "citationStyles": {
            "bibtex": "@Article{He2023DeepTW,\n author = {Bobby He and James Martens and Guodong Zhang and Aleksandar Botev and Andy Brock and Samuel L. Smith and Y. Teh},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation},\n volume = {abs/2302.10322},\n year = {2023}\n}\n"
        }
    },
    "1018_ase+ace": {
        "paperId": "8a7acaf6469c06ae5876d92f013184db5897bb13",
        "externalIds": {
            "MAG": "1583833196",
            "DBLP": "journals/tsmc/BartoSA83",
            "DOI": "10.1109/TSMC.1983.6313077",
            "CorpusId": 1522994
        },
        "corpusId": 1522994,
        "publicationVenue": {
            "id": "336446b6-e859-4f7b-9121-d2d40357fe0a",
            "name": "IEEE Transactions on Systems, Man and Cybernetics",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Syst Man Cybern",
                "IEEE Transactions on Systems, Man, and Cybernetics"
            ],
            "issn": "0018-9472",
            "alternate_issns": [
                "2168-2909"
            ],
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=21"
        },
        "url": "https://www.semanticscholar.org/paper/8a7acaf6469c06ae5876d92f013184db5897bb13",
        "title": "Neuronlike adaptive elements that can solve difficult learning control problems",
        "abstract": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.",
        "venue": "IEEE Transactions on Systems, Man and Cybernetics",
        "year": 1983,
        "referenceCount": 27,
        "citationCount": 3394,
        "influentialCitationCount": 193,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem and the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1983-09-01",
        "journal": {
            "name": "IEEE Transactions on Systems, Man, and Cybernetics",
            "pages": "834-846",
            "volume": "SMC-13"
        },
        "citationStyles": {
            "bibtex": "@Article{Barto1983NeuronlikeAE,\n author = {A. Barto and R. Sutton and C. Anderson},\n booktitle = {IEEE Transactions on Systems, Man and Cybernetics},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics},\n pages = {834-846},\n title = {Neuronlike adaptive elements that can solve difficult learning control problems},\n volume = {SMC-13},\n year = {1983}\n}\n"
        }
    },
    "1020_6-layer-tensor-transformer+adahessian": {
        "paperId": "20438e2a38a0c4723fbd9de50b44b7335f6f43cb",
        "externalIds": {
            "DBLP": "journals/corr/abs-2006-00719",
            "MAG": "3031420959",
            "ArXiv": "2006.00719",
            "DOI": "10.1609/aaai.v35i12.17275",
            "CorpusId": 219176656
        },
        "corpusId": 219176656,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/20438e2a38a0c4723fbd9de50b44b7335f6f43cb",
        "title": "ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning",
        "abstract": "Incorporating second-order curvature information into machine learning optimization algorithms can be subtle, and doing so na\u00efvely can lead to high per-iteration costs associated with forming the Hessian and performing the associated linear system solve. To address this, we introduce ADAHESSIAN, a new stochastic optimization algorithm. ADAHESSIAN directly incorporates approximate curvature information from the loss function, and it includes several novel performance-improving features, including: (i) a fast Hutchinson based method to approximate the curvature matrix with low computational overhead; (ii) a spatial averaging to reduce the variance of the second derivative; and (iii) a root-mean-square exponential moving average to smooth out variations of the second-derivative across different iterations. We perform extensive tests on NLP, CV, and recommendation system tasks, and ADAHESSIAN achieves state-of-the-art results. In particular, we find that ADAHESSIAN: (i) outperforms AdamW for transformers by0.13/0.33 BLEU score on IWSLT14/WMT14, 2.7/1.0 PPLon PTB/Wikitext-103; (ii) outperforms AdamW for Squeeze-Bert by 0.41 points on GLUE; (iii) achieves 1.45%/5.55%higher accuracy on ResNet32/ResNet18 on Cifar10/ImageNetas compared to Adam; and (iv) achieves 0.032% better score than Adagrad for DLRM on the Criteo Ad Kaggle dataset. The cost per iteration of ADAHESSIANis comparable to first-order methods, and ADAHESSIAN exhibits improved robustness towards variations in hyperparameter values. The code for ADAHESSIAN is open-sourced and publicly-available [1].",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2020,
        "referenceCount": 84,
        "citationCount": 178,
        "influentialCitationCount": 34,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/17275/17082",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "ADAHESSIAN is a new stochastic optimization algorithm that directly incorporates approximate curvature information from the loss function, and it includes several novel performance-improving features, including a fast Hutchinson based method to approximate the curvature matrix with low computational overhead."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-06-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2006.00719"
        },
        "citationStyles": {
            "bibtex": "@Article{Yao2020ADAHESSIANAA,\n author = {Z. Yao and A. Gholami and Sheng Shen and K. Keutzer and Michael W. Mahoney},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning},\n volume = {abs/2006.00719},\n year = {2020}\n}\n"
        }
    },
    "1021_gpt3-6.7b_+_mup": {
        "paperId": "0b0d7d87c58d41b92d907347b778032be5966f60",
        "externalIds": {
            "ArXiv": "2203.03466",
            "DBLP": "journals/corr/abs-2203-03466",
            "DOI": "10.48550/arXiv.2203.03466",
            "CorpusId": 247292726
        },
        "corpusId": 247292726,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/0b0d7d87c58d41b92d907347b778032be5966f60",
        "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
        "abstract": "Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost. A Pytorch implementation of our technique can be found at github.com/microsoft/mup and installable via `pip install mup`.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 69,
        "citationCount": 77,
        "influentialCitationCount": 10,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.03466",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Physics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Physics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work shows that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes, which leads to a new HP tuning paradigm, muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-03-07",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2203.03466"
        },
        "citationStyles": {
            "bibtex": "@Article{Yang2022TensorPV,\n author = {Greg Yang and J. E. Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and J. Pachocki and Weizhu Chen and Jianfeng Gao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},\n volume = {abs/2203.03466},\n year = {2022}\n}\n"
        }
    },
    "1022_pagnol-xl": {
        "paperId": "92bc69500c16fce47bcfd06ada14ffc4a7e8ddca",
        "externalIds": {
            "ArXiv": "2110.08554",
            "ACL": "2022.lrec-1.455",
            "DBLP": "conf/lrec/LaunayTPBCCPS22",
            "CorpusId": 239016819
        },
        "corpusId": 239016819,
        "publicationVenue": {
            "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
            "name": "International Conference on Language Resources and Evaluation",
            "type": "conference",
            "alternate_names": [
                "LREC",
                "Int Conf Lang Resour Evaluation"
            ],
            "url": "http://www.lrec-conf.org/"
        },
        "url": "https://www.semanticscholar.org/paper/92bc69500c16fce47bcfd06ada14ffc4a7e8ddca",
        "title": "PAGnol: An Extra-Large French Generative Model",
        "abstract": "Access to large pre-trained models of varied architectures, in many different languages, is central to the democratization of NLP. We introduce PAGnol, a collection of French GPT models. Using scaling laws, we efficiently train PAGnol-XL (1.5B parameters) with the same computational budget as CamemBERT, a model 13 times smaller. PAGnol-XL is the largest model trained from scratch for the French language. We plan to train increasingly large and performing versions of PAGnol, exploring the capabilities of French extreme-scale models. For this first release, we focus on the pre-training and scaling calculations underlining PAGnol. We fit a scaling law for compute for the French language, and compare it with its English counterpart. We find the pre-training dataset significantly conditions the quality of the outputs, with common datasets such as OSCAR leading to low-quality offensive text. We evaluate our models on discriminative and generative tasks in French, comparing to other state-of-the-art French and multilingual models, and reaching the state of the art in the abstract summarization task. Our research was conducted on the public GENCI Jean Zay supercomputer, and our models up to the Large are made publicly available.",
        "venue": "International Conference on Language Resources and Evaluation",
        "year": 2021,
        "referenceCount": 57,
        "citationCount": 4,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The pre-training dataset significantly conditions the quality of the outputs, with common datasets such as OSCAR leading to low-quality offensive text, and a scaling law for compute for the French language is fit, and it is compared with its English counterpart."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-16",
        "journal": {
            "pages": "4275-4284"
        },
        "citationStyles": {
            "bibtex": "@Article{Launay2021PAGnolAE,\n author = {Julien Launay and E. L. Tommasone and B. Pannier and Franccois Boniface and A. Chatelain and Alessandro Cappelli and Iacopo Poli and Djam\u00e9 Seddah},\n booktitle = {International Conference on Language Resources and Evaluation},\n pages = {4275-4284},\n title = {PAGnol: An Extra-Large French Generative Model},\n year = {2021}\n}\n"
        }
    },
    "1023_pointnet++": {
        "paperId": "8674494bd7a076286b905912d26d47f7501c4046",
        "externalIds": {
            "DBLP": "conf/nips/QiYSG17",
            "MAG": "2963121255",
            "ArXiv": "1706.02413",
            "CorpusId": 1745976
        },
        "corpusId": 1745976,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/8674494bd7a076286b905912d26d47f7501c4046",
        "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
        "abstract": "Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "referenceCount": 34,
        "citationCount": 7914,
        "influentialCitationCount": 1722,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set and proposes novel set learning layers to adaptively combine features from multiple scales to learn deep point set features efficiently and robustly."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-06-07",
        "journal": {
            "pages": "5099-5108"
        },
        "citationStyles": {
            "bibtex": "@Article{Qi2017PointNetDH,\n author = {C. Qi and L. Yi and Hao Su and L. Guibas},\n booktitle = {Neural Information Processing Systems},\n pages = {5099-5108},\n title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n year = {2017}\n}\n"
        }
    },
    "1024_gencnn_+_dyn_eval": {
        "paperId": "8645643ad5dfe662fa38f61615432d5c9bdf2ffb",
        "externalIds": {
            "ACL": "P15-1151",
            "MAG": "2949558408",
            "DBLP": "journals/corr/WangLLJL15",
            "ArXiv": "1503.05034",
            "DOI": "10.3115/v1/P15-1151",
            "CorpusId": 5453533
        },
        "corpusId": 5453533,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/8645643ad5dfe662fa38f61615432d5c9bdf2ffb",
        "title": "genCNN: A Convolutional Architecture for Word Sequence Prediction",
        "abstract": "We propose a novel convolutional architecture, named $gen$CNN, for word sequence prediction. Different from previous work on neural network-based language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and $n$-best re-ranking in machine translation show that $gen$CNN outperforms the state-of-the-arts with big margins.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2015,
        "referenceCount": 47,
        "citationCount": 28,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P15-1151.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is argued that the proposed novel convolutional architecture, named $gen$CNN, can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-03-17",
        "journal": {
            "pages": "1567-1576"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2015genCNNAC,\n author = {Mingxuan Wang and Zhengdong Lu and Hang Li and Wenbin Jiang and Qun Liu},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {1567-1576},\n title = {genCNN: A Convolutional Architecture for Word Sequence Prediction},\n year = {2015}\n}\n"
        }
    },
    "1025_deepstack": {
        "paperId": "4312ae64a058d555ff1656ccad713dbd81200e79",
        "externalIds": {
            "DBLP": "journals/corr/MoravcikSBLMBDW17",
            "CorpusId": 12274704
        },
        "corpusId": 12274704,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/4312ae64a058d555ff1656ccad713dbd81200e79",
        "title": "DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker",
        "abstract": "Arti\ufb01cial intelligence has seen a number of breakthroughs in recent years, with games often serving as signi\ufb01cant milestones. A common feature of games with these successes is that they involve information symmetry among the players, where all players have identical information. This property of perfect information , though, is far more common in games than in real-world problems. Poker is the quintessential game of imperfect information , and it has been a longstanding challenge problem in arti\ufb01cial intelligence. In this paper we introduce DeepStack, a new algorithm for imperfect information settings such as poker. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition about arbitrary poker situations that is automatically learned from self-play games using deep learning. In a study involving dozens of participants and 44,000 hands of poker, DeepStack becomes the \ufb01rst computer program to beat professional poker players in heads-up no-limit Texas hold\u2019em. Furthermore, we show this approach dramatically reduces worst-case exploitability compared to the abstraction paradigm that has been favored for over a decade. Games have long served as a benchmark and set of milestones for progress in arti\ufb01cial",
        "venue": "arXiv.org",
        "year": 2017,
        "referenceCount": 34,
        "citationCount": 212,
        "influentialCitationCount": 5,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "DeepStack is introduced, a new algorithm for imperfect information settings such as poker that combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition about arbitrary poker situations that is automatically learned from self-play games using deep learning"
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1701.01724"
        },
        "citationStyles": {
            "bibtex": "@Article{Moravc\u00edk2017DeepStackEA,\n author = {Matej Moravc\u00edk and Martin Schmid and Neil Burch and V. Lis\u00fd and Dustin Morrill and Nolan Bard and Trevor Davis and K. Waugh and Michael Bradley Johanson and Michael Bowling},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker},\n volume = {abs/1701.01724},\n year = {2017}\n}\n"
        }
    },
    "1026_bert-rbp": {
        "paperId": "a76c11fd3119c44520f7487b4141ff7e833bc5d0",
        "externalIds": {
            "PubMedCentral": "9710633",
            "DOI": "10.1093/bioadv/vbac023",
            "CorpusId": 233478600,
            "PubMed": "36699410"
        },
        "corpusId": 233478600,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/a76c11fd3119c44520f7487b4141ff7e833bc5d0",
        "title": "Prediction of RNA\u2013protein interactions using a nucleotide language model",
        "abstract": "Motivation The accumulation of sequencing data has enabled researchers to predict the interactions between RNA sequences and RNA-binding proteins (RBPs) using novel machine learning techniques. However, existing models are often difficult to interpret and require additional information to sequences. Bidirectional encoder representations from Transformer (BERT) is a language-based deep learning model that is highly interpretable. Therefore, a model based on BERT architecture can potentially overcome such limitations. Results Here, we propose BERT-RBP as a model to predict RNA-RBP interactions by adapting the BERT architecture pre-trained on a human reference genome. Our model outperformed state-of-the-art prediction models using the eCLIP-seq data of 154 RBPs. The detailed analysis further revealed that BERT-RBP could recognize both the transcript region type and RNA secondary structure only from sequence information. Overall, the results provide insights into the fine-tuning mechanism of BERT in biological contexts and provide evidence of the applicability of the model to other RNA-related problems. Availability Python source codes are freely available at https://github.com/kkyamada/bert-rbp. Contact mhamada@waseda.jp",
        "venue": "bioRxiv",
        "year": 2021,
        "referenceCount": 73,
        "citationCount": 20,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://academic.oup.com/bioinformaticsadvances/article-pdf/2/1/vbac023/43625170/vbac023.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Medicine",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The detailed analysis revealed that BERT-RBP could recognize both the transcript region type and RNA secondary structure only from sequence information, providing insights into the fine-tuning mechanism of BERT in biological contexts and providing evidence of the applicability of the model to other RNA-related problems."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-04-28",
        "journal": {
            "name": "Bioinformatics Advances",
            "volume": "2"
        },
        "citationStyles": {
            "bibtex": "@Article{Yamada2021PredictionOR,\n author = {Keisuke Yamada and Michiaki Hamada},\n booktitle = {bioRxiv},\n journal = {Bioinformatics Advances},\n title = {Prediction of RNA\u2013protein interactions using a nucleotide language model},\n volume = {2},\n year = {2021}\n}\n"
        }
    },
    "1027_capsnet_(multimnist)": {
        "paperId": "c4c06578f4870e4b126e6837907929f3c900b99f",
        "externalIds": {
            "ArXiv": "1710.09829",
            "MAG": "2963703618",
            "DBLP": "journals/corr/abs-1710-09829",
            "CorpusId": 3603485
        },
        "corpusId": 3603485,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/c4c06578f4870e4b126e6837907929f3c900b99f",
        "title": "Dynamic Routing Between Capsules",
        "abstract": "A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "referenceCount": 22,
        "citationCount": 4001,
        "influentialCitationCount": 875,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-10-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1710.09829"
        },
        "citationStyles": {
            "bibtex": "@Article{Sabour2017DynamicRB,\n author = {S. Sabour and Nicholas Frosst and Geoffrey E. Hinton},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Dynamic Routing Between Capsules},\n volume = {abs/1710.09829},\n year = {2017}\n}\n"
        }
    },
    "1028_scatterbrain": {
        "paperId": "5f895e84c1fea75de07b4f90da518273c2e57291",
        "externalIds": {
            "DBLP": "journals/corr/abs-2110-15343",
            "ArXiv": "2110.15343",
            "CorpusId": 248498407
        },
        "corpusId": 248498407,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/5f895e84c1fea75de07b4f90da518273c2e57291",
        "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation",
        "abstract": "Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve 2.1x lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 77,
        "citationCount": 73,
        "influentialCitationCount": 9,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Psychology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, Scatterbrain is proposed, a novel way to unify sparse and low-rank attention for accurate and efficient approximation and is unbiased with provably low error."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-28",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2110.15343"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2021ScatterbrainUS,\n author = {Beidi Chen and Tri Dao and Eric Winsor and Zhao Song and A. Rudra and C. R\u00e9},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Scatterbrain: Unifying Sparse and Low-rank Attention Approximation},\n volume = {abs/2110.15343},\n year = {2021}\n}\n"
        }
    },
    "1030_incoder-6.7b": {
        "paperId": "5288b9f3a9f575543f44c39e1d3b78b3ca4c99da",
        "externalIds": {
            "DBLP": "journals/corr/abs-2204-05999",
            "ArXiv": "2204.05999",
            "DOI": "10.48550/arXiv.2204.05999",
            "CorpusId": 248157108
        },
        "corpusId": 248157108,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/5288b9f3a9f575543f44c39e1d3b78b3ca4c99da",
        "title": "InCoder: A Generative Model for Code Infilling and Synthesis",
        "abstract": "Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https://sites.google.com/view/incoder-code-models",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 81,
        "citationCount": 320,
        "influentialCitationCount": 60,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2204.05999",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "InCoder is introduced, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling) with the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-04-12",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2204.05999"
        },
        "citationStyles": {
            "bibtex": "@Article{Fried2022InCoderAG,\n author = {Daniel Fried and Armen Aghajanyan and Jessy Lin and Sida I. Wang and Eric Wallace and Freda Shi and Ruiqi Zhong and Wen-tau Yih and Luke Zettlemoyer and M. Lewis},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {InCoder: A Generative Model for Code Infilling and Synthesis},\n volume = {abs/2204.05999},\n year = {2022}\n}\n"
        }
    },
    "1031_byte-mlstm+emb+wn+vd": {
        "paperId": "55cf59bfbb25d6363cab87cb747648ebe8a096e5",
        "externalIds": {
            "DBLP": "conf/iclr/Krause0RL17",
            "MAG": "2952262370",
            "ArXiv": "1609.07959",
            "CorpusId": 215826764
        },
        "corpusId": 215826764,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/55cf59bfbb25d6363cab87cb747648ebe8a096e5",
        "title": "Multiplicative LSTM for sequence modelling",
        "abstract": "We introduce multiplicative LSTM (mLSTM), a recurrent neural network architecture for sequence modelling that combines the long short-term memory (LSTM) and multiplicative recurrent neural network architectures. mLSTM is characterised by its ability to have different recurrent transition functions for each possible input, which we argue makes it more expressive for autoregressive density estimation. We demonstrate empirically that mLSTM outperforms standard LSTM and its deep variants for a range of character level language modelling tasks. In this version of the paper, we regularise mLSTM to achieve 1.27 bits/char on text8 and 1.24 bits/char on Hutter Prize. We also apply a purely byte-level mLSTM on the WikiText-2 dataset to achieve a character level entropy of 1.26 bits/char, corresponding to a word level perplexity of 88.8, which is comparable to word level LSTMs regularised in similar ways on the same task.",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "referenceCount": 42,
        "citationCount": 190,
        "influentialCitationCount": 30,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated empirically that mLSTM outperforms standard LSTM and its deep variants for a range of character level language modelling tasks, and is argued makes it more expressive for autoregressive density estimation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-09-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1609.07959"
        },
        "citationStyles": {
            "bibtex": "@Article{Krause2016MultiplicativeLF,\n author = {Ben Krause and Liang Lu and Iain Murray and S. Renals},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Multiplicative LSTM for sequence modelling},\n volume = {abs/1609.07959},\n year = {2016}\n}\n"
        }
    },
    "1032_shortformer": {
        "paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "externalIds": {
            "DBLP": "journals/corr/abs-2012-15832",
            "ACL": "2021.acl-long.427",
            "ArXiv": "2012.15832",
            "DOI": "10.18653/v1/2021.acl-long.427",
            "CorpusId": 229924221
        },
        "corpusId": 229924221,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "title": "Shortformer: Better Language Modeling using Shorter Inputs",
        "abstract": "Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 27,
        "citationCount": 67,
        "influentialCitationCount": 9,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.acl-long.427.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work identifies conditions where shorter inputs are not harmful, and achieves perplexity and efficiency improvements through two new methods that decrease input length, and shows how to improve the efficiency of recurrence methods in transformers."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-01-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2012.15832"
        },
        "citationStyles": {
            "bibtex": "@Article{Press2021ShortformerBL,\n author = {Ofir Press and Noah A. Smith and M. Lewis},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Shortformer: Better Language Modeling using Shorter Inputs},\n volume = {abs/2012.15832},\n year = {2021}\n}\n"
        }
    },
    "1034_imagen": {
        "paperId": "9695824d7a01fad57ba9c01d7d76a519d78d65e7",
        "externalIds": {
            "DBLP": "journals/corr/abs-2205-11487",
            "ArXiv": "2205.11487",
            "DOI": "10.48550/arXiv.2205.11487",
            "CorpusId": 248986576
        },
        "corpusId": 248986576,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9695824d7a01fad57ba9c01d7d76a519d78d65e7",
        "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
        "abstract": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "referenceCount": 108,
        "citationCount": 2782,
        "influentialCitationCount": 302,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2205.11487",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding, and finds that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment."
        },
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "publicationDate": "2022-05-23",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2205.11487"
        },
        "citationStyles": {
            "bibtex": "@Article{Saharia2022PhotorealisticTD,\n author = {Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily L. Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and S. S. Mahdavi and Raphael Gontijo Lopes and Tim Salimans and Jonathan Ho and David J. Fleet and Mohammad Norouzi},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},\n volume = {abs/2205.11487},\n year = {2022}\n}\n"
        }
    },
    "1035_resnet-rs": {
        "paperId": "9efe8dbde586d6248ecfc69f08b918012e2ac478",
        "externalIds": {
            "ArXiv": "2103.07579",
            "DBLP": "journals/corr/abs-2103-07579",
            "CorpusId": 232233729
        },
        "corpusId": 232233729,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9efe8dbde586d6248ecfc69f08b918012e2ac478",
        "title": "Revisiting ResNets: Improved Training and Scaling Strategies",
        "abstract": "Novel computer vision architectures monopolize the spotlight, but the impact of the model architecture is often conflated with simultaneous changes to training methodology and scaling strategies. Our work revisits the canonical ResNet (He et al., 2015) and studies these three aspects in an effort to disentangle them. Perhaps surprisingly, we find that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art models. We show that the best performing scaling strategy depends on the training regime and offer two new scaling strategies: (1) scale model depth in regimes where overfitting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended (Tan&Le, 2019). Using improved training and scaling strategies, we design a family of ResNet architectures, ResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while achieving similar accuracies on ImageNet. In a large-scale semi-supervised learning setup, ResNet-RS achieves 86.2% top-1 ImageNet accuracy, while being 4.7x faster than EfficientNet NoisyStudent. The training techniques improve transfer performance on a suite of downstream tasks (rivaling state-of-the-art self-supervised algorithms) and extend to video classification on Kinetics-400. We recommend practitioners use these simple revised ResNets as baselines for future research.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 78,
        "citationCount": 215,
        "influentialCitationCount": 23,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-03-13",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2103.07579"
        },
        "citationStyles": {
            "bibtex": "@Article{Bello2021RevisitingRI,\n author = {Irwan Bello and W. Fedus and Xianzhi Du and E. D. Cubuk and A. Srinivas and Tsung-Yi Lin and Jonathon Shlens and Barret Zoph},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Revisiting ResNets: Improved Training and Scaling Strategies},\n volume = {abs/2103.07579},\n year = {2021}\n}\n"
        }
    },
    "1036_thumbs_up?": {
        "paperId": "12d0353ce8b41b7e5409e5a4a611110aee33c7bc",
        "externalIds": {
            "DBLP": "journals/corr/cs-CL-0205070",
            "ArXiv": "cs/0205070",
            "MAG": "2951278869",
            "ACL": "W02-1011",
            "DOI": "10.3115/1118693.1118704",
            "CorpusId": 7105713
        },
        "corpusId": 7105713,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/12d0353ce8b41b7e5409e5a4a611110aee33c7bc",
        "title": "Thumbs up? Sentiment Classification using Machine Learning Techniques",
        "abstract": "We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2002,
        "referenceCount": 39,
        "citationCount": 9024,
        "influentialCitationCount": 796,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1118693.1118704",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work considers the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative, and concludes by examining factors that make the sentiment classification problem more challenging."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "publicationDate": "2002-05-27",
        "journal": {
            "name": "ArXiv",
            "volume": "cs.CL/0205070"
        },
        "citationStyles": {
            "bibtex": "@Article{Pang2002ThumbsUS,\n author = {B. Pang and Lillian Lee and Shivakumar Vaithyanathan},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Thumbs up? Sentiment Classification using Machine Learning Techniques},\n volume = {cs.CL/0205070},\n year = {2002}\n}\n"
        }
    },
    "1037_stable_diffusion_(ldm-kl-8-g)": {
        "paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "externalIds": {
            "ArXiv": "2112.10752",
            "DBLP": "journals/corr/abs-2112-10752",
            "DOI": "10.1109/CVPR52688.2022.01042",
            "CorpusId": 245335280
        },
        "corpusId": 245335280,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "referenceCount": 110,
        "citationCount": 5469,
        "influentialCitationCount": 1662,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2112.10752",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "These latent diffusion models achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-12-20",
        "journal": {
            "name": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "10674-10685"
        },
        "citationStyles": {
            "bibtex": "@Article{Rombach2021HighResolutionIS,\n author = {Robin Rombach and A. Blattmann and Dominik Lorenz and Patrick Esser and B. Ommer},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {10674-10685},\n title = {High-Resolution Image Synthesis with Latent Diffusion Models},\n year = {2021}\n}\n"
        }
    },
    "1038_spn-4+kn5": {
        "paperId": "2a76c2121eee30af82a24058b4e149f05bcda911",
        "externalIds": {
            "DBLP": "conf/interspeech/ChengKPCC14",
            "MAG": "2403625461",
            "DOI": "10.21437/Interspeech.2014-476",
            "CorpusId": 11826506
        },
        "corpusId": 11826506,
        "publicationVenue": {
            "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
            "name": "Interspeech",
            "type": "conference",
            "alternate_names": [
                "Conf Int Speech Commun Assoc",
                "INTERSPEECH",
                "Conference of the International Speech Communication Association"
            ],
            "issn": "2308-457X",
            "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
            "alternate_urls": [
                "http://www.isca-speech.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/2a76c2121eee30af82a24058b4e149f05bcda911",
        "title": "Language modeling with sum-product networks",
        "abstract": "Sum product networks (SPNs) are a new class of deep probabilistic models. They can contain multiple hidden layers while keeping their inference and training times tractable. An SPN consists of interleaving layers of sum nodes and product nodes. A sum node can be interpreted as a hidden variable, and a product node can be viewed as a feature capturing rich interactions among an SPN\u2019s inputs. We show that the ability of SPN to use hidden layers to model complex dependencies among words, and its tractable inference and learning times, make it a suitable framework for a language model. Even though SPNs have been applied to a variety of vision problems [1, 2], we are the first to use it for language modeling. Our empirical comparisons with six previous language models indicate that our SPN has superior performance.",
        "venue": "Interspeech",
        "year": 2014,
        "referenceCount": 16,
        "citationCount": 78,
        "influentialCitationCount": 6,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that the ability of SPN to use hidden layers to model complex dependencies among words, and its tractable inference and learning times, make it a suitable framework for a language model."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "2098-2102"
        },
        "citationStyles": {
            "bibtex": "@Article{Cheng2014LanguageMW,\n author = {W. Cheng and Stanley Kok and Hoai Vu Pham and Hai Leong Chieu and K. M. A. Chai},\n booktitle = {Interspeech},\n pages = {2098-2102},\n title = {Language modeling with sum-product networks},\n year = {2014}\n}\n"
        }
    },
    "1039_instructgpt": {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "externalIds": {
            "DBLP": "conf/nips/Ouyang0JAWMZASR22",
            "ArXiv": "2203.02155",
            "CorpusId": 246426909
        },
        "corpusId": 246426909,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback",
        "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "referenceCount": 83,
        "citationCount": 4998,
        "influentialCitationCount": 707,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent and showing improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-03-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2203.02155"
        },
        "citationStyles": {
            "bibtex": "@Article{Ouyang2022TrainingLM,\n author = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and P. Welinder and P. Christiano and J. Leike and Ryan J. Lowe},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Training language models to follow instructions with human feedback},\n volume = {abs/2203.02155},\n year = {2022}\n}\n"
        }
    },
    "1040_gl-lwgc-awd-mos-lstm_+_dynamic_evaluation_(wt2)": {
        "paperId": "553be2229337c27ffc265bb3ee3cecfa2111f95e",
        "externalIds": {
            "ArXiv": "1708.08863",
            "DBLP": "journals/corr/abs-1708-08863",
            "MAG": "2751361084",
            "CorpusId": 2050695
        },
        "corpusId": 2050695,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/553be2229337c27ffc265bb3ee3cecfa2111f95e",
        "title": "Gradual Learning of Deep Recurrent Neural Networks",
        "abstract": "Recurrent Neural Networks (RNNs) achieve state-of-the-art results in many sequence-to-sequence modeling tasks. However, RNNs are difficult to train and tend to suffer from overfitting. Motivated by the Data Processing Inequality (DPI), we formulate the multi-layered network as a Markov chain, introducing a training method that comprises training the network gradually and using layer-wise gradient clipping. We found that applying our methods, combined with previously introduced regularization and optimization methods, resulted in improvements in state-of-the-art architectures operating in language modeling tasks.",
        "venue": "arXiv.org",
        "year": 2017,
        "referenceCount": 28,
        "citationCount": 4,
        "influentialCitationCount": 1,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work forms the multi-layered network as a Markov chain, introducing a training method that comprises training the network gradually and using layer-wise gradient clipping, which resulted in improvements in state-of-the-art architectures operating in language modeling tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-08-29",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1708.08863"
        },
        "citationStyles": {
            "bibtex": "@Article{Aharoni2017GradualLO,\n author = {Ziv Aharoni and Gal Rattner and H. Permuter},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Gradual Learning of Deep Recurrent Neural Networks},\n volume = {abs/1708.08863},\n year = {2017}\n}\n"
        }
    },
    "1041_spectrally_normalized_gan": {
        "paperId": "84de7d27e2f6160f634a483e8548c499a2cda7fa",
        "externalIds": {
            "MAG": "2785678896",
            "DBLP": "journals/corr/abs-1802-05957",
            "ArXiv": "1802.05957",
            "CorpusId": 3366315
        },
        "corpusId": 3366315,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/84de7d27e2f6160f634a483e8548c499a2cda7fa",
        "title": "Spectral Normalization for Generative Adversarial Networks",
        "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "referenceCount": 48,
        "citationCount": 3858,
        "influentialCitationCount": 571,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator and confirms that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-02-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1802.05957"
        },
        "citationStyles": {
            "bibtex": "@Article{Miyato2018SpectralNF,\n author = {Takeru Miyato and Toshiki Kataoka and Masanori Koyama and Yuichi Yoshida},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Spectral Normalization for Generative Adversarial Networks},\n volume = {abs/1802.05957},\n year = {2018}\n}\n"
        }
    },
    "1042_clip_(vit_l_14@336px)": {
        "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "externalIds": {
            "ArXiv": "2103.00020",
            "DBLP": "conf/icml/RadfordKHRGASAM21",
            "CorpusId": 231591445
        },
        "corpusId": 231591445,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision",
        "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 222,
        "citationCount": 11731,
        "influentialCitationCount": 3605,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-02-26",
        "journal": {
            "pages": "8748-8763"
        },
        "citationStyles": {
            "bibtex": "@Article{Radford2021LearningTV,\n author = {Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and I. Sutskever},\n booktitle = {International Conference on Machine Learning},\n pages = {8748-8763},\n title = {Learning Transferable Visual Models From Natural Language Supervision},\n year = {2021}\n}\n"
        }
    },
    "1043_deeplabv3": {
        "paperId": "ee4a012a4b12d11d7ab8c0e79c61e807927a163c",
        "externalIds": {
            "ArXiv": "1706.05587",
            "DBLP": "journals/corr/ChenPSA17",
            "MAG": "2630837129",
            "CorpusId": 22655199
        },
        "corpusId": 22655199,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/ee4a012a4b12d11d7ab8c0e79c61e807927a163c",
        "title": "Rethinking Atrous Convolution for Semantic Image Segmentation",
        "abstract": "In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.",
        "venue": "arXiv.org",
        "year": 2017,
        "referenceCount": 96,
        "citationCount": 6814,
        "influentialCitationCount": 988,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The proposed `DeepLabv3' system significantly improves over the previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-06-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1706.05587"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2017RethinkingAC,\n author = {Liang-Chieh Chen and G. Papandreou and Florian Schroff and Hartwig Adam},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Rethinking Atrous Convolution for Semantic Image Segmentation},\n volume = {abs/1706.05587},\n year = {2017}\n}\n"
        }
    },
    "1044_cutout-regularized_net": {
        "paperId": "eb35fdc11a325f21a8ce0ca65058f7480a2fc91f",
        "externalIds": {
            "MAG": "2746314669",
            "ArXiv": "1708.04552",
            "DBLP": "journals/corr/abs-1708-04552",
            "CorpusId": 23714201
        },
        "corpusId": 23714201,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/eb35fdc11a325f21a8ce0ca65058f7480a2fc91f",
        "title": "Improved Regularization of Convolutional Neural Networks with Cutout",
        "abstract": "Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56%, 15.20%, and 1.30% test error respectively. Code is available at this https URL",
        "venue": "arXiv.org",
        "year": 2017,
        "referenceCount": 23,
        "citationCount": 3002,
        "influentialCitationCount": 403,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper shows that the simple regularization technique of randomly masking out square regions of input during training, which is called cutout, can be used to improve the robustness and overall performance of convolutional neural networks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-08-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1708.04552"
        },
        "citationStyles": {
            "bibtex": "@Article{Devries2017ImprovedRO,\n author = {Terrance Devries and Graham W. Taylor},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Improved Regularization of Convolutional Neural Networks with Cutout},\n volume = {abs/1708.04552},\n year = {2017}\n}\n"
        }
    },
    "1045_katago": {
        "paperId": "f244ffb549a61806d00f614e70fa1c3fbe5fffc6",
        "externalIds": {
            "DBLP": "journals/corr/abs-1902-10565",
            "ArXiv": "1902.10565",
            "MAG": "2918318309",
            "CorpusId": 67855419
        },
        "corpusId": 67855419,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/f244ffb549a61806d00f614e70fa1c3fbe5fffc6",
        "title": "Accelerating Self-Play Learning in Go",
        "abstract": "By introducing several improvements to the AlphaZero process and architecture, we greatly accelerate self-play learning in Go, achieving a 50x reduction in computation over comparable methods. Like AlphaZero and replications such as ELF OpenGo and Leela Zero, our bot KataGo only learns from neural-net-guided Monte Carlo tree search self-play. But whereas AlphaZero required thousands of TPUs over several days and ELF required thousands of GPUs over two weeks, KataGo surpasses ELF's final model after only 19 days on fewer than 30 GPUs. Much of the speedup involves non-domain-specific improvements that might directly transfer to other problems. Further gains from domain-specific techniques reveal the remaining efficiency gap between the best methods and purely general methods such as AlphaZero. Our work is a step towards making learning in state spaces as large as Go possible without large-scale computational resources.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 26,
        "citationCount": 72,
        "influentialCitationCount": 16,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "By introducing several improvements to the AlphaZero process and architecture, this work greatly accelerate self-play learning in Go, achieving a 50x reduction in computation over comparable methods."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-02-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1902.10565"
        },
        "citationStyles": {
            "bibtex": "@Article{Wu2019AcceleratingSL,\n author = {David J. Wu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Accelerating Self-Play Learning in Go},\n volume = {abs/1902.10565},\n year = {2019}\n}\n"
        }
    },
    "1046_fixres_resnext-101_wsl": {
        "paperId": "c0aaee2337e5af680e5dca1bfc349a737dfec573",
        "externalIds": {
            "ArXiv": "1906.06423",
            "MAG": "2949863037",
            "DBLP": "conf/nips/TouvronVDJ19",
            "CorpusId": 189928444
        },
        "corpusId": 189928444,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/c0aaee2337e5af680e5dca1bfc349a737dfec573",
        "title": "Fixing the train-test resolution discrepancy",
        "abstract": "Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the typical size of the objects seen by the classifier at train and test time. We experimentally validate that, for a target test resolution, using a lower train resolution offers better classification at test time. \nWe then propose a simple yet effective and efficient strategy to optimize the classifier performance when the train and test resolutions differ. It involves only a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128x128 images, and 79.8% with one trained on 224x224 image. In addition, if we use extra training data we get 82.5% with the ResNet-50 train with 224x224 images. \nConversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop). To the best of our knowledge this is the highest ImageNet single-crop, top-1 and top-5 accuracy to date.",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "referenceCount": 50,
        "citationCount": 354,
        "influentialCitationCount": 45,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is experimentally validated that, for a target test resolution, using a lower train resolution offers better classification at test time, and a simple yet effective and efficient strategy to optimize the classifier performance when the train and test resolutions differ is proposed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-06-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1906.06423"
        },
        "citationStyles": {
            "bibtex": "@Article{Touvron2019FixingTT,\n author = {Hugo Touvron and A. Vedaldi and Matthijs Douze and H. J\u00e9gou},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Fixing the train-test resolution discrepancy},\n volume = {abs/1906.06423},\n year = {2019}\n}\n"
        }
    },
    "1047_innervator": {
        "paperId": "e89cb97bc83badf8c6cc0e2439ee4a035cba72d9",
        "externalIds": {
            "DBLP": "conf/icga/MillerTH89",
            "MAG": "1581066146",
            "CorpusId": 23166548
        },
        "corpusId": 23166548,
        "publicationVenue": {
            "id": "34fcc5a6-2504-4caa-be37-c8a9911df57c",
            "name": "International Conference on Genetic Algorithms",
            "type": "conference",
            "alternate_names": [
                "Int Conf Genet Algorithm",
                "international conference on Genetic algorithms",
                "ICGA",
                "int conf Genet algorithm"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e89cb97bc83badf8c6cc0e2439ee4a035cba72d9",
        "title": "Designing Neural Networks using Genetic Algorithms",
        "abstract": "RoboCup has come a long way since it\u2019s creation in \u201997 [1] and is a respected place for machine learning researchers to try out new algorithms in a competitive fashion. RoboCup is now an international competition that draws many teams and respected researchers looking for a chance to create the best team. Originally we set out to create a team to compete in RoboCup. This was an ambitious project, and we had hopes to finish within the next year. For this semester, we chose to scale down the RoboCup team towards a smaller research area to try our learning algorithm on. The scaled down version of the RoboCup soccer environment is known as the \u201dKeepaway Testbed\u201d and was started by Peter Stone, University of Texas [2]. Here the task is simple, you have two teams on the field each with the same number of players. Instead of trying to score a goal on the opponent the teams are given tasks, and one team is labeled the keepers and the other is labeled the takers. It is the task of the keepers to maintain possesion of the ball and it is the task of the takers to take the ball. The longer the keepers are able to maintain possesion of the ball the better the team. There are several advantages to this environment. First, it provides some of the essential characteristics of a real soccer game. Typically it is believed that if a team is able to maintain possesion of the ball for long periods of time they will win the match. Secondly, it provides realistic behavior much the same as the original RoboCup server. This is accomplished by introducing noise into the system similar to the original RoboCup, and similar to what would be received by real robots. Finally, when you want to go through the learning process this environment is capable of stopping play once the takers have touched the ball, and the environment is capable of starting a new trial based on that occurrence. Although the RoboCup Keepaway Machine Learning testbed provided an excellent environment to train our agents, we still needed to scale down the problem in order to do a feasibility study. Based on the Keepaway testbed, we created a simulation world with one simple task. One agent is placed into the world and has to locate the position of the goal. This can be thought of as an agent in a soccer environment needing to locate either the ball or another teammate. It was in this environment where we tested our methods for learning autonomous agents.",
        "venue": "International Conference on Genetic Algorithms",
        "year": 1989,
        "referenceCount": 0,
        "citationCount": 970,
        "influentialCitationCount": 38,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This semester, the RoboCup Keepaway Machine Learning testbed provided an excellent environment to train the authors' agents, but still needed to scale down the problem in order to do a feasibility study."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1989-06-01",
        "journal": {
            "pages": "379-384"
        },
        "citationStyles": {
            "bibtex": "@Article{Miller1989DesigningNN,\n author = {G. Miller and P. Todd and Shailesh U. Hegde},\n booktitle = {International Conference on Genetic Algorithms},\n pages = {379-384},\n title = {Designing Neural Networks using Genetic Algorithms},\n year = {1989}\n}\n"
        }
    },
    "1048_character-enriched_word2vec": {
        "paperId": "e2dba792360873aef125572812f3673b1a85d850",
        "externalIds": {
            "MAG": "2952566282",
            "ACL": "Q17-1010",
            "ArXiv": "1607.04606",
            "DBLP": "journals/tacl/BojanowskiGJM17",
            "DOI": "10.1162/tacl_a_00051",
            "CorpusId": 207556454
        },
        "corpusId": 207556454,
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/e2dba792360873aef125572812f3673b1a85d850",
        "title": "Enriching Word Vectors with Subword Information",
        "abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
        "venue": "Transactions of the Association for Computational Linguistics",
        "year": 2016,
        "referenceCount": 53,
        "citationCount": 8772,
        "influentialCitationCount": 971,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00051",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new approach based on the skipgram model, where each word is represented as a bag of character n-grams, with words being represented as the sum of these representations, which achieves state-of-the-art performance on word similarity and analogy tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-07-15",
        "journal": {
            "name": "Transactions of the Association for Computational Linguistics",
            "pages": "135-146",
            "volume": "5"
        },
        "citationStyles": {
            "bibtex": "@Article{Bojanowski2016EnrichingWV,\n author = {Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {135-146},\n title = {Enriching Word Vectors with Subword Information},\n volume = {5},\n year = {2016}\n}\n"
        }
    },
    "1050_anthropic_lm_175b": {
        "paperId": "6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552",
        "externalIds": {
            "DBLP": "journals/corr/abs-2302-07459",
            "ArXiv": "2302.07459",
            "DOI": "10.48550/arXiv.2302.07459",
            "CorpusId": 256868727
        },
        "corpusId": 256868727,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552",
        "title": "The Capacity for Moral Self-Correction in Large Language Models",
        "abstract": "We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to\"morally self-correct\"-- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 70,
        "citationCount": 91,
        "influentialCitationCount": 8,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.07459",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Psychology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-02-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2302.07459"
        },
        "citationStyles": {
            "bibtex": "@Article{Ganguli2023TheCF,\n author = {Deep Ganguli and Amanda Askell and Nicholas Schiefer and Thomas Liao and Kamil.e Lukovsiut.e and Anna Chen and Anna Goldie and Azalia Mirhoseini and Catherine Olsson and Danny Hernandez and Dawn Drain and Dustin Li and Eli Tran-Johnson and Ethan Perez and John Kernion and Jamie Kerr and J. Mueller and J. Landau and Kamal Ndousse and Karina Nguyen and Liane Lovitt and Michael Sellitto and Nelson Elhage and Noem'i Mercado and Nova DasSarma and R. Lasenby and Robin Larson and Sam Ringer and Sandipan Kundu and Saurav Kadavath and Scott Johnston and S. Kravec and S. E. Showk and Tamera Lanham and Timothy Telleen-Lawton and T. Henighan and Tristan Hume and Yuntao Bai and Zac Hatfield-Dodds and Benjamin Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom B. Brown and C. Olah and Jack Clark and Sam Bowman and Jared Kaplan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Capacity for Moral Self-Correction in Large Language Models},\n volume = {abs/2302.07459},\n year = {2023}\n}\n"
        }
    },
    "1051_switch": {
        "paperId": "fdacf2a732f55befdc410ea927091cad3b791f13",
        "externalIds": {
            "DBLP": "journals/corr/abs-2101-03961",
            "ArXiv": "2101.03961",
            "CorpusId": 231573431
        },
        "corpusId": 231573431,
        "publicationVenue": {
            "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
            "name": "Journal of machine learning research",
            "type": "journal",
            "alternate_names": [
                "Journal of Machine Learning Research",
                "J mach learn res",
                "J Mach Learn Res"
            ],
            "issn": "1532-4435",
            "alternate_issns": [
                "1533-7928"
            ],
            "url": "http://www.ai.mit.edu/projects/jmlr/",
            "alternate_urls": [
                "http://jmlr.csail.mit.edu/",
                "http://www.jmlr.org/",
                "http://portal.acm.org/affiliated/jmlr"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/fdacf2a732f55befdc410ea927091cad3b791f13",
        "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
        "abstract": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the\"Colossal Clean Crawled Corpus\"and achieve a 4x speedup over the T5-XXL model.",
        "venue": "Journal of machine learning research",
        "year": 2021,
        "referenceCount": 74,
        "citationCount": 1114,
        "influentialCitationCount": 220,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work simplifies the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs and shows large sparse models may be trained, for the first time, with lower precision formats."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-01-11",
        "journal": {
            "name": "J. Mach. Learn. Res.",
            "pages": "120:1-120:39",
            "volume": "23"
        },
        "citationStyles": {
            "bibtex": "@Article{Fedus2021SwitchTS,\n author = {W. Fedus and Barret Zoph and Noam M. Shazeer},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {120:1-120:39},\n title = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},\n volume = {23},\n year = {2021}\n}\n"
        }
    },
    "1053_back-propagation": {
        "paperId": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
        "externalIds": {
            "MAG": "1498436455",
            "DOI": "10.1038/323533a0",
            "CorpusId": 205001834
        },
        "corpusId": 205001834,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/052b1d8ce63b07fec3de9dbb583772d860b7c769",
        "title": "Learning representations by back-propagating errors",
        "abstract": null,
        "venue": "Nature",
        "year": 1986,
        "referenceCount": 2,
        "citationCount": 24263,
        "influentialCitationCount": 776,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
        },
        "publicationTypes": null,
        "publicationDate": "1986-10-01",
        "journal": {
            "name": "Nature",
            "pages": "533-536",
            "volume": "323"
        },
        "citationStyles": {
            "bibtex": "@Article{Rumelhart1986LearningRB,\n author = {D. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},\n booktitle = {Nature},\n journal = {Nature},\n pages = {533-536},\n title = {Learning representations by back-propagating errors},\n volume = {323},\n year = {1986}\n}\n"
        }
    },
    "1054_swift": {
        "paperId": "228acf81ba342d96b6ff0c4feb950791e406cffb",
        "externalIds": {
            "PubMedCentral": "10468397",
            "DBLP": "journals/nature/KaufmannBL0K023",
            "DOI": "10.1038/s41586-023-06419-4",
            "CorpusId": 261357832,
            "PubMed": "37648758"
        },
        "corpusId": 261357832,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/228acf81ba342d96b6ff0c4feb950791e406cffb",
        "title": "Champion-level drone racing using deep reinforcement learning",
        "abstract": null,
        "venue": "Nature",
        "year": 2023,
        "referenceCount": 64,
        "citationCount": 46,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.nature.com/articles/s41586-023-06419-4.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces Swift, an autonomous system that can race physical vehicles at the level of the human world champions, and represents a milestone for mobile robotics and machine intelligence, which may inspire the deployment of hybrid learning-based solutions in other physical systems."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-08-01",
        "journal": {
            "name": "Nature",
            "pages": "982 - 987",
            "volume": "620"
        },
        "citationStyles": {
            "bibtex": "@Article{Kaufmann2023ChampionlevelDR,\n author = {Elia Kaufmann and L. Bauersfeld and Antonio Loquercio and Matthias M\u00fcller and V. Koltun and Davide Scaramuzza},\n booktitle = {Nature},\n journal = {Nature},\n pages = {982 - 987},\n title = {Champion-level drone racing using deep reinforcement learning},\n volume = {620},\n year = {2023}\n}\n"
        }
    },
    "1056_llava": {
        "paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8",
        "externalIds": {
            "DBLP": "journals/corr/abs-2304-08485",
            "ArXiv": "2304.08485",
            "DOI": "10.48550/arXiv.2304.08485",
            "CorpusId": 258179774
        },
        "corpusId": 258179774,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/a5036f31f0e629dc661f120b8c3b1f374d479ab8",
        "title": "Visual Instruction Tuning",
        "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "referenceCount": 63,
        "citationCount": 783,
        "influentialCitationCount": 253,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2304.08485",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-04-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2304.08485"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2023VisualIT,\n author = {Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Visual Instruction Tuning},\n volume = {abs/2304.08485},\n year = {2023}\n}\n"
        }
    },
    "1057_blstm_for_handwriting_(1)": {
        "paperId": "43fa5235e49fa6f16d047c999234d1b93df360b0",
        "externalIds": {
            "MAG": "2587486463",
            "CorpusId": 5668166
        },
        "corpusId": 5668166,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/43fa5235e49fa6f16d047c999234d1b93df360b0",
        "title": "A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks",
        "abstract": "In this paper we introduce a new connectionist approach to on-line handwriting recognition and address in particular the problem of recognizing handwritten whiteboard notes. The approach uses a bidirectional recurrent neural network with the long short-term memory architecture. We use a recently introduced objective function, known as Connectionist Temporal Classification (CTC), that directly trains the network to label unsegmented sequence data. Our new system achieves a word recognition rate of 74.0%, compared with 65.4% using a previously developed HMMbased recognition system.",
        "venue": "",
        "year": 2007,
        "referenceCount": 19,
        "citationCount": 200,
        "influentialCitationCount": 10,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new connectionist approach to on-line handwriting recognition and address in particular the problem of recognizing handwritten whiteboard notes using a recently introduced objective function, known as Connectionist Temporal Classification (CTC), that directly trains the network to label unsegmented sequence data."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Liwicki2007ANA,\n author = {M. Liwicki and Alex Graves and H. Bunke and J. Schmidhuber},\n title = {A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks},\n year = {2007}\n}\n"
        }
    },
    "1058_true-regularization+finetune": {
        "paperId": "5a2304ba4e4401db2e0df8188a5f761646b52480",
        "externalIds": {
            "MAG": "2953242772",
            "DBLP": "journals/corr/abs-1904-04163",
            "ArXiv": "1904.04163",
            "DOI": "10.1109/ICASSP.2019.8683533",
            "CorpusId": 89606968
        },
        "corpusId": 89606968,
        "publicationVenue": {
            "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
            "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "type": "conference",
            "alternate_names": [
                "Int Conf Acoust Speech Signal Process",
                "IEEE Int Conf Acoust Speech Signal Process",
                "ICASSP",
                "International Conference on Acoustics, Speech, and Signal Processing"
            ],
            "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
        },
        "url": "https://www.semanticscholar.org/paper/5a2304ba4e4401db2e0df8188a5f761646b52480",
        "title": "Knowledge Distillation for Recurrent Neural Network Language Modeling with Trust Regularization",
        "abstract": "Recurrent Neural Networks (RNNs) have dominated language modeling because of their superior performance over traditional N-gram based models. In many applications, a large Recurrent Neural Network language model (RNNLM) or an ensemble of several RNNLMs is used. These models have large memory footprints and require heavy computation. In this paper, we examine the effect of applying knowledge distillation in reducing the model size for RNNLMs. In addition, we propose a trust regularization method to improve the knowledge distillation training for RNNLMs. Using knowledge distillation with trust regularization, we reduce the parameter size to a third of that of the previously published best model while maintaining the state-of-the-art perplexity result on Penn Treebank data. In a speech recognition N-best rescoring task, we reduce the RNNLM model size to 18.5% of the baseline system, with no degradation in word error rate (WER) performance on Wall Street Journal data set.",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "year": 2019,
        "referenceCount": 25,
        "citationCount": 23,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.04163",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper reduces the RNNLM model size to 18.5% of the baseline system, with no degradation in word error rate (WER) performance on Wall Street Journal data set, and proposes a trust regularization method to improve the knowledge distillation training for RNNLMs."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-04-08",
        "journal": {
            "name": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "pages": "7230-7234"
        },
        "citationStyles": {
            "bibtex": "@Article{Shi2019KnowledgeDF,\n author = {Yangyang Shi and M. Hwang and X. Lei and Haoyu Sheng},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {7230-7234},\n title = {Knowledge Distillation for Recurrent Neural Network Language Modeling with Trust Regularization},\n year = {2019}\n}\n"
        }
    },
    "1059_transformer": {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "externalIds": {
            "MAG": "2950858113",
            "DBLP": "conf/nips/VaswaniSPUJGKP17",
            "ArXiv": "1706.03762",
            "CorpusId": 13756489
        },
        "corpusId": 13756489,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need",
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "referenceCount": 41,
        "citationCount": 84885,
        "influentialCitationCount": 13899,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-06-12",
        "journal": {
            "pages": "5998-6008"
        },
        "citationStyles": {
            "bibtex": "@Article{Vaswani2017AttentionIA,\n author = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n booktitle = {Neural Information Processing Systems},\n pages = {5998-6008},\n title = {Attention is All you Need},\n year = {2017}\n}\n"
        }
    },
    "1060_deep_lstm_for_video_classification": {
        "paperId": "5418b2a482720e013d487a385c26fae0f017c6a6",
        "externalIds": {
            "MAG": "1923404803",
            "ArXiv": "1503.08909",
            "DBLP": "conf/cvpr/NgHVVMT15",
            "DOI": "10.1109/CVPR.2015.7299101",
            "CorpusId": 4245530
        },
        "corpusId": 4245530,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/5418b2a482720e013d487a385c26fae0f017c6a6",
        "title": "Beyond short snippets: Deep networks for video classification",
        "abstract": "Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 73.0%).",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "referenceCount": 29,
        "citationCount": 2229,
        "influentialCitationCount": 188,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1503.08909",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes and evaluates several deep neural network architectures to combine image information across a video over longer time periods than previously attempted, and proposes two methods capable of handling full length videos."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-03-30",
        "journal": {
            "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "4694-4702"
        },
        "citationStyles": {
            "bibtex": "@Article{Ng2015BeyondSS,\n author = {Joe Yue-Hei Ng and Matthew J. Hausknecht and Sudheendra Vijayanarasimhan and O. Vinyals and R. Monga and G. Toderici},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4694-4702},\n title = {Beyond short snippets: Deep networks for video classification},\n year = {2015}\n}\n"
        }
    },
    "1061_gshard_(dense)": {
        "paperId": "1882f194cb43828852cc052887671e55a80f945a",
        "externalIds": {
            "MAG": "3040573126",
            "DBLP": "conf/iclr/LepikhinLXCFHKS21",
            "ArXiv": "2006.16668",
            "CorpusId": 220265858
        },
        "corpusId": 220265858,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/1882f194cb43828852cc052887671e55a80f945a",
        "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "abstract": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "referenceCount": 99,
        "citationCount": 612,
        "influentialCitationCount": 103,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding and it is demonstrated that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-06-30",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2006.16668"
        },
        "citationStyles": {
            "bibtex": "@Article{Lepikhin2020GShardSG,\n author = {Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and M. Krikun and Noam M. Shazeer and Z. Chen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},\n volume = {abs/2006.16668},\n year = {2020}\n}\n"
        }
    },
    "1062_deberta": {
        "paperId": "05f5f8b2065a520846d89771ebaea2bb1534e9c6",
        "externalIds": {
            "DBLP": "journals/corr/abs-2006-03654",
            "MAG": "3033187248",
            "ArXiv": "2006.03654",
            "CorpusId": 219531210
        },
        "corpusId": 219531210,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/05f5f8b2065a520846d89771ebaea2bb1534e9c6",
        "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
        "abstract": "Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and pre-trained models will be made publicly available at this https URL.",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "referenceCount": 71,
        "citationCount": 1592,
        "influentialCitationCount": 310,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) is proposed that improves the BERT and RoBERTa models using two novel techniques that significantly improve the efficiency of model pre-training and performance of downstream tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-06-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2006.03654"
        },
        "citationStyles": {
            "bibtex": "@Article{He2020DeBERTaDB,\n author = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},\n volume = {abs/2006.03654},\n year = {2020}\n}\n"
        }
    },
    "1063_eve": {
        "paperId": "0537c62e32299b8e9b4e3cac44d82ffa123cbbe1",
        "externalIds": {
            "DOI": "10.1038/s41586-021-04043-8",
            "CorpusId": 240071819,
            "PubMed": "34707284"
        },
        "corpusId": 240071819,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/0537c62e32299b8e9b4e3cac44d82ffa123cbbe1",
        "title": "Disease variant prediction with deep generative models of evolutionary data",
        "abstract": null,
        "venue": "Nature",
        "year": 2021,
        "referenceCount": 46,
        "citationCount": 287,
        "influentialCitationCount": 32,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2020/12/22/2020.12.21.423785.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classification."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-10-27",
        "journal": {
            "name": "Nature",
            "pages": "91 - 95",
            "volume": "599"
        },
        "citationStyles": {
            "bibtex": "@Article{Frazer2021DiseaseVP,\n author = {J. Frazer and Pascal Notin and M. Dias and Aidan N. Gomez and Joseph K Min and Kelly P. Brock and Y. Gal and D. Marks},\n booktitle = {Nature},\n journal = {Nature},\n pages = {91 - 95},\n title = {Disease variant prediction with deep generative models of evolutionary data},\n volume = {599},\n year = {2021}\n}\n"
        }
    },
    "1064_bert-large": {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "externalIds": {
            "MAG": "2951055169",
            "ACL": "N19-1423",
            "DBLP": "journals/corr/abs-1810-04805",
            "ArXiv": "1810.04805",
            "DOI": "10.18653/v1/N19-1423",
            "CorpusId": 52967399
        },
        "corpusId": 52967399,
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "url": "https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "referenceCount": 63,
        "citationCount": 69903,
        "influentialCitationCount": 17426,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "4171-4186"
        },
        "citationStyles": {
            "bibtex": "@Article{Devlin2019BERTPO,\n author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {4171-4186},\n title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n year = {2019}\n}\n"
        }
    },
    "1065_nllb": {
        "paperId": "e19b54ad4c1c8af045069e9cac350ffc2ce60e1a",
        "externalIds": {
            "ArXiv": "2207.04672",
            "DBLP": "journals/corr/abs-2207-04672",
            "DOI": "10.48550/arXiv.2207.04672",
            "CorpusId": 250425961
        },
        "corpusId": 250425961,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/e19b54ad4c1c8af045069e9cac350ffc2ce60e1a",
        "title": "No Language Left Behind: Scaling Human-Centered Machine Translation",
        "abstract": "Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 0,
        "citationCount": 486,
        "influentialCitationCount": 39,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2207.04672",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages is developed, laying important groundwork towards realizing a universal translation system."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-07-11",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2207.04672"
        },
        "citationStyles": {
            "bibtex": "@Article{team2022NoLL,\n author = {Nllb team and M. Costa-juss\u00e0 and James Cross and Onur cCelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Alison Youngblood and Bapi Akula and Lo\u00efc Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon L. Spruit and C. Tran and Pierre Yves Andrews and N. F. Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzm'an and Philipp Koehn and Alexandre Mourachko and C. Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {No Language Left Behind: Scaling Human-Centered Machine Translation},\n volume = {abs/2207.04672},\n year = {2022}\n}\n"
        }
    },
    "1067_fast_r-cnn": {
        "paperId": "7ffdbc358b63378f07311e883dddacc9faeeaf4b",
        "externalIds": {
            "ArXiv": "1504.08083",
            "CorpusId": 206770307
        },
        "corpusId": 206770307,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/7ffdbc358b63378f07311e883dddacc9faeeaf4b",
        "title": "Fast R-CNN",
        "abstract": "This paper proposes Fast R-CNN, a clean and fast framework for object detection. Compared to traditional R-CNN, and its accelerated version SPPnet, Fast R-CNN trains networks using a multi-task loss in a single training stage. The multi-task loss simplifies learning and improves detection accuracy. Unlike SPPnet, all network layers can be updated during fine-tuning. We show that this difference has practical ramifications for very deep networks, such as VGG16, where mAP suffers when only the fully-connected layers are updated. Compared to\"slow\"R-CNN, Fast R-CNN is 9x faster at training VGG16 for detection, 213x faster at test-time, and achieves a significantly higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn",
        "venue": "",
        "year": 2015,
        "referenceCount": 23,
        "citationCount": 20994,
        "influentialCitationCount": 3168,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes Fast R-CNN, a clean and fast framework for object detection that trains networks using a multi-task loss in a single training stage and achieves a significantly higher mAP on PASCAL VOC 2012."
        },
        "publicationTypes": null,
        "publicationDate": "2015-04-29",
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Girshick2015FastR,\n author = {Ross B. Girshick},\n title = {Fast R-CNN},\n year = {2015}\n}\n"
        }
    },
    "1068_vall-e": {
        "paperId": "c2f91f35df893714418cc29096083dce0b441229",
        "externalIds": {
            "DBLP": "journals/corr/abs-2301-02111",
            "ArXiv": "2301.02111",
            "DOI": "10.48550/arXiv.2301.02111",
            "CorpusId": 255440307
        },
        "corpusId": 255440307,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/c2f91f35df893714418cc29096083dce0b441229",
        "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
        "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 49,
        "citationCount": 201,
        "influentialCitationCount": 48,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2301.02111",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity, and is found to preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-01-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2301.02111"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2023NeuralCL,\n author = {Chengyi Wang and Sanyuan Chen and Yu Wu and Zi-Hua Zhang and Long Zhou and Shujie Liu and Zhuo Chen and Yanqing Liu and Huaming Wang and Jinyu Li and Lei He and Sheng Zhao and Furu Wei},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers},\n volume = {abs/2301.02111},\n year = {2023}\n}\n"
        }
    },
    "1069_ltm": {
        "paperId": "8732ce8366ecd03f12fd77325683433012f640d3",
        "externalIds": {
            "MAG": "2979178745",
            "DBLP": "journals/corr/abs-1904-08936",
            "ArXiv": "1904.08936",
            "DOI": "10.1109/IJCNN.2019.8851909",
            "CorpusId": 125946459
        },
        "corpusId": 125946459,
        "publicationVenue": {
            "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
            "name": "IEEE International Joint Conference on Neural Network",
            "type": "conference",
            "alternate_names": [
                "IJCNN",
                "IEEE Int Jt Conf Neural Netw",
                "Int Jt Conf Neural Netw",
                "International Joint Conference on Neural Network"
            ],
            "url": "http://www.wikicfp.com/cfp/program?id=1573"
        },
        "url": "https://www.semanticscholar.org/paper/8732ce8366ecd03f12fd77325683433012f640d3",
        "title": "Language Modeling through Long-Term Memory Network",
        "abstract": "Recurrent Neural Networks (RNN), Long Short-Term Memory Networks (LSTM), and Memory Networks which contain memory are popularly used to learn patterns in sequential data. Sequential data has long sequences that hold relationships. RNN can handle long sequences but suffers from the vanishing and exploding gradient problems. While LSTM and other memory networks address this problem, they are not capable of handling long sequences (50 or more data points long sequence patterns). Language modelling requiring learning from longer sequences are affected by the need for more information in memory. This paper introduces Long Term Memory network (LTM), which can tackle the exploding and vanishing gradient problems and handles long sequences without forgetting. LTM is designed to scale data in the memory and gives a higher weight to the input in the sequence. LTM avoid overfitting by scaling the cell state after achieving the optimal results. The LTM is tested on Penn treebank dataset, and Text8 dataset and LTM achieves test perplexities of 83 and 82 respectively. 650 LTM cells achieved a test perplexity of 67 for Penn treebank, and 600 cells achieved a test perplexity of 77 for Text8. LTM achieves state of the art results by only using ten hidden LTM cells for both datasets.",
        "venue": "IEEE International Joint Conference on Neural Network",
        "year": 2019,
        "referenceCount": 30,
        "citationCount": 14,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.08936",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper introduces Long Term Memory network (LTM), which can tackle the exploding and vanishing gradient problems and handles long sequences without forgetting."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-04-18",
        "journal": {
            "name": "2019 International Joint Conference on Neural Networks (IJCNN)",
            "pages": "1-6"
        },
        "citationStyles": {
            "bibtex": "@Article{Nugaliyadde2019LanguageMT,\n author = {A. Nugaliyadde and K. Wong and Ferdous Sohel and Hong Xie},\n booktitle = {IEEE International Joint Conference on Neural Network},\n journal = {2019 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-6},\n title = {Language Modeling through Long-Term Memory Network},\n year = {2019}\n}\n"
        }
    },
    "1072_srn-encoded_grammatical_structures": {
        "paperId": "605d738a39df3c5e596613ab0ca6925f0eecdf35",
        "externalIds": {
            "DBLP": "journals/ml/Elman91",
            "MAG": "2087946919",
            "DOI": "10.1023/A:1022699029236",
            "CorpusId": 7069311
        },
        "corpusId": 7069311,
        "publicationVenue": {
            "id": "22c9862f-a25e-40cd-9d31-d09e68a293e6",
            "name": "Machine-mediated learning",
            "type": "journal",
            "alternate_names": [
                "Mach learn",
                "Machine Learning",
                "Mach Learn"
            ],
            "issn": "0732-6718",
            "alternate_issns": [
                "0885-6125"
            ],
            "url": "http://www.springer.com/computer/artificial/journal/10994",
            "alternate_urls": [
                "https://link.springer.com/journal/10994",
                "http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/605d738a39df3c5e596613ab0ca6925f0eecdf35",
        "title": "Distributed representations, simple recurrent networks, and grammatical structure",
        "abstract": null,
        "venue": "Machine-mediated learning",
        "year": 1991,
        "referenceCount": 84,
        "citationCount": 854,
        "influentialCitationCount": 64,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1023/A:1022699029236.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Using a prediction task, a simple recurrent network is trained on multiclausal sentences which contain multiply-embedded relative clauses and principal component analysis of the hidden unit activation patterns reveals that the network solves the task by developing complex distributed representations which encode the relevant grammatical relations and hierarchical constituent structure."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1991-09-01",
        "journal": {
            "name": "Machine Learning",
            "pages": "195-225",
            "volume": "7"
        },
        "citationStyles": {
            "bibtex": "@Article{Elman1991DistributedRS,\n author = {J. Elman},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {195-225},\n title = {Distributed representations, simple recurrent networks, and grammatical structure},\n volume = {7},\n year = {1991}\n}\n"
        }
    },
    "1074_lstm-large+behaviorial-gating": {
        "paperId": "6798ff4bf77e3a8732c9465ad0f945d9d340bb62",
        "externalIds": {
            "MAG": "2971393230",
            "DBLP": "journals/corr/abs-1909-00107",
            "ArXiv": "1909.00107",
            "CorpusId": 202540825
        },
        "corpusId": 202540825,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/6798ff4bf77e3a8732c9465ad0f945d9d340bb62",
        "title": "Behavior Gated Language Models",
        "abstract": "Most current language modeling techniques only exploit co-occurrence, semantic and syntactic information from the sequence of words. However, a range of information such as the state of the speaker and dynamics of the interaction might be useful. In this work we derive motivation from psycholinguistics and propose the addition of behavioral information into the context of language modeling. We propose the augmentation of language models with an additional module which analyzes the behavioral state of the current context. This behavioral information is used to gate the outputs of the language model before the final word prediction output. We show that the addition of behavioral context in language models achieves lower perplexities on behavior-rich datasets. We also confirm the validity of the proposed models on a variety of model architectures and improve on previous state-of-the-art models with generic domain Penn Treebank Corpus.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 26,
        "citationCount": 1,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work derives motivation from psycholinguistics and proposes the addition of behavioral information into the context of language modeling with an additional module which analyzes the behavioral state of the current context."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-08-31",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1909.00107"
        },
        "citationStyles": {
            "bibtex": "@Article{Shivakumar2019BehaviorGL,\n author = {P. G. Shivakumar and Shao-Yen Tseng and P. Georgiou and Shrikanth S. Narayanan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Behavior Gated Language Models},\n volume = {abs/1909.00107},\n year = {2019}\n}\n"
        }
    },
    "1075_hogwild!": {
        "paperId": "36f49b05d764bf5c10428b082c2d96c13c4203b9",
        "externalIds": {
            "DBLP": "journals/corr/abs-1106-5730",
            "MAG": "2951781666",
            "ArXiv": "1106.5730",
            "CorpusId": 6108215
        },
        "corpusId": 6108215,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/36f49b05d764bf5c10428b082c2d96c13c4203b9",
        "title": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
        "abstract": "Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.",
        "venue": "Neural Information Processing Systems",
        "year": 2011,
        "referenceCount": 29,
        "citationCount": 2152,
        "influentialCitationCount": 254,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking, and presents an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2011-06-28",
        "journal": {
            "pages": "693-701"
        },
        "citationStyles": {
            "bibtex": "@Article{Recht2011HogwildAL,\n author = {B. Recht and Christopher R\u00e9 and Stephen J. Wright and Feng Niu},\n booktitle = {Neural Information Processing Systems},\n pages = {693-701},\n title = {Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent},\n year = {2011}\n}\n"
        }
    },
    "1076_nasv3_(cifar-10)": {
        "paperId": "67d968c7450878190e45ac7886746de867bf673d",
        "externalIds": {
            "MAG": "2952431534",
            "ArXiv": "1611.01578",
            "DBLP": "journals/corr/ZophL16",
            "CorpusId": 12713052
        },
        "corpusId": 12713052,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/67d968c7450878190e45ac7886746de867bf673d",
        "title": "Neural Architecture Search with Reinforcement Learning",
        "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "referenceCount": 73,
        "citationCount": 4734,
        "influentialCitationCount": 577,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper uses a recurrent network to generate the model descriptions of neural networks and trains this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-11-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1611.01578"
        },
        "citationStyles": {
            "bibtex": "@Article{Zoph2016NeuralAS,\n author = {Barret Zoph and Quoc V. Le},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Neural Architecture Search with Reinforcement Learning},\n volume = {abs/1611.01578},\n year = {2016}\n}\n"
        }
    },
    "1077_adaptive_subgrad": {
        "paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
        "externalIds": {
            "MAG": "2405601855",
            "DBLP": "journals/jmlr/DuchiHS11",
            "DOI": "10.5555/1953048.2021068",
            "CorpusId": 538820
        },
        "corpusId": 538820,
        "publicationVenue": {
            "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
            "name": "Journal of machine learning research",
            "type": "journal",
            "alternate_names": [
                "Journal of Machine Learning Research",
                "J mach learn res",
                "J Mach Learn Res"
            ],
            "issn": "1532-4435",
            "alternate_issns": [
                "1533-7928"
            ],
            "url": "http://www.ai.mit.edu/projects/jmlr/",
            "alternate_urls": [
                "http://jmlr.csail.mit.edu/",
                "http://www.jmlr.org/",
                "http://portal.acm.org/affiliated/jmlr"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/413c1142de9d91804d6d11c67ff3fed59c9fc279",
        "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
        "abstract": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.",
        "venue": "Journal of machine learning research",
        "year": 2011,
        "referenceCount": 55,
        "citationCount": 9561,
        "influentialCitationCount": 1416,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work describes and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal functions that can be chosen in hindsight."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2011-02-01",
        "journal": {
            "name": "J. Mach. Learn. Res.",
            "pages": "2121-2159",
            "volume": "12"
        },
        "citationStyles": {
            "bibtex": "@Article{Duchi2011AdaptiveSM,\n author = {John C. Duchi and Elad Hazan and Y. Singer},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2121-2159},\n title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},\n volume = {12},\n year = {2011}\n}\n"
        }
    },
    "1078_gan-advancer": {
        "paperId": "571b0750085ae3d939525e62af510ee2cee9d5ea",
        "externalIds": {
            "DBLP": "conf/nips/SalimansGZCRCC16",
            "ArXiv": "1606.03498",
            "MAG": "2949938177",
            "CorpusId": 1687220
        },
        "corpusId": 1687220,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/571b0750085ae3d939525e62af510ee2cee9d5ea",
        "title": "Improved Techniques for Training GANs",
        "abstract": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "referenceCount": 28,
        "citationCount": 7583,
        "influentialCitationCount": 890,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work focuses on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic, and presents ImageNet samples with unprecedented resolution and shows that the methods enable the model to learn recognizable features of ImageNet classes."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-06-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1606.03498"
        },
        "citationStyles": {
            "bibtex": "@Article{Salimans2016ImprovedTF,\n author = {Tim Salimans and I. Goodfellow and Wojciech Zaremba and Vicki Cheung and Alec Radford and Xi Chen},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Improved Techniques for Training GANs},\n volume = {abs/1606.03498},\n year = {2016}\n}\n"
        }
    },
    "1080_papa": {
        "paperId": "c3a20b9aa86033cec29f08e69f4bc81e8b329ae2",
        "externalIds": {
            "MAG": "2054879295",
            "DOI": "10.1007/BF02822639",
            "CorpusId": 121274808
        },
        "corpusId": 121274808,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/c3a20b9aa86033cec29f08e69f4bc81e8b329ae2",
        "title": "Further experiments with PAPA",
        "abstract": null,
        "venue": "",
        "year": 1961,
        "referenceCount": 0,
        "citationCount": 22,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Physics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Physics",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The P A P A m a c h i n e, b u i l t in the authors' I n s t i t u t e, has been d e s c r i b e d e l sewhere, to recognize s imple g e o m e t r i c a l p a t t e r n s."
        },
        "publicationTypes": null,
        "publicationDate": "1961-09-01",
        "journal": {
            "name": "Il Nuovo Cimento (1955-1965)",
            "pages": "112-115",
            "volume": "20"
        },
        "citationStyles": {
            "bibtex": "@Article{Gamba1961FurtherEW,\n author = {A. Gamba and L. Gamberini and G. Palmieri and R. Sanna},\n journal = {Il Nuovo Cimento (1955-1965)},\n pages = {112-115},\n title = {Further experiments with PAPA},\n volume = {20},\n year = {1961}\n}\n"
        }
    },
    "1082_unified-io": {
        "paperId": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c",
        "externalIds": {
            "DBLP": "journals/corr/abs-2206-08916",
            "ArXiv": "2206.08916",
            "DOI": "10.48550/arXiv.2206.08916",
            "CorpusId": 249848272
        },
        "corpusId": 249848272,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/8b5eab31e1c5689312fff3181a75bfbf5c13e51c",
        "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",
        "abstract": "We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos for Unified-IO are available at: https://unified-io.allenai.org.",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "referenceCount": 129,
        "citationCount": 211,
        "influentialCitationCount": 20,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2206.08916",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-06-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2206.08916"
        },
        "citationStyles": {
            "bibtex": "@Article{Lu2022UnifiedIOAU,\n author = {Jiasen Lu and Christopher Clark and Rowan Zellers and Roozbeh Mottaghi and Aniruddha Kembhavi},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks},\n volume = {abs/2206.08916},\n year = {2022}\n}\n"
        }
    },
    "1083_cpm-large": {
        "paperId": "cc50f846ed7222698d130cddbc58ed4d547914ed",
        "externalIds": {
            "DBLP": "journals/corr/abs-2012-00413",
            "ArXiv": "2012.00413",
            "MAG": "3107315802",
            "DOI": "10.1016/j.aiopen.2021.07.001",
            "CorpusId": 227238757
        },
        "corpusId": 227238757,
        "publicationVenue": {
            "id": "6c35576a-a87d-4dc1-a576-780572d8d0e6",
            "name": "AI Open",
            "type": "journal",
            "issn": "2666-6510",
            "url": "https://www.keaipublishing.com/en/journals/ai-open/"
        },
        "url": "https://www.semanticscholar.org/paper/cc50f846ed7222698d130cddbc58ed4d547914ed",
        "title": "CPM: A Large-scale Generative Chinese Pre-trained Language Model",
        "abstract": null,
        "venue": "AI Open",
        "year": 2020,
        "referenceCount": 42,
        "citationCount": 90,
        "influentialCitationCount": 8,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-12-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2012.00413"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2020CPMAL,\n author = {Zhengyan Zhang and Xu Han and Hao Zhou and Pei Ke and Yuxian Gu and Deming Ye and Yujia Qin and Yusheng Su and Haozhe Ji and Jian Guan and Fanchao Qi and Xiaozhi Wang and Yanan Zheng and Guoyang Zeng and Huanqi Cao and S. Chen and Daixuan Li and Zhenbo Sun and Zhiyuan Liu and Minlie Huang and Wentao Han and Jie Tang and Juan-Zi Li and Xiaoyan Zhu and Maosong Sun},\n booktitle = {AI Open},\n journal = {ArXiv},\n title = {CPM: A Large-scale Generative Chinese Pre-trained Language Model},\n volume = {abs/2012.00413},\n year = {2020}\n}\n"
        }
    },
    "1084_deepnash": {
        "paperId": "b947cfc64517ac2e1b809e74b506e40eb4e76235",
        "externalIds": {
            "DBLP": "journals/corr/abs-2206-15378",
            "ArXiv": "2206.15378",
            "DOI": "10.1126/science.add4679",
            "CorpusId": 250144392,
            "PubMed": "36454847"
        },
        "corpusId": 250144392,
        "publicationVenue": {
            "id": "f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
            "name": "Science",
            "type": "journal",
            "issn": "0193-4511",
            "alternate_issns": [
                "0036-8075"
            ],
            "url": "https://www.jstor.org/journal/science",
            "alternate_urls": [
                "https://www.sciencemag.org/",
                "http://www.sciencemag.org/",
                "http://www.jstor.org/journals/00368075.html",
                "http://www.sciencemag.org/archive/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/b947cfc64517ac2e1b809e74b506e40eb4e76235",
        "title": "Mastering the game of Stratego with model-free multiagent reinforcement learning",
        "abstract": "We introduce DeepNash, an autonomous agent that plays the imperfect information game Stratego at a human expert level. Stratego is one of the few iconic board games that artificial intelligence (AI) has not yet mastered. It is a game characterized by a twin challenge: It requires long-term strategic thinking as in chess, but it also requires dealing with imperfect information as in poker. The technique underpinning DeepNash uses a game-theoretic, model-free deep reinforcement learning method, without search, that learns to master Stratego through self-play from scratch. DeepNash beat existing state-of-the-art AI methods in Stratego and achieved a year-to-date (2022) and all-time top-three ranking on the Gravon games platform, competing with human expert players. Description Machine learning to play Stratego Stratego is a popular two-player imperfect information board game. Because of its complexity stemming from its enormous game tree, decision-making under imperfect information, and a piece deployment phase at the start, Stratego poses a challenge for artificial intelligence (AI). Previous computer programs only performed at an amateur level at best. Perolat et al. introduce a model-free multiagent reinforcement learning methodology and show that it can achieve human expert\u2013level performance in Stratego. The present work not only adds to the growing list of games that AI systems can play as well or even better than humans but may also facilitate further applications of reinforcement learning methods in real-world, large-scale multiagent problems that are characterized by imperfect information and thus are currently unsolvable. \u2014YS Reinforcement learning achieves human expert\u2013level performance in the large-scale imperfect information board game Stratego.",
        "venue": "Science",
        "year": 2022,
        "referenceCount": 33,
        "citationCount": 105,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2206.15378",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A model-free multiagent reinforcement learning methodology is introduced and it is shown that it can achieve human expert\u2013level performance in Stratego, a popular two-player imperfect information board game."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-06-30",
        "journal": {
            "name": "Science",
            "pages": "990 - 996",
            "volume": "378"
        },
        "citationStyles": {
            "bibtex": "@Article{P\u00e9rolat2022MasteringTG,\n author = {J. P\u00e9rolat and B. D. Vylder and Daniel Hennes and Eugene Tarassov and Florian Strub and V. D. Boer and Paul Muller and Jerome T. Connor and Neil Burch and Thomas W. Anthony and S. McAleer and R. \u00c9lie and Sarah H. Cen and Zhe Wang and A. Gruslys and Aleksandra Malysheva and Mina Khan and Sherjil Ozair and Finbarr Timbers and Tobias Pohlen and Tom Eccles and Mark Rowland and Marc Lanctot and Jean-Baptiste Lespiau and Bilal Piot and Shayegan Omidshafiei and Edward Lockhart and L. Sifre and Nathalie Beauguerlange and R. Munos and David Silver and Satinder Singh and D. Hassabis and K. Tuyls},\n booktitle = {Science},\n journal = {Science},\n pages = {990 - 996},\n title = {Mastering the game of Stratego with model-free multiagent reinforcement learning},\n volume = {378},\n year = {2022}\n}\n"
        }
    },
    "1086_rbm_image_classifier": {
        "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
        "externalIds": {
            "MAG": "2945315962",
            "CorpusId": 18268744
        },
        "corpusId": 18268744,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086",
        "title": "Learning Multiple Layers of Features from Tiny Images",
        "abstract": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.",
        "venue": "",
        "year": 2009,
        "referenceCount": 15,
        "citationCount": 27639,
        "influentialCitationCount": 7259,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex, using a novel parallelization algorithm to distribute the work among multiple machines connected on a network."
        },
        "publicationTypes": null,
        "publicationDate": null,
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Inproceedings{Krizhevsky2009LearningML,\n author = {A. Krizhevsky},\n title = {Learning Multiple Layers of Features from Tiny Images},\n year = {2009}\n}\n"
        }
    },
    "1087_spatial_pyramid_matching": {
        "paperId": "6dbaff29d3898cf60f63f5a34cb9610ebb75220c",
        "externalIds": {
            "DBLP": "conf/cvpr/LazebnikSP06",
            "MAG": "2162915993",
            "DOI": "10.1109/CVPR.2006.68",
            "CorpusId": 2421251
        },
        "corpusId": 2421251,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/6dbaff29d3898cf60f63f5a34cb9610ebb75220c",
        "title": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories",
        "abstract": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u2019s \"gist\" and Lowe\u2019s SIFT descriptors.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2006,
        "referenceCount": 31,
        "citationCount": 8576,
        "influentialCitationCount": 1327,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://hal.inria.fr/docs/00/54/85/85/PDF/cvpr06_lana.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence that exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2006-06-17",
        "journal": {
            "name": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
            "pages": "2169-2178",
            "volume": "2"
        },
        "citationStyles": {
            "bibtex": "@Article{Lazebnik2006BeyondBO,\n author = {Svetlana Lazebnik and C. Schmid and J. Ponce},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},\n pages = {2169-2178},\n title = {Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories},\n volume = {2},\n year = {2006}\n}\n"
        }
    },
    "1088_multi-scale_dilated_cnn": {
        "paperId": "7f5fc84819c0cf94b771fe15141f65b123f7b8ec",
        "externalIds": {
            "ArXiv": "1511.07122",
            "MAG": "2949927980",
            "DBLP": "journals/corr/YuK15",
            "CorpusId": 17127188
        },
        "corpusId": 17127188,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7f5fc84819c0cf94b771fe15141f65b123f7b8ec",
        "title": "Multi-Scale Context Aggregation by Dilated Convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "referenceCount": 40,
        "citationCount": 7420,
        "influentialCitationCount": 458,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work develops a new convolutional network module that is specifically designed for dense prediction, and shows that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2015-11-23",
        "journal": {
            "name": "CoRR",
            "volume": "abs/1511.07122"
        },
        "citationStyles": {
            "bibtex": "@Article{Yu2015MultiScaleCA,\n author = {F. Yu and V. Koltun},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Multi-Scale Context Aggregation by Dilated Convolutions},\n volume = {abs/1511.07122},\n year = {2015}\n}\n"
        }
    },
    "1089_wenet_(penn_treebank)": {
        "paperId": "aed145657eb6074cb23e226f699b217527596369",
        "externalIds": {
            "MAG": "2928299337",
            "DBLP": "journals/corr/abs-1904-03819",
            "ArXiv": "1904.03819",
            "CorpusId": 102351005
        },
        "corpusId": 102351005,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/aed145657eb6074cb23e226f699b217527596369",
        "title": "WeNet: Weighted Networks for Recurrent Network Architecture Search",
        "abstract": "In recent years, there has been increasing demand for automatic architecture search in deep learning. Numerous approaches have been proposed and led to state-of-the-art results in various applications, including image classification and language modeling. In this paper, we propose a novel way of architecture search by means of weighted networks (WeNet), which consist of a number of networks, with each assigned a weight. These weights are updated with back-propagation to reflect the importance of different networks. Such weighted networks bear similarity to mixture of experts. We conduct experiments on Penn Treebank and WikiText-2. We show that the proposed WeNet can find recurrent architectures which result in state-of-the-art performance.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 38,
        "citationCount": 4,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A novel way of architecture search by means of weighted networks (WeNet), which consist of a number of networks, with each assigned a weight, which are updated with back-propagation to reflect the importance of different networks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-04-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1904.03819"
        },
        "citationStyles": {
            "bibtex": "@Article{Huang2019WeNetWN,\n author = {Zhiheng Huang and Bing Xiang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {WeNet: Weighted Networks for Recurrent Network Architecture Search},\n volume = {abs/1904.03819},\n year = {2019}\n}\n"
        }
    },
    "1090_tf-lm-discourse_lstm_(wt2)": {
        "paperId": "daf6e4731470f6dbec9508d7ebdf026dd8527f26",
        "externalIds": {
            "ACL": "L18-1470",
            "DBLP": "conf/lrec/VerwimphW18",
            "MAG": "2806989935",
            "CorpusId": 21710652
        },
        "corpusId": 21710652,
        "publicationVenue": {
            "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
            "name": "International Conference on Language Resources and Evaluation",
            "type": "conference",
            "alternate_names": [
                "LREC",
                "Int Conf Lang Resour Evaluation"
            ],
            "url": "http://www.lrec-conf.org/"
        },
        "url": "https://www.semanticscholar.org/paper/daf6e4731470f6dbec9508d7ebdf026dd8527f26",
        "title": "TF-LM: TensorFlow-based Language Modeling Toolkit",
        "abstract": "Recently, an abundance of deep learning toolkits has been made freely available. These toolkits typically offer the building blocks and sometimes simple example scripts, but designing and training a model still takes a considerable amount of time and knowledge. We present language modeling scripts based on TensorFlow that allow one to train and test competitive models directly, by using a pre-de\ufb01ned con\ufb01guration or changing it to their needs. There are several options for input features (words, characters, words combined with characters, character n -grams) and for batching (sentence-or discourse-level). The models can be used to test the perplexity, predict the next word(s), re-score hypotheses or generate debugging \ufb01les for interpolation with n -gram models. Additionally, we make available LSTM language models trained on a variety of Dutch texts and English benchmarks, that can be used immediately, thereby avoiding the time and computationally expensive training process. The toolkit is open source and can be found at https://github.com/lverwimp/tf-lm .",
        "venue": "International Conference on Language Resources and Evaluation",
        "year": 2018,
        "referenceCount": 33,
        "citationCount": 11,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents language modeling scripts based on TensorFlow that allow one to train and test competitive models directly, by using a pre-de\ufb01ned con\ufb01guration or changing it to their needs, thereby avoiding the time and computationally expensive training process."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-05-01",
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Verwimp2018TFLMTL,\n author = {Lyan Verwimp and H. V. hamme and P. Wambacq},\n booktitle = {International Conference on Language Resources and Evaluation},\n title = {TF-LM: TensorFlow-based Language Modeling Toolkit},\n year = {2018}\n}\n"
        }
    },
    "1092_flan-palm_540b": {
        "paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "externalIds": {
            "DBLP": "journals/corr/abs-2210-11416",
            "ArXiv": "2210.11416",
            "DOI": "10.48550/arXiv.2210.11416",
            "CorpusId": 253018554
        },
        "corpusId": 253018554,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
        "title": "Scaling Instruction-Finetuned Language Models",
        "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 106,
        "citationCount": 1466,
        "influentialCitationCount": 210,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.11416",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups, and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation)."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-10-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2210.11416"
        },
        "citationStyles": {
            "bibtex": "@Article{Chung2022ScalingIL,\n author = {Hyung Won Chung and Le Hou and S. Longpre and Barret Zoph and Yi Tay and W. Fedus and Eric Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and S. Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Wei Yu and Vincent Zhao and Yanping Huang and Andrew M. Dai and Hongkun Yu and Slav Petrov and E. Chi and J. Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Scaling Instruction-Finetuned Language Models},\n volume = {abs/2210.11416},\n year = {2022}\n}\n"
        }
    },
    "1093_adam_(cifar-10)": {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "externalIds": {
            "MAG": "2964121744",
            "DBLP": "journals/corr/KingmaB14",
            "ArXiv": "1412.6980",
            "CorpusId": 6628106
        },
        "corpusId": 6628106,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization",
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "referenceCount": 26,
        "citationCount": 130960,
        "influentialCitationCount": 20747,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-12-22",
        "journal": {
            "name": "CoRR",
            "volume": "abs/1412.6980"
        },
        "citationStyles": {
            "bibtex": "@Article{Kingma2014AdamAM,\n author = {Diederik P. Kingma and Jimmy Ba},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Adam: A Method for Stochastic Optimization},\n volume = {abs/1412.6980},\n year = {2014}\n}\n"
        }
    },
    "1094_glove_(6b)": {
        "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
        "externalIds": {
            "DBLP": "conf/emnlp/PenningtonSM14",
            "ACL": "D14-1162",
            "MAG": "2250539671",
            "DOI": "10.3115/v1/D14-1162",
            "CorpusId": 1957433
        },
        "corpusId": 1957433,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5",
        "title": "GloVe: Global Vectors for Word Representation",
        "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2014,
        "referenceCount": 32,
        "citationCount": 28882,
        "influentialCitationCount": 3738,
        "isOpenAccess": true,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods and produces a vector space with meaningful substructure."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-10-01",
        "journal": {
            "pages": "1532-1543"
        },
        "citationStyles": {
            "bibtex": "@Article{Pennington2014GloVeGV,\n author = {Jeffrey Pennington and R. Socher and Christopher D. Manning},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1532-1543},\n title = {GloVe: Global Vectors for Word Representation},\n year = {2014}\n}\n"
        }
    },
    "1096_welm": {
        "paperId": "205eab69e430b4da93ecf3fd9115f0919b448040",
        "externalIds": {
            "ArXiv": "2209.10372",
            "DBLP": "journals/corr/abs-2209-10372",
            "DOI": "10.48550/arXiv.2209.10372",
            "CorpusId": 252407522
        },
        "corpusId": 252407522,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/205eab69e430b4da93ecf3fd9115f0919b448040",
        "title": "WeLM: A Well-Read Pre-trained Language Model for Chinese",
        "abstract": "Large Language Models pre-trained with self-supervised learning have demonstrated impressive zero-shot generalization capabilities on a wide spectrum of tasks. In this work, we present WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations. WeLM is trained with 10B parameters by\"reading\"a curated high-quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly outperform existing pre-trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi-lingual and code-switching understanding, outperforming existing multilingual language models pre-trained on 30 languages. Furthermore, We collected human-written prompts for a large set of supervised datasets in Chinese and fine-tuned WeLM with multi-prompted training. The resulting model can attain strong generalization on unseen types of tasks and outperform the unsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has basic skills at explaining and calibrating the decisions from itself, which can be promising directions for future research. Our models can be applied from https://welm.weixin.qq.com/docs/api/.",
        "venue": "arXiv.org",
        "year": 2022,
        "referenceCount": 85,
        "citationCount": 10,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2209.10372",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations and has basic skills at explaining and calibrating the decisions from itself."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-09-21",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2209.10372"
        },
        "citationStyles": {
            "bibtex": "@Article{Su2022WeLMAW,\n author = {Hui Su and Xiao Zhou and Houjin Yu and Yuwen Chen and Zilin Zhu and Yang Yu and Jie Zhou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {WeLM: A Well-Read Pre-trained Language Model for Chinese},\n volume = {abs/2209.10372},\n year = {2022}\n}\n"
        }
    },
    "1097_madaline_iii": {
        "paperId": "63f5a3a89a94fd1c672317f816cc49bdbdb0697d",
        "externalIds": {
            "DBLP": "journals/pieee/WidrowL90",
            "MAG": "2116424792",
            "DOI": "10.1109/5.58323",
            "CorpusId": 195704643
        },
        "corpusId": 195704643,
        "publicationVenue": {
            "id": "6faaccca-1cc4-45a9-aeb6-96a4901d2606",
            "name": "Proceedings of the IEEE",
            "type": "journal",
            "alternate_names": [
                "Proc IEEE"
            ],
            "issn": "0018-9219",
            "alternate_issns": [
                "1558-2256"
            ],
            "url": "http://www.ieee.org/portal/pages/pubs/proceedings/",
            "alternate_urls": [
                "http://www.ieee.org/products/onlinepubs/pub/about_conference.html",
                "https://ieeexplore.ieee.org/servlet/opac?punumber=5",
                "http://proceedingsoftheieee.ieee.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/63f5a3a89a94fd1c672317f816cc49bdbdb0697d",
        "title": "30 years of adaptive neural networks: perceptron, Madaline, and backpropagation",
        "abstract": "Fundamental developments in feedforward artificial neural networks from the past thirty years are reviewed. The history, origination, operating characteristics, and basic theory of several supervised neural-network training algorithms (including the perceptron rule, the least-mean-square algorithm, three Madaline rules, and the backpropagation technique) are described. The concept underlying these iterative adaptation algorithms is the minimal disturbance principle, which suggests that during training it is advisable to inject new information into a network in a manner that disturbs stored information to the smallest extent possible. The two principal kinds of online rules that have developed for altering the weights of a network are examined for both single-threshold elements and multielement networks. They are error-correction rules, which alter the weights of a network to correct error in the output response to the present input pattern, and gradient rules, which alter the weights of a network during each pattern presentation by gradient descent with the objective of reducing mean-square error (averaged over all training patterns). >",
        "venue": "Proceedings of the IEEE",
        "year": 1990,
        "referenceCount": 137,
        "citationCount": 2362,
        "influentialCitationCount": 82,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The history, origination, operating characteristics, and basic theory of several supervised neural-network training algorithms (including the perceptron rule, the least-mean-square algorithm, three Madaline rules, and the backpropagation technique) are described."
        },
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "publicationDate": "1990-09-01",
        "journal": {
            "name": "Proc. IEEE",
            "pages": "1415-1442",
            "volume": "78"
        },
        "citationStyles": {
            "bibtex": "@Article{Widrow199030YO,\n author = {B. Widrow and Michael A. Lehr},\n booktitle = {Proceedings of the IEEE},\n journal = {Proc. IEEE},\n pages = {1415-1442},\n title = {30 years of adaptive neural networks: perceptron, Madaline, and backpropagation},\n volume = {78},\n year = {1990}\n}\n"
        }
    },
    "1098_simplenet": {
        "paperId": "9356de29250603ca08eeb75b87712016c8a2bb2b",
        "externalIds": {
            "DBLP": "journals/corr/HasanPourRVS16",
            "ArXiv": "1608.06037",
            "MAG": "2616274621",
            "CorpusId": 3388909
        },
        "corpusId": 3388909,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/9356de29250603ca08eeb75b87712016c8a2bb2b",
        "title": "Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",
        "abstract": "Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded system or system with computational and memory limitations. We achieved state-of-the-art result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and competitive results on CIFAR100 and SVHN. Models are made available at: this https URL",
        "venue": "arXiv.org",
        "year": 2016,
        "referenceCount": 61,
        "citationCount": 102,
        "influentialCitationCount": 10,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a simple architecture, called SimpleNet, based on a set of designing principles, with which it empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-08-22",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1608.06037"
        },
        "citationStyles": {
            "bibtex": "@Article{HasanPour2016LetsKI,\n author = {S. H. HasanPour and Mohammad Rouhani and Mohsen Fayyaz and M. Sabokrou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures},\n volume = {abs/1608.06037},\n year = {2016}\n}\n"
        }
    },
    "1099_rnn_for_speech": {
        "paperId": "88c19ad27b6f3d8026a955c7298d1a7e05e93cb6",
        "externalIds": {
            "MAG": "2106564373",
            "DBLP": "journals/taslp/ChenHW98",
            "DOI": "10.1109/89.668817",
            "CorpusId": 18250581
        },
        "corpusId": 18250581,
        "publicationVenue": {
            "id": "cd5799dd-1165-414f-87b0-ea5e184781c0",
            "name": "IEEE Transactions on Speech and Audio Processing",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Speech Audio Process"
            ],
            "issn": "1063-6676",
            "alternate_issns": [
                "1558-2353"
            ],
            "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=89",
            "alternate_urls": [
                "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=89"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/88c19ad27b6f3d8026a955c7298d1a7e05e93cb6",
        "title": "An RNN-based prosodic information synthesizer for Mandarin text-to-speech",
        "abstract": "A new RNN-based prosodic information synthesizer for Mandarin Chinese text-to-speech (TTS) is proposed in this paper. Its four-layer recurrent neural network (RNN) generates prosodic information such as syllable pitch contours, syllable energy levels, syllable initial and final durations, as well as intersyllable pause durations. The input layer and first hidden layer operate with a word-synchronized clock to represent current-word phonologic states within the prosodic structure of text to be synthesized. The second hidden layer and output layer operate on a syllable-synchronized clock and use outputs from the preceding layers, along with additional syllable-level inputs fed directly to the second hidden layer, to generate desired prosodic parameters. The RNN was trained on a large set of actual utterances accompanied by associated texts, and can automatically learn many human-prosody phonologic rules, including the well-known Sandhi Tone 3 F0-change rule. Experimental results show that all synthesized prosodic parameter sequences matched quite well with their original counterparts, and a pitch-synchronous-overlap-add-based (PSOLA-based) Mandarin TTS system was also used for testing of our approach. While subjective tests are difficult to perform and remain to be done in the future, we have carried out informal listening tests by a significant number of native Chinese speakers and the results confirmed that all synthesized speech sounded quite natural.",
        "venue": "IEEE Transactions on Speech and Audio Processing",
        "year": 1998,
        "referenceCount": 66,
        "citationCount": 189,
        "influentialCitationCount": 12,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Experimental results show that all synthesized prosodic parameter sequences matched quite well with their original counterparts, and a pitch-synchronous-overlap-add-based (PSOLA-based) Mandarin TTS system was also used for testing of the approach."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1998-05-01",
        "journal": {
            "name": "IEEE Trans. Speech Audio Process.",
            "pages": "226-239",
            "volume": "6"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen1998AnRP,\n author = {Sin-Horng Chen and Shaw-Hwa Hwang and Yih-Ru Wang},\n booktitle = {IEEE Transactions on Speech and Audio Processing},\n journal = {IEEE Trans. Speech Audio Process.},\n pages = {226-239},\n title = {An RNN-based prosodic information synthesizer for Mandarin text-to-speech},\n volume = {6},\n year = {1998}\n}\n"
        }
    },
    "1100_multiband_diffusion": {
        "paperId": "bca9507a2f6bd3379db34ab2ef4295dbdf503a67",
        "externalIds": {
            "ArXiv": "2308.02560",
            "DBLP": "journals/corr/abs-2308-02560",
            "DOI": "10.48550/arXiv.2308.02560",
            "CorpusId": 260682832
        },
        "corpusId": 260682832,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/bca9507a2f6bd3379db34ab2ef4295dbdf503a67",
        "title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion",
        "abstract": "Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "referenceCount": 51,
        "citationCount": 1,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2308.02560",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a high-fidelity multi-band diffusion-based framework that generates any type of audio modality from low-bitrate discrete representations and outperforms state-of-the-art generative techniques in terms of perceptual quality."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-08-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2308.02560"
        },
        "citationStyles": {
            "bibtex": "@Article{Roman2023FromDT,\n author = {Robin San Roman and Yossi Adi and Antoine Deleforge and R. Serizel and Gabriel Synnaeve and Alexandre D'efossez},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion},\n volume = {abs/2308.02560},\n year = {2023}\n}\n"
        }
    },
    "1102_mistral_7b": {
        "paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
        "externalIds": {
            "ArXiv": "2310.06825",
            "DBLP": "journals/corr/abs-2310-06825",
            "DOI": "10.48550/arXiv.2310.06825",
            "CorpusId": 263830494
        },
        "corpusId": 263830494,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/db633c6b1c286c0386f0078d8a2e6224e03a6227",
        "title": "Mistral 7B",
        "abstract": "We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 29,
        "citationCount": 181,
        "influentialCitationCount": 30,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.06825",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency, which leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-10-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2310.06825"
        },
        "citationStyles": {
            "bibtex": "@Article{Jiang2023Mistral7,\n author = {Albert Qiaochu Jiang and Alexandre Sablayrolles and A. Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de Las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L'elio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth\u00e9e Lacroix and William El Sayed},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Mistral 7B},\n volume = {abs/2310.06825},\n year = {2023}\n}\n"
        }
    },
    "1103_unipi": {
        "paperId": "da2fe6cd385194b0274d04d04ee72e8caf3854d4",
        "externalIds": {
            "DBLP": "journals/corr/abs-2302-00111",
            "ArXiv": "2302.00111",
            "DOI": "10.48550/arXiv.2302.00111",
            "CorpusId": 256459809
        },
        "corpusId": 256459809,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/da2fe6cd385194b0274d04d04ee72e8caf3854d4",
        "title": "Learning Universal Policies via Text-Guided Video Generation",
        "abstract": "A goal of artificial intelligence is to construct an agent that can solve a wide variety of tasks. Recent progress in text-guided image synthesis has yielded models with an impressive ability to generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by this success, we investigate whether such tools can be used to construct more general-purpose agents. Specifically, we cast the sequential decision making problem as a text-conditioned video generation problem, where, given a text-encoded specification of a desired goal, a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video. By leveraging text as the underlying goal specification, we are able to naturally and combinatorially generalize to novel goals. The proposed policy-as-video formulation can further represent environments with different state and action spaces in a unified space of images, which, for example, enables learning and generalization across a variety of robot manipulation tasks. Finally, by leveraging pretrained language embeddings and widely available videos from the internet, the approach enables knowledge transfer through predicting highly realistic video plans for real robots.",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "referenceCount": 51,
        "citationCount": 64,
        "influentialCitationCount": 8,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.00111",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work casts the sequential decision making problem as a text-conditioned video generation problem, where a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-01-31",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2302.00111"
        },
        "citationStyles": {
            "bibtex": "@Article{Du2023LearningUP,\n author = {Yilun Du and Mengjiao Yang and Bo Dai and H. Dai and Ofir Nachum and J. Tenenbaum and D. Schuurmans and P. Abbeel},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Learning Universal Policies via Text-Guided Video Generation},\n volume = {abs/2302.00111},\n year = {2023}\n}\n"
        }
    },
    "1104_clip_(resnet-50)": {
        "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "externalIds": {
            "ArXiv": "2103.00020",
            "DBLP": "conf/icml/RadfordKHRGASAM21",
            "CorpusId": 231591445
        },
        "corpusId": 231591445,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision",
        "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "referenceCount": 222,
        "citationCount": 11731,
        "influentialCitationCount": 3605,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is demonstrated that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-02-26",
        "journal": {
            "pages": "8748-8763"
        },
        "citationStyles": {
            "bibtex": "@Article{Radford2021LearningTV,\n author = {Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and I. Sutskever},\n booktitle = {International Conference on Machine Learning},\n pages = {8748-8763},\n title = {Learning Transferable Visual Models From Natural Language Supervision},\n year = {2021}\n}\n"
        }
    },
    "1105_iss": {
        "paperId": "ca1060c50642f9f05735d3007873439347b3bea5",
        "externalIds": {
            "MAG": "2951835141",
            "ArXiv": "1709.05027",
            "DBLP": "journals/corr/abs-1709-05027",
            "CorpusId": 36483539
        },
        "corpusId": 36483539,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/ca1060c50642f9f05735d3007873439347b3bea5",
        "title": "Learning Intrinsic Sparse Structures within Long Short-term Memory",
        "abstract": "Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Based on group Lasso regularization, our method achieves 10.59x speedup without losing any perplexity of a language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to non- LSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is publicly available at this https URL",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "referenceCount": 39,
        "citationCount": 135,
        "influentialCitationCount": 28,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work aims to learn structurally-sparse Long Short-Term Memory by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs, by proposing Intrinsic Sparse Structures (ISS) in LSTMs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-09-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1709.05027"
        },
        "citationStyles": {
            "bibtex": "@Article{Wen2017LearningIS,\n author = {W. Wen and Yuxiong He and Samyam Rajbhandari and Minjia Zhang and Wenhan Wang and Fang Liu and Bin Hu and Yiran Chen and H. Li},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning Intrinsic Sparse Structures within Long Short-term Memory},\n volume = {abs/1709.05027},\n year = {2017}\n}\n"
        }
    },
    "1106_adversarial_joint_adaptation_network_(resnet)": {
        "paperId": "ae9e5e72aefd19b81c1fe75d7baf6c0bedad75e5",
        "externalIds": {
            "MAG": "2408201877",
            "ArXiv": "1605.06636",
            "DBLP": "conf/icml/LongZ0J17",
            "CorpusId": 12757870
        },
        "corpusId": 12757870,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/ae9e5e72aefd19b81c1fe75d7baf6c0bedad75e5",
        "title": "Deep Transfer Learning with Joint Adaptation Networks",
        "abstract": "Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "referenceCount": 46,
        "citationCount": 2021,
        "influentialCitationCount": 272,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "JAN is presented, which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-05-21",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1605.06636"
        },
        "citationStyles": {
            "bibtex": "@Article{Long2016DeepTL,\n author = {Mingsheng Long and Hanhua Zhu and Jianmin Wang and Michael I. Jordan},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Deep Transfer Learning with Joint Adaptation Networks},\n volume = {abs/1605.06636},\n year = {2016}\n}\n"
        }
    },
    "1107_flamingo": {
        "paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
        "externalIds": {
            "ArXiv": "2204.14198",
            "DBLP": "journals/corr/abs-2204-14198",
            "CorpusId": 248476411
        },
        "corpusId": 248476411,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
        "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
        "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "referenceCount": 182,
        "citationCount": 1445,
        "influentialCitationCount": 182,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces Flamingo, a family of Visual Language Models (VLM) with this ability to bridge powerful pretrained vision-only and language-only models, handle sequences of arbitrarily interleaved visual and textual data, and seamlessly ingest images or videos as inputs."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-04-29",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2204.14198"
        },
        "citationStyles": {
            "bibtex": "@Article{Alayrac2022FlamingoAV,\n author = {Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and A. Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and O. Vinyals and Andrew Zisserman and K. Simonyan},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Flamingo: a Visual Language Model for Few-Shot Learning},\n volume = {abs/2204.14198},\n year = {2022}\n}\n"
        }
    },
    "1108_codet5-base": {
        "paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896",
        "externalIds": {
            "DBLP": "conf/emnlp/0034WJH21",
            "ACL": "2021.emnlp-main.685",
            "ArXiv": "2109.00859",
            "DOI": "10.18653/v1/2021.emnlp-main.685",
            "CorpusId": 237386541
        },
        "corpusId": 237386541,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/a30f912f8c5e2a2bfb06351d4578e1ba3fa37896",
        "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
        "abstract": "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "referenceCount": 37,
        "citationCount": 789,
        "influentialCitationCount": 206,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.685.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-09-02",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2109.00859"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2021CodeT5IU,\n author = {Yue Wang and Weishi Wang and Shafiq R. Joty and S. Hoi},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},\n volume = {abs/2109.00859},\n year = {2021}\n}\n"
        }
    },
    "1109_pangu-\u03b1": {
        "paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f",
        "externalIds": {
            "DBLP": "journals/corr/abs-2104-12369",
            "MAG": "3158631574",
            "ArXiv": "2104.12369",
            "CorpusId": 233394012
        },
        "corpusId": 233394012,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f",
        "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation",
        "abstract": "Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with \\textit{few-shot in-context} learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-$\\alpha$, with up to 200 billion parameters. PanGu-$\\alpha$ is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-$\\alpha$, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-$\\alpha$ in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-$\\alpha$ in performing various tasks under few-shot or zero-shot settings.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 42,
        "citationCount": 154,
        "influentialCitationCount": 18,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The experimental results demonstrate the superior capabilities of PanGu-\u03b1 in performing various tasks under few-shot or zero-shot settings and investigate the effect of model scales on the few- shot performances across a broad range of Chinese NLP tasks."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-04-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2104.12369"
        },
        "citationStyles": {
            "bibtex": "@Article{Zeng2021PanGu\u03b1LA,\n author = {Wei Zeng and Xiaozhe Ren and Teng Su and Hui Wang and Yi Liao and Zhiwei Wang and Xin Jiang and ZhenZhang Yang and Kaisheng Wang and Xiaoda Zhang and Chen Li and Ziyan Gong and Yifan Yao and Xinjing Huang and Jun Wang and Jianfeng Yu and Qiwei Guo and Yue Yu and Yan Zhang and Jin Wang and Heng Tao and Dasen Yan and Z. Yi and Fang Peng and Fan Jiang and Han Zhang and Lingfeng Deng and Yehong Zhang and Zhengping Lin and Chao Zhang and Shaojie Zhang and Mingyue Guo and Shanzhi Gu and Gaojun Fan and Yaowei Wang and Xuefeng Jin and Qun Liu and Yonghong Tian},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation},\n volume = {abs/2104.12369},\n year = {2021}\n}\n"
        }
    },
    "1110_st-moe": {
        "paperId": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
        "externalIds": {
            "ArXiv": "2202.08906",
            "CorpusId": 248496391
        },
        "corpusId": 248496391,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
        "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models",
        "abstract": "Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).",
        "venue": "",
        "year": 2022,
        "referenceCount": 91,
        "citationCount": 52,
        "influentialCitationCount": 9,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work scales a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B), and for the first time achieves state-of theart performance in transfer learning."
        },
        "publicationTypes": null,
        "publicationDate": "2022-02-17",
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{Zoph2022STMoEDS,\n author = {Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and J. Dean and Noam M. Shazeer and W. Fedus},\n title = {ST-MoE: Designing Stable and Transferable Sparse Expert Models},\n year = {2022}\n}\n"
        }
    },
    "1111_lstm+noise(beta)": {
        "paperId": "672f28e2772b7d7895c5ce08ccd07eac3e60219e",
        "externalIds": {
            "DBLP": "conf/icml/DiengRAB18",
            "MAG": "2952556607",
            "ArXiv": "1805.01500",
            "CorpusId": 3276568
        },
        "corpusId": 3276568,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/672f28e2772b7d7895c5ce08ccd07eac3e60219e",
        "title": "Noisin: Unbiased Regularization for Recurrent Neural Networks",
        "abstract": "Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "referenceCount": 49,
        "citationCount": 20,
        "influentialCitationCount": 4,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data, which preserves the underlying RNN on average."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-05-03",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1805.01500"
        },
        "citationStyles": {
            "bibtex": "@Article{Dieng2018NoisinUR,\n author = {Adji B. Dieng and R. Ranganath and Jaan Altosaar and D. Blei},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Noisin: Unbiased Regularization for Recurrent Neural Networks},\n volume = {abs/1805.01500},\n year = {2018}\n}\n"
        }
    },
    "1112_differentiable_neural_computer": {
        "paperId": "784ee73d5363c711118f784428d1ab89f019daa5",
        "externalIds": {
            "MAG": "2530887700",
            "DBLP": "journals/nature/GravesWRHDGCGRA16",
            "DOI": "10.1038/nature20101",
            "CorpusId": 205251479,
            "PubMed": "27732574"
        },
        "corpusId": 205251479,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/784ee73d5363c711118f784428d1ab89f019daa5",
        "title": "Hybrid computing using a neural network with dynamic external memory",
        "abstract": null,
        "venue": "Nature",
        "year": 2016,
        "referenceCount": 54,
        "citationCount": 1438,
        "influentialCitationCount": 150,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ora.ox.ac.uk/objects/uuid:dd8473bd-2d70-424d-881b-86d9c9c66b51/download_file?safe_filename=outline.pdf&file_format=application%2Fpdf&type_of_work=Journal+article",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-10-12",
        "journal": {
            "name": "Nature",
            "pages": "471-476",
            "volume": "538"
        },
        "citationStyles": {
            "bibtex": "@Article{Graves2016HybridCU,\n author = {Alex Graves and Greg Wayne and Malcolm Reynolds and Tim Harley and Ivo Danihelka and A. Grabska-Barwinska and Sergio Gomez Colmenarejo and Edward Grefenstette and Tiago Ramalho and J. Agapiou and Adri\u00e0 Puigdom\u00e8nech Badia and K. Hermann and Yori Zwols and Georg Ostrovski and Adam Cain and Helen King and C. Summerfield and Phil Blunsom and K. Kavukcuoglu and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {471-476},\n title = {Hybrid computing using a neural network with dynamic external memory},\n volume = {538},\n year = {2016}\n}\n"
        }
    },
    "1113_alphago_lee": {
        "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
        "externalIds": {
            "DBLP": "journals/nature/SilverHMGSDSAPL16",
            "MAG": "2257979135",
            "DOI": "10.1038/nature16961",
            "CorpusId": 515925,
            "PubMed": "26819042"
        },
        "corpusId": 515925,
        "publicationVenue": {
            "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
            "name": "Nature",
            "type": "journal",
            "issn": "0028-0836",
            "url": "https://www.nature.com/",
            "alternate_urls": [
                "http://www.nature.com/nature/",
                "https://www.nature.com/nature/",
                "http://www.nature.com/nature/archive/index.html"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490",
        "title": "Mastering the game of Go with deep neural networks and tree search",
        "abstract": null,
        "venue": "Nature",
        "year": 2016,
        "referenceCount": 72,
        "citationCount": 14771,
        "influentialCitationCount": 522,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.nature.com/articles/nature16961.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Medicine",
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Using this search algorithm, the program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0.5, the first time that a computer program has defeated a human professional player in the full-sized game of Go."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2016-01-27",
        "journal": {
            "name": "Nature",
            "pages": "484-489",
            "volume": "529"
        },
        "citationStyles": {
            "bibtex": "@Article{Silver2016MasteringTG,\n author = {David Silver and Aja Huang and Chris J. Maddison and A. Guez and L. Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and S. Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and I. Sutskever and T. Lillicrap and M. Leach and K. Kavukcuoglu and T. Graepel and D. Hassabis},\n booktitle = {Nature},\n journal = {Nature},\n pages = {484-489},\n title = {Mastering the game of Go with deep neural networks and tree search},\n volume = {529},\n year = {2016}\n}\n"
        }
    },
    "1114_mask_r-cnn": {
        "paperId": "1a0912bb76777469295bb2c059faee907e7f3258",
        "externalIds": {
            "ArXiv": "1703.06870",
            "CorpusId": 54465873
        },
        "corpusId": 54465873,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/1a0912bb76777469295bb2c059faee907e7f3258",
        "title": "Mask R-CNN",
        "abstract": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.",
        "venue": "",
        "year": 2017,
        "referenceCount": 40,
        "citationCount": 21388,
        "influentialCitationCount": 3551,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents a conceptually simple, flexible, and general framework for object instance segmentation, which extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition."
        },
        "publicationTypes": null,
        "publicationDate": "2017-03-20",
        "journal": null,
        "citationStyles": {
            "bibtex": "@Inproceedings{He2017MaskR,\n author = {Kaiming He and Georgia Gkioxari and Piotr Doll\u00e1r and Ross B. Girshick},\n title = {Mask R-CNN},\n year = {2017}\n}\n"
        }
    },
    "1115_dropout-lstm+noise(bernoulli)_(wt2)": {
        "paperId": "672f28e2772b7d7895c5ce08ccd07eac3e60219e",
        "externalIds": {
            "DBLP": "conf/icml/DiengRAB18",
            "MAG": "2952556607",
            "ArXiv": "1805.01500",
            "CorpusId": 3276568
        },
        "corpusId": 3276568,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/672f28e2772b7d7895c5ce08ccd07eac3e60219e",
        "title": "Noisin: Unbiased Regularization for Recurrent Neural Networks",
        "abstract": "Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "referenceCount": 49,
        "citationCount": 20,
        "influentialCitationCount": 4,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data, which preserves the underlying RNN on average."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-05-03",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1805.01500"
        },
        "citationStyles": {
            "bibtex": "@Article{Dieng2018NoisinUR,\n author = {Adji B. Dieng and R. Ranganath and Jaan Altosaar and D. Blei},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Noisin: Unbiased Regularization for Recurrent Neural Networks},\n volume = {abs/1805.01500},\n year = {2018}\n}\n"
        }
    },
    "1116_emdr": {
        "paperId": "6d4a9f1c41b078846901362ba0dce8295dd6a2a8",
        "externalIds": {
            "ArXiv": "2106.05346",
            "DBLP": "journals/corr/abs-2106-05346",
            "CorpusId": 235390519
        },
        "corpusId": 235390519,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/6d4a9f1c41b078846901362ba0dce8295dd6a2a8",
        "title": "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering",
        "abstract": "We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than staged-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3% absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "referenceCount": 57,
        "citationCount": 101,
        "influentialCitationCount": 12,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers and demonstrates the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-06-09",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2106.05346"
        },
        "citationStyles": {
            "bibtex": "@Article{Sachan2021EndtoEndTO,\n author = {Devendra Singh Sachan and Siva Reddy and William Hamilton and Chris Dyer and Dani Yogatama},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering},\n volume = {abs/2106.05346},\n year = {2021}\n}\n"
        }
    },
    "1117_discriminator_guidance": {
        "paperId": "4ff52e24e02116b675abf085642b0de38f30b1eb",
        "externalIds": {
            "DBLP": "conf/icml/KimKKKM23",
            "ArXiv": "2211.17091",
            "DOI": "10.48550/arXiv.2211.17091",
            "CorpusId": 254096299
        },
        "corpusId": 254096299,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/4ff52e24e02116b675abf085642b0de38f30b1eb",
        "title": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
        "abstract": "The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "referenceCount": 77,
        "citationCount": 39,
        "influentialCitationCount": 5,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2211.17091",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-11-28",
        "journal": {
            "pages": "16567-16598"
        },
        "citationStyles": {
            "bibtex": "@Article{Kim2022RefiningGP,\n author = {Dongjun Kim and Yeongmin Kim and Wanmo Kang and Il-Chul Moon},\n booktitle = {International Conference on Machine Learning},\n pages = {16567-16598},\n title = {Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models},\n year = {2022}\n}\n"
        }
    },
    "1118_musicgen": {
        "paperId": "4cc8e18f5eece0b0d8e1abcb8ee10fb33680fbb2",
        "externalIds": {
            "DBLP": "journals/corr/abs-2306-05284",
            "ArXiv": "2306.05284",
            "DOI": "10.48550/arXiv.2306.05284",
            "CorpusId": 259108357
        },
        "corpusId": 259108357,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/4cc8e18f5eece0b0d8e1abcb8ee10fb33680fbb2",
        "title": "Simple and Controllable Music Generation",
        "abstract": "We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft.",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "referenceCount": 48,
        "citationCount": 69,
        "influentialCitationCount": 19,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.05284",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces MusicGen, a single Language Model that operates over several streams of compressed discrete music representation, i.e., tokens, and demonstrates how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-06-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2306.05284"
        },
        "citationStyles": {
            "bibtex": "@Article{Copet2023SimpleAC,\n author = {Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre D'efossez},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Simple and Controllable Music Generation},\n volume = {abs/2306.05284},\n year = {2023}\n}\n"
        }
    },
    "1119_transformer-xl_large": {
        "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
        "externalIds": {
            "ArXiv": "1901.02860",
            "DBLP": "conf/acl/DaiYYCLS19",
            "ACL": "P19-1285",
            "MAG": "2911109671",
            "DOI": "10.18653/v1/P19-1285",
            "CorpusId": 57759363
        },
        "corpusId": 57759363,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
        "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
        "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "referenceCount": 71,
        "citationCount": 2990,
        "influentialCitationCount": 379,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1285.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-01-09",
        "journal": {
            "pages": "2978-2988"
        },
        "citationStyles": {
            "bibtex": "@Article{Dai2019TransformerXLAL,\n author = {Zihang Dai and Zhilin Yang and Yiming Yang and J. Carbonell and Quoc V. Le and R. Salakhutdinov},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2978-2988},\n title = {Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},\n year = {2019}\n}\n"
        }
    },
    "1120_probabilistic_modeling_for_object_recognition": {
        "paperId": "bbe488bb190d75f4b665d43e306bcab1ab228890",
        "externalIds": {
            "DBLP": "conf/cvpr/SchneidermanK98",
            "MAG": "2166713160",
            "DOI": "10.1109/CVPR.1998.698586",
            "CorpusId": 1060186
        },
        "corpusId": 1060186,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/bbe488bb190d75f4b665d43e306bcab1ab228890",
        "title": "Probabilistic modeling of local appearance and spatial relationships for object recognition",
        "abstract": "In this paper, we describe an algorithm for object recognition that explicitly models and estimated the posterior probability function, P(object/image). We have chosen a functional form of the posterior probability function that captures the joint statistics of local appearance and position on the object as well as the statistics of local appearance in the visual world at large. We use a discrete representation of local appearance consisting of approximately 10/sup 6/ patterns. We compute an estimate of P(object/image) in closed form by counting the frequency of occurrence of these patterns over various sets of training images. We have used this method for detecting human faces from frontal and profile views. The algorithm for frontal views has shown a detection rate of 93.0% with 88 false alarms on a set of 125 images containing 483 faces combining the MIT test set of Sung and Poggio with the CMU test sets of Rowley, Baluja, and Kanade. The algorithm for detection of profile views has also demonstrated promising results.",
        "venue": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)",
        "year": 1998,
        "referenceCount": 14,
        "citationCount": 454,
        "influentialCitationCount": 31,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.cs.cmu.edu/afs/cs.cmu.edu/user/hws/www/CVPR98.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "An algorithm for object recognition that explicitly models and estimated the posterior probability function, P(object/image) in closed form is described, which captures the joint statistics of local appearance and position on the object as well as the statistics ofLocal appearance in the visual world at large."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "1998-06-23",
        "journal": {
            "name": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)",
            "pages": "45-51"
        },
        "citationStyles": {
            "bibtex": "@Article{Schneiderman1998ProbabilisticMO,\n author = {Henry Schneiderman and T. Kanade},\n booktitle = {Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)},\n journal = {Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)},\n pages = {45-51},\n title = {Probabilistic modeling of local appearance and spatial relationships for object recognition},\n year = {1998}\n}\n"
        }
    },
    "1121_mt-dnn": {
        "paperId": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac",
        "externalIds": {
            "MAG": "2963854351",
            "ACL": "P19-1441",
            "ArXiv": "1901.11504",
            "DBLP": "journals/corr/abs-1901-11504",
            "DOI": "10.18653/v1/P19-1441",
            "CorpusId": 59523594
        },
        "corpusId": 59523594,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/658721bc13b0fa97366d38c05a96bf0a9f4bb0ac",
        "title": "Multi-Task Deep Neural Networks for Natural Language Understanding",
        "abstract": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "referenceCount": 35,
        "citationCount": 1096,
        "influentialCitationCount": 227,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1441.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks that allows domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-01-31",
        "journal": {
            "pages": "4487-4496"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2019MultiTaskDN,\n author = {Xiaodong Liu and Pengcheng He and Weizhu Chen and Jianfeng Gao},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {4487-4496},\n title = {Multi-Task Deep Neural Networks for Natural Language Understanding},\n year = {2019}\n}\n"
        }
    },
    "1122_deq-transformer_(medium,_adaptive_embedding)": {
        "paperId": "9a618cca0d2fc78db1be1aed70517401cb3f3859",
        "externalIds": {
            "MAG": "2970900903",
            "DBLP": "journals/corr/abs-1909-01377",
            "ArXiv": "1909.01377",
            "CorpusId": 202539738
        },
        "corpusId": 202539738,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/9a618cca0d2fc78db1be1aed70517401cb3f3859",
        "title": "Deep Equilibrium Models",
        "abstract": "We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective \u201cdepth\u201d of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88% memory reduction in our experiments. The code is available at https://github.com/locuslab/deq.",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "referenceCount": 54,
        "citationCount": 492,
        "influentialCitationCount": 86,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that DEQs often improve performance over these state-of-the-art models (for similar parameter counts); have similar computational requirements to existing models; and vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88% memory reduction in the authors' experiments."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-09-01",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1909.01377"
        },
        "citationStyles": {
            "bibtex": "@Article{Bai2019DeepEM,\n author = {Shaojie Bai and J. Z. Kolter and V. Koltun},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Deep Equilibrium Models},\n volume = {abs/1909.01377},\n year = {2019}\n}\n"
        }
    },
    "1124_meena": {
        "paperId": "d08463bd665589d04619f04dbde84183ffcf2e63",
        "externalIds": {
            "ArXiv": "2001.09977",
            "DBLP": "journals/corr/abs-2001-09977",
            "MAG": "3000779003",
            "CorpusId": 210920238
        },
        "corpusId": 210920238,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/d08463bd665589d04619f04dbde84183ffcf2e63",
        "title": "Towards a Human-like Open-Domain Chatbot",
        "abstract": "We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",
        "venue": "arXiv.org",
        "year": 2020,
        "referenceCount": 87,
        "citationCount": 786,
        "influentialCitationCount": 90,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations, is presented and a human evaluation metric called Sensibleness and Specificity Average (SSA) is proposed, which captures key elements of a human-like multi- turn conversation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2020-01-27",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2001.09977"
        },
        "citationStyles": {
            "bibtex": "@Article{Freitas2020TowardsAH,\n author = {Daniel De Freitas and Minh-Thang Luong and David R. So and Jamie Hall and Noah Fiedel and R. Thoppilan and Zi Yang and Apoorv Kulshreshtha and Gaurav Nemade and Yifeng Lu and Quoc V. Le},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Towards a Human-like Open-Domain Chatbot},\n volume = {abs/2001.09977},\n year = {2020}\n}\n"
        }
    },
    "1125_imagebind": {
        "paperId": "7dc6da87eaa6f830354feb2db14023cab8678c91",
        "externalIds": {
            "DBLP": "journals/corr/abs-2305-05665",
            "ArXiv": "2305.05665",
            "DOI": "10.1109/CVPR52729.2023.01457",
            "CorpusId": 258564264
        },
        "corpusId": 258564264,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/7dc6da87eaa6f830354feb2db14023cab8678c91",
        "title": "ImageBind One Embedding Space to Bind Them All",
        "abstract": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications \u2018out-of-the-box\u2019 including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2023,
        "referenceCount": 87,
        "citationCount": 255,
        "influentialCitationCount": 46,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.05665",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that all combinations of paired data are not necessary to train a joint embedding, and only image-paired data is sufficient to bind the modalities together, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-05-09",
        "journal": {
            "name": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "15180-15190"
        },
        "citationStyles": {
            "bibtex": "@Article{Girdhar2023ImageBindOE,\n author = {Rohit Girdhar and Alaaeldin El-Nouby and Zhuang Liu and Mannat Singh and Kalyan Vasudev Alwala and Armand Joulin and Ishan Misra},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {15180-15190},\n title = {ImageBind One Embedding Space to Bind Them All},\n year = {2023}\n}\n"
        }
    },
    "1126_decaying_fast_weights_transformer": {
        "paperId": "f6d8beb02771791d628f7e0773d8906261ce707c",
        "externalIds": {
            "ArXiv": "2210.04243",
            "ACL": "2022.emnlp-main.697",
            "DBLP": "conf/emnlp/Mao22",
            "DOI": "10.48550/arXiv.2210.04243",
            "CorpusId": 252781097
        },
        "corpusId": 252781097,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/f6d8beb02771791d628f7e0773d8906261ce707c",
        "title": "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights",
        "abstract": "Autoregressive Transformers are strong language models but incur O(T) complexity during per-token generation due to the self-attention mechanism. Recent work proposes kernel-based methods to approximate causal self-attention by replacing it with recurrent formulations with various update rules and feature maps to achieve O(1) time and memory complexity. We explore these approaches and find that they are unnecessarily complex, and propose a simple alternative - decaying fast weights - that runs fast on GPU, outperforms prior methods, and retains 99% of attention\u2019s performance for GPT-2. We also show competitive performance on WikiText-103 against more complex attention substitutes.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2022,
        "referenceCount": 40,
        "citationCount": 3,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.04243",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work explores kernel-based methods to approximate causal self-attention by replacing it with recurrent formulations with various update rules and feature maps and proposes a simple alternative that runs fast on GPU, outperforms prior methods, and retains 99% of attention\u2019s performance for GPT-2."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-10-09",
        "journal": {
            "pages": "10236-10242"
        },
        "citationStyles": {
            "bibtex": "@Article{Mao2022FineTuningPT,\n author = {H. H. Mao},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {10236-10242},\n title = {Fine-Tuning Pre-trained Transformers into Decaying Fast Weights},\n year = {2022}\n}\n"
        }
    },
    "1128_pythia-12b": {
        "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "externalIds": {
            "DBLP": "conf/icml/BidermanSABOHKP23",
            "ArXiv": "2304.01373",
            "DOI": "10.48550/arXiv.2304.01373",
            "CorpusId": 257921893
        },
        "corpusId": 257921893,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/be55e8ec4213868db08f2c3168ae666001bea4b8",
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
        "abstract": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}.",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "referenceCount": 101,
        "citationCount": 380,
        "influentialCitationCount": 45,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2304.01373",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters is introduced, demonstrating that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2023-04-03",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2304.01373"
        },
        "citationStyles": {
            "bibtex": "@Article{Biderman2023PythiaAS,\n author = {Stella Biderman and Hailey Schoelkopf and Quentin G. Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},\n volume = {abs/2304.01373},\n year = {2023}\n}\n"
        }
    },
    "1129_tf-lm-discourse_lstm_(ptb)": {
        "paperId": "daf6e4731470f6dbec9508d7ebdf026dd8527f26",
        "externalIds": {
            "ACL": "L18-1470",
            "DBLP": "conf/lrec/VerwimphW18",
            "MAG": "2806989935",
            "CorpusId": 21710652
        },
        "corpusId": 21710652,
        "publicationVenue": {
            "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
            "name": "International Conference on Language Resources and Evaluation",
            "type": "conference",
            "alternate_names": [
                "LREC",
                "Int Conf Lang Resour Evaluation"
            ],
            "url": "http://www.lrec-conf.org/"
        },
        "url": "https://www.semanticscholar.org/paper/daf6e4731470f6dbec9508d7ebdf026dd8527f26",
        "title": "TF-LM: TensorFlow-based Language Modeling Toolkit",
        "abstract": "Recently, an abundance of deep learning toolkits has been made freely available. These toolkits typically offer the building blocks and sometimes simple example scripts, but designing and training a model still takes a considerable amount of time and knowledge. We present language modeling scripts based on TensorFlow that allow one to train and test competitive models directly, by using a pre-de\ufb01ned con\ufb01guration or changing it to their needs. There are several options for input features (words, characters, words combined with characters, character n -grams) and for batching (sentence-or discourse-level). The models can be used to test the perplexity, predict the next word(s), re-score hypotheses or generate debugging \ufb01les for interpolation with n -gram models. Additionally, we make available LSTM language models trained on a variety of Dutch texts and English benchmarks, that can be used immediately, thereby avoiding the time and computationally expensive training process. The toolkit is open source and can be found at https://github.com/lverwimp/tf-lm .",
        "venue": "International Conference on Language Resources and Evaluation",
        "year": 2018,
        "referenceCount": 33,
        "citationCount": 11,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents language modeling scripts based on TensorFlow that allow one to train and test competitive models directly, by using a pre-de\ufb01ned con\ufb01guration or changing it to their needs, thereby avoiding the time and computationally expensive training process."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-05-01",
        "journal": {
            "name": "",
            "volume": ""
        },
        "citationStyles": {
            "bibtex": "@Article{Verwimp2018TFLMTL,\n author = {Lyan Verwimp and H. V. hamme and P. Wambacq},\n booktitle = {International Conference on Language Resources and Evaluation},\n title = {TF-LM: TensorFlow-based Language Modeling Toolkit},\n year = {2018}\n}\n"
        }
    },
    "1130_gpt-2_(1.5b,_curriculum_learning_45k)": {
        "paperId": "d931f84abfc4550c10ceb113b142c8eb3e07571e",
        "externalIds": {
            "DBLP": "journals/corr/abs-2108-06084",
            "CorpusId": 237048462
        },
        "corpusId": 237048462,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/d931f84abfc4550c10ceb113b142c8eb3e07571e",
        "title": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training",
        "abstract": "Recent works have demonstrated great success in training large autoregressive language models (e.g., GPT-3) on unlabeled text corpus for text generation. To reduce their expensive training cost, practitioners attempt to increase the batch sizes and learning rates. However, increasing them often cause training instabilities and poor generalization. On the other side, using smaller batch sizes or learning rates would reduce the training ef\ufb01ciency, signi\ufb01cantly increasing training time and cost. We investigate this stability-ef\ufb01ciency dilemma and identify that long sequence length is one of the main causes of training instability in large-scale GPT model pre-training. Based on our analysis, we present a novel sequence length warmup method that simultaneously improves training stability and ef\ufb01ciency. As a kind of curriculum learning approach, our method improves the training convergence speed of autoregressive models. More importantly, our in-depth analysis shows that our method exerts a gradient variance reduction effect and regular-izes early stages of training where the amount of training data is much smaller than the model capacity. This enables stable training with much larger batch sizes and learning rates, further improving the training speed. Evaluations show that our approach enables stable GPT-2 (117M and 1.5B) pre-training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot WikiText-103/LAMBADA evaluation results, our approach reduces the required number of pre-training to-kens and wall clock time by up to 55% and 73%, respectively.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 42,
        "citationCount": 23,
        "influentialCitationCount": 3,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents a novel sequence length warmup method that simultaneously improves training stability and ef\ufb01ciency and improves the training convergence speed of autoregressive models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2108.06084"
        },
        "citationStyles": {
            "bibtex": "@Article{Li2021CurriculumLA,\n author = {Conglong Li and Minjia Zhang and Yuxiong He},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training},\n volume = {abs/2108.06084},\n year = {2021}\n}\n"
        }
    },
    "1133_gpt2+corelm+fine-tuning": {
        "paperId": "84b06e7c731d621a46376e6714baa94f2cea7a65",
        "externalIds": {
            "ArXiv": "2111.02687",
            "ACL": "2021.crac-1.8",
            "DBLP": "journals/corr/abs-2111-02687",
            "DOI": "10.18653/v1/2021.crac-1.8",
            "CorpusId": 241583458
        },
        "corpusId": 241583458,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/84b06e7c731d621a46376e6714baa94f2cea7a65",
        "title": "CoreLM: Coreference-aware Language Model Fine-Tuning",
        "abstract": "Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the fine-tuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models\u2019 performance in terms of Accuracy in LAMBADA and Children\u2019s Book Test, with and without the use of model-created coreference annotations.",
        "venue": "CRAC",
        "year": 2021,
        "referenceCount": 32,
        "citationCount": 1,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.crac-1.8.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A Fine-Tuning framework is proposed, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information, which results in a better Language Model for a fraction of the computational cost."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-11-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2111.02687"
        },
        "citationStyles": {
            "bibtex": "@Article{Stylianou2021CoreLMCL,\n author = {Nikolaos Stylianou and I. Vlahavas},\n booktitle = {CRAC},\n journal = {ArXiv},\n title = {CoreLM: Coreference-aware Language Model Fine-Tuning},\n volume = {abs/2111.02687},\n year = {2021}\n}\n"
        }
    },
    "1134_transformer-xl_+_rmt": {
        "paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca",
        "externalIds": {
            "ArXiv": "2207.06881",
            "DBLP": "journals/corr/abs-2207-06881",
            "DOI": "10.48550/arXiv.2207.06881",
            "CorpusId": 250526424
        },
        "corpusId": 250526424,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/a8cf0f7a20f886acfb332071c2daaf58ba86a5ca",
        "title": "Recurrent Memory Transformer",
        "abstract": "Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "referenceCount": 56,
        "citationCount": 49,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2207.06881",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-07-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2207.06881"
        },
        "citationStyles": {
            "bibtex": "@Article{Bulatov2022RecurrentMT,\n author = {Aydar Bulatov and Yuri Kuratov and M. Burtsev},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Recurrent Memory Transformer},\n volume = {abs/2207.06881},\n year = {2022}\n}\n"
        }
    },
    "1135_s_+_i-attention_(3)": {
        "paperId": "26b16f5815407ab59d2cc4589459bd71c92ae32e",
        "externalIds": {
            "ArXiv": "1806.10090",
            "MAG": "2798287132",
            "ACL": "P18-2043",
            "DBLP": "journals/corr/abs-1806-10090",
            "DOI": "10.18653/v1/P18-2043",
            "CorpusId": 49430466
        },
        "corpusId": 49430466,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/26b16f5815407ab59d2cc4589459bd71c92ae32e",
        "title": "Conditional Generators of Words Definitions",
        "abstract": "We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words\u2019 ambiguity and polysemy leads to performance improvement.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "referenceCount": 15,
        "citationCount": 52,
        "influentialCitationCount": 16,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-2043.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work studies the problem of word ambiguities in definition modeling and proposes a possible solution by employing latent variable modeling and soft attention mechanisms and shows that taking into account words\u2019 ambiguity and polysemy leads to performance improvement."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-06-01",
        "journal": {
            "pages": "266-271"
        },
        "citationStyles": {
            "bibtex": "@Article{Gadetsky2018ConditionalGO,\n author = {Artyom Gadetsky and Ilya Yakubovskiy and D. Vetrov},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {266-271},\n title = {Conditional Generators of Words Definitions},\n year = {2018}\n}\n"
        }
    },
    "1136_pali-x": {
        "paperId": "3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a",
        "externalIds": {
            "ArXiv": "2305.18565",
            "DBLP": "journals/corr/abs-2305-18565",
            "DOI": "10.48550/arXiv.2305.18565",
            "CorpusId": 258967670
        },
        "corpusId": 258967670,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a",
        "title": "PaLI-X: On Scaling up a Multilingual Vision and Language Model",
        "abstract": "We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 96,
        "citationCount": 81,
        "influentialCitationCount": 11,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.18565",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "PaLI-X, a multilingual vision and language model, advances the state-of-the-art on most vision-and-language benchmarks considered and observes emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-05-29",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2305.18565"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2023PaLIXOS,\n author = {Xi Chen and Josip Djolonga and Piotr Padlewski and Basil Mustafa and Soravit Changpinyo and Jialin Wu and Carlos Riquelme Ruiz and Sebastian Goodman and Xiao Wang and Yi Tay and Siamak Shakeri and Mostafa Dehghani and Daniel M. Salz and Mario Lucic and M. Tschannen and Arsha Nagrani and Hexiang Hu and Mandar Joshi and Bo Pang and Ceslee Montgomery and Paulina Pietrzyk and Marvin Ritter and A. Piergiovanni and Matthias Minderer and Filip Pavetic and Austin Waters and Gang Li and Ibrahim M. Alabdulmohsin and Lucas Beyer and J. Amelot and Kenton Lee and A. Steiner and Yang Li and Daniel Keysers and Anurag Arnab and Yuanzhong Xu and Keran Rong and Alexander Kolesnikov and Mojtaba Seyedhosseini and A. Angelova and Xiaohua Zhai and N. Houlsby and Radu Soricut},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {PaLI-X: On Scaling up a Multilingual Vision and Language Model},\n volume = {abs/2305.18565},\n year = {2023}\n}\n"
        }
    },
    "1137_statistical_shape_constellations": {
        "paperId": "1cf1527807ebb16020b04d4166e7ba8d27652302",
        "externalIds": {
            "MAG": "1949116567",
            "DBLP": "conf/eccv/WeberWP00",
            "DOI": "10.1007/3-540-45054-8_2",
            "CorpusId": 8970876
        },
        "corpusId": 8970876,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/1cf1527807ebb16020b04d4166e7ba8d27652302",
        "title": "Unsupervised Learning of Models for Recognition",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2000,
        "referenceCount": 18,
        "citationCount": 753,
        "influentialCitationCount": 55,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/3-540-45054-8_2.pdf",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A method to learn object class models from unlabeled and unsegmented cluttered cluttered scenes for the purpose of visual object recognition that achieves very good classification results on human faces and rear views of cars."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2000-06-26",
        "journal": {
            "pages": "18-32"
        },
        "citationStyles": {
            "bibtex": "@Article{Weber2000UnsupervisedLO,\n author = {Markus Weber and M. Welling and P. Perona},\n booktitle = {European Conference on Computer Vision},\n pages = {18-32},\n title = {Unsupervised Learning of Models for Recognition},\n year = {2000}\n}\n"
        }
    },
    "1138_transformerxl+relationlm": {
        "paperId": "4d1e6c441a0b6aa2b9ec500a5064dc307d797b0c",
        "externalIds": {
            "DBLP": "journals/tacl/LiuYB22",
            "ArXiv": "2201.09680",
            "ACL": "2022.tacl-1.32",
            "DOI": "10.1162/tacl_a_00476",
            "CorpusId": 246240585
        },
        "corpusId": 246240585,
        "publicationVenue": {
            "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
            "name": "Transactions of the Association for Computational Linguistics",
            "type": "journal",
            "alternate_names": [
                "Trans Assoc Comput Linguistics",
                "TACL"
            ],
            "issn": "2307-387X",
            "url": "https://www.mitpressjournals.org/loi/tacl",
            "alternate_urls": [
                "http://www.transacl.org/"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/4d1e6c441a0b6aa2b9ec500a5064dc307d797b0c",
        "title": "Relational Memory-Augmented Language Models",
        "abstract": "We present a memory-augmented approach to condition an autoregressive language model on a knowledge graph. We represent the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation. Experiments on WikiText-103, WMT19, and enwik8 English datasets demonstrate that our approach produces a better language model in terms of perplexity and bits per character. We also show that relational memory improves coherence, is complementary to token-based memory, and enables causal interventions. Our model provides a simple yet effective way to combine an autoregressive language model and a knowledge graph for more coherent and logical generation.",
        "venue": "Transactions of the Association for Computational Linguistics",
        "year": 2022,
        "referenceCount": 99,
        "citationCount": 23,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00476/2020721/tacl_a_00476.pdf",
            "status": "GOLD"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A memory-augmented approach to condition an autoregressive language model on a knowledge graph that represents the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2022-01-24",
        "journal": {
            "name": "Transactions of the Association for Computational Linguistics",
            "pages": "555-572",
            "volume": "10"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2022RelationalML,\n author = {Qi Liu and Dani Yogatama and Phil Blunsom},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {555-572},\n title = {Relational Memory-Augmented Language Models},\n volume = {10},\n year = {2022}\n}\n"
        }
    },
    "1139_efficientdet": {
        "paperId": "41c67d04be2d1632c0d3b0880c21c9fe797cdab8",
        "externalIds": {
            "MAG": "2990268359",
            "ArXiv": "1911.09070",
            "DBLP": "journals/corr/abs-1911-09070",
            "DOI": "10.1109/cvpr42600.2020.01079",
            "CorpusId": 208175544
        },
        "corpusId": 208175544,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/41c67d04be2d1632c0d3b0880c21c9fe797cdab8",
        "title": "EfficientDet: Scalable and Efficient Object Detection",
        "abstract": "Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfficientNet backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfficientDetD7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs, being 4x \u2013 9x smaller and using 13x \u2013 42x fewer FLOPs than previous detector.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "referenceCount": 45,
        "citationCount": 3391,
        "influentialCitationCount": 468,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1911.09070",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Engineering",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper systematically study neural network architecture design choices for object detection and proposes a weighted bi-directional feature pyramid network (BiFPN) and a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-11-20",
        "journal": {
            "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "10778-10787"
        },
        "citationStyles": {
            "bibtex": "@Article{Tan2019EfficientDetSA,\n author = {Mingxing Tan and Ruoming Pang and Quoc V. Le},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {10778-10787},\n title = {EfficientDet: Scalable and Efficient Object Detection},\n year = {2019}\n}\n"
        }
    },
    "1140_alphax-1": {
        "paperId": "32c3892a07e16a32604de7d2724f993f3f2ad4af",
        "externalIds": {
            "MAG": "2803461142",
            "ArXiv": "1903.11059",
            "DBLP": "journals/corr/abs-1903-11059",
            "CorpusId": 29159963
        },
        "corpusId": 29159963,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/32c3892a07e16a32604de7d2724f993f3f2ad4af",
        "title": "AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search",
        "abstract": "We present AlphaX, a fully automated agent that designs complex neural architectures from scratch. AlphaX explores the exponentially grown search space with a distributed Monte Carlo Tree Search (MCTS) and a Meta-Deep Neural Network (DNN). MCTS intrinsically improves the search efficiency by dynamically balancing the exploration and exploitation at fine-grained states, while Meta-DNN predicts the network accuracy to guide the search, and to provide an estimated reward to speed up the rollout. As the search progresses, AlphaX also generates the training data for Meta-DNN. So, the learning of Meta-DNN is end-to-end. In 14 days with only 16 GPUs (1832 samples), AlphaX found an architecture that reaches the state-of-the-art accuracies on both CIFAR-10(97.18%) and ImageNet(75.5% top-1 and 92.2% top-5). This demonstrates up to 10x speedup over the original searching for NASNet that used 500 GPUs in 4 days (20000 samples). On NASBench-101, AlphaX demonstrates 3x and 2.8x speedup over Random Search and Regularized Evolution. Finally, we show the searched architecture improves a variety of vision applications from Neural Style Transfer, to Image Captioning and Object Detection. Our implementation is available at this https URL.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 64,
        "citationCount": 83,
        "influentialCitationCount": 14,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The searched architecture improves a variety of vision applications from Neural Style Transfer, to Image Captioning and Object Detection, and 3x and 2.8x speedup over Random Search and Regularized Evolution are shown."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-03-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1805.07440"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2019AlphaXEN,\n author = {Linnan Wang and Yiyang Zhao and Yuu Jinnai},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search},\n volume = {abs/1805.07440},\n year = {2019}\n}\n"
        }
    },
    "1141_pnasnet-5": {
        "paperId": "5f79398057bf0bbda9ff50067bc1f2950c2a2266",
        "externalIds": {
            "ArXiv": "1712.00559",
            "MAG": "2949714964",
            "DBLP": "journals/corr/abs-1712-00559",
            "DOI": "10.1007/978-3-030-01246-5_2",
            "CorpusId": 40430109
        },
        "corpusId": 40430109,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/5f79398057bf0bbda9ff50067bc1f2950c2a2266",
        "title": "Progressive Neural Architecture Search",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2017,
        "referenceCount": 53,
        "citationCount": 1799,
        "influentialCitationCount": 283,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1712.00559",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms using a sequential model-based optimization (SMBO) strategy."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2017-12-02",
        "journal": {
            "pages": "19-35"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2017ProgressiveNA,\n author = {Chenxi Liu and Barret Zoph and Jonathon Shlens and Wei Hua and Li-Jia Li and Li Fei-Fei and A. Yuille and Jonathan Huang and K. Murphy},\n booktitle = {European Conference on Computer Vision},\n pages = {19-35},\n title = {Progressive Neural Architecture Search},\n year = {2017}\n}\n"
        }
    },
    "1144_optimized_multi-scale_edge_detection": {
        "paperId": "fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec",
        "externalIds": {
            "MAG": "2182096904",
            "DBLP": "journals/pami/Canny86a",
            "DOI": "10.1109/TPAMI.1986.4767851",
            "CorpusId": 13284142,
            "PubMed": "21869365"
        },
        "corpusId": 13284142,
        "publicationVenue": {
            "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
            "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "type": "journal",
            "alternate_names": [
                "IEEE Trans Pattern Anal Mach Intell"
            ],
            "issn": "0162-8828",
            "url": "http://www.computer.org/tpami/",
            "alternate_urls": [
                "http://www.computer.org/portal/web/tpami",
                "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec",
        "title": "A Computational Approach to Edge Detection",
        "abstract": "This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "year": 1986,
        "referenceCount": 33,
        "citationCount": 30403,
        "influentialCitationCount": 1610,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://cmp.felk.cvut.cz/~cernyad2/TextCaptchaPdf/A Computational Approach to Edge Detection.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science",
            "Medicine"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Medicine",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "There is a natural uncertainty principle between detection and localization performance, which are the two main goals, and with this principle a single operator shape is derived which is optimal at any scale."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "1986-06-01",
        "journal": {
            "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "pages": "679-698",
            "volume": "PAMI-8"
        },
        "citationStyles": {
            "bibtex": "@Article{Canny1986ACA,\n author = {J. Canny},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {679-698},\n title = {A Computational Approach to Edge Detection},\n volume = {PAMI-8},\n year = {1986}\n}\n"
        }
    },
    "1145_curl": {
        "paperId": "79e14a09ff070e06ab9df598ccd885b929164ef9",
        "externalIds": {
            "DBLP": "journals/corr/abs-2004-04136",
            "MAG": "3015437096",
            "ArXiv": "2004.04136",
            "CorpusId": 215415964
        },
        "corpusId": 215415964,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/79e14a09ff070e06ab9df598ccd885b929164ef9",
        "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
        "abstract": "We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at this https URL.",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "referenceCount": 65,
        "citationCount": 808,
        "influentialCitationCount": 143,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features and is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-04-08",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2004.04136"
        },
        "citationStyles": {
            "bibtex": "@Article{Srinivas2020CURLCU,\n author = {A. Srinivas and M. Laskin and P. Abbeel},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {CURL: Contrastive Unsupervised Representations for Reinforcement Learning},\n volume = {abs/2004.04136},\n year = {2020}\n}\n"
        }
    },
    "1146_objectnet": {
        "paperId": "639174f32a71ecfe9041ad05ff30eb39bd4977bf",
        "externalIds": {
            "MAG": "2970692043",
            "DBLP": "conf/nips/BarbuMALWGTK19",
            "CorpusId": 202777185
        },
        "corpusId": 202777185,
        "publicationVenue": {
            "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
            "name": "Neural Information Processing Systems",
            "type": "conference",
            "alternate_names": [
                "Neural Inf Process Syst",
                "NeurIPS",
                "NIPS"
            ],
            "url": "http://neurips.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/639174f32a71ecfe9041ad05ff30eb39bd4977bf",
        "title": "ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",
        "abstract": "We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet (objects are largely centered and unoccluded) and harder (due to the controls). Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "referenceCount": 25,
        "citationCount": 424,
        "influentialCitationCount": 49,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A highly automated platform that enables gathering datasets with controls at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers is developed."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": null,
        "journal": {
            "pages": "9448-9458"
        },
        "citationStyles": {
            "bibtex": "@Article{Barbu2019ObjectNetAL,\n author = {Andrei Barbu and David Mayo and Julian Alverio and William Luo and Christopher Wang and Dan Gutfreund and J. Tenenbaum and Boris Katz},\n booktitle = {Neural Information Processing Systems},\n pages = {9448-9458},\n title = {ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models},\n year = {2019}\n}\n"
        }
    },
    "1147_amoebanet-a_(f=190)": {
        "paperId": "50bdda28de3dcf82a0e10f9ec13eea248b19edb5",
        "externalIds": {
            "MAG": "2785430118",
            "DBLP": "journals/corr/abs-1802-01548",
            "ArXiv": "1802.01548",
            "DOI": "10.1609/aaai.v33i01.33014780",
            "CorpusId": 3640974
        },
        "corpusId": 3640974,
        "publicationVenue": {
            "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
            "name": "AAAI Conference on Artificial Intelligence",
            "type": "conference",
            "alternate_names": [
                "National Conference on Artificial Intelligence",
                "National Conf Artif Intell",
                "AAAI Conf Artif Intell",
                "AAAI"
            ],
            "url": "http://www.aaai.org/"
        },
        "url": "https://www.semanticscholar.org/paper/50bdda28de3dcf82a0e10f9ec13eea248b19edb5",
        "title": "Regularized Evolution for Image Classifier Architecture Search",
        "abstract": "The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier\u2014 AmoebaNet-A\u2014that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "referenceCount": 90,
        "citationCount": 2575,
        "influentialCitationCount": 381,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4405/4283",
            "status": "BRONZE"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work evolves an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time and gives evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-02-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1802.01548"
        },
        "citationStyles": {
            "bibtex": "@Article{Real2018RegularizedEF,\n author = {Esteban Real and A. Aggarwal and Yanping Huang and Quoc V. Le},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Regularized Evolution for Image Classifier Architecture Search},\n volume = {abs/1802.01548},\n year = {2018}\n}\n"
        }
    },
    "1148_grus": {
        "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
        "externalIds": {
            "MAG": "2950635152",
            "DBLP": "conf/emnlp/ChoMGBBSB14",
            "ACL": "D14-1179",
            "ArXiv": "1406.1078",
            "DOI": "10.3115/v1/D14-1179",
            "CorpusId": 5590763
        },
        "corpusId": 5590763,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e",
        "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
        "abstract": "In this paper, we propose a novel neural network model called RNN Encoder\u2010 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2010Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2014,
        "referenceCount": 33,
        "citationCount": 20169,
        "influentialCitationCount": 2930,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/D14-1179.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Qualitatively, the proposed RNN Encoder\u2010Decoder model learns a semantically and syntactically meaningful representation of linguistic phrases."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2014-06-03",
        "journal": {
            "pages": "1724-1734"
        },
        "citationStyles": {
            "bibtex": "@Article{Cho2014LearningPR,\n author = {Kyunghyun Cho and B. V. Merrienboer and \u00c7aglar G\u00fcl\u00e7ehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1724-1734},\n title = {Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation},\n year = {2014}\n}\n"
        }
    },
    "1149_multi-cell_lstm": {
        "paperId": "590205f9d8b38bd395cbfdf25f54e79ae42de2cf",
        "externalIds": {
            "ArXiv": "1811.06477",
            "MAG": "2901701848",
            "DBLP": "journals/corr/abs-1811-06477",
            "CorpusId": 53425684
        },
        "corpusId": 53425684,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/590205f9d8b38bd395cbfdf25f54e79ae42de2cf",
        "title": "Multi-cell LSTM Based Neural Language Model",
        "abstract": "Language models, being at the heart of many NLP problems, are always of great interest to researchers. Neural language models come with the advantage of distributed representations and long range contexts. With its particular dynamics that allow the cycling of information within the network, `Recurrent neural network' (RNN) becomes an ideal paradigm for neural language modeling. Long Short-Term Memory (LSTM) architecture solves the inadequacies of the standard RNN in modeling long-range contexts. In spite of a plethora of RNN variants, possibility to add multiple memory cells in LSTM nodes was seldom explored. Here we propose a multi-cell node architecture for LSTMs and study its applicability for neural language modeling. The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup.",
        "venue": "arXiv.org",
        "year": 2018,
        "referenceCount": 20,
        "citationCount": 3,
        "influentialCitationCount": 0,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup and study its applicability for neural language modeling."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2018-11-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1811.06477"
        },
        "citationStyles": {
            "bibtex": "@Article{Cherian2018MulticellLB,\n author = {T. Cherian and Akshay Badola and V. Padmanabhan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Multi-cell LSTM Based Neural Language Model},\n volume = {abs/1811.06477},\n year = {2018}\n}\n"
        }
    },
    "1151_peephole_lstm": {
        "paperId": "545a4e23bf00ddbc1d3325324b4c61f57cf45081",
        "externalIds": {
            "MAG": "2100649405",
            "DBLP": "conf/ijcnn/GersS00",
            "DOI": "10.1109/IJCNN.2000.861302",
            "CorpusId": 36867983
        },
        "corpusId": 36867983,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/545a4e23bf00ddbc1d3325324b4c61f57cf45081",
        "title": "Recurrent nets that time and count",
        "abstract": "The size of the time intervals between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While hidden Markov models tend to ignore this information, recurrent neural networks (RNN) can in principle learn to make use of it. We focus on long short-term memory (LSTM) because it usually outperforms other RNN. Surprisingly, LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps, without the help of any short training exemplars. Without external resets or teacher forcing or loss of performance on tasks reported earlier, our LSTM variant also learns to generate very stable sequences of highly nonlinear, precisely timed spikes. This makes LSTM a promising approach for real-world tasks that require to time and count.",
        "venue": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium",
        "year": 2000,
        "referenceCount": 10,
        "citationCount": 593,
        "influentialCitationCount": 24,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Surprisingly, LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps, without the help of any short training exemplars."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2000-01-26",
        "journal": {
            "name": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium",
            "pages": "189-194 vol.3",
            "volume": "3"
        },
        "citationStyles": {
            "bibtex": "@Article{Gers2000RecurrentNT,\n author = {Felix Alexander Gers and J. Schmidhuber},\n booktitle = {Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},\n journal = {Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},\n pages = {189-194 vol.3},\n title = {Recurrent nets that time and count},\n volume = {3},\n year = {2000}\n}\n"
        }
    },
    "1153_msa_transformer": {
        "paperId": "deee48c5e0ac0407a1e002905caaf2b174bdb0e6",
        "externalIds": {
            "DBLP": "conf/icml/RaoLVMCASR21",
            "DOI": "10.1101/2021.02.12.430858",
            "CorpusId": 231939146
        },
        "corpusId": 231939146,
        "publicationVenue": {
            "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
            "name": "bioRxiv",
            "type": "journal",
            "url": "http://biorxiv.org/"
        },
        "url": "https://www.semanticscholar.org/paper/deee48c5e0ac0407a1e002905caaf2b174bdb0e6",
        "title": "MSA Transformer",
        "abstract": "Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evo lutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models.",
        "venue": "bioRxiv",
        "year": 2021,
        "referenceCount": 55,
        "citationCount": 326,
        "influentialCitationCount": 30,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.biorxiv.org/content/biorxiv/early/2021/08/27/2021.02.12.430858.full.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science",
            "Biology"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Biology",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Biology",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A protein language model which takes as input a set of sequences in the form of a multiple sequence alignment and is trained with a variant of the masked language modeling objective across many protein families surpasses current state-of-the-art unsupervised structure learning methods by a wide margin."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-02-13",
        "journal": {
            "name": "bioRxiv"
        },
        "citationStyles": {
            "bibtex": "@Article{Rao2021MSAT,\n author = {Roshan Rao and Jason Liu and Robert Verkuil and Joshua Meier and J. Canny and P. Abbeel and Tom Sercu and Alexander Rives},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {MSA Transformer},\n year = {2021}\n}\n"
        }
    },
    "1154_fusion_in_encoder": {
        "paperId": "471a49220cea2069e8b8a76821b1d2434204a732",
        "externalIds": {
            "DBLP": "journals/corr/abs-2211-10147",
            "ACL": "2022.emnlp-main.285",
            "ArXiv": "2211.10147",
            "DOI": "10.48550/arXiv.2211.10147",
            "CorpusId": 253708199
        },
        "corpusId": 253708199,
        "publicationVenue": {
            "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
            "name": "Conference on Empirical Methods in Natural Language Processing",
            "type": "conference",
            "alternate_names": [
                "Empir Method Nat Lang Process",
                "Empirical Methods in Natural Language Processing",
                "Conf Empir Method Nat Lang Process",
                "EMNLP"
            ],
            "url": "https://www.aclweb.org/portal/emnlp"
        },
        "url": "https://www.semanticscholar.org/paper/471a49220cea2069e8b8a76821b1d2434204a732",
        "title": "FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering",
        "abstract": "Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2022,
        "referenceCount": 47,
        "citationCount": 5,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2211.10147",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work proposes to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples, and proposes an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-11-18",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2211.10147"
        },
        "citationStyles": {
            "bibtex": "@Article{Kedia2022FiEBA,\n author = {Akhil Kedia and Mohd Abbas Zaidi and Haejun Lee},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering},\n volume = {abs/2211.10147},\n year = {2022}\n}\n"
        }
    },
    "1155_phrasecond": {
        "paperId": "55f4fd0fa6335720df9bd74ce4102a8656c9592c",
        "externalIds": {
            "MAG": "2765627424",
            "ArXiv": "1710.10504",
            "DBLP": "journals/corr/abs-1710-10504",
            "CorpusId": 1679521
        },
        "corpusId": 1679521,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/55f4fd0fa6335720df9bd74ce4102a8656c9592c",
        "title": "Phase Conductor on Multi-layered Attentions for Machine Comprehension",
        "abstract": "Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question-aware passage attention model and self-matching attention model. Our research proposes phase conductor (PhaseCond) for attention models in two meaningful ways. First, PhaseCond, an architecture of multi-layered attention models, consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow. Second, we extend and improve the dot-product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives. We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models. We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models.",
        "venue": "arXiv.org",
        "year": 2017,
        "referenceCount": 20,
        "citationCount": 22,
        "influentialCitationCount": 5,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This research proposes phase conductor (PhaseCond) for attention models in two meaningful ways, and demonstrates the effectiveness of the proposed model PhaseCond on the SQuAD dataset, showing that the model significantly outperforms both state-of-the-art single-layered and multiple-layering attention models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2017-10-28",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1710.10504"
        },
        "citationStyles": {
            "bibtex": "@Article{Liu2017PhaseCO,\n author = {R. Liu and Wei Wei and Weiguang Mao and M. Chikina},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Phase Conductor on Multi-layered Attentions for Machine Comprehension},\n volume = {abs/1710.10504},\n year = {2017}\n}\n"
        }
    },
    "1156_albert-xxlarge": {
        "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
        "externalIds": {
            "MAG": "2996428491",
            "DBLP": "journals/corr/abs-1909-11942",
            "ArXiv": "1909.11942",
            "CorpusId": 202888986
        },
        "corpusId": 202888986,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/7a064df1aeada7e69e5173f7d4c8606f4470365b",
        "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
        "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "referenceCount": 72,
        "citationCount": 5133,
        "influentialCitationCount": 851,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work presents two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT, and uses a self-supervised loss that focuses on modeling inter-sentence coherence."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-09-26",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1909.11942"
        },
        "citationStyles": {
            "bibtex": "@Article{Lan2019ALBERTAL,\n author = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\n volume = {abs/1909.11942},\n year = {2019}\n}\n"
        }
    },
    "1157_igpt-xl": {
        "paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "externalIds": {
            "MAG": "3034445277",
            "DBLP": "conf/icml/ChenRC0JLS20",
            "CorpusId": 219781060
        },
        "corpusId": 219781060,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "title": "Generative Pretraining From Pixels",
        "abstract": "Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we \ufb01nd that a GPT-2 scale model learns strong image representations as measured by linear probing, \ufb01ne-tuning, and low-data classi\ufb01cation. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full \ufb01ne-tuning, matching the top supervised pre-trained models. An even larger model trained on a mix-ture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "referenceCount": 79,
        "citationCount": 1132,
        "influentialCitationCount": 93,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is found that a GPT-2 scale model learns strong image representations as measured by linear probing, \ufb01ne-tuning, and low-data classi\ufb01cation, despite training on low-resolution ImageNet without labels."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2020-07-12",
        "journal": {
            "pages": "1691-1703"
        },
        "citationStyles": {
            "bibtex": "@Article{Chen2020GenerativePF,\n author = {Mark Chen and Alec Radford and Jeff Wu and Heewoo Jun and Prafulla Dhariwal and D. Luan and I. Sutskever},\n booktitle = {International Conference on Machine Learning},\n pages = {1691-1703},\n title = {Generative Pretraining From Pixels},\n year = {2020}\n}\n"
        }
    },
    "1158_yolox-x": {
        "paperId": "c01b385205e488a731c8c8c11c0c494d426beb03",
        "externalIds": {
            "DBLP": "journals/corr/abs-2107-08430",
            "ArXiv": "2107.08430",
            "CorpusId": 236088010
        },
        "corpusId": 236088010,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/c01b385205e488a731c8c8c11c0c494d426beb03",
        "title": "YOLOX: Exceeding YOLO Series in 2021",
        "abstract": "In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3% AP on COCO, outperforming the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/Megvii-BaseDetection/YOLOX.",
        "venue": "arXiv.org",
        "year": 2021,
        "referenceCount": 40,
        "citationCount": 2126,
        "influentialCitationCount": 387,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Physics",
                "source": "s2-fos-model"
            },
            {
                "category": "Engineering",
                "source": "s2-fos-model"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This report switches the YOLO detector to an anchor-free manner and conducts other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-07-18",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2107.08430"
        },
        "citationStyles": {
            "bibtex": "@Article{Ge2021YOLOXEY,\n author = {Zheng Ge and Songtao Liu and Feng Wang and Zeming Li and Jian Sun},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {YOLOX: Exceeding YOLO Series in 2021},\n volume = {abs/2107.08430},\n year = {2021}\n}\n"
        }
    },
    "1159__tinyllama-1.1b_(3t_tokens_checkpoint)": {
        "paperId": "560c6f24c335c2dd27be0cfa50dbdbb50a9e4bfd",
        "externalIds": {
            "DBLP": "journals/corr/abs-2401-02385",
            "ArXiv": "2401.02385",
            "DOI": "10.48550/arXiv.2401.02385",
            "CorpusId": 266755802
        },
        "corpusId": 266755802,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/560c6f24c335c2dd27be0cfa50dbdbb50a9e4bfd",
        "title": "TinyLlama: An Open-Source Small Language Model",
        "abstract": "We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.",
        "venue": "arXiv.org",
        "year": 2024,
        "referenceCount": 34,
        "citationCount": 27,
        "influentialCitationCount": 4,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "TinyLlama is presented, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs that significantly outperforms existing open-source language models with comparable sizes."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2024-01-04",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2401.02385"
        },
        "citationStyles": {
            "bibtex": "@Article{Zhang2024TinyLlamaAO,\n author = {Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {TinyLlama: An Open-Source Small Language Model},\n volume = {abs/2401.02385},\n year = {2024}\n}\n"
        }
    },
    "1160_batchnorm": {
        "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
        "externalIds": {
            "ArXiv": "1502.03167",
            "MAG": "1836465849",
            "DBLP": "journals/corr/IoffeS15",
            "CorpusId": 5808102
        },
        "corpusId": 5808102,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/995c5f5e62614fcb4d2796ad2faab969da51713e",
        "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "referenceCount": 33,
        "citationCount": 38702,
        "influentialCitationCount": 1968,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2015-02-10",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1502.03167"
        },
        "citationStyles": {
            "bibtex": "@Article{Ioffe2015BatchNA,\n author = {Sergey Ioffe and Christian Szegedy},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},\n volume = {abs/1502.03167},\n year = {2015}\n}\n"
        }
    },
    "1161_dbn_for_nlp": {
        "paperId": "c8933a793d5c2b423c2053cb2a0e1ec243ab6071",
        "externalIds": {
            "MAG": "1964812476",
            "DBLP": "journals/taslp/SarikayaHD14",
            "DOI": "10.1109/TASLP.2014.2303296",
            "CorpusId": 6488286
        },
        "corpusId": 6488286,
        "publicationVenue": {
            "id": "309e00f7-4bbd-461f-ab37-a90cd14ef21d",
            "name": "IEEE/ACM Transactions on Audio Speech and Language Processing",
            "alternate_names": [
                "IEEE/ACM Trans Audio Speech Lang Process"
            ],
            "issn": "2329-9290",
            "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=6570655",
            "alternate_urls": [
                "https://signalprocessingsociety.org/publications-resources/ieeeacm-transactions-audio-speech-and-language-processing/ieeeacm"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/c8933a793d5c2b423c2053cb2a0e1ec243ab6071",
        "title": "Application of Deep Belief Networks for Natural Language Understanding",
        "abstract": "Applications of Deep Belief Nets (DBN) to various problems have been the subject of a number of recent studies ranging from image classification and speech recognition to audio classification. In this study we apply DBNs to a natural language understanding problem. The recent surge of activity in this area was largely spurred by the development of a greedy layer-wise pretraining method that uses an efficient learning algorithm called Contrastive Divergence (CD). CD allows DBNs to learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms: Support Vector Machines (SVM), boosting and Maximum Entropy (MaxEnt). The plain DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models. However, using additional unlabeled data for DBN pre-training and combining DBN-based learned features with the original features provides significant gains over SVMs, which, in turn, performed better than both MaxEnt and Boosting.",
        "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "year": 2014,
        "referenceCount": 21,
        "citationCount": 424,
        "influentialCitationCount": 9,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://www.cs.toronto.edu/~hinton/absps/ruhijournal.pdf",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "The plain DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models, however, using additional unlabeled data for DBN pre-training and combining DBN-based learned features with the original features provides significant gains over SVMs, which, in turn, performed better than both MaxEnt and Boosting."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2014-04-01",
        "journal": {
            "name": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
            "pages": "778-784",
            "volume": "22"
        },
        "citationStyles": {
            "bibtex": "@Article{Sarikaya2014ApplicationOD,\n author = {R. Sarikaya and Geoffrey E. Hinton and Anoop Deoras},\n booktitle = {IEEE/ACM Transactions on Audio Speech and Language Processing},\n journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},\n pages = {778-784},\n title = {Application of Deep Belief Networks for Natural Language Understanding},\n volume = {22},\n year = {2014}\n}\n"
        }
    },
    "1162_baai_bge-reranker-large": {
        "paperId": "2ff694e20f492a7acf7fd0646c5e1576f0b3c901",
        "externalIds": {
            "DBLP": "journals/corr/abs-2309-07597",
            "ArXiv": "2309.07597",
            "DOI": "10.48550/arXiv.2309.07597",
            "CorpusId": 261823330
        },
        "corpusId": 261823330,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/2ff694e20f492a7acf7fd0646c5e1576f0b3c901",
        "title": "C-Pack: Packaged Resources To Advance General Chinese Embedding",
        "abstract": "We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 83,
        "citationCount": 72,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.07597",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This work introduces C-Pack, a package of resources that significantly advance the field of general Chinese embeddings and releases the entire suite of training methods for C-TEM, a family of embedding models covering multiple sizes."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-09-14",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2309.07597"
        },
        "citationStyles": {
            "bibtex": "@Article{Xiao2023CPackPR,\n author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n volume = {abs/2309.07597},\n year = {2023}\n}\n"
        }
    },
    "1163_palm_2": {
        "paperId": "b6d6c33298b852cf63edac233deca70530d69a2a",
        "externalIds": {
            "DBLP": "journals/corr/abs-2305-10403",
            "ArXiv": "2305.10403",
            "DOI": "10.48550/arXiv.2305.10403",
            "CorpusId": 258740735
        },
        "corpusId": 258740735,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/b6d6c33298b852cf63edac233deca70530d69a2a",
        "title": "PaLM 2 Technical Report",
        "abstract": "We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities. When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 166,
        "citationCount": 549,
        "influentialCitationCount": 47,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2305.10403",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "PaLM 2 is a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM and enables inference-time control over toxicity without additional overhead or impact on other capabilities."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-05-17",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2305.10403"
        },
        "citationStyles": {
            "bibtex": "@Article{Anil2023PaLM2T,\n author = {Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and A. Passos and Siamak Shakeri and E. Taropa and Paige Bailey and Z. Chen and Eric Chu and J. Clark and Laurent El Shafey and Yanping Huang and K. Meier-Hellstern and Gaurav Mishra and Erica Moreira and Mark Omernick and Kevin Robinson and Sebastian Ruder and Yi Tay and Kefan Xiao and Yuanzhong Xu and Yujing Zhang and Gustavo Hern\u00e1ndez Abrego and Junwhan Ahn and Jacob Austin and Paul Barham and Jan A. Botha and James Bradbury and Siddhartha Brahma and K. Brooks and Michele Catasta and Yongzhou Cheng and Colin Cherry and Christopher A. Choquette-Choo and Aakanksha Chowdhery and C. Cr\u00e9py and Shachi Dave and Mostafa Dehghani and Sunipa Dev and Jacob Devlin and M. D'iaz and Nan Du and Ethan Dyer and Vladimir Feinberg and Fan Feng and Vlad Fienber and Markus Freitag and Xavier Garc\u00eda and Sebastian Gehrmann and Lucas Gonz\u00e1lez and Guy Gur-Ari and S. Hand and Hadi Hashemi and Le Hou and Joshua Howland and A. Hu and Jeffrey Hui and Jeremy Hurwitz and M. Isard and Abe Ittycheriah and Matthew Jagielski and W. Jia and Kathleen Kenealy and M. Krikun and Sneha Kudugunta and Chang Lan and Katherine Lee and Benjamin Lee and Eric Li and Mu-Li Li and Wei Li and Yaguang Li and Jun Yu Li and Hyeontaek Lim and Han Lin and Zhong-Zhong Liu and Frederick Liu and Marcello Maggioni and Aroma Mahendru and Joshua Maynez and Vedant Misra and Maysam Moussalem and Zachary Nado and John Nham and Eric Ni and A. Nystrom and Alicia Parrish and Marie Pellat and M. Polacek and Oleksandr Polozov and Reiner Pope and Siyuan Qiao and Emily Reif and Bryan Richter and Parker Riley and A. Ros and Aurko Roy and Brennan Saeta and R. Samuel and R. Shelby and Ambrose Slone and D. Smilkov and David R. So and Daniela Sohn and Simon Tokumine and Dasha Valter and Vijay Vasudevan and Kiran Vodrahalli and Xuezhi Wang and Pidong Wang and Zirui Wang and Tao Wang and J. Wieting and Yuhuai Wu and Ke Xu and Yunhan Xu and L. Xue and Pengcheng Yin and Jiahui Yu and Qiaoling Zhang and Steven Zheng and Ce Zheng and Wei Zhou and Denny Zhou and Slav Petrov and Yonghui Wu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {PaLM 2 Technical Report},\n volume = {abs/2305.10403},\n year = {2023}\n}\n"
        }
    },
    "1164_impala": {
        "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "externalIds": {
            "MAG": "2950708659",
            "DBLP": "conf/icml/EspeholtSMSMWDF18",
            "ArXiv": "1802.01561",
            "CorpusId": 3645060
        },
        "corpusId": 3645060,
        "publicationVenue": {
            "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
            "name": "International Conference on Machine Learning",
            "type": "conference",
            "alternate_names": [
                "ICML",
                "Int Conf Mach Learn"
            ],
            "url": "https://icml.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
        "abstract": "In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "referenceCount": 42,
        "citationCount": 1338,
        "influentialCitationCount": 230,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Mathematics",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) is developed that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-02-05",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1802.01561"
        },
        "citationStyles": {
            "bibtex": "@Article{Espeholt2018IMPALASD,\n author = {L. Espeholt and Hubert Soyer and R. Munos and K. Simonyan and Volodymyr Mnih and Tom Ward and Yotam Doron and Vlad Firoiu and Tim Harley and Iain Dunning and S. Legg and K. Kavukcuoglu},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures},\n volume = {abs/1802.01561},\n year = {2018}\n}\n"
        }
    },
    "1165_cerebras-gpt-13b": {
        "paperId": "ece77610adfb0fb162dd22ef694f2777393c319a",
        "externalIds": {
            "DBLP": "journals/corr/abs-2304-03208",
            "ArXiv": "2304.03208",
            "DOI": "10.48550/arXiv.2304.03208",
            "CorpusId": 257985427
        },
        "corpusId": 257985427,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/ece77610adfb0fb162dd22ef694f2777393c319a",
        "title": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster",
        "abstract": "We study recent research advances that improve large language models through efficient pre-training and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the predictable power-law scaling and compare Cerebras-GPT with other publicly-available models to show all Cerebras-GPT models have state-of-the-art training efficiency on both pre-training and downstream objectives. We describe our learnings including how Maximal Update Parameterization ($\\mu$P) can further improve large model scaling, improving accuracy and hyperparameter predictability at scale. We release our pre-trained models and code, making this paper the first open and reproducible work comparing compute-optimal model scaling to models trained on fixed dataset sizes. Cerebras-GPT models are available on HuggingFace: https://huggingface.co/cerebras.",
        "venue": "arXiv.org",
        "year": 2023,
        "referenceCount": 57,
        "citationCount": 48,
        "influentialCitationCount": 7,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2304.03208",
            "status": "CLOSED"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper releases the first open and reproducible work comparing compute-optimal model scaling to models trained on fixed dataset sizes, and describes the learnings including how Maximal Update Parameterization can further improve large model scaling, improving accuracy and hyperparameter predictability at scale."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2023-04-06",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2304.03208"
        },
        "citationStyles": {
            "bibtex": "@Article{Dey2023CerebrasGPTOC,\n author = {Nolan Dey and G. Gosal and Zhiming Chen and Hemant Khachane and William Marshall and Ribhu Pathria and Marvin Tom and Joel Hestness},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster},\n volume = {abs/2304.03208},\n year = {2023}\n}\n"
        }
    },
    "1166_tsn": {
        "paperId": "ea3d7de6c0880e14455b9acb28f1bc1234321456",
        "externalIds": {
            "DBLP": "journals/corr/WangXW0LTG16",
            "MAG": "2507009361",
            "ArXiv": "1608.00859",
            "DOI": "10.1007/978-3-319-46484-8_2",
            "CorpusId": 5711057
        },
        "corpusId": 5711057,
        "publicationVenue": {
            "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
            "name": "European Conference on Computer Vision",
            "type": "conference",
            "alternate_names": [
                "ECCV",
                "Eur Conf Comput Vis"
            ],
            "url": "https://link.springer.com/conference/eccv"
        },
        "url": "https://www.semanticscholar.org/paper/ea3d7de6c0880e14455b9acb28f1bc1234321456",
        "title": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
        "abstract": null,
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "referenceCount": 42,
        "citationCount": 3335,
        "influentialCitationCount": 625,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1608.00859",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2016-08-02",
        "journal": {
            "pages": "20-36"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2016TemporalSN,\n author = {Limin Wang and Yuanjun Xiong and Zhe Wang and Y. Qiao and Dahua Lin and Xiaoou Tang and L. Gool},\n booktitle = {European Conference on Computer Vision},\n pages = {20-36},\n title = {Temporal Segment Networks: Towards Good Practices for Deep Action Recognition},\n year = {2016}\n}\n"
        }
    },
    "1167_photo-geometric_autoencoder": {
        "paperId": "73fc37d751279286856bfb3b110c5e6ee7968e00",
        "externalIds": {
            "MAG": "3035523051",
            "ArXiv": "1911.11130",
            "DBLP": "conf/ijcai/Wu0V21",
            "DOI": "10.1109/cvpr42600.2020.00008",
            "CorpusId": 208267756
        },
        "corpusId": 208267756,
        "publicationVenue": {
            "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
            "name": "Computer Vision and Pattern Recognition",
            "type": "conference",
            "alternate_names": [
                "CVPR",
                "Comput Vis Pattern Recognit"
            ],
            "issn": "1063-6919",
            "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
            "alternate_urls": [
                "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/73fc37d751279286856bfb3b110c5e6ee7968e00",
        "title": "Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild",
        "abstract": "We propose a method to learn 3D deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination. In order to disentangle these components without supervision, we use the fact that many object categories have, at least in principle, a symmetric structure. We show that reasoning about illumination allows us to exploit the underlying object symmetry even if the appearance is not symmetric due to shading. Furthermore, we model objects that are probably, but not certainly, symmetric by predicting a symmetry probability map, learned end-to-end with the other components of the model. Our experiments show that this method can recover very accurately the 3D shape of human faces, cat faces and cars from single-view images, without any supervision or a prior shape model. On benchmarks, we demonstrate superior accuracy compared to another method that uses supervision at the level of 2D image correspondences.",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "referenceCount": 107,
        "citationCount": 263,
        "influentialCitationCount": 41,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1911.11130",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper proposes a method to learn 3D deformable object categories from raw single-view images, without external supervision, based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2019-11-25",
        "journal": {
            "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "pages": "1-10"
        },
        "citationStyles": {
            "bibtex": "@Article{Wu2019UnsupervisedLO,\n author = {Shangzhe Wu and C. Rupprecht and A. Vedaldi},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1-10},\n title = {Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild},\n year = {2019}\n}\n"
        }
    },
    "1168_deep_boltzmann_machines": {
        "paperId": "ddc45fad8d15771d9f1f8579331458785b2cdd93",
        "externalIds": {
            "MAG": "189596042",
            "DBLP": "journals/jmlr/SalakhutdinovH09",
            "CorpusId": 877639
        },
        "corpusId": 877639,
        "publicationVenue": {
            "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
            "name": "International Conference on Artificial Intelligence and Statistics",
            "type": "conference",
            "alternate_names": [
                "AISTATS",
                "Int Conf Artif Intell Stat"
            ]
        },
        "url": "https://www.semanticscholar.org/paper/ddc45fad8d15771d9f1f8579331458785b2cdd93",
        "title": "Deep Boltzmann Machines",
        "abstract": "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "year": 2009,
        "referenceCount": 22,
        "citationCount": 2197,
        "influentialCitationCount": 234,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Mathematics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new learning algorithm for Boltzmann machines that contain many layers of hidden variables that is made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2009-04-15",
        "journal": {
            "pages": "448-455"
        },
        "citationStyles": {
            "bibtex": "@Article{Salakhutdinov2009DeepBM,\n author = {R. Salakhutdinov and Geoffrey E. Hinton},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n pages = {448-455},\n title = {Deep Boltzmann Machines},\n year = {2009}\n}\n"
        }
    },
    "1170_bert-large-cas_(ptb+wt2+wt103)": {
        "paperId": "8cd4054b41936ba0889edc26be8969c3dc8491d8",
        "externalIds": {
            "ArXiv": "1904.09408",
            "MAG": "2936497627",
            "DBLP": "journals/corr/abs-1904-09408",
            "CorpusId": 127980590
        },
        "corpusId": 127980590,
        "publicationVenue": {
            "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
            "name": "arXiv.org",
            "alternate_names": [
                "ArXiv"
            ],
            "issn": "2331-8422",
            "url": "https://arxiv.org"
        },
        "url": "https://www.semanticscholar.org/paper/8cd4054b41936ba0889edc26be8969c3dc8491d8",
        "title": "Language Models with Transformers",
        "abstract": "The Transformer architecture is superior to RNN-based models in computational efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer models on various NLP tasks using pre-trained language models on large-scale corpora. Surprisingly, these Transformer architectures are suboptimal for language model itself. Neither self-attention nor the positional encoding in the Transformer is able to efficiently incorporate the word-level sequential context crucial to language modeling. \nIn this paper, we explore effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient. We propose Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model. Experimental results on the PTB, WikiText-2, and WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs. The source code is publicly available.",
        "venue": "arXiv.org",
        "year": 2019,
        "referenceCount": 43,
        "citationCount": 94,
        "influentialCitationCount": 7,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper explores effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient, and proposes Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2019-04-20",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1904.09408"
        },
        "citationStyles": {
            "bibtex": "@Article{Wang2019LanguageMW,\n author = {Chenguang Wang and Mu Li and Alex Smola},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Language Models with Transformers},\n volume = {abs/1904.09408},\n year = {2019}\n}\n"
        }
    },
    "1171_cnn_best_practices": {
        "paperId": "5562a56da3a96dae82add7de705e2bd841eb00fc",
        "externalIds": {
            "DBLP": "conf/icdar/SimardSP03",
            "MAG": "2156163116",
            "DOI": "10.1109/ICDAR.2003.1227801",
            "CorpusId": 4659176
        },
        "corpusId": 4659176,
        "publicationVenue": null,
        "url": "https://www.semanticscholar.org/paper/5562a56da3a96dae82add7de705e2bd841eb00fc",
        "title": "Best practices for convolutional neural networks applied to visual document analysis",
        "abstract": "Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple \"do-it-yourself\" implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependentlearning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages.",
        "venue": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.",
        "year": 2003,
        "referenceCount": 10,
        "citationCount": 2767,
        "influentialCitationCount": 157,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A set of concrete bestpractices that document analysis researchers can use to get good results with neural networks, including a simple \"do-it-yourself\" implementation of convolution with a flexible architecture suitable for many visual document problems."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2003-08-03",
        "journal": {
            "name": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.",
            "pages": "958-963"
        },
        "citationStyles": {
            "bibtex": "@Article{Simard2003BestPF,\n author = {P. Simard and David Steinkraus and John C. Platt},\n booktitle = {Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.},\n journal = {Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.},\n pages = {958-963},\n title = {Best practices for convolutional neural networks applied to visual document analysis},\n year = {2003}\n}\n"
        }
    },
    "1172_ernie-doc_(247m)": {
        "paperId": "7434fb6c79a50bfedd4ad1c872f9e4fb1f25571a",
        "externalIds": {
            "ACL": "2021.acl-long.227",
            "DBLP": "conf/acl/DingSWSTW020",
            "ArXiv": "2012.15688",
            "DOI": "10.18653/v1/2021.acl-long.227",
            "CorpusId": 229923177
        },
        "corpusId": 229923177,
        "publicationVenue": {
            "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
            "name": "Annual Meeting of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "Annu Meet Assoc Comput Linguistics",
                "Meeting of the Association for Computational Linguistics",
                "ACL",
                "Meet Assoc Comput Linguistics"
            ],
            "url": "https://www.aclweb.org/anthology/venues/acl/"
        },
        "url": "https://www.semanticscholar.org/paper/7434fb6c79a50bfedd4ad1c872f9e4fb1f25571a",
        "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
        "abstract": "Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "referenceCount": 47,
        "citationCount": 34,
        "influentialCitationCount": 2,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.acl-long.227.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": null
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2021-01-01",
        "journal": {
            "pages": "2914-2927"
        },
        "citationStyles": {
            "bibtex": "@Article{Ding2021ERNIEDocAR,\n author = {Siyu Ding and Junyuan Shang and Shuohuan Wang and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2914-2927},\n title = {ERNIE-Doc: A Retrospective Long-Document Modeling Transformer},\n year = {2021}\n}\n"
        }
    },
    "1174_lamemo": {
        "paperId": "a99cbe3d7b2e201491a2fa1f4ba57b1e80cca3a3",
        "externalIds": {
            "DBLP": "journals/corr/abs-2204-07341",
            "ArXiv": "2204.07341",
            "ACL": "2022.naacl-main.422",
            "DOI": "10.48550/arXiv.2204.07341",
            "CorpusId": 248218547
        },
        "corpusId": 248218547,
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "url": "https://www.semanticscholar.org/paper/a99cbe3d7b2e201491a2fa1f4ba57b1e80cca3a3",
        "title": "LaMemo: Language Modeling with Look-Ahead Memory",
        "abstract": "Although Transformers with fully connected self-attentions are powerful to model long-term dependencies, they are struggling to scale to long texts with thousands of words in language modeling. One of the solutions is to equip the model with a recurrence memory. However, existing approaches directly reuse hidden states from the previous segment that encodes contexts in a uni-directional way. As a result, this prohibits the memory to dynamically interact with the current context that provides up-to-date information for token prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo) that enhances the recurrence memory by incrementally attending to the right-side tokens and interpolating with the old memory states to maintain long-term information in the history. LaMemo embraces bi-directional attention and segment recurrence with an additional computation overhead only linearly proportional to the memory length. Experiments on widely used language modeling benchmarks demonstrate its superiority over the baselines equipped with different types of memory mechanisms.",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2022,
        "referenceCount": 35,
        "citationCount": 2,
        "influentialCitationCount": 1,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2204.07341",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "Look-Ahead Memory (LaMemo) is proposed that enhances the recurrence memory by incrementally attending to the right-side tokens and interpolating with the old memory states to maintain long-term information in the history."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2022-04-15",
        "journal": {
            "pages": "5747-5762"
        },
        "citationStyles": {
            "bibtex": "@Article{Ji2022LaMemoLM,\n author = {Haozhe Ji and Rongsheng Zhang and Zhenyu Yang and Zhipeng Hu and Minlie Huang},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {5747-5762},\n title = {LaMemo: Language Modeling with Look-Ahead Memory},\n year = {2022}\n}\n"
        }
    },
    "1175_elmo": {
        "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
        "externalIds": {
            "DBLP": "conf/naacl/PetersNIGCLZ18",
            "MAG": "2962739339",
            "ArXiv": "1802.05365",
            "ACL": "N18-1202",
            "DOI": "10.18653/v1/N18-1202",
            "CorpusId": 3626819
        },
        "corpusId": 3626819,
        "publicationVenue": {
            "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
            "name": "North American Chapter of the Association for Computational Linguistics",
            "type": "conference",
            "alternate_names": [
                "North Am Chapter Assoc Comput Linguistics",
                "NAACL"
            ],
            "url": "https://www.aclweb.org/portal/naacl"
        },
        "url": "https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f",
        "title": "Deep Contextualized Word Representations",
        "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2018,
        "referenceCount": 64,
        "citationCount": 10572,
        "influentialCitationCount": 1460,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N18-1202.pdf",
            "status": "HYBRID"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            },
            {
                "category": "Linguistics",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "A new type of deep contextualized word representation is introduced that models both complex characteristics of word use and how these uses vary across linguistic contexts, allowing downstream models to mix different types of semi-supervision signals."
        },
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "publicationDate": "2018-02-15",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/1802.05365"
        },
        "citationStyles": {
            "bibtex": "@Article{Peters2018DeepCW,\n author = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Deep Contextualized Word Representations},\n volume = {abs/1802.05365},\n year = {2018}\n}\n"
        }
    },
    "1177_dlrm-12t": {
        "paperId": "88bb275161782df0d0873de01797964e6ca7b9d4",
        "externalIds": {
            "ArXiv": "2104.05158",
            "DBLP": "conf/isca/MudigereHHJT0LO22",
            "DOI": "10.1145/3470496.3533727",
            "CorpusId": 237414938
        },
        "corpusId": 237414938,
        "publicationVenue": {
            "id": "deedf64a-dd5c-4b33-b345-ff83bfb93d71",
            "name": "International Symposium on Computer Architecture",
            "type": "conference",
            "alternate_names": [
                "Int Symp Comput Archit",
                "ISCA"
            ],
            "url": "http://www.cs.wisc.edu/~arch/www/"
        },
        "url": "https://www.semanticscholar.org/paper/88bb275161782df0d0873de01797964e6ca7b9d4",
        "title": "Software-hardware co-design for fast and scalable training of deep learning recommendation models",
        "abstract": "Deep learning recommendation models (DLRMs) have been used across many business-critical services at Meta and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper, we present Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs. Neo employs a novel 4D parallelism strategy that combines table-wise, row-wise, column-wise, and data parallelism for training massive embedding operators in DLRMs. In addition, Neo enables extremely high-performance and memory-efficient embedding computations using a variety of critical systems optimizations, including hybrid kernel fusion, software-managed caching, and quality-preserving compression. Finally, Neo is paired with ZionEX, a new hardware platform co-designed with Neo's 4D parallelism for optimizing communications for large-scale DLRM training. Our evaluation on 128 GPUs using 16 ZionEX nodes shows that Neo outperforms existing systems by up to 40\u00d7 for training 12-trillion-parameter DLRM models deployed in production.",
        "venue": "International Symposium on Computer Architecture",
        "year": 2021,
        "referenceCount": 75,
        "citationCount": 87,
        "influentialCitationCount": 12,
        "isOpenAccess": true,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2104.05158",
            "status": "GREEN"
        },
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "This paper presents Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs that employs a novel 4D parallelism strategy that combines table-wise, row- Wise, column- wise, and data parallelism for training massive embedding operators inDLRMs."
        },
        "publicationTypes": [
            "JournalArticle",
            "Book",
            "Conference"
        ],
        "publicationDate": "2021-04-12",
        "journal": {
            "name": "Proceedings of the 49th Annual International Symposium on Computer Architecture"
        },
        "citationStyles": {
            "bibtex": "@Article{Mudigere2021SoftwarehardwareCF,\n author = {Dheevatsa Mudigere and Y. Hao and Jianyu Huang and Zhihao Jia and Andrew Tulloch and Srinivas Sridharan and Xing Liu and Mustafa Ozdal and Jade Nie and Jongsoo Park and Liangchen Luo and J. Yang and Leon Gao and Dmytro Ivchenko and Aarti Basant and Yuxi Hu and Jiyan Yang and E. K. Ardestani and Xiaodong Wang and Rakesh Komuravelli and Ching-Hsiang Chu and Serhat Yilmaz and Huayu Li and Jiyuan Qian and Zhuobo Feng and Yi-An Ma and Junjie Yang and Ellie Wen and Hong Li and Lin Yang and Chonglin Sun and Whitney Zhao and Dimitry Melts and Krishnaveni Dhulipala and Kranthi G. Kishore and Tyler N. Graf and Assaf Eisenman and Kiran Kumar Matam and Adi Gangidi and Guoqiang Jerry Chen and M. Krishnan and A. Nayak and Krishnakumar Nair and Bharath Muthiah and Mahmoud khorashadi and P. Bhattacharya and Petr Lapukhov and M. Naumov and A. Mathews and Lin Qiao and M. Smelyanskiy and Bill Jia and Vijay Rao},\n booktitle = {International Symposium on Computer Architecture},\n journal = {Proceedings of the 49th Annual International Symposium on Computer Architecture},\n title = {Software-hardware co-design for fast and scalable training of deep learning recommendation models},\n year = {2021}\n}\n"
        }
    },
    "1178_flan_137b": {
        "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "externalIds": {
            "DBLP": "journals/corr/abs-2109-01652",
            "ArXiv": "2109.01652",
            "CorpusId": 237416585
        },
        "corpusId": 237416585,
        "publicationVenue": {
            "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
            "name": "International Conference on Learning Representations",
            "type": "conference",
            "alternate_names": [
                "Int Conf Learn Represent",
                "ICLR"
            ],
            "url": "https://iclr.cc/"
        },
        "url": "https://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "title": "Finetuned Language Models Are Zero-Shot Learners",
        "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "referenceCount": 170,
        "citationCount": 1831,
        "influentialCitationCount": 254,
        "isOpenAccess": false,
        "openAccessPdf": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "s2FieldsOfStudy": [
            {
                "category": "Computer Science",
                "source": "external"
            },
            {
                "category": "Computer Science",
                "source": "s2-fos-model"
            }
        ],
        "tldr": {
            "model": "tldr@v2.0.0",
            "text": "It is shown that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks and outperforms few-shot GPT-3 by a large margin."
        },
        "publicationTypes": [
            "JournalArticle"
        ],
        "publicationDate": "2021-09-03",
        "journal": {
            "name": "ArXiv",
            "volume": "abs/2109.01652"
        },
        "citationStyles": {
            "bibtex": "@Article{Wei2021FinetunedLM,\n author = {Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Finetuned Language Models Are Zero-Shot Learners},\n volume = {abs/2109.01652},\n year = {2021}\n}\n"
        }
    }
}