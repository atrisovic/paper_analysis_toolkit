model.name,model.title,citation.openAccessPdf,How does it use <model.name> ?,Secondary use of the <model.name> (if applicable),Citation Sentence,Does the paper introduce a new model?,"If yes, what is the name of the model? (not named is fine)",Does the paper use AI model?,"If yes, which model it is using?",Notes
Spatially-Sparse CNN,Spatially-sparse convolutional neural networks,https://www.mdpi.com/1424-8220/23/14/6427/pdf?version=1689577851,,,,No,,No,,
MobileNet,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,https://www.frontiersin.org/articles/10.3389/fpls.2023.1308528/pdf?isPublishedV2=False,Uses,,Depthwise separable convolutions boosted computational efficiency in MobileNetV1 Howard et al. (2017). MobileNetV2 added a resource-efficient block with inverted residuals and linear bottlenecks to improve efficiency Howard et al. (2018).,No,,Yes,,Good example how models detect plant disease in agricultural applications.
AudioGen,AudioGen: Textually Guided Audio Generation,http://arxiv.org/pdf/2303.03857,Background,,,No,,Yes,,
Robot Parkour,Robot Parkour Learning,https://arxiv.org/pdf/2310.04828,Background,,"Recent advancements in reinforcement learning (RL) and optimal control have empowered quadrupedal robots to per- form a series of dynamic tasks, such as navigating diverse terrains in the wild [1], [2], [3], achieving high-speed run- ning [4], [5], engaging in parkour [6], executing jumps [7], and standing up on hind legs [8], [9].",Yes,,Yes,,
ReLU (NORB),Rectified Linear Units Improve Restricted Boltzmann Machines,https://arxiv.org/pdf/2308.16316,Background,,"However, DM-GAN’s computational complexity and memory management pose challenges, and it relies on labeled data [154], [155].",Yes,,Yes,,
ERNIE 3.0,ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,https://arxiv.org/pdf/2211.05994,Background,,Incorporating knowledge into PLMs can empower their memorization and reasoning [10].,No,,No,,
,,,Background,,ERNIE [62] proposes a multi-stage knowledge masking strategy of both entity level and phase level.,,,,,
ContextNet,ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context,https://arxiv.org/pdf/2210.15781,Extends,,"TitaNet-LID-BxRxC consists of an encoder which is a 1D depth-wise channel separable convolutional model with a ContextNet-like [20, 23] architecture and a decoder.",Yes,,Yes,,
,,,Background,,"Among several proposed methods [15, 18, 19, 20], ContextNet, proposed by [20], builds upon previous work [21] and incorporates a Squeeze-and-Excitation (SE) layer [22] to compress a sequence of local feature vectors into a single global context vector.",,,,,
,,,Background,,"In the ASR domain, [20] demonstrated that incorporating global context improved accuracy.",,,,,
EfficientNetV2,EfficientNetV2: Smaller Models and Faster Training,https://arxiv.org/pdf/2301.10750,,,,,,,,
R-FCN,R-FCN: Object Detection via Region-based Fully Convolutional Networks,https://www.mdpi.com/1424-8220/23/8/3852/pdf?version=1681106508,,,,,,,,
GPT-3 175B (davinci),Language Models are Few-Shot Learners,https://www.mdpi.com/2226-471X/9/2/47/pdf?version=1706518556,,,,,,,,
CURL,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,https://arxiv.org/pdf/2305.18510,,,,,,,,
SemExp,Object Goal Navigation using Goal-Oriented Semantic Exploration,http://arxiv.org/pdf/2210.09435,,,,,,,,
data2vec (speech),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://ieeexplore.ieee.org/ielx7/6287639/6514899/10235987.pdf,,,,,,,,
Pluribus,Superhuman AI for multiplayer poker,https://ojs.aaai.org/index.php/AAAI/article/download/20431/20190,Background,,"Among these, the most successful examples include the CFR algorithm (Zinkevich et al. 2007) and its modern variants (Tammelin 2014; Moravcík et al. 2017; ˇ
Brown and Sandholm 2017b,a, 2019a,b; Davis, Waugh, and Bowling 2019; Farina, Kroer, and Sandholm 2021b; Morrill et al. 2021), and methods based on accelerated firstorder methods such as EGT (Nesterov 2005; Hoda et al. 2010; Kroer, Farina, and Sandholm 2018; Farina, Kroer, and Sandholm 2021a) and Mirror Prox (Nemirovski 2004; Kroer
2019; Farina, Kroer, and Sandholm 2021a), which are able to scale to large two-player extensive-form games and compute approximate Nash equilibria for moderate approximation gaps.",Yes,,Yes,,
,,,Background,,"Hundreds of papers have been published on it, the AAAI Annual Computer Poker Competition was organized, and superhuman AI performance has been achieved (Bowling et al. 2015; Brown and Sandholm 2017b, 2019b).",,,,,
DeBERTa,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,https://arxiv.org/pdf/2308.06552,,,,,,,,
GloVe (32B),GloVe: Global Vectors for Word Representation,https://journals.tubitak.gov.tr/cgi/viewcontent.cgi?article=4022&context=elektrik,,,,,,,,
TransE,Translating Embeddings for Modeling Multi-relational Data,https://www.nature.com/articles/s42256-023-00684-8.pdf,,,,,,,,
ESM2-15B,Evolutionary-scale prediction of atomic level protein structure with a language model,https://www.biorxiv.org/content/biorxiv/early/2023/12/01/2023.11.30.569352.full.pdf,Uses,,"To achieve this, we first used ESM-2 8M (version 2 with 8 million 215 parameters) to generate the vectors for a set of genes",Yes,,Yes,,
IBM-5,The Mathematics of Statistical Machine Translation: Parameter Estimation,https://arxiv.org/pdf/1607.08692,,,,,,,,Can't find the reference of this model
LUKE,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,https://aclanthology.org/2021.eval4nlp-1.5.pdf,Background,,"We examined several papers with state of the art NER results on the CoNLL 2003 dataset considering guidelines 1, 2, and 3. Of these papers Liu et al. (2019) follow 1, 2, and 3. Yamada et al. (2020) explicitly follows guidelines 2 and 3. Luoma and Pyysalo (2020) met guideline 1.",No,,Yes,,
ALIGN,Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision,https://aclanthology.org/2023.emnlp-main.301.pdf,Background,,"Underlying several popular multimodal models like CLIP (Radford et al., 2021), DALL-E 2 (Ramesh et al., 2022) and ALIGN (Jia et al., 2021) is a pooled text encoder, i.e., a text representation model that outputs a single vector for a given input caption",No,,Yes,,
XLM,Cross-lingual Language Model Pretraining,https://aclanthology.org/2021.emnlp-main.685.pdf,Motivation,,"As different tasks have different dataset sizes, we follow Conneau and Lample (2019) to employ a balanced sampling strategy",Yes,CodeT5,Yes,CodeT5,An example to show how the paper uses a similar sampling method as the one used for the AI model
LSTM,Long Short-Term Memory,https://www.biorxiv.org/content/biorxiv/early/2024/01/25/2024.01.24.577123.full.pdf,Background,,"However, the journey is not without its challenges. Despite all recent successes, optimization remains a key concern for deep learning, and while innovations such as LSTMs (39), ResNet’s skipped-connections (40), reward shaping (41) and sensorimotor priors (42, 43) have made strides in addressing these challenges, the true potential of RL lies in its adaptability",Yes,SDS,Yes,SDS,Good example of using a previous AI model to develop a new model that finds motor control policies in complex object manipulation tasks.
,,,Motivation,,"We used the powerful onpolicy RL algorithm PPO (16) from the Stable Baselines 3 library (46) with a recurrent architecture that has LSTM layers (39) in both the actor and critic, which allowed us to deal with the partially observable environment (Fig.1).",,,,,
S4,Efficiently Modeling Long Sequences with Structured State Spaces,https://arxiv.org/pdf/2306.16524,Background,,"Earlier examples of SSM layers in deep learning model includes Structured State Space(S4) 31, its variants 32,33 and Gated State Space (GSS)34",Yes,Hyena,Yes,Hyena,
NLLB,No Language Left Behind: Scaling Human-Centered Machine Translation,https://aclanthology.org/2023.wmt-1.24.pdf,Background,,"However, it is worth noting that GPT4-5shot displayed subpar performance when applied to legal data, while NLLB_Greedy demonstrated comparatively lower performance in the context of environmental data.",No,,Yes,,Evaluation on different machine translation systems
Relational Memory Core,Relational recurrent neural networks,https://link.springer.com/content/pdf/10.1007/s10489-022-03550-z.pdf,Background,,"A variety of RL-related methods have been proposed to make RL feasible in large-scale applications. One approach is to augment the neural network with a “memory” to enhance sample efficiency in complicated environments [127, 128].",No,,No,,
Base LM + kNN LM + Continuous Cache,Generalization through Memorization: Nearest Neighbor Language Models,https://arxiv.org/pdf/2309.14928,Motivation,,"Inspired by the adapter idea in supervised methods [11, 20, 32, 52, 52], NtUA introduces weighted key-value cache which formulates the CLIP-extracted visual features as keys, the predicted pseudo-labels of target samples as values, and the corresponding pseudo-label confidence as weights of the key-value pairs as illustrated in Fig. 1.",Yes,NtUA,Yes,NtUA,
VGG19,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://www.mdpi.com/1424-8220/24/1/89/pdf?version=1703323668,Differences,,"Unlike other common networks such as ResNet and VGGNet that encode images into low-resolution representations through a sequential arrangement of convolutional layers [41–43], HRNet adopts a parallel processing framework.",Yes,,,,
S-Norm,Simple and Effective Multi-Paragraph Reading Comprehension,https://www.aclweb.org/anthology/D19-5815.pdf,Background,,"Such findings indicate that there are certain shortcuts in solving reading comprehension tasks that allow a model to find the answer by superficial clues such as lexical overlap and entity types (Clark and Gardner, 2018; Sugawara et al., 2018)",No,,No,,
ProBERTa,Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks,https://www.biorxiv.org/content/biorxiv/early/2023/02/09/2023.02.03.526917.full.pdf,Background,,"There is growing interest in developing protein language models (pLMs) at the scale of evolution due to the abundance of 1D amino acid sequences, such as the series of ESM (Rives et al., 2019; Lin et al., 2022), TAPE (Rao et al., 2019), ProtTrans (Elnaggar et al., 2021), PRoBERTa (Nambiar et al., 2020), PMLM (He et al., 2021), ProteinLM (Xiao et al., 2021), PLUS (Min et al., 2021), Adversarial MLM (McDermott et al., 2021), ProteinBERT (Brandes et al., 2022), CARP (Yang et al., 2022a) in masked language modeling (MLM) fashion, ProtGPT2 (Ferruz et al., 2022) in causal language modeling fashion, and several others (Melnyk et al., 2022a; Madani et al., 2021; Unsal et al., 2022; Nourani et al., 2021; Lu et al., 2020; Sturmfels et al., 2020; Strodthoff et al., 2020).",Yes,LM-DESIGN,Yes,LM-DESIGN,
Heuristic problem solving for AI,Steps toward Artificial Intelligence,https://aclanthology.org/2022.trustnlp-1.1.pdf,Background,,"Attribution analysis, or credit assignment, concerns how individual components of a system contribute to its overall performance (Minsky, 1961).",Yes,,Yes,,
Switch,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/pdf/2102.04906,Background,,"On NLP tasks, sparely gated MoE [16] and switch Transformer [81] embeds hard MoE in a long shortterm memory (LSTM) [82] network and a Transformer [6], respectively. y. Instead of making choice with binary gates as in [80], only the branches corresponding to the top-K elements of the real-valued gates are activated in [16], [78], [81].",No,,No,,
SemVec,Linguistic Regularities in Continuous Space Word Representations,http://arxiv.org/pdf/2210.02771,Uses,,"We test this approach for k = 1 and k = 3, using embeddings from GloVe (Pennington et al., 2014) and Skip-gram8 (Mikolov et al., 2013), and using the embeddings predicted by our BERT-base encoder pre-trained on MSCG+PREFIX+GKB.",Yes,,Yes,SemVec,use embeddings from the AI model to test new approach
Sandwich Transformer,Improving Transformer Models by Reordering their Sublayers,https://www.mdpi.com/2076-3417/12/9/4502/pdf?version=1651215512,Background,,"In [16], the sub-layers of the encoder and decoder in the Transformer, such as the self-attention and feed forward layers, were reordered to reduce the model’s perplexity and increase the model’s robustness. In this study, we also aim to improve the efficiency of the refined model and shorten the training time.",Yes,X-Transformer,Yes,X-Transformer,
,,,Similarities,,"Finally, we chose s-sf-sf as our proposed model because it has fewer model parameters, while the BLEU score is almost the same. A similar notion was also proposed in [16]. If the self-attention mechanism appears earlier in the model and stacks more than the number of feed forward layers, the perplexity of the language model can be much lower.",,,,,use a similiar idea in the AI model to improve a current model
Transformer (Adaptive Input Embeddings),Adaptive Input Representations for Neural Language Modeling,https://www.aclweb.org/anthology/D19-1539.pdf,Uses,,"We adapt the transformer implementation available in the fairseq toolkit to our two tower architecture (Ott et al., 2019). For hyper-parameter and optimization choices we mostly follow Baevski and Auli (2018).",Yes,,Yes,Transformer (Adaptive Input Embeddings),
MT-DNN,Multi-Task Deep Neural Networks for Natural Language Understanding,https://arxiv.org/pdf/2308.08234,Background,,"One such approach, MT-DNN (Liu et al., 2019b), batches all the GLUE tasks (Wang et al., 2018) together and updates the model accordingly.",Yes,CMTL,Yes,CMTL,
,,,Background,,"Two learning types were mainly discussed. First, joint learning was used in an MTL setting where all the tasks are equally important (Kendall et al., 2018; Liu et al., 2019b).",,,,,
Culturome,Quantitative Analysis of Culture Using Millions of Digitized Books,https://serval.unil.ch/resource/serval:BIB_9947DF4A7067.P001/REF.pdf,Background,,"But the larger and the more ‘complete’ a corpus is, the greater the danger to succumb to an ‘implicit essentialism’22 and to mistake the model for the original: it seems to contain ‘everything’, so it must be ‘true’ (something that can often be observed in the field of cultoromics,23 when arguments are being made on the basis of the Google Books Ngram Corpus).",No,,No,,
Regularized SVD for Collaborative Filtering,Improving regularized singular value decomposition for collaborative filtering,https://eprints.sztaki.hu/9319/1/Palovics_400_3311628_ny.pdf,Background,,"Alpenglow1 is a free and open source C++ based framework with easy-to-use Python API especially suited for conjoint batch and online learning. We experiment in Aplenglow with • non-personalized temporal popularity and item-to-item recommender models, • time-aware variants of nearest neighbor [9], • SGD based batch and online matrix factorization [10], • asymmetric matrix factorization [8].",No,,No,,
GSM,Gated Self-Matching Networks for Reading Comprehension and Question Answering,https://link.springer.com/content/pdf/10.1007/s44267-023-00010-1.pdf,Uses,,"The predicted relatedness R(xi, xj) is the average value of two related attention scores rij and rji: where WrQ and WrM are two learnable matrices for query and memory according to the query-memory attention mechanism [41].",Yes,,Yes,GSM,
MnasNet-A3,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://arxiv.org/pdf/2212.06524,Uses,,"Concretely, a lightweight MnasNet [24] is applied for image feature Ft,color extraction.",Yes,SST,Yes,MnasNet-A3,
Statistical Shape Constellations,Unsupervised Learning of Models for Recognition,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0203897&type=printable,Background,,"In computer vision field, traditional methods [19–21] and deep learning based methods [22–28] are applied to solve the object detection problems",Yes,Enhanced Region Proposal Network (ERPN),Yes,Region Proposal Network (RPN),
PolyCoder,A systematic evaluation of large language models of code,http://arxiv.org/pdf/2305.19213,Background,,"Recently, large language models of code (Code-LLMs) are receiving increasing attention (Chen et al., 2021; Xu et al., 2022).",No,,Yes,"DAVINCI002, CODEX","compared to textonly LLMs, Code-LLMs with code prompts are significantly better in causal reasoning"
EDSR,Enhanced Deep Residual Networks for Single Image Super-Resolution,https://arxiv.org/pdf/2211.06770,Background,,"The problem of image restoration and enhancement has been addressed in several papers, though many were dealing only with particular aspects such as super-resolution [4, 8, 21,24,29,32,35,44,45,52], denoising [1,2,11,15,43,53,54], color and tone mapping [36, 38, 49, 50], luminance, gamma and contrast adjustment [5, 10, 51].",Yes,MicroISP,Yes,MicroISP,
ContextNet,ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context,https://arxiv.org/pdf/2210.17098,,,,,,,,Not sure how to record this because it's cited in a table
TransE,Translating Embeddings for Modeling Multi-relational Data,https://arxiv.org/pdf/2310.08917,Background,,"This technique involves transforming entities and relations into low-dimensional vectors and using a scoring function (Bordes et al., 2013; Wang et al., 2017) to assess the plausibility of a triplet (consisting of a head entity, a relation, and a tail entity).",Yes,RelEns-DSC,Yes,TransE,
,,,Background,,"Well-known scoring functions, such as TransE (Bordes et al., 2013), ComplEx (Trouillon et al., 2017), ConvE (Dettmers et al., 2018), and CompGCN (Vashishth et al., 2020), have demonstrated remarkable success in learning from KGs.",,,,,
,,,Motivation,,"Following (Bordes et al., 2013; Trouillon et al., 2017; Sun et al., 2019; Vashishth et al., 2020), we adopt mean reciprocal ranking (MRR) as the evaluation metric. Larger MRR indicates better performance",,,,,
,,,Uses,,"We select some representative embedding models as our base models Fi , including: (i) translational distance models TransE (Bordes et al., 2013), RotatE (Sun et al., 2019), HousE (Li et al., 2022); (ii) bilinear model ComplEx (Trouillon et al., 2017); (iii) neural network model ConvE (Dettmers et al., 2018); and (iv) GNN based model CompGCN (Vashishth et al., 2020)",,,,,
ResNeXt-50,Aggregated Residual Transformations for Deep Neural Networks,http://arxiv.org/pdf/2306.05401,Uses,,"Mean accuracy of different backbone architectures on CCC-Medium. Accuracy reported is an average across 9 runs. Backbones used: [3, 10, 23, 43, 51], †: AugMix [14], ‡: DeepAugment [11].",Yes,RDumb,Yes,ResNeXt-50,
ViT-G (model soup),Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,http://arxiv.org/pdf/2301.10092,Background,Uses,The publication ”Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time” [1] proposes to average the weights of the different models to maximize the use of all the training without wasting learning time.,Yes,Pruned Soup,Yes,"Model Soups, ResNet, ViT, EfficientNet",first test the Model Souping (the AI model) and then introduce a new model based on the original AI model 
SmooCT,Self-play Monte-Carlo tree search in computer poker,https://ojs.aaai.org/index.php/AAAI/article/download/25661/25433,Background,,"Others used Monte Carlo simulation-based approaches (Schweizer et al. 2009; Broeck, Driessens, and Ramon 2009) including Monte Carlo Tree Search (Heinrich and Silver 2014), which is a very popular and successful technique in games in general (Swiechowski et al. 2023).","Yes, fine tuned",DEVN,Yes,Counterfactual Values (CFVs),
DeepNet,"DeepNet: Scaling Transformers to 1, 000 Layers",https://arxiv.org/pdf/2310.03724,Uses,Differences,"We compare our results to massively multilingual supervised models, M2M100 [20] and Deepnet [21].",Yes,,Yes,"M2M100, Deepnet, T-Modules","The source paper compares their model with the target paper, not sure if i catogrize it into the correct 'use' labels?"
RBM Image Classifier,Learning Multiple Layers of Features from Tiny Images,https://ieeexplore.ieee.org/ielx7/6570653/7076742/10233848.pdf,Uses,,"The framework was tested using the ResNet18 model [24] on the CIFAR-10 dataset [25], which consists of 50000 training images and 10000 test images classified into 10 classes.",Yes,F-2T2R,Yes,F-2T2R,
Megatron-Turing NLG 530B,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",http://arxiv.org/pdf/2302.10360,Uses,Motivation,"We used a variety of real models that have been introduced by other works, and then designed our family of hypothetical future models FUTURE-* in a similar fashion, keeping a reasonable sequence length, increasing the embedding dimension drastically, and following the trend of recent large models like PaLM (Chowdhery et al., 2022) and MT-NLG (Smith et al., 2022) of increasing the ratio d/h, which results in favorable energy calculations due to the lower fraction of memory operations in attention.",No,,Yes,"PaLM, MT-NLG (target paper)",
Youtube recommendation model,Deep Neural Networks for YouTube Recommendations,http://arxiv.org/pdf/2302.00158,,,,,,,,no access
Universal approximation via Feedforward Networks,Multilayer feedforward networks are universal approximators,https://www.biorxiv.org/content/biorxiv/early/2023/06/23/2023.05.05.539427.full.pdf,Background,,"Neural networks are machine learning models [17, 18] capable of approximating any function [19]",Yes,Dawnn,Yes,Dawnn,
Denoising Diffusion Probabilistic Models (LSUN Bedroom),Denoising Diffusion Probabilistic Models,https://www.mdpi.com/1424-8220/24/3/829/pdf?version=1706284599,Background,,"Representatively, denoising diffusion probabilistic models (DDPMs) [12] have rendered image generation plausible by gradually denoising sampled data from a Gaussian distribution.",Yes,HDPose,Yes,DDPM (target paper),
,,,Uses,,"Following DDPM [12], this process can be expressed as: xt := √ α¯tx0 + p 1 − α¯tϵ (3) where αt := 1 − βt , α¯t := Πt s=1 αs and ϵ ∼ N (0, I) Gaussian noise ϵ. We can optimize L by randomly sampling t during training, thereby exploiting these properties.",,,,,
ReLU (NORB),Rectified Linear Units Improve Restricted Boltzmann Machines,http://arxiv.org/pdf/2306.12929,Uses,,The configuration “MLP” parameterizes each Gi with a feed-forward net with one hidden layer of size nhid and a ReLU non-linearity [47].,Yes,fine tuned,Yes,,
ProBERTa,Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks,https://www.biorxiv.org/content/biorxiv/early/2024/02/08/2024.02.06.579188.full.pdf,Background,,"Recently, deep learning has been an accelerating force for protein engineers, and large attention based models are at the forefront.[16–18]",Yes,NOMELT,Yes,NOMELT,
Motion-Driven 3D Feature Tracking,A Combined Corner and Edge Detector,https://arxiv.org/pdf/2112.12812,,,,,,,,no access to the target paper
TD(0),An Adaptive Optimal Controller for Discrete-Time Markov Environments,https://www.jstage.jst.go.jp/article/kikaic/79/801/79_1754/_pdf,,,,,,,,Paper written in Japanese
ProtT5-XXL,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,https://www.biorxiv.org/content/biorxiv/early/2021/07/20/2020.11.27.401232.full.pdf,Background,Future Work,"Standard transformer models have already been used to embed unaligned protein sequences (52–54), but the most efficient transformer models released in the last year are now capable of handling sequence lengths even in the millions, and so this suggests that a single deep transformer model with a compressed self-attention mechanism could, in principle, embed a whole MSA in one go by essentially treating it as a single sequence.",Yes,,Yes,,
BASIC-L + Lion,Symbolic Discovery of Optimization Algorithms,https://ieeexplore.ieee.org/ielx7/92/4359553/10313117.pdf,Background,,"Although both target the classification of images, best-inclass classifiers differ by multiple orders of magnitude in terms of model parameters, i.e., 1.5M [51] for MNIST and 2440M [52] for ImageNet.",Yes,,Yes,,
ShuffleNet v2,ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design,https://www.nature.com/articles/s41598-023-42577-1.pdf,Differences,,"Although EfcientNet-B0 consumes more computational resources than these models, it did improve identifcation accuracy when comparing with other lightweight CNN such as SqueezeNet and ShufeNetV2.",Yes,EfcientNet-CA,Yes,EfcientNet-B0,An interesting paper presents a deep learning-based model designed for the rapid and precise identifcation of common horseshoe bats
ShuffleNet v1,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,https://www.mdpi.com/2079-9292/12/22/4617/pdf?version=1699696562,Background,,"MobileNet [20], ShuffleNet [26], and EffNet [27] are examples of common artificially built lightweight models.",Yes,Arc_EffNet,Yes,Arc_EffNet,
Agent57,Agent57: Outperforming the Atari Human Benchmark,https://arxiv.org/pdf/2107.02195,Background,,"Reinforcement learning (RL) algorithms have reached tremendous success in the field of embodied intelligence, including human-level control in Atari games [1], [2] and in firstperson games [3], [4], and super-human control in competitive games [5], [6].",Yes,,Yes,,
Fast R-CNN,Fast R-CNN,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0297059&type=printable,Background,,"Common two-stage detectors include R-CNN [12], Fast R-CNN [13] and Faster R-CNN [14].",Yes,PO-YOLOv5,Yes,PO-YOLOv5,
GShard (dense),GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,https://arxiv.org/pdf/2309.13850,Background,,"These benefits have been empirically demonstrated in several deep learning applications, including natural language processing (Lepikhin et al., 2021; Du et al., 2022; Fedus et al., 2022b; Zhou et al., 2023; Pham et al., 2024), speech recognition (Peng et al., 1996; Gulati et al., 2020; You et al., 2022), computer vision (Dosovitskiy et al., 2021; Riquelme et al., 2021; Liang et al., 2022; Bao et al., 2022), multi-task learning (Hazimeh et al., 2021; Gupta et al., 2022) and other applications (Rives et al., 2021; Chow et al., 2023; Li et al., 2023; Han et al., 2024).",Yes,,Yes,,
Symmetric Residual Encoder-Decoder Net,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,https://arxiv.org/pdf/2303.14934,Background,,"Image denoising aims to restore clean images from noisy observations [5, 11, 14], and it has achieved noticeable improvement with the advances in deep networks",Yes,not named,Yes,"blind-neighborhood network (BNN), locally aware network (LAN)",Discusses a method of denoising images by using BNN for supervision in flat regions of noisy images and LAN for supervision in textured regions of noisy images. 
,,,Background,,"Other learning based methods, such as RED30 [33], MemNet [40], and MWCNN [31], are also developed with advanced architectures",,,,,
Denoising Autoencoders,Extracting and composing robust features with denoising autoencoders,http://www.cell.com/article/S2405844023006515/pdf,,,Recent advancements in Artificial Intelligence (AI) have proven that deep learning models provide promising outcomes in comparison with conventional (shallow) machine learning algorithms in different domains,No,,Yes,"Deep convolutional neural networks (DCNNs), VGG16, VGG19, ResNet50,
InceptionV3, and MobileNet","Interesting article on using models for biomedical image classification, differentiating between beign and malignag cancers. It trains and fine-tunes multiple models (VGG16 and VGG19, ResNet50, InceptionV3, and MobileNet) and finds that ResNet50 does exceptionally well in terms of classification accuracy compared to other models used."
Word Representations,Word Representations: A Simple and General Method for Semi-Supervised Learning,http://ijece.iaescore.com/index.php/IJECE/article/download/11416/11082,,,,,,,,
Fully Convolutional Networks,Fully convolutional networks for semantic segmentation,https://arxiv.org/pdf/2310.15150,,,,,,,,
SEER,Self-supervised Pretraining of Visual Features in the Wild,https://www.pure.ed.ac.uk/ws/files/241297605/Paper_Generative_Models_with_Negative_Retraining.pdf,Background,,Few-shot learning uses prior knowledge to augment the learned classes [12]–[15].,Yes,REtraining with Few-shots Generative Adversarial Network (REFGAN),No,,
,,,Extends,,"REFGAN is adaptive and takes into account OoC as they are introduced. We adapt the prior to detect OoC by moving away from a priori known OoC using two poles, one positive and one negative, in line with Contrastive Learning [15].",,,,,
,,,Background,,"It intersects with other methods such as [28], [15].",,,,,
,,,Extends,,Our REFGAN methodology can perform multi-class classification and recognize the class of OoC using Nearest Neighbors with f-divergences [15].,,,,,
,,,Future Work,,"In the future, we will evaluate REFGAN using a large number of classes using (9) to avoid softmax [39] and compare it to [15] and meta-learning methods that address the few-shot learning setting [25], [40], [41].",,,,,
AmoebaNet-A (F=448),Regularized Evolution for Image Classifier Architecture Search,https://ojs.aaai.org/index.php/AAAI/article/download/25118/24890,Background,,"Similarly, for AmobaNet(Real et al. 2019), to search for clean accuracy leveraging Evolutionary Algorithms, 8 days would be required on 10 GPUs",Yes,Wsr-NAS,Yes,WsrNet,
PaLM (540B),PaLM: Scaling Language Modeling with Pathways,https://arxiv.org/pdf/2310.03026,Background,,"The remarkable achievements of LLMs are undeniably captivating, demonstrating LLM’s human-like reasoning skills and generalization of human commonsense (Bian et al., 2023; Nay, 2022; Chowdhery et al., 2022; Ouyang et al., 2022; Chung et al., 2022)",Yes,LanguageMPC,No,,About automated driving LLM that combinds RL and MPC
GLM-130B,GLM-130B: An Open Bilingual Pre-trained Model,https://arxiv.org/pdf/2308.07107,Background,,"To our knowledge, it is the first model to directly fine-tune LLMs, including ChatGLM [68, 114], ChatGLM2.0 [68, 114], Baichuan [115], and Qwen [116], specifically for the query rewriting task",No,,No,,
VQGAN + CLIP,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/pdf/2211.00680,Uses,,"In this Section we present the results of experiments carried out on images generated by several state-of-the-art generative models including GANs, transformers, and DMs: ProGAN [20], StyleGAN2 [22], StyleGAN3 [11], BigGAN [21], EG3D [30], Taming Transformer [23], DALL·E Mini [24], DALL·E 2 [3], GLIDE [5], Latent Diffusion [25], Stable Diffusion [4] and ADM (Ablated Diffusion Model) [26].",No,,Yes,"ProGAN, StyleGAN2, StyleGAN3, BigGAN, EG3D, Taming Transformer, DALL·E Mini, DALL·E 2, GLIDE, Latent Diffusion, Stable Diffusion and ADM (Ablated Diffusion Model)",Cool application on using AI to generate synthetic images
HMM Word Alignment,HMM-Based Word Alignment in Statistical Translation,http://dl.acm.org/ft_gateway.cfm?id=1118045&type=pdf,,,,,,,,Cannot download securely
PaLI,PaLI: A Jointly-Scaled Multilingual Language-Image Model,http://arxiv.org/pdf/2303.07226,Background,,"However, state-of-the-art vision language models like Flamingo-80B [1], BEIT-3-1.9B[66], and PaLI-17B [6] can be computationally expensive and difficult to train, which has motivated researchers to explore ways of improving their efficiency and effectiveness.",Yes,VL-MoE,No,,
Cognitron,Cognitron: A self-organizing multilayered neural network,https://academic.oup.com/mnras/article-pdf/507/3/4425/40365089/stab2142.pdf,Background,,"The concept of machine learning in computational science started from Fukushima (1975, 1980) and Fukushima, Miyake & Ito (1983).",Yes,"No name just says ""our CNN classifier""",Yes,CNN,
Deeply-recursive ConvNet,Deeply-Recursive Convolutional Network for Image Super-Resolution,https://www.mdpi.com/2072-4292/14/2/257/pdf?version=1641475856,Background,,"In particular, the deep learning-based SRR methods, either using residual networks [37–39], recursive networks[40,41], attention-based networks [42,43], and/or using generative adversarial networks (GANs) [44–46], have become more and more popular over the last decade, not only in the field of picture/photo enhancement, but also in the field of Earth observation for improving the quality and resolution of satellite imagery [34–36].",No,,Yes,MARSGAN SRR,
Base LM + kNN LM + Continuous Cache,Generalization through Memorization: Nearest Neighbor Language Models,http://arxiv.org/pdf/2210.09340,Background,,"k-Nearest Neighbors (kNN)-based approaches have been successfully used in the literature for an array of tasks such as language modeling (Khandelwal et al., 2020), question answering (Kassner and Schütze, 2020), dialogue generation (Fan et al., 2021), etc.",Yes,"No name: "" a framework for transferring knowledge to a low-resource HS corpus by incorporating neighborhood information with Optimal Transport""",Yes,LaBSE,
SemExp,Object Goal Navigation using Goal-Oriented Semantic Exploration,https://arxiv.org/pdf/2307.15320,Background,,"Data-driven policy learning allows to acquire reactive robotic skills and has recently shown impressive results both in simulation and in the real world for tasks such as dexterous manipulation [1], [2], [3], [4], robotic arm manipulation [5], [6], [7], quadruped locomotion [8], [9], [10], [11] and navigation [12], [13], [14], [15].",Yes,,Yes,"DA, DM",
Conv-DBN,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://www.nature.com/articles/s41598-020-69201-w.pdf,Background,,"A DBN, consists of probabilistic models composed of multiple layers of random variables51.",No,,Yes,ResNet-50,
WGAN-GP,Improved Training of Wasserstein GANs,https://arxiv.org/pdf/2308.13295,Uses,,"In practice, the Wasserstein GAN with Gradient Penalty (WGAN-GP) [28] is used.",Yes,,Yes,"OL-GAN, FNN",
DeepNash,Mastering the game of Stratego with model-free multiagent reinforcement learning,https://arxiv.org/pdf/2307.13922,Background,,"Such examples include strong performance in competitive games (Brown & Sandholm, 2019; Perolat et al., 2022), resource allocation (Parise et al., 2020; Amelina et al., 2015) and robotics (Hamann, 2018; Hernandez et al. ´ , 2013).",No,,Yes,Q-Learning dynamics,
LDA,Latent Dirichlet Allocation,https://www.medrxiv.org/content/medrxiv/early/2023/11/17/2023.11.16.23298640.full.pdf,Uses,,"LDA is a generative probabilistic model which applies a distribution of topics over each document, assuming topics are drawn from a Dirichlet distribution.27",Yes,EHR-BERT,Yes,LDA,
Gen-1,Structure and Content-Guided Video Synthesis with Diffusion Models,https://arxiv.org/pdf/2307.10373,,,,,,,,
Adaptive Subgrad,Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,https://arxiv.org/pdf/2302.00195,,,,,,,,
Stacked hourglass network,Stacked Hourglass Networks for Human Pose Estimation,https://www.mdpi.com/1424-8220/23/15/6713/pdf?version=1690437230,,,,,,,,
PNAS-net,Progressive Neural Architecture Search,https://www.mdpi.com/2073-431X/12/9/174/pdf?version=1693565686,Background,,"Similarly, the majority of recent literature on AutoML for DL models concentrates on automating the design of DL models through the neural architecture search (NAS) process [24–28].",No,,No,,
GPT-4,GPT-4 Technical Report,https://arxiv.org/pdf/2310.08889,Background,,"Further, as large language models (LLMs) are drawing much attention in the NLP community, how strong LLMs behave in connecting discrete and continuous space perturbations remains unexplored, especially when GPT-4 (OpenAI, 2023) is known to support images and texts. As these models are not open-source to the public, we leave exploring the perturbation in LLMs in future works.",No,,No,,
DOT(S)-RNN,How to Construct Deep Recurrent Neural Networks,https://link.springer.com/content/pdf/bfm:978-3-030-59338-4/1?pdf=chapter%20toc,Other,,,,,,,could not find citation
Ankh_base,Ankh ☥: Optimized Protein Language Model Unlocks General-Purpose Modelling,https://www.biorxiv.org/content/biorxiv/early/2024/02/01/2024.01.29.577750.full.pdf,Uses,,"Using these mined sequences, we finetuned the complete encoder-decoder Ankh base model27
.
As a masking strategy, we employ unigram T5 span masking (Experiment 4 in the Ankh paper27)
and dynamically sample new masking tokens every epoch. We set the maximum sequence
length to 512 and randomly select 20% of tokens for masking, following the approach described
in the Ankh paper",No,,Yes, fine tuned,
Contriever,Unsupervised Dense Information Retrieval with Contrastive Learning,http://arxiv.org/pdf/2302.01626,Background,,"mContriever (Izacard et al., 2021b) and CCP (Wu et al., 2022a) similarly mine positive pairs by cropping two spans in a document. T",No,,No,,
Two-stream ConvNets for action recognition,Two-Stream Convolutional Networks for Action Recognition in Videos,http://arxiv.org/pdf/2305.01111,Background,,Mentioned in a table,No,,No,,
AlphaX-1,AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://ojs.aaai.org/index.php/AAAI/article/download/17233/17040,Background,,"We compare BANANAS to a host of popular NAS algorithms including random search (Li and Talwalkar 2019), DARTS (Liu, Simonyan, and Yang 2018), regularized evolution (Real et al. 2019), BOHB (Falkner, Klein, and Hutter 2018), NASBOT (Kandasamy et al. 2018b), local search (White, Nolen, and Savani 2020), TPE (Bergstra et al. 2011), BONAS (Shi et al. 2019), BOHAMIANN (Springenberg et al. 2016), REINFORCE (Williams 1992), GP-based BO (Snoek, Larochelle, and Adams 2012), AlphaX",No,,No,,
ALBERT-xxlarge,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,https://dl.acm.org/doi/pdf/10.1145/3583781.3590259,Background,,"Since the
introduction of the first transformer in 2017 [2], powerful
transformer-based pre-trained natural language processing (NLP)
models, such as BERT [3] and Albert [4], and computer vision
models, such as the Vision Transformer [5] have emerged.",no,,Yes,Albert ,
ASE+ACE,Neuronlike adaptive elements that can solve difficult learning control problems,http://manuscript.elsevier.com/S0893608022001009/pdf/S0893608022001009.pdf,,,,,,,,access denied
PreTrans-3L-250H,Speech recognition with deep recurrent neural networks,https://www.mdpi.com/2073-4433/14/3/542/pdf?version=1678952808,Background,,"They
incorporate a feedback loop in which the output of the RNN layer is taken as an input into
the next layer. Hence, RNNs are especially viable for analyzing time series in problems
related to text generation [69], speech recognition [70], or forecasting tasks [71].",no,,no,,
ObjectNet,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,https://arxiv.org/pdf/2212.04825,Background,,"We also show the results of LLE on other OOD variants of ImageNet, including ImageNet-A [37] (IN-A), ImageNetV2 [69] (IN-V2), ObjectNet [9], and ImageNet-D [71,72] (IN-D). IN-D has rendition images similar to IN-R except for having additional domain annotations, e.g., clipart, infograph, etc.",yes,ImageNet-W,no,,
DLRM-2020,Deep Learning Recommendation Model for Personalization and Recommendation Systems,http://arxiv.org/pdf/2206.02626,Background,,"On the contrary, for recommendation tasks, there always has been a debate of linear vs. non-linear
networks [29, 65], along with the importance of increasing the width vs. depth of the network
[11, 39].",yes,DISTILL-CF,no,,
Once for All,Once for All: Train One Network and Specialize it for Efficient Deployment,https://arxiv.org/pdf/2102.05610,Background,,"When
designing fast models for inference with NAS, previous
work employed multi-objective search [53, 17, 12, 27, 61,
25, 11, 20, 37, 15] to consider accuracy together with performance/efficiency.",yes,EfficientNet-X,no,,
T5-3B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://www.nature.com/articles/s41598-023-48594-4.pdf,Uses,,"a. We use a pretrained Opus-MT-based53 sequence-to-sequence model pretrained on the English-French language for back translation. T5 abstractive summarization: To address the issue of data scarcity, text summarization is conducted on input texts, generating concise summaries by employing the T5-large model5",no,,yes,t5-large,
BIDAF,Bidirectional Attention Flow for Machine Comprehension,https://arxiv.org/pdf/2010.10019,Uses,,"Even though our results are behind [2], we wish to point out that their context matching model with the bi-directional attention flow (BiDAF) [55] significantly contributes to the performance.",no,,yes,biDAF,
VD-RHN,Recurrent Highway Networks,https://ieeexplore.ieee.org/ielx7/6287639/9668973/09762315.pdf,Background,,"For example, the quality of neural language modeling [11] was significantly improved by using Highway layers [12], [13] or tying input and output embeddings [14].",no,,no,,
EfficientNet-L2,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://www.mdpi.com/2072-6694/16/2/430/pdf?version=1705650909,Uses,,"The ResNet 50 [38], EfficeientNet B3 [39], and ConViT (Small) [40] models were fine-tuned on the LVI datasets.",yes,ensemble,yes,finetuned,
Hybrid H3-2.7B,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,http://arxiv.org/pdf/2305.13504,Background,,"Moreover, state space models [64, 66, 65, 39] are getting popular at handling long-range dependencies better than transformer architecture and exploring this direction for code generation is an interesting and exciting research direction.",no,,no,,
HRA,Hybrid Reward Architecture for Reinforcement Learning,https://link.springer.com/content/pdf/10.1007/s10994-021-05961-4.pdf,,,!!couldnt find,,,,,
Pointer Sentinel-LSTM (medium),Pointer Sentinel Mixture Models,https://arxiv.org/pdf/2012.04632,Background,,"Other well-known approaches to address this challenge are [ 9 ,14 , 6 ,16 ,17 , 4]. In this paper, we argue that a key step in designing recurrent neural architectures is to understand the decay of dependence in sequential data and to use this understanding to inform the setting of the relevant hyper-parameters of the architecture.",yes,DilatedRNNs,no,,
FAST,Machine Learning for High-Speed Corner Detection,https://arxiv.org/pdf/2109.00210,Background,,"The image-based local feature extraction and description can be grouped into hand-crafted [22, 6, 32] and deep-learned [9, 40, 13, 35, 31] methods.",no,,no,,
PyramidNet,Deep Pyramidal Residual Networks,http://arxiv.org/pdf/2209.08473,Uses,,"Finally, according to the theory of [26] and [17], wider models are more easily interpreted as convex minimization problems.",yes,Wide PyramidNet272,yes,Wide PyramidNet272,
Transformer local-attention (NesT-B),"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",https://arxiv.org/pdf/2308.05128,Background,,"This field is evolving rapidly, so methods such as transformers [53], large language models [5], knowledge distillation [8], wild and large datasets [31], unsupervised learning [9], and matching loss functions [44], etc. are constantly advancing the state-of-the-art. L",yes,N3HL153gpt,no,,
FTW,Human-level performance in 3D multiplayer games with population-based reinforcement learning,https://arxiv.org/pdf/2109.10665,Background,,"It has powerful representation learning and function approximation properties to be applied across various fields [17], [18], e.g., games [19] and robotics [20].",No,,No,,
Transformer ELMo,Dissecting Contextual Word Embeddings: Architecture and Representation,https://www.aclweb.org/anthology/2020.emnlp-main.397.pdf,Background,,"found the reference, couldnt find it in the text",,,,,
StarGAN v2,StarGAN v2: Diverse Image Synthesis for Multiple Domains,https://dl.acm.org/doi/pdf/10.1145/3588432.3591500,Background,,"found the reference, couldnt find it in the text",,,,,
Agent57,Agent57: Outperforming the Atari Human Benchmark,https://www.mdpi.com/2079-9292/12/16/3508/pdf?version=1692357476,Background,,". For instance, though Agent57 [8] stands as the premier deep reinforcement learning algorithm capable of surpassing the average human player across all 57 Atari games, it generally mandates orders of magnitude more interactions than its human counterpart.",yes,E2S2RND,no,,
LaMDA,LaMDA: Language Models for Dialog Applications,https://arxiv.org/pdf/2308.09490,Background,,"While some large-scale models are completely closedsource, such as OpenAI’s GPT-3 [3] or Google’s Bard [49], and are only accessible through an API, many other models are available as open-source models, usually including the code to train",no,,no,,
Pattern recognition and reading by machine,Pattern recognition and reading by machine,https://mhp-assets.s3.amazonaws.com/marketing/AccessScience%20brochure%202016.pdf,,,no citation,,,,,
ProxylessNAS,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,https://dl.acm.org/doi/pdf/10.1145/3584945,Background,,"Differentiable methods [5, 22, 38, 65, 73] have been widely used in the image classification task.",yes,fine grained PAS-NE,no,,
Wide & Deep,"STRING v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets",https://www.biorxiv.org/content/biorxiv/early/2023/10/12/2023.09.19.558413.full.pdf,Uses,,"The positive interactions were extracted from STRING (version 11) [56], a database encompassing diverse PPI networks and consolidating comprehensive information from various primary sources.",yes,SENSE-PPI,yes,version 11,
Thumbs Up?,Thumbs up? Sentiment Classification using Machine Learning Techniques,https://www.mdpi.com/1424-8220/21/4/1330/pdf?version=1613983067,Background,,"The most common way to do that is by analyzing certain traits of words (commonly their frequency within the text) [32,34]",no,,no,,
Hiero,A Hierarchical Phrase-Based Model for Statistical Machine Translation,http://dl.acm.org/ft_gateway.cfm?id=1220670&type=pdf,,,could not access,,,,,
DITTO,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,https://arxiv.org/pdf/2310.01041,Background,,Xu et al. (2022) proposed the DITTO objective that penalizes sentence-level repetition loop.,yes,DAEMON,no,,
Convolutional Pose Machines,Convolutional Pose Machines,http://nrl.northumbria.ac.uk/id/eprint/43318/1/Paper.pdf,Background,,could not find citation in text,,,,,
Enhanced Neighborhood-Based Filtering,Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights,https://www.atlantis-press.com/article/125950416.pdf,Background,,An incremental algorithm to update user factors is presented [25] by utilizing a simple method of the batch process.,no,,no,,
Prototypical networks,Prototypical Networks for Few-shot Learning,https://arxiv.org/pdf/2307.07286,Uses,,"For example, Snell et al. [32] presented the Prototypical Networks that compute distances between a datapoint and class-wise prototypes",yes,no name,yes,ProtoNet,
Pandemonium (morse),Pandemonium: a paradigm for learning,https://karger.com/ofa/article-pdf/2/6/374/3300852/000260906.pdf,Background,,"To a point, the mission of such constellations of molecules might be compared with the recognition of real-life patterns, such as handwritten characters formulated by Oliver Selfridge 50 years ago in a whimsically named feature-analysis approach, the ‘Pandemonium model’ [34, 35]",no,,no,,
ReLU (LFW),Rectified Linear Units Improve Restricted Boltzmann Machines,https://link.springer.com/content/pdf/10.1007/s44196-023-00366-8.pdf,Uses,,"In Eq. (5), a two-layer fully connected feed forward network with Rectifed Linear Units (ReLU) activation [34].",yes, (LN‑GTM),yes,ReLu,
Make-A-Video,Make-A-Video: Text-to-Video Generation without Text-Video Data,http://arxiv.org/pdf/2303.17599,Uses,,"Leveraging a pre-trained text-to-image diffusion model, MakeA-Video [34] performs video generation through a spatialtemporal decoder trained only on unlabeled video data.",yes,vid2vid-zero,yes,Make-A-Video,
Transformer local-attention (NesT-B),"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",https://arxiv.org/pdf/2301.10750,Background,,ResNeSt is a modularized architecture that uses channel-wise attention to focus on various network branches to take advantage of their success in capturing cross-feature interactions and learning various representations [78].,yes,ConvNext,yes,ResNet,
Switch,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/pdf/2211.08842,Background,,"Meanwhile, PTMs are also getting bigger and bigger. In the past two years, large models with trillions of parameters are popping up around the world, including GPT-3 [15], Switch Transformer [16], and M6 [17].",yes,ELBERT model and the acceleration method,no,,
EfficientDet,EfficientDet: Scalable and Efficient Object Detection,http://arxiv.org/pdf/2303.04884,Uses,,". EfficientDet (Tan et al., 2020), an augmented variant of YOLOv3, exploits a pyramid network to enable the detection of scaling targets",yes,O2RNet-ResNet101,yes,"EfficientDet-b0 (Oerke, 2006) 0.45 0.89 0.85 0.30 0.82 0.71 0.77 EfficientDet-b1 (Oerke, 2006) 0.45 0.89 0.86 0.30 0.82 0.72 0.77 EfficientDet-b2 (Oerke, 2006) 0.46 0.89 0.87 0.30 0.82 0.73 0.78 EfficientDet-b3 (Oerke, 2006) 0.49 0.93 0.91 0.32 0.84 0.75 0.81 EfficientDet-b4 (Oerke, 2006) 0.50 0.94 0.92 0.34 0.88 0.78 0.82 EfficientDet-b5 (Oerke, 2006)",
VGG16,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://www.mdpi.com/2306-5354/11/1/19/pdf?version=1703413051,Background,,"Research has constantly improved network architecture around this fundamental principle, including architectural milestones like VGG [87], ResNet [88], Xception [89], or EfficientNet [90].",no,,no,,
PolyCoder,A systematic evaluation of large language models of code,http://arxiv.org/pdf/2302.05226,Background,,couldnt find citation in text,,,,,
DrLIM,Dimensionality Reduction by Learning an Invariant Mapping,https://link.springer.com/content/pdf/10.1007/s11042-023-14876-2.pdf,Background,,"For example, the contrastive loss [7] needs to construct sample pairs to train the model.",yes,ResNet-MLB,no,,
DiT-XL/2,Scalable Diffusion Models with Transformers,https://arxiv.org/pdf/2307.02270,Background,,"Recently, denoising diffusion models demonstrating great potential in various computer vision fields including super-resolution [52], [53], image generation [54]–[61], object detection and segmentation [62]–[65], etc.",yes,SVDM,no,,
Naive Bayes,Pattern classification and scene analysis,https://sol.sbc.org.br/index.php/ercemapi/article/download/21955/21778,Uses,,"Finally, the resulting image is normalized between 0 and 1. To evaluate the efficiency of the method used, the following validation metrics: accuracy (Acc), sensitivity (Sens), specificity (Spec), and area under roc curve (AUC) [Duda 1973].",yes,no name,yes,?,
ConvNet similarity metric,"Learning a similarity metric discriminatively, with application to face verification",http://arxiv.org/pdf/2303.05161,Background,,"Segregation of class manifolds is a powerful conceptualisation that informs the design of distancebased losses in metric learning and contrastive learning [13–17] and underlies several approaches aimed at quantifying expressivity and generalisation, in artificial neural networks as well as in neuroscience [18– 23].",no,no name,no,,
YOLOv3,YOLOv3: An Incremental Improvement,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0296992&type=printable,Extends,,Examples of one-stage algorithms include the Single-Shot Multibox Detector (SSD) series [10] and the You Only Look Once (YOLO) series [11–17].,yes,YOLOv7-LDS. YOLOv7-LDS,yes,YOLOv7-LDS. YOLOv7-LDS,
AbLang,AbLang: an antibody language model for completing antibody sequences,https://www.nature.com/articles/s42004-023-01037-7.pdf,Uses,,"Finally, since LLMs have shown promising results on similar tasks, we also used an antibodyspecific pretrained LLM (AbLang22)",yes,,yes,AbLang,
HMM Word Alignment,HMM-Based Word Alignment in Statistical Translation,https://www.mdpi.com/2297-8747/24/1/14/pdf?version=1551266155,Background,Uses,Word alignment was also studied in connection with statistical translation by using a HMM by Vogel et al.,No,,Yes,Hidden Markov Models (HMMs) ,"Interesting application of HMM to the field of linguistics. The Voynich manuscript is an undeciphered script that we are still trying to decipher today. This paper aims to use HMM to prove that this manuscript is neither a hoax nor intentional cipher, but an actual language, through having the model identify evidence of the existence of two states associated with the symbols in the manuscript (like the consonants and vowels in English). "
Gated HORNN (3rd order),Higher Order Recurrent Neural Networks,https://arxiv.org/pdf/2206.01261,Motivation,,"In the context of recurrent models, (Soltani and Jiang, 2016) introduced weighted skip connections between subsequent states of an unfolded RNN learn long-term dependencies.",No,,Yes,"CNN, RNN",Introducing entangled residual mappings as a method of understanding the role of residual mappings across different learning models while preserving the iterative feature refinement. 
FAIRSEQ Adaptive Inputs,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling",https://arxiv.org/pdf/2201.05742,Uses,,We use RoBERTa as the backbone of Kformer and implement our method using Facebook’s tool fairseq [13],Yes,Kformer,Yes,Kformer,
ShuffleNet v1,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,https://arxiv.org/pdf/2302.07483,,,,,,,,"No citation found, only in References"
ESM1-670M (UR50/D),Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/jcb.30490,Uses,,"We reviewed and compared the most recent and frequently used DL‐based protein embeddings (namely: UniRep,20 SeqVec,21 ProteinBERT,25 and ESM‐1b23) in predicting transporter proteins and their relative substrates, in combination with several machine learning approaches.",Yes,PortPred,Yes,PortPred,
Spatially-Sparse CNN,Spatially-sparse convolutional neural networks,https://arxiv.org/pdf/1703.09438,Motivation,,"Our approach is largely inspired by Graham’s sparse convolutional networks [16, 17], which enable efficient shape analysis by storing a sparse set of non-trivial features instead of dense feature maps.",Yes,Octree Generating Networks (OGN),Yes,Octree Generating Networks (OGN),"Interesting application of OGNs to create high resolution 3D outputs, such that the higher the octree level, the more efficient the representation is."
XGLM,Few-shot Learning with Multilingual Language Models,http://arxiv.org/pdf/2208.01448,Background,Differences,"We show that AlexaTM 20B provides SOTA performance in zero-shot setting for all of these tasks, across all supported languages, improving on previous SOTA achieved by the XGLM 7.5B model (Lin et al., 2021).",Yes,AlexaTM 20B,Yes,AlexaTM 20B,Evaluates how AlexaTM 20B has overall better performance than other models (which includes the XGLM 7.5B model referenced).
,,,Similarities,,"The CLM mode is similar to the way previous large-scale decoder-only models have been using these models for generation and scoring for in-context learning (Brown et al., 2020b; Chowdhery et al., 2022; Lin et al., 2021).",,,,,
,,,Differences,,"Additionally, we evaluate AlexaTM 20B on a few multilingual datasets including XNLI, XCOPA, Paws-X, and XWinograd and show that AlexaTM 20B achieves SOTA numbers in zero-shot on all these tasks across all languages better than XGLM 7.5B model (Lin et al., 2021).",,,,,
,,,Differences,,"As can be seen, AlexaTM 20B outperforms the supervised M2M-124 615M model from Goyal et al. (2022) and XGLM 7.5B, which is a multilingual decoder-only model (Lin et al., 2021), across almost all pairs.",,,,,
,,,Background,,We follow Lin et al. (2021) and evaluate AlexaTM 20B on four multilingual data sets to evaluate its performance on non-English tasks.,,,,,
,,,Differences,,"As can be seen in Table 10, AlexaTM 20B performs better or on par to XGLM 7.5B (Lin et al., 2021) across all tasks and languages (supported by both models).",,,,,
,,,Background,,XGLM 7.5B numbers for XNLI and XCOPA are as reported by Lin et al. (2021).,,,,,
,,,Differences,,"Among public multilingual encoders, the most successful efforts include mBERT (a version of BERT pre-trained on Wikipedia in 104 languages), mBART (Liu et al., 2020b), XLM-R (Conneau et al., 2020), and XGLM (Lin et al., 2021).",,,,,
T5-3B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/pdf/2312.05092,Uses,,"CodeT5 [62] uses the T5 architecture [63], which unifies all tasks as text generation tasks",No,,Yes,"CodeBERT, CodeBERTa, GraphCodeBERT, JavaBERT, PLBART, CodeT5, UniXCoder, CodeReviewer","Doesn't further train the pre-trained models, just want to analyze the extent that these pre-trained models learn about the specific aspects of source code through probing. "
ShuffleNet v1,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,https://arxiv.org/pdf/2307.03110,,,,,,,,No citations found besides in References
ViT-Base/32,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://bmcmedimaging.biomedcentral.com/counter/pdf/10.1186/s12880-023-01177-1,Background,,"In order to further highlight the efect of our model,we compare the proposed method with four methods: Efcient-b2 [14], Resnet34 [8], Swin transformer [15], and Vision transformer [16], as show in Fig. 9",No,,Yes,ResNet,Impactful extension of attention-enhanced architecture in the medical field to better identify pneumonia in chest X-ray images
BiLSTM for Speech,Framewise phoneme classification with bidirectional LSTM and other neural network architectures,https://www.mdpi.com/2220-9964/12/3/98/pdf?version=1677404230,Motivation,,"With the same or similar setup parameters, a bidirectional network achieved more desired results than a unidirectional network in many fields [58].",Yes,Att-DBGRU,Yes,Att-DBGRU,Interesting use of AI models to deciding how to manage tourist attractions by forecasting tourism volume. 
Learning deep architectures,Learning Deep Architectures for AI,https://www.nature.com/articles/s41598-023-35190-9.pdf,Uses,,Deep learning utilizes layers of non-linear processing elements for feature extraction and conversion.,No,,Yes,"You Only Look Once (YOLO), Single Shot Detector (SSD)",Good example of how AI models can be used in the real world for security - in this case an early weaopn detection framwork by using CNN-based models like YOLO and SSD. 
Zoneout + Variational LSTM (WT2),Pointer Sentinel Mixture Models,http://arxiv.org/pdf/2207.09519,Similarities,,"The cache model has been equipped on various models to boost the performance for vision or language models, including kNNLMs [30], Unbounded Cache [18], Matching Network [60] and others [43,53].",Yes,Tip-Adapter,Yes,"Tip-Adapter, Contrastive Vision-Language Pre-training (CLIP)",Purpose of citation: to say that these others models also equipped the cache models in their models to boost performance for vision and language models. 
LeNet-5,Gradient-based learning applied to document recognition,https://www.nature.com/articles/s41598-023-49334-4.pdf,Background,,"Due to the increase of convolution layer, successive multi-layer gradient multiplication may cause minimum amount of information that is captured according to the chain rule in back-propagation, resulting in the problems of Gradient Vanishing and Gradient Exploding and the decline of the accuracy of the training set.",No,,Yes,"ResNet50, SVR",Impactful example of how using the ResNet50 + SVR model combination can help rapidly conduct an excellent evaluation of pulmonary function parameters when patients cough. This is helpful especially since traditional clinical evaluations of respiratory diseases involve a complex process that the pateitns can't monitor daily.
Context-dependent RNN,Context dependent recurrent neural network language model,https://www.aclweb.org/anthology/N15-1186.pdf,Motivation,,"Many recent proposals in the literature use wordrepresentations as the basic units for tackling sentence-level tasks such as language modeling (Mnih and Hinton, 2007; Mikolov and Zweig, 2012), paraphrase detection (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b), discriminative parsing (Collobert, 2011), as well as similar tasks involving larger units such as documents (Glorot et al., 2011; Huang et al., 2012; Le and Mikolov, 2014).",Yes,"SG, SG+Morph",Yes,"SG and SG+Morph models trained from scratch, for all languages that they considered","Cool example of a method for training models to discover morphological rules in languages. The citation mentions previous works that have developed language models for generation of words, but also mentions that these models treated words as units and neglected the relationship between the words, thereby affecting the word semantics. This paper presents a method for analyzing these relationships between the words, so that models are able to account for word semantics. "
PLATO-XL,PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation,http://arxiv.org/pdf/2206.14000,Extends,,"We experiment on multiple Chinese pretrained language models, including T5 (Xue et al., 2021), BART (Liu et al., 2020), EVA2.0 (Gu et al., 2022), and PLATO (Bao et al., 2021b).",No,,Yes,"PLATO-SINC, PLATO, PLATO-FT, PLATO-FID, BART, T5, EVA2.0",
Elastic weight consolidation,Overcoming catastrophic forgetting in neural networks,http://arxiv.org/pdf/2306.08200,Background,,"One approach is to constrain the parameters of the model learned on a new task to deviate little from their values before this learning, e.g. by using distillation [1–4] or parameter regularization [5, 6] methods.",Yes,Prompt Of Prompts (POP),Yes,Prompt Of Prompts (POP),
,,,Background,,"EWC [5], GEM [21] and NSCL [6], on the other hand, suggested to constrain the gradient and parameters of the model, so as to remain close to those of the old model.",,,,,
RBM Image Classifier,Learning Multiple Layers of Features from Tiny Images,https://arxiv.org/pdf/2312.04918,Uses,,We started by training a standard VGG-16 [11] on the CIFAR-10 dataset [42],No,,Yes,"ResNet50, MobileNetV2",
Support Vector Machines,Support-Vector Networks,https://www.ias-iss.org/ojs/IAS/article/download/2928/1181,Uses,,"SVMs (Cortes and Vapnik 1995) are supervised learning method for classification, regression, and outlier’s detection",Yes,HSI classification (modification of Minimum Noise Fraction (MNF) + Edge Preserving Features (EPFs)),Yes,Support-vector networks,
VGG16,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://www.nature.com/articles/s41598-023-49739-1.pdf,Background,,The most famous algorithm utilized for speeding up the evaluation of the gradient and for providing training to the neural networks is backpropagations. ,Yes,CNN-based model that isn't specifically named,Yes,"CNN-based model that has convolutional layers, fully-connected layers, and the output",Using CNNs to classify human activity recognition (HAR) tasks 
UL2,UL2: Unifying Language Learning Paradigms,https://arxiv.org/pdf/2307.12856,Uses,,"It is pre-trained using a mixture of long-span denoising objective (Tay et al., 2022) on a large-scale HTML corpus extracted from CommonCrawl.",Yes,HTML-T5,Yes,HTML-T5,
,,,,,"HTML-T5 consists of (1) local and global attention mechanisms (Ainslie et al., 2020; Guo et al., 2022) and (2) a mixture of denoising objectives (Tay et al., 2022) with longer-span corruption on large-scale HTML corpus.",,,,,
,,,,,"HTML-T5 is pre-trained on a large-scale HTML corpus curated from CommonCrawl using a mixture of long-span denoising objectives (Tay et al., 2022), and then finetuned it for each downstream task.",,,,,
,,,,,"The objective is then to predict the masked spans using the remaining tokens in the HTML document (Raffel et al., 2020; Tay et al., 2022; Ainslie et al., 2023).",,,,,
,,,,,"HTML-denoising comparison with different mixtures of span length (Raffel et al., 2020; Tay et al., 2022).",,,,,
BatchNorm,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://researchopenworld.com/wp-content/uploads/2023/10/CST-8-833.pdf,Background,,"Importantly, these studies identified instances of cancer even in men in their twenties, with a prevalence ranging from 8% to 11%, highlighting the long period of latency between the development of prostate cancer and the appearance of symptoms in some individuals [19,20].",Yes,modified CNN system,Yes,modified CNN system - hybrid CNN-Deep Learning technique (CNN-RNN),
,,,Motivation,,Deep Learning [19] systems are built just for that purpose (Figure 4).,,,,,
MnasNet-A1 + SSDLite,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://arxiv.org/pdf/2204.04705,,,,,,,,
Learnability theory of language development,Language learnability and language development,https://pure.uva.nl/ws/files/39880573/The_linking_problem_is_a_special_case.pdf,,,,,,,,
Megatron-Turing NLG 530B,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",http://arxiv.org/pdf/2203.07259,,,,,,,,
PaLM 2,PaLM 2 Technical Report,https://www.medrxiv.org/content/medrxiv/early/2023/06/05/2023.06.04.23290939.full.pdf,,,,,,,,
Word2Vec (small),Distributed Representations of Words and Phrases and their Compositionality,https://repository.kaust.edu.sa/bitstream/10754/686283/1/ROLE_Rotated_Lorentzian_Graph_Embedding_Model_for_Asymmetric_Proximity.pdf,,,,,,,,
CTC-Trained LSTM,Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,http://arxiv.org/pdf/2306.08329,,,,,,,,
LSTM with forget gates,Learning to Forget: Continual Prediction with LSTM,https://downloads.hindawi.com/journals/cin/2023/8585839.pdf,,,,,,,,
RoboCat,RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation,https://arxiv.org/pdf/2309.13037,,,,,,,,
TransformerXL + spectrum control,Improving Neural Language Generation with Spectrum Control,https://aclanthology.org/2023.acl-short.103.pdf,,,,,,,,
SqueezeNet,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size,http://arxiv.org/pdf/2302.09973,,,,,,,,
PLATO-XL,PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation,https://dl.acm.org/doi/pdf/10.1145/3477495.3532069,,,,,,,,
Part-of-sentence tagging model,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,https://link.springer.com/content/pdf/10.1007/s42979-020-00270-4.pdf,,,,,,,,
ISS,Learning Intrinsic Sparse Structures within Long Short-term Memory,https://arxiv.org/pdf/2004.09031,,,,,,,,
W2v-BERT,w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,https://arxiv.org/pdf/2203.05936,,,,,,,,
BPE,Neural Machine Translation of Rare Words with Subword Units,http://arxiv.org/pdf/2210.14389,,,,,,,,
LSTM + dynamic eval,Dynamic Evaluation of Neural Sequence Models,https://www.aclweb.org/anthology/D18-1491.pdf,,,,,,,,
λ-WASP,Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus,https://aclanthology.org/P14-1091.pdf,,,,,,,,
PointNet++,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,https://www.nature.com/articles/s41598-024-52343-6.pdf,,,,,,,,
Cascaded LNet-ANet,Deep Learning Face Attributes in the Wild,https://www.mdpi.com/1999-4893/16/3/175/pdf?version=1679903097,,,,,,,,
DALL-E,Zero-Shot Text-to-Image Generation,https://arxiv.org/pdf/2307.04292,,,,,,,,
DNABERT,DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/9A30B0A8F17FC59DF55FAA0401474AED/S2634460223000377a.pdf/div-class-title-short-term-forecasting-of-ozone-air-pollution-across-europe-with-transformers-div.pdf,,,,,,,,
GLM-130B,GLM-130B: An Open Bilingual Pre-trained Model,https://arxiv.org/pdf/2211.17148,,,,,,,,
TransE,Translating Embeddings for Modeling Multi-relational Data,https://journalofcloudcomputing.springeropen.com/counter/pdf/10.1186/s13677-023-00585-6,,,,,,,,
MT-DNN,Multi-Task Deep Neural Networks for Natural Language Understanding,https://aclanthology.org/2021.ecnlp-1.7.pdf,,,,,,,,
MoCo,Momentum Contrast for Unsupervised Visual Representation Learning,https://arxiv.org/pdf/2309.16077,,,,,,,,
Hopfield network,Neural networks and physical systems with emergent collective computational abilities.,https://iopscience.iop.org/article/10.1088/2634-4386/acb2ef/pdf,,,,,,,,
ESM1-670M (UR50/D),Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,https://www.biorxiv.org/content/biorxiv/early/2023/06/10/2023.06.09.544229.full.pdf,,,,,,,,
Xception,Xception: Deep Learning with Depthwise Separable Convolutions,https://arxiv.org/pdf/2308.12494,Differences,Future Work,"To improve this, Xception [7] proposes to use separable convolution, a depthwise convolution followed by a pointwise convolution, which requires visiting each pixel only once by depthwise convolution.",no,n/a,no,n/a,"Not a model, but a roadmap to improve image recognition models. Quote: ""Extensive experiments demonstrate that our approach decreases runtime by up to 13% and reduces the number of parameters by up to 23%, while increasing PSNR and SSIM on several image restoration datasets."""
KEPLER,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,https://aclanthology.org/2021.findings-emnlp.292.pdf,Extends,,"KEPLER(Wanget al., 2019) uses Knowledge Embedding objective, i.e., TransE, to guide embedding encoded over entity description.",yes,Relation-Guided Pre-Training (RGPT-QA),no,n/a,It proposes a framewrok model that can more accurate answer QA than privious models.
,,,Differences,,"From the table,KEPLER trained via TransE performs slightly better than KnowBERT trained via entity linking, and RGPT-QA out performs KEPLER by 2.8%, 2.1%, 5.7% on the three datasets.",yes,Relation-Guided Pre-Training (RGPT-QA),no,n/a,It proposes a framewrok model that can more accurate answer QA than privious models.
GShard (dense),GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,http://arxiv.org/pdf/2303.10845,Extends,,"Comparing to the expensive computational cost for training dense Transformer model, sparse architectures such as Mixtureof-Experts (MoE) [13, 14, 15, 22] are considered to be an appealing choice to scale model size up without incuring linear increase in computational cost.",yes,PanGu-Σ,yes,Ascend 910 AI,"PanGu-Σ provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when f ine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation."
WGAN-GP,Improved Training of Wasserstein GANs,https://arxiv.org/pdf/2309.13508,Extends,,"Because the gradient penalty enforces the Lipschitz constraint on the critic, limiting its update, we had to increase the number of critic training iterations per actor iteration to 5, a recommended value in WGAN-GP [14].",yes,Guided Cooperation via Modelbased Rollout (GCMR),yes,hierarchical reinforcement learning (HRL),"Here, we present a goal-conditioned HRL framework with Guided Cooperation via Modelbased Rollout (GCMR) 1 , which estimates forward dynamics to promote inter-level cooperation. The GCMR alleviates the state-transition error within off-policy correction through a model-based rollout, further improving the sample efficiency."
Transformer local-attention (NesT-B),"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",http://arxiv.org/pdf/2304.13991,Extends,,"In Zhang et al. [40], a convolution is used to aggregate hierarchical transformer blocks in their Nested Hierarchical Transformer (NesT).",no,no,yes,Vision Transformers (ViT),"In addition, to use the CNN, we proposed to reconstruct the image data after the self-attention in a reverse embedding layer. Through the evaluation, we demonstrate that the proposed convolutions help improve the classification ability of ViT."
,,,Extends,,"NesT incorporates a convolution into the aggregation function of hierarchical Transformer blocks. NesT-T is used for the evaluation which includes three hierarchical layers of 8, 4, and 1 Transformer blocks each.",no,no,yes,Convolutional Neural Networks (CNN),"In addition, to use the CNN, we proposed to reconstruct the image data after the self-attention in a reverse embedding layer. Through the evaluation, we demonstrate that the proposed convolutions help improve the classification ability of ViT."
Max-Margin Markov Networks,Max-Margin Markov Networks,https://dl.acm.org/doi/pdf/10.3115/1220575.1220588,Motivation,Background,"Additionally, we use several pre-existing classifiers as features. This are simple maximum entropy Markov models trained off of the MUC6data, the MUC7 data and our ACE data.",yes,new joint EDTmodel (no name),yes,Entity detection and tracking (EDT),"In contrast, by modeling both aspects of the EDT task simultaneously, we are able to learn using highly complex, non-local features. We develop a new joint EDTmodel and explore the utility of many features, demonstrating their effectiveness on this task."
,,,Extends,,"The boundary decision features include: the second and third order Markov features over entity type, entity subtype and mention type; features appearing at the previous (and next) words within a window of three; the words that appear and the previous and next mention boundaries, specified also by entity type, entity subtype and mention type.",yes,new joint EDTmodel (no name),yes,Entity detection and tracking (EDT),"In contrast, by modeling both aspects of the EDT task simultaneously, we are able to learn using highly complex, non-local features. We develop a new joint EDTmodel and explore the utility of many features, demonstrating their effectiveness on this task."
,,,Differences,,"This non-locality makes models like Markov networks intractable, and LaSO provides an excellent framework for tackling this problem.",yes,new joint EDTmodel (no name),yes,Entity detection and tracking (EDT),"In contrast, by modeling both aspects of the EDT task simultaneously, we are able to learn using highly complex, non-local features. We develop a new joint EDTmodel and explore the utility of many features, demonstrating their effectiveness on this task."
GPT-2 (1.5B),Language Models are Unsupervised Multitask Learners,https://www.emerald.com/insight/content/doi/10.1108/JEBDE-08-2023-0015/full/pdf?title=unraveling-the-landscape-of-large-language-models-a-systematic-review-and-future-perspectives,Motivation,,"These models, such as GPT-1, GPT-2, GPT-3, InstructGPT and GPT-4 (Brownetal.,2020;OpenAI,2023;Ouyangetal., 2022; Radford, Narasimhan,Salimans,&Sutskever,2018;Radfordetal.,2019), are based on the architecture of the transformer (Vaswani et al., 2017), which is one of the famous architectures for developing LLMs and is well-known for its attention mechanisms, resulting in a simpler architecture than the recurrent neural networks (RNNs) (Schuster & Paliwal, 1997).",no,n/a,yes,Transformer Architecture,"This paper aims to present a comprehensive examination of the research landscape in LLMs, providing an overview of the prevailing themes and topics within this dynamic domain."
,,,Differences,,"The GPT-2 model (Radfordetal.,2019) was proposed to further reduce the need for labeled datasets. It is a 1.5 billion transformer and uses a zero-shot setting, demonstrating that LLMs can learn various natural language tasks without explicit supervision. Compared with GPT-1 and GPT-2 models, GPT-3 model (Brown et al., 2020)has 175 billion parameters, which is much larger and improves the performance of LLMs further.",no,n/a,yes,Transformer Architecture,"This paper aims to present a comprehensive examination of the research landscape in LLMs, providing an overview of the prevailing themes and topics within this dynamic domain."
OverFeat,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",http://arxiv.org/pdf/2207.11523,Differences,Similarities,OverFeat [44] - Chart,yes,new road detection method (no name),yes,VGG-16,We propose a method to detect and segment roads with a random forest classifier of local experts with superpixel based machine-learned features.
Adaptive Subgrad,Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,https://www.mdpi.com/2076-3417/13/5/2782/pdf?version=1677048989,Extends,,ADAM combines the AdaGrad’s ability to deal with sparse gradients [28] and RMSProp’s ability to deal with non-stationary objectives [29].,yes,"CNN and LSTM (both, no name)",yes,Adaptive Moment Estimation (ADAM),"We have developed both a CNN and an LSTM architecture for frequency-domain feature detection and approximation. The CNN models, one for phase and one for amplitude data, take short-distance Fourier transforms (SDFTs) representing the change in the signal over multiple observation points as input. The LSTM model takes the change in phase or amplitude points at each lateral location as a comma-separated value (CSV) input."
Restricted Bolzmann machines,Restricted Boltzmann machines for collaborative filtering,https://jwcn-eurasipjournals.springeropen.com/track/pdf/10.1186/s13638-019-1525-y,Differences,Background,"In the field of recommended systems, the restricted Boltzmann machine was first proposed to simulate a user’s explicit rating of items [18], and autoencoders and denoising autoencoders were also used for recommendations [19–21].",yes,neighborhood-aware deep learning method,yes,multi-layer perceptron (MLP) and convolution neural networks (CNN),"In the field of recommended systems, the restricted Boltzmann machine was first proposed to simulate a user’s explicit rating of items [18], and autoencoders and denoising autoencoders were also used for recommendations [19–21]. However, research using deep learning in the field of services recommendation is seldom. Therefore, we propose the neighborhood-aware deep learning method to predict the service quality of web services."
Statement Curriculum Learning,Formal Mathematics Statement Curriculum Learning,https://arxiv.org/pdf/2308.15605,Similarities,,"To train AIs to complete hard tasks, it is common to use reinforcement learning using outcomes as rewards, such as wins in games [19, 26] and successful proofs in proof generation systems [23, 30].",no,n/a,yes,LLMs,"In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models."
FAIRSEQ Adaptive Inputs,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling",https://arxiv.org/pdf/2204.08858,Uses,,Two different ASR frameworks are used: ESPnet [22] and an internal toolkit based on fair seqand PyTorch [23].,no,n/a,yes,GTC-T,"In this work, we are investigating GTC-T with a CTC-like and a MonoRNN-T graph using a large-scale data set of 145K hours."
,,,Uses,,"For the results presented here, an Emformer-based streaming ASR model architecture is used that is implemented using an internal fairseq-based ASR framework [23,24].",no,n/a,yes,MonoRNN-T,"In this work, we are investigating GTC-T with a CTC-like and a MonoRNN-T graph using a large-scale data set of 145K hours."
Walking Minotaur robot,Learning to Walk via Deep Reinforcement Learning,http://arxiv.org/pdf/2111.12557,Motivation,,"However, these approaches are far from providing any practical solutions to the problem at hand and they are shown to be only effective on simpler practical robots [26]–[33] and demonstrations in more complex problems such as those offered by legged robots are missing.",no,n/a,yes,Explicit Reference Governors (ERG),"In this work, we will report our preliminary results in the modeling, control design, and development of a legged robot called NU’s Husky Carbon (shown in Fig. 1). Our objective is to integrate legged and aerial mobility into a single platform."
MnasNet-A3,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://www.frontiersin.org/articles/10.3389/fphy.2023.1159266/pdf,Extends,,"Traditional NAS work uses the reinforcement learning algorithm (RL) [7], evolutionary algorithm (EA) [8], and the gradient-based method to conduct architecture search.",yes,CK-βNAS-CLR,no,n/a,This paper proposes a circular kernel convolution-β-decay regulation NAS-confident learning rate (CK-βNAS-CLR) framework to automatically design the neural network structure for HSI classification.
DeepStack,DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker,https://figshare.com/articles/journal_contribution/Mathematical_consistency_and_long-term_behaviour_of_a_dynamical_system_with_a_self-organising_vector_field/11861349/1/files/21742305.pdf,Background,,"Artificial intelligence (AI) is nowadays widely spread [36], has demonstrated very impressive and sometimes superhuman levels of performance in a range of applications, such as face recognition [33], medical diagnostics from image analysis [15, 8], games of Go [31] and poker [25], and is often recognised as the most important technology of the future [24].",yes,n/a (brain model),no,na,"Here we analyse a mathematical construct, which was recently proposed in [19] with the goal to develop AI both explainable and inspired by the brain."
Neuro-Symbolic Concept Learner,"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",http://arxiv.org/pdf/2205.14268,Motivation,,"These approaches use neural models to build executable logical programs. Examples include Liang et al. (2017) and Mao et al. (2019). We highlight Logic Tensor Networks (LTNs) [Badreddine et al., 2022], as we include this approach in our empirical evaluation. LTNs connect neural predictions into functions representing symbolic relations with real-valued or fuzzy logic semantics. Neural networks as predicates: This line of work int",yes,Neural Probabilistic Soft Logic (NeuPSL),yes,probabilistic soft logic (PSL),"This paper introduces Neural Probabilistic Soft Logic (NeuPSL), a novel NeSy method that integrates deep neural networks with a symbolic method designed for fast joint learning and inference"
AlexNet,ImageNet classification with deep convolutional neural networks,https://www.preprints.org/manuscript/202310.1446/v1/download,Background,,"In 2012, AlexNet, proposed by Alex Krizhevsky et al. [5] made a splash in the ImageNet image recognition competition, crushing the classification performance of the second place support vector machines (SVM)",yes,attention mechanism for multiscale receptive fields convolution block (AMMRF),yes,common objects in context (COCO),The new detection method is more lightweight and works well for multi-scale ship target detection in SAR images.Our contributions can be summarized as follows.
,,,Motivation,,Applying deep learning to image processing can significantly improve detection accuracy and speed for tasks such as target detection and instance segmentation [5].,yes,You Only Look Once SAR Ship Identification (YOLO-SARSI),yes,PASCAL visual object classes (PASCAL VOC),The new detection method is more lightweight and works well for multi-scale ship target detection in SAR images.Our contributions can be summarized as follows.
SimCLR,A Simple Framework for Contrastive Learning of Visual Representations,https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btae020/56167979/btae020.pdf,Motivation,,The contrastive loss used in contrastive learning strengthens the similarity between positive sample pairs (i.e. a sample and its augmented sample) while increasing the distance between negative sample pairs (i.e. a sample and other samples) (Chen et al. 2020b).,yes,"autoencoder-based method, scMAE",no,n/a,"The masked autoencoder introduces a masking predictor, which captures relationships among genes by predicting whether gene expression values are masked."
DnCNN,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,https://www.mdpi.com/1424-8220/23/3/1486/pdf?version=1674977686,Similarities,,"However, these methods all face two problems [7]: (1) the optimization problem in the test stage is very complex, making the denoising process time-consuming; (2) parameters need to be manually adjusted to obtain a better image-denoising effect.",yes,image recognition model,yes,AlexNet,"We propose a new image-denoising model that aims to extract the local features of the image through CNN and focus on the global information of the image through the attention similarity module (ASM), especially the global similarity details of the image"
,,,Background,,"For example, residual learning and bath normalization were applied to a deep convolutional neural network for image denoising (DnCNN) [7] to enhance the learning ability of the model.",yes,image recognition model,yes,VGG,"We propose a new image-denoising model that aims to extract the local features of the image through CNN and focus on the global information of the image through the attention similarity module (ASM), especially the global similarity details of the image"
,,,Background,,"Zhang et al. [7] first designed deep CNNs for image denoising (DnCNN), and they improved the performance of the model by stacking multiple convolutional layers, residual learning [10], and batch normalization [21].",yes,image recognition model,yes,ResNet,"We propose a new image-denoising model that aims to extract the local features of the image through CNN and focus on the global information of the image through the attention similarity module (ASM), especially the global similarity details of the image"
,,,Uses,,The color image datasets included the Waterloo Exploration Database and BSD432 [7].,yes,image recognition model,yes,DnCNN (our searched model),"We propose a new image-denoising model that aims to extract the local features of the image through CNN and focus on the global information of the image through the attention similarity module (ASM), especially the global similarity details of the image"
,,,Uses,,The gray image datasets consisted of Set12 and BSD68 [7].,yes,image recognition model,yes,BRDNet,"We propose a new image-denoising model that aims to extract the local features of the image through CNN and focus on the global information of the image through the attention similarity module (ASM), especially the global similarity details of the image"
,,,Differences,,"For gray image denoising, we chose several state-of-art denoising methods with the same test datasets, including BM3D [5], DnCNN [7], FFDNet [11], BRDNet [14], ADNet [15], and RDN [13].",yes,image recognition model,yes,RDASM,"We propose a new image-denoising model that aims to extract the local features of the image through CNN and focus on the global information of the image through the attention similarity module (ASM), especially the global similarity details of the image"
Tensorized Transformer (257M),A Tensorized Transformer for Language Modeling,https://aclanthology.org/2021.findings-emnlp.67.pdf,Background,,Ma et al. (2019) combine low rank approximate and parameter sharing to construct a tensorized Transformer.,no,improved transformer model,no,n/a,"In this paper, we present an approach to learning a hard retrieval attention where an attention head only attends to one token in the sentence rather than all tokens."
Perceiver IO,Perceiver IO: A General Architecture for Structured Inputs & Outputs,http://arxiv.org/pdf/2204.05994,Extends,,"Our proposed architecture is based on the Perceiver/PerceiverIO [8, 9] architectures.",yes,Malceiver,yes,Perceiver/PerceiverIO,"We propose the Malceiver, a hierarchical Perceiver model for Android malware detection that makes use of multi-modal features."
,,,Background,,"A recent pair of promising approaches, the Perceiver [9] and PerceiverIO [8], change the attention mechanism to use cross-attention rather than self-attention.",yes,Malceiver,yes,Perceiver/PerceiverIO,"We propose the Malceiver, a hierarchical Perceiver model for Android malware detection that makes use of multi-modal features."
,,,Motivation,,The PerceiverIO has been shown to be effective even with input sequences with over 2 million raw elements [8].,yes,Malceiver,yes,Perceiver/PerceiverIO,"We propose the Malceiver, a hierarchical Perceiver model for Android malware detection that makes use of multi-modal features."
,,,Extends,,"We therefore build on the Perceiver/PerceiverIO network architecture [9, 8].",yes,Malceiver,yes,Perceiver/PerceiverIO,"We propose the Malceiver, a hierarchical Perceiver model for Android malware detection that makes use of multi-modal features."
GPT-NeoX-20B,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,https://www.biorxiv.org/content/biorxiv/early/2023/07/08/2023.07.06.547963.full.pdf,Future Work,,"With the increasing availability of large-scale annotated MS/MS data, the use of large language models such as LLaMA (Touvron et al., 2023), GPT-NeoX (Black et al., 2022) or Chinchilla (Hoffmann et al., 2022) seems to be a highly promising methodological avenue to train a new generation of structure prediction models in the future.",yes,deep neural netwrok for molecular structure construction,yes,GNPS,Here we report a deep neural network method to predict chemical structures solely from high-resolution MS/MS spectra. This novel approach initially relies on the encoding of SMILES strings from chemical structures using a continuous chemical descriptor space that had been previously implemented for molecule design
AWD-LSTM,On the State of the Art of Evaluation in Neural Language Models,https://europepmc.org/articles/pmc6612801?pdf=render,Background,,"A fair and unbiased comparison can be very challenging especially when considering the sensitivity of deep learning methods to the step of model selection: deep neural networks have many hyper-parameters that require careful tuning, and differences in performance can be the result of the use of different model selection strategies (Lipton and Steinhardt, 2018; Melis et al., 2018).",no,n/a,yes,CNNs + RNNs,"In this study we present a systematic exploration of deep learning architectures for predicting DNA- and RNA-binding specificity. For this purpose, we present deepRAM, an end-to-end deep learning tool that provides an implementation of a wide selection of architectures; its fully automatic model selection procedure allows us to perform a fair and unbiased comparison of deep learning architectures"
Multi-scale Dilated CNN,Multi-Scale Context Aggregation by Dilated Convolutions,https://arxiv.org/pdf/2307.06005,Uses,,"In addition, to capture long-range dependency between words, we also incorporate the dilated convolutional layer [51] that introduces “holes” into each convolution filter.",yes,Discretized Differentiable Neural Architecture Search (DDNAS),yes,Neural Architecture Search (NAS),"This paper presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification."
AlexNet,ImageNet classification with deep convolutional neural networks,https://dergipark.org.tr/tr/download/article-file/2940213,Idk the language,,,,,,,
SVM for face detection,Training support vector machines: an application to face detection,https://www.mdpi.com/2073-8994/13/4/615/pdf?version=1618226433,Background,,Data classification problems arise and are solved in many areas of human activity [1–7].,yes,two-stage hybrid SVM-kNN classifiers,yes,SVM classifier,The paper considers a solution to the problem of developing two-stage hybrid SVM-kNN classifiers with the aim to increase the data classification quality by refining the classification decisions near the class boundary defined by the SVM classifier.
,,,Background,,"Such problems include the problems of credit risk analysis [1], medical diagnostics [2], text categorization [4], the identification of facial images [7], etc.",yes,two-stage hybrid SVM-kNN classifiers,yes,SVM classifier,The paper considers a solution to the problem of developing two-stage hybrid SVM-kNN classifiers with the aim to increase the data classification quality by refining the classification decisions near the class boundary defined by the SVM classifier.
,,,Motivation,,"Many classification problems are successfully solved using the SVM algorithm [1–7,17–20,24–29].",yes,two-stage hybrid SVM-kNN classifiers,yes,SVM classifier,The paper considers a solution to the problem of developing two-stage hybrid SVM-kNN classifiers with the aim to increase the data classification quality by refining the classification decisions near the class boundary defined by the SVM classifier.
MCDNN (MNIST),Multi-column deep neural networks for image classification,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/bme2.12004,Uses,,These preprocessing steps are applied consistently to all images to artificially increase the dataset size using label‐preserving transformations [46].,yes,"robust deep learning approach for glasses detection, DL architecture",yes,CNN,"In this paper, we present a robust deep learning approach for glasses detection from selfie photos full/ partial frontal body non‐standard images captured in real‐life uncontrolled environments that do not utilize any facial landmarks."
ReLU (LFW),Rectified Linear Units Improve Restricted Boltzmann Machines,https://www.biorxiv.org/content/biorxiv/early/2023/12/02/2023.12.01.569515.1.full.pdf,Uses,,"To learn non-linear patterns, each layer in MetageNN is followed by a ReLU activation function [40].",yes,MetageNN,yes,Rectified Linear Units Improve Restricted Boltzmann Machines,"We present MetageNN, a memory-efficient long-read taxonomic classifier that is robust to sequencing errors and missing genomes. MetageNN is a neural network model that uses short k-mer profiles of sequences to reduce the impact of distribution shifts on error-prone long reads."
Transformer local-attention (NesT-B),"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",http://arxiv.org/pdf/2210.05958,Motivation,,"The insufficient training data makes ViT hard to derive the inductive bias of attending locality, thus many recent works strive to introduce local inductive bias by integrating convolution into ViTs [18, 15, 30, 31, 32] and modify it to hierarchical structure [33, 34, 16, 17, 35], making ViTs more like traditional CNNs.",no,n/a,yes,Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs),"In this paper, we further consider this problem and point out two weaknesses of ViTs in inductive biases, that is, the spatial relevance and diverse channel representation."
,,,Motivation,,"To make ViTs more similar to standard CNNs, [16, 54, 17, 34, 33, 35, 57, 32] re-design the spatial and channel dimension of vanilla ViT, producing a series of hierarchical style vision transformer.",no,n/a,yes,Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs),"In this paper, we further consider this problem and point out two weaknesses of ViTs in inductive biases, that is, the spatial relevance and diverse channel representation."
,,,Uses,,"The structure of vision transformer also guarantees this mechanism because the length of input tokens is variable, except for the hierarchical structure vision transformer with window attention such as[17, 35].",no,n/a,yes,Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs),"In this paper, we further consider this problem and point out two weaknesses of ViTs in inductive biases, that is, the spatial relevance and diverse channel representation."
VD-LSTM+REAL Large,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,http://arxiv.org/pdf/2203.12644,Uses,,"The word embedding and softmax matrices are tied (Press and Wolf, 2017; Inan et al., 2017)",yes,MemSizer,no,n/a,"We propose MemSizer, an approach towards closing the performance gap while improving the efficiency even with short generation. It projects the source sequences into lower dimension representations like Linformer, while enjoying efficient recurrent-style incremental computation similar to kernel-based transformers."
,,,Motivation,,"CNN allowed to achieve impressive results in image recognition, with near human performance on MNIST dataset and outperformed humans on traffic sign recognition by a factor of two [19].",yes,HydraNet CNNs,yes,U-Net architecture,"Based on U-Net architecture we develop HydraNet CNNs which allow segmenting worms accurately into anterior, mid-body and posterior parts"
MCDNN (MNIST),Multi-column deep neural networks for image classification,https://www.aging-us.com/article/203916/pdf,Motivation,,"CNN allowed to achieve impressive results in image recognition, with near human performance on MNIST dataset and outperformed humans on traffic sign recognition by a factor of two [19].",yes,WormNet,yes,CNN,"We designed WormNet - a convolutional neural network (CNN) to predict the worm lifespan class based on young adult images (day 1 – day 3 old adults) and showed that WormNet, as well as, InceptionV3 CNN can successfully classify lifespan."
CodeT5+,CodeT5+: Open Code Large Language Models for Code Understanding and Generation,https://arxiv.org/pdf/2308.02582,Differences,,"We have not used other open-source models pretrained on code, such as CodeT5+ (Wang et al., 2023) (768 token length) or Codegen (2048 token length) (Nijkamp et al., 2022) for our study as they do not offer the token length required for our prompt (4K).",yes,LTMP-DA-GP,yes,LLMs,"In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP)"
DNABERT,DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome,https://bmcgenomics.biomedcentral.com/counter/pdf/10.1186/s12864-021-08246-1,Motivation,Differences,"The DNAbert model [22] is more related to the present work. It is a transformer neural network, which in the pre-training is trained to predict k-mers (k=3-6) from the surrounding sequence context.",yes,bidirectional Markov model,yes,nucleotide models,We develop a bidirectional Markov model that use an average of the probability from a Markov model applied to both strands of the sequence and thus depends on up to 14 bases to each side of the nucleotide.
Reading Twice for NLU,Dynamic Integration of Background Knowledge in Neural NLU Systems,https://ojs.aaai.org/index.php/AAAI/article/download/17490/17297,Motivation,,"These approaches usually leverage knowledge to enhance a specific CQA component: 1) enhancing representations (Weissenborn, Kocisk ˇ y, and Dyer 2017; Bauer, Wang, and Bansal 2018; ` Mihaylov and Frank 2018; Ma et al. 2019); 2) enhancing attention mechanism (Chen et al. 2018; Wang and Jiang 2019); and 3) enhancing reasoning mechanism (Lin et al. 2019; Lv et al. 2020).",no,n/a,yes,Commonsense Question Answering (CQA) approaches,"? To answer these questions, we benchmark knowledge-enhanced CQA by conducting extensive experiments on multiple standard CQA datasets using a simple and effective knowledgeto-text transformation framework."
Hybrid H3-2.7B,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,https://arxiv.org/pdf/2309.13600,Motivation,,"These approaches provide superior performance in several areas, such as NLP (Mehta et al. 2022; Wang et al. 2022; Dao et al. 2022), speech (Saon, Gupta, and Cui 2023), RL (Lu et al. 2023; David et al. 2022), time series analysis, and more, especially in tasks that require capturing long-range dependencies.",no,n/a,yes,Hyena N-D,"Our empirical findings indicate that the proposed Hyena N-D layer boosts the performance of various Vision Transformer architectures, such as ViT, Swin, and DeiT across multiple datasets"
,,,Motivation,,"Recently, several novel sequence layers showed impressive results in 1-D sequence modeling, specifically in improving complexity (Peng et al. 2023; Poli et al. 2023; Dao et al. 2022).",no,n/a,yes,Hyena N-D,"Our empirical findings indicate that the proposed Hyena N-D layer boosts the performance of various Vision Transformer architectures, such as ViT, Swin, and DeiT across multiple datasets"
Transformer + Simple Recurrent Unit,Simple Recurrent Units for Highly Parallelizable Recurrence,http://arxiv.org/pdf/2305.13048,Uses,,"The element-wise WKV computation is time-dependent but can be readily parallelized along the other two dimensions (Lei et al., 2018) 3 .",yes,Receptance Weighted Key Value (RWKV),yes,RNNs,"We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs."
fastText,Bag of Tricks for Efficient Text Classification,http://arxiv.org/pdf/2301.08006,Background,,"In [12], authors employed FastText word representation in conjunction with strategies such as bag of n-gram characteristics and demonstrated that FastText outperformed deep learning approaches while being faster.",yes,Word2Vec remodel,yes,Word2Vec,This paper proposes two novel models for the keyword suggestion task trained on scientific literature. Our techniques adapt the architecture of Word2Vec and FastText to generate keyword embeddings by leveraging documents’ keyword co-occurrence.
,,,Background,,"In [12], authors employed FastText word representation in conjunction with strategies such as bag of n-gram characteristics and demonstrated that FastText outperformed deep learning approaches while being faster.",yes,FastText remodel,yes,FastText,This paper proposes two novel models for the keyword suggestion task trained on scientific literature. Our techniques adapt the architecture of Word2Vec and FastText to generate keyword embeddings by leveraging documents’ keyword co-occurrence.
AmoebaNet-A (F=190),Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/pdf/2303.08308,Uses,,"Specifically, we design a stage-wise hyperspace to include many candidate search spaces and leverage aging evolution [30] to perform random mutations of elastic stages for search space evolution.",yes,SpaceEvo,yes,Regularized Evolution for Image Classifier Architecture Search,"To address this challenge, we propose SpaceEvo, an automatic method for designing a dedicated, quantizationfriendly search space for each target hardware."
,,,Extends,,"By factorizing the search space into a sequence of elastic stages ((Sec. 4.3), we enable traditional aging evolution methods, such as the aging evolution [30], to be directly applied to search the space (Sec. 4.4).",yes,SpaceEvo,yes,Regularized Evolution for Image Classifier Architecture Search,"To address this challenge, we propose SpaceEvo, an automatic method for designing a dedicated, quantizationfriendly search space for each target hardware."
,,,Uses,,"Taking this advantage, we leverage aging evolution [30] to search the large hyperspace.",yes,SpaceEvo,yes,Neural Architecture Search (NAS),"To address this challenge, we propose SpaceEvo, an automatic method for designing a dedicated, quantizationfriendly search space for each target hardware."
DNABERT,DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome,https://www.biorxiv.org/content/biorxiv/early/2023/08/17/2023.08.15.553415.full.pdf,Background,,"Different approaches to tackle these issues have emerged, including: (1) Developing specific architectures for long sequences (Lin et al. 2021; Rao et al. 2021); (2) Splitting the data into smaller segments (Dotan et al. 2023); (3) K-mer representation of all possible nucleotides (Ji et al. 2021).",no,n/a,yes,"BPE, Unigram, WordPiece, “words”, and “pairs”.","In this work, we study the effect of alternative tokenization algorithms on eight different tasks in biology, from predicting the function of proteins and their stability, through nucleotide sequence alignment, to classifying proteins to specific families."
data2vec (vision),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://arxiv.org/pdf/2308.14553,Motivation,,"In the field of automatic speech recognition (ASR), the self-supervised pre-trained models Wav2vec2.0 [13], HuBERT [14], Data2vec [15] and WavLM [16] were proposed recently, and in [17] using the pretrained models to learn different levels of information at different layers was analyzed.",no,n/a,yes,TTS models,"In this work, we therefore explore pre-trained models to improve the noise robustness of TTS models."
BatchNorm,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://www.mdpi.com/1422-0067/24/21/15681/pdf?version=1698416591,Uses,,The output from the last hidden state is subjected to batch normalization to reduce the covariate shift and provide additional regularization [25].,yes,LSTM4piRNA,yes,LSTM network,"To address these issues, we propose LSTM4piRNA, a highly efficient deep learning-based method for predicting piRNAs in large-scale genome databases."
Inception-ResNet-V2,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",https://arxiv.org/pdf/2308.07470,Uses,,"To understand the batching quality of DNN model serving systems, we ran a single copy of ResNet50 [11] (SLO 25 ms) and InceptionResNetV2 [37] (SLO 70 ms) separately on 8 GPUs using Clockwork, Nexus, Shepherd, and Symphony.",yes,Symphony,yes,InceptionResNetV2,"We propose Symphony, a DNN serving system that explores deferred batch scheduling to optimize system efficiency and throughput."
,,,Uses,,"We collected a mixed model zoo consisting of 37 widelyused DNN models, including variants of DenseNet [17], EfficientNet [39, 40], Inception [37, 38], MobileNet [13, 14, 32], NASNet [49], ResNet [11, 12], VGG [35], Xception [3], SSDMobileNet [23], and Bert [8].",yes,Symphony,yes,DenseNet,"We propose Symphony, a DNN serving system that explores deferred batch scheduling to optimize system efficiency and throughput."
,,,Uses,,"To understand how batch size affects goodput and to study how close is the batching behavior to the ideal staggered execution, we run a single copy of ResNet50 [11] and InceptionResNetV2 [37] separately with 8 GPUs on Symphony, Clockwork, and Nexus. Requests arrive in a Poisson distribution.",yes,Symphony,yes,ResNet50,"We propose Symphony, a DNN serving system that explores deferred batch scheduling to optimize system efficiency and throughput."
BERT-Large-CAS (PTB+WT2+WT103),Language Models with Transformers,https://journals.nauss.edu.sa/index.php/JISCR/article/download/1488/1011,Uses,,"The transformer model [85, 133] uses the attention to boost the training speed.",no,n/a,yes,too many,"In this study, research directions and the theoretical foundation in this area are investigated."
Make-A-Video,Make-A-Video: Text-to-Video Generation without Text-Video Data,http://arxiv.org/pdf/2305.08850,Background,Differences,"Diffusion-based generative models have demonstrated remarkable success in generating photorealistic and diverse images [27, 28, 31] and videos [34, 38, 41] conditioned on text.",yes,Make-AProtagonist,yes,Stable UnCLIP,"In this regard, we propose a generic video editing framework called Make-AProtagonist, which utilizes textual and visual clues to edit videos with the goal of empowering individuals to become the protagonists."
,,,Motivation,,"In view of the impressive performance in T2I, text-to-video generation (T2V) [11, 12, 20, 34, 41] has attracted much attention.",yes,Make-AProtagonist,yes,Stable UnCLIP,"In this regard, we propose a generic video editing framework called Make-AProtagonist, which utilizes textual and visual clues to edit videos with the goal of empowering individuals to become the protagonists."
LDM-1.45B,High-Resolution Image Synthesis with Latent Diffusion Models,https://arxiv.org/pdf/2312.08494,Motivation,Uses,"Diffusion models [4, 5, 6] have shown unprecedented success in multiple modalities, such as text-conditional image and audio generation [7, 8], de novo protein design [9], natural language processing [10], and medical imaging [11, 12]. For an extensive survey on the current state of diffusion models and their applications, see [13].",yes,PerMod,yes,Difussion Models,"Towards allowing greater perceptual control over voice, we introduce PerMod, a conditional latent diffusion model that takes in an input voice and a perceptual qualities vector, and produces a voice with the matching perceptual qualities."
Maxout Networks,Maxout Networks,https://www.frontiersin.org/articles/10.3389/fpls.2020.611622/pdf,Uses,,"Multilayer perceptron is a powerful machine learning technique that can characterize the features of the samples and learn the appropriate classification features from the samples (Goodfellow et al., 2016).",no,n/a,yes,Multilayer perceptrons and Vector Machine,"We used hyperspectral imaging data and machine learning to explore the possibility of fast, accurate and automated discrimination of weeds in pastures where ryegrass and clovers are the sown species."
A3C FF hs,Asynchronous Methods for Deep Reinforcement Learning,http://arxiv.org/pdf/2302.02298,Uses,,"This means that one can select other feature extraction models to replace VGG-16 in the first block and can also consider other state-of-the-art algorithms [13]–[15] instead of A3C [31] that is implemented in [21], as long as it can approximate a value function [4].",no,n/a,yes,Deep Reinforced Learning,"In this paper, we review two publications that investigate the mentioned issues of DRL and propose effective solutions."
SPN-4+KN5,Language modeling with sum-product networks,https://link.springer.com/content/pdf/bfm:978-3-030-58449-8/1?pdf=chapter%20toc,This are lecture notes from a class?,,,,,,,
YOLOv3,YOLOv3: An Incremental Improvement,https://dl.acm.org/doi/pdf/10.1145/3581783.3612412,Background,,"While many architectures have attempted to train on different scales [11, 31], they are all data-driven and fail to provide the necessary guarantees.",yes,ScaleFlow,yes,Neural Networks,"To overcome these limitations, in this work, we propose ScaleFlow, a closed-loop scale-adaptive inference that can reduce model inference time by progressively processing vision data with increasing resolution but decreasing spatial size, achieving speedup without compromising accuracy"
,,,Background,,"However, they only provide a way of quantifying class uncertainty, not location uncertainty [11, 29– 31, 42, 44].",yes,ScaleFlow,yes,Neural Networks,"To overcome these limitations, in this work, we propose ScaleFlow, a closed-loop scale-adaptive inference that can reduce model inference time by progressively processing vision data with increasing resolution but decreasing spatial size, achieving speedup without compromising accuracy"
,,,Differences,,"We select several representative object detection neural network models, YOLOv3 [31], CenterNet [44], RetinaNet [23], and FCOS [37] to evaluate our ScaleFlow as an object-detection service with both live streaming from a webcam and pseudo streaming from the COCO dataset [24].",yes,ScaleFlow,yes,Neural Networks,"To overcome these limitations, in this work, we propose ScaleFlow, a closed-loop scale-adaptive inference that can reduce model inference time by progressively processing vision data with increasing resolution but decreasing spatial size, achieving speedup without compromising accuracy"
,,,Uses,,"In this section, we explore opportunities for improving neural network speed through scale-adaptive designs, using a motivational study with YOLOv3 [31] object detection on COCO dataset [24].",yes,ScaleFlow,yes,Neural Networks,"To overcome these limitations, in this work, we propose ScaleFlow, a closed-loop scale-adaptive inference that can reduce model inference time by progressively processing vision data with increasing resolution but decreasing spatial size, achieving speedup without compromising accuracy"
,,,Differences,,"We choose five neural network architectures designed for object detection, YOLOv3 [31], CenterNet [44], RetinaNet [23], and FCOS [37], EfficientDet [36]. Due to space constraints, some experiments only use YOLOv3 and CenterNet as representative anchorbased and anchor-free model",yes,ScaleFlow,yes,Neural Networks,"To overcome these limitations, in this work, we propose ScaleFlow, a closed-loop scale-adaptive inference that can reduce model inference time by progressively processing vision data with increasing resolution but decreasing spatial size, achieving speedup without compromising accuracy"
WizardCoder-15.5B,WizardCoder: Empowering Code Large Language Models with Evol-Instruct,https://arxiv.org/pdf/2310.05103,Motivation,,"Recently, WizardCoder (Luo et al., 2023) significantly outperforms other open-source Code LLMs by employing the Evol-Instruct method for complex instruction fine-tuning.",no,n/a,yes,DetectGPT,"This work proposes a training-free approach for the detection of LLMs-generated codes, mitigating the risks associated with their indiscriminate usage."
ShuffleNet v1,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,https://arxiv.org/pdf/2308.13343,Motivation,,"There are architectures like Structured transform networks [11], Deep fried convnets [15], Shufflenet [19] which have considered computational aspects as well",yes,SaEnet,yes,CNN,"We propose SaEnet, Squeeze aggregated excitation network, for learning global channelwise representation in between layers."
VQGAN + CLIP,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/pdf/2304.08945,Background,,"From then on, transformer-based models have been exploited for different tasks, including object detection [29], image generation [30]), video understanding [31], etc.",yes,DIRFA,yes,transformer architecture,"This paper presents DIRFA, a novel method that can generate talking faces with diverse yet realistic facial animations from the same driving audio."
BellKor 2007,The BellKor solution to the Netflix Prize,https://ojs.aaai.org/index.php/ICWSM/article/download/15013/14863,Background,,Such applications have made extensive use of Matrix Factorization methods that have been popularized during the Netflix Prize (Bell and Koren 2007).,yes,n/a,no,n/a,"However, collaboration patterns are difficult to capture when the relationships between users are not directly observable, since they need to be inferred from the user actions. In this work, we propose a solution to this problem by adopting a systemic view of collaboration."
Transformer + Simple Recurrent Unit,Simple Recurrent Units for Highly Parallelizable Recurrence,https://ieeexplore.ieee.org/ielx7/6287639/8600701/08786773.pdf,Motivation,,"The first architecture of RNNs is based on the simple recurrent units (SRUs), which is simple and fast. However, it suffers from the vanishing gradient problem [29].",yes,n/a,yes,SRUs and GRUs in neural networks,"In this paper, we propose an effective multi-sensors-based framework for human activity recognition using a hybrid deep learning model, which combines the simple recurrent units (SRUs) with the gated recurrent units (GRUs) of neural networks."
Conformer + Wav2vec 2.0 + Noisy Student,Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition,http://arxiv.org/pdf/2210.07677,Differences,,"Namely, we compare against the high-performing self-supervised speech representations models wav2vec 2.0 [2], Conformer [23], and w2v-BERT models [4].",yes,TransFusion,yes,LibriSpeech,"Specifically, we propose TransFusion: a transcribing diffusion model which iteratively denoises a random character sequence into coherent text corresponding to the transcript of a conditioning utterance"
,,,Differences,,"We compare against three state-of-the-art models for ASR: wav2vec 2.0 [2], Conformer [23], and w2v-BERT [4].",yes,TransFusion,yes,LibriSpeech,"Specifically, we propose TransFusion: a transcribing diffusion model which iteratively denoises a random character sequence into coherent text corresponding to the transcript of a conditioning utterance"
,,,Differences,,"This differs from the baselines, all of which have been trained with substantial data augmentation to further improve performance [2, 23, 4].",yes,TransFusion,yes,LibriSpeech,"Specifically, we propose TransFusion: a transcribing diffusion model which iteratively denoises a random character sequence into coherent text corresponding to the transcript of a conditioning utterance"
LSTM with forget gates,Learning to Forget: Continual Prediction with LSTM,https://www.mdpi.com/2075-1680/12/3/266/pdf?version=1679978199,Background,,"Different types of ANNs include shallow networks, deep networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs) [29,30].",no,n/a,yes,long short-term memory (LSTM),"In the current study, the use of an LSTM and a bidirectional LSTM (BiLSTM) is proposed for dealing with a data collection that, besides the time series values denoting the solar energy generation, also comprises corresponding information about the weather."
Part-of-sentence tagging model,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,https://www.aclweb.org/anthology/2020.coling-main.245.pdf,Uses,,"Secondly, we propose to use a BiLSTM-CNN architecture (Ma, 2016) that integrates character embeddings as additional features, using a convolution layer.",no,,yes,Neural Networks,"Since the MEDIA task seems to be one of the most difficult, according to several previous studies, we propose to explore Neural Networks approaches focusing of three aspects: firstly, the Neural Network inputs and more specifically the word embeddings; secondly, we compared French version of BERT against the best setup through different ways; Finally, the comparison against State-of-the-Art approaches."
,,,Extends,,"To further improve the performance of our SLU model, we propose to use a BiLSTM-CNN (convolutional neural network) architecture (Ma, 2016) that integrates character embeddings using a convolution layer, in addition to the word embeddings.",no,,yes,Neural Networks,"Since the MEDIA task seems to be one of the most difficult, according to several previous studies, we propose to explore Neural Networks approaches focusing of three aspects: firstly, the Neural Network inputs and more specifically the word embeddings; secondly, we compared French version of BERT against the best setup through different ways; Finally, the comparison against State-of-the-Art approaches."
,,,Extends,,"In this section, we propose to use a BiLSTM-CNN architecture (Ma, 2016) that integrates character embeddings as additional features, using a convolution layer.",no,,yes,Neural Networks,"Since the MEDIA task seems to be one of the most difficult, according to several previous studies, we propose to explore Neural Networks approaches focusing of three aspects: firstly, the Neural Network inputs and more specifically the word embeddings; secondly, we compared French version of BERT against the best setup through different ways; Finally, the comparison against State-of-the-Art approaches."
Unsupervised Scale-Invariant Learning,Object class recognition by unsupervised scale-invariant learning,https://isprs-archives.copernicus.org/articles/XL-1-W1/333/2013/isprsarchives-XL-1-W1-333-2013.pdf,Background,,"Later, Webber et. al (2000) represent objects as constellations of rigid parts, and recognized objects with a join probability density function on the shape of rigid parts by similarity matching. Fergus et. al (2003) and Opelt et. al (2004) proposed category models composed of some more flexible parts, and estimated the parameters of the parts using expectation-maximization algorithm.",yes,hierarchical semantic graph model,no,n/a,"In this paper, we propose a hierarchical semantic graph model to detect and recognize man-made objects in high resolution remote sensing images automatically."
ProGen2-xlarge,ProGen2: Exploring the Boundaries of Protein Language Models,https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1011162&type=printable,Background,,"Recently, self-supervised masked language models of biological sequences have been used to study proteins [14–21], DNA [22], RNA [23, 24], and glycans [25, 26].",yes,n/a,yes,BERT,"Here, we introduce a selfsupervised learning approach designed to identify and characterize BGCs from such data."
Transformer-XL Large + Phrase Induction,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",https://arxiv.org/pdf/2005.11153,"Quote not present, but it is the smae author",,,,,,,
GSM,Gated Self-Matching Networks for Reading Comprehension and Question Answering,https://ojs.aaai.org/index.php/AAAI/article/download/4591/4469,Background,,"It means that hp, the state at the position that we attend to, is used for comparing with the sentence representations to encode position information, and ht is used for matching the sentence representations against itself (self-matching) to collect information from the context (Wang et al. 2017).",yes,n/a,yes,NLP,"To this end, we first design a tagging scheme to generate n tag sequences for an n-word sentence. Then a position-attention mechanism is introduced to produce different sentence representations for every query position to model these n tag sequences. In this way, our method can simultaneously extract all entities and their type, as well as all overlapping relations"
,,,Motivation,,"Attention mechanisms are used to model dependencies of input and output sequences, and are successfully applied in various NLP tasks (Bahdanau, Cho, and Bengio 2014; Vaswani et al. 2017; Cheng, Dong, and Lapata 2016; Wang et al. 2017",yes,n/a,yes,NLP,"To this end, we first design a tagging scheme to generate n tag sequences for an n-word sentence. Then a position-attention mechanism is introduced to produce different sentence representations for every query position to model these n tag sequences. In this way, our method can simultaneously extract all entities and their type, as well as all overlapping relations"
Part-of-sentence tagging model,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,https://arxiv.org/pdf/2008.12014,Extends,,"For both PoS tagging and NER, we experiment with an established neural sequence tagging model, dubbed BILSTM-CNN-CRF, introduced by Ma and Hovy [22].",yes,GREEK-BERT,yes,NLP,"In this paper, we present GREEK-BERT, a monolingual BERT-based language model for modern Greek."
ALIGN,Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision,http://arxiv.org/pdf/2304.09172,Uses,,"Approaches such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) have catalyzed a lot of recent progress in computer vision by showing that Transformer-based (Vaswani et al., 2017) models trained using large amounts of image-text data from the internet can yield transferable representations, and such models can perform zero-shot recognition and retrieval using natural language queries.",yes,MERU,yes,CLIP,"We propose MERU, a contrastive model that yields hyperbolic representations of images and text."
,,,Uses,,"Our method conceptually resembles current state-of-the-art contrastive methods (Jia et al., 2021; Radford et al., 2021).",yes,MERU,yes,CLIP,"We propose MERU, a contrastive model that yields hyperbolic representations of images and text."
,,,Uses,,"More recent approaches like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) use contrastive metric learning to pre-train Vision Transformers (Dosovitskiy et al., 2021) and have helped to better realize the motivations of the earlier works in practice.",yes,MERU,yes,CLIP,"We propose MERU, a contrastive model that yields hyperbolic representations of images and text."
DOT(S)-RNN,How to Construct Deep Recurrent Neural Networks,http://manuscript.elsevier.com/S127096382100211X/pdf/S127096382100211X.pdf,Forbidden Page,,,,,,,
EDSR,Enhanced Deep Residual Networks for Single Image Super-Resolution,https://www.mdpi.com/1424-8220/23/21/8717/pdf?version=1698285540,Background,,"For example, EDSR [26] has more than 40M parameters, and the huge computing power requirement makes it almost impossible to deploy in embedded devices, which limits its practical application",yes,dynamic attention mechanism-based thermal image super-resolution network (LDASRNet),yes,"shallow feature extraction (SFE) module, a deep feature extraction (DFE) module and a feature reconstruction (FRec) module.","In this paper, we propose a dynamic attention mechanism-based thermal image super-resolution network for infrared sensors. Specifically, the dynamic attention modules adaptively reweight the outputs of the attention and non-attention branches according to features at different depths of the network"
,,,Differences,,"Unlike SRResNet [33], EDSR [26] consists of 32 residual blocks that remove batch normalization, reducing memory consumption and artifacts",yes,dynamic attention mechanism-based thermal image super-resolution network (LDASRNet),yes,"shallow feature extraction (SFE) module, a deep feature extraction (DFE) module and a feature reconstruction (FRec) module.","In this paper, we propose a dynamic attention mechanism-based thermal image super-resolution network for infrared sensors. Specifically, the dynamic attention modules adaptively reweight the outputs of the attention and non-attention branches according to features at different depths of the network"
,,,Differences,,"To verify that LDASRNet has comparable or even better performance than larger networks, we select models AWSRN [65], SRMDNF [67], CARN [68], ChaSNet [44], MPRANet [1] and MDSR [26] with parameters ranging from 1.4M to 6.5M for comparison.",yes,dynamic attention mechanism-based thermal image super-resolution network (LDASRNet),yes,"shallow feature extraction (SFE) module, a deep feature extraction (DFE) module and a feature reconstruction (FRec) module.","In this paper, we propose a dynamic attention mechanism-based thermal image super-resolution network for infrared sensors. Specifically, the dynamic attention modules adaptively reweight the outputs of the attention and non-attention branches according to features at different depths of the network"
,,,Differences,,"In addition, we select RCAN [24] and EDSR [26], two networks with parameters exceeding 10M (EDSR has more than 40M parameters) as reference.",yes,dynamic attention mechanism-based thermal image super-resolution network (LDASRNet),yes,"shallow feature extraction (SFE) module, a deep feature extraction (DFE) module and a feature reconstruction (FRec) module.","In this paper, we propose a dynamic attention mechanism-based thermal image super-resolution network for infrared sensors. Specifically, the dynamic attention modules adaptively reweight the outputs of the attention and non-attention branches according to features at different depths of the network"
Megatron-BERT,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://www.nature.com/articles/s41746-022-00742-2.pdf,Uses,,We used a total number of 992 NVIDIA DGX A100 GPUs from 124 superPOD nodes at UF’s HiPerGator-AI cluster to train GatorTron models by leveraging both data-level and model-level parallelisms implemented by the Megatron-LM package [43].,no,n/a,yes,GatorTron,We examine how (1) scaling up the number of parameters and (2) scaling up the size of the training data could benefit these NLP tasks.
LSTM,Long Short-Term Memory,https://journal.uob.edu.bh:443/bitstream/123456789/5190/3/IJCDS150130_1570911792.pdf,Uses,,We employed VGG16 [20] and LSTM [21] networks for image captioning. Figure 3 block diagram for image captioning.,yes,CNBD-Combinational Network for Bullying Detection,yes,Binary Encoder Image Transformer (BEiT) and Multi-Layer Perceptron (MLP) network,"We proposed a deep learning technique named as CNBD-Combinational Network for Bullying Detection (CNBD), which is a combination of two networks: Binary Encoder Image Transformer (BEiT) and Multi-Layer Perceptron (MLP) network."
Galactica,Galactica: A Large Language Model for Science,http://arxiv.org/pdf/2305.16636,Uses,,"When constructing our training set, we use incontext few-shot learning with the 6.7B parameter version of Galactica (Taylor et al., 2022).",yes,DataFinder Dataset,yes,GPT-3 and Galactica,"To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expertannotated evaluation set (392 queries)."
,,,Uses,,"We simulate query collection with the 6.7B parameter version of Galactica (Taylor et al., 2022), a large scientific language model that supports fewshot learning.",yes,DataFinder Dataset,yes,GPT-3 and Galactica,"To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expertannotated evaluation set (392 queries)."
,,,Uses,,"To encourage more efficient labeling (Wang et al., 2021), we provided autosuggestions for each field from GPT-3 (Brown et al., 2020) and Galactica 6.7B (Taylor et al., 2022) to help annotators.",yes,DataFinder Dataset,yes,GPT-3 and Galactica,"To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expertannotated evaluation set (392 queries)."
Meena,Towards a Human-like Open-Domain Chatbot,https://arxiv.org/pdf/2010.03450,Background,,"However, recent success on many language understanding problems (Wang et al., 2019), the impressive generation capabilities of modern dialog systems (Zhang et al., 2019; Adiwardana et al., 2020), as well as the huge interest in yes/no question-answering (Choi et al., 2018; Clark et al., 2019) have created a conducive environment for revisiting this hard task.",yes,Circa,no,n/a,"We create and release1 the first large-scale English language corpus ‘Circa’ with 34,268 (polar question, indirect answer) pairs to enable progress on this task."
LDA,Latent Dirichlet Allocation,https://arxiv.org/pdf/2310.07739,Uses,,LDA works by selecting the number of clusters that yielded the largest coherence value [49].,no,n/a,yes,LDA,"In this paper, we investigate the supply-demand dynamics of the four presidential candidates of Taiwan, using 911,510 Facebook posts from public figures, pages, and public groups, provided by CrowdTangle."
,,,Uses,,"After filtering the data sources by policy issues and candidates, we employed the two keyword extraction methods: frequency counts and Latent Dirichlet Allocation (LDA) for topic modeling.",no,n/a,yes,LDA,"In this paper, we investigate the supply-demand dynamics of the four presidential candidates of Taiwan, using 911,510 Facebook posts from public figures, pages, and public groups, provided by CrowdTangle."
DINOv2,DINOv2: Learning Robust Visual Features without Supervision,http://arxiv.org/pdf/2305.19256,Uses,,"Specifically, we generate 10000 images from each model and we use DINO [9]-v2 [42] to compute top-1 similarity to the training images.",yes,first diffusion-based framework that can learn an unknown distribution,yes,DINOv2,We present the first diffusion-based framework that can learn an unknown distribution using only highly-corrupted samples.
Temporal Convolutional Attention-based Network(TCAN) (WT2),Temporal Convolutional Attention-based Network For Sequence Modeling,https://arxiv.org/pdf/2211.13114,Future Work,,Our future work will investigate modifying TCNs [27] and attentionbased TCNs [28] to handle variable-length sequences and applying them to signal-level SC.,no,n/a,yes,LSTM,"To circumvent these requirements, we present a novel SC approach utilizing many-to-one attention-based LSTM. With the proposed LSTM network, SC is solved as a regression problem, taking the entire sensor signal as input and the step count as the output. The analysis shows that the attention-based LSTM automatically learned the pattern of steps even in the absence of groundtruth labels."
Enhanced Neighborhood-Based Filtering,Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights,https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470874233.fmatter,Wrong link?,,,,,,,
Enhanced Neighborhood-Based Filtering,Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights,https://arxiv.org/pdf/1509.09130,Motivation,,Taking into account the popularity of items in order to improve the recommendation has been considered before by [8] who design a greedy sequential preprocessing procedure aimed at subtracting different explanatory effects that may have an influence on the data,no,n/a,no,n/a,"In this contribution, we provide statistical evidence that existing movie recommendation datasets reveal a significant positive association between the rating of items and the propensity to select these items."
,,,Background,,These include standard user and item rating effects as well as popularity —referred to as “support” in [8]— and time-related effects,no,n/a,no,n/a,"In this contribution, we provide statistical evidence that existing movie recommendation datasets reveal a significant positive association between the rating of items and the propensity to select these items."
,,,Motivation,,"Interestingly, [8] uses successive Bayesian regressions for each effect to reduce the variability inherent in using a linear model with many missing observations.",no,n/a,no,n/a,"In this contribution, we provide statistical evidence that existing movie recommendation datasets reveal a significant positive association between the rating of items and the propensity to select these items."
,,,Future Work,,"An alternative would be to use a Bayesian mean estimate, as in [8], to shrink the estimates towards the global mean rating for scarcely observed items.",no,n/a,no,n/a,"In this contribution, we provide statistical evidence that existing movie recommendation datasets reveal a significant positive association between the rating of items and the propensity to select these items."
Llama 2-70B,Llama 2: Open Foundation and Fine-Tuned Chat Models,https://www.biorxiv.org/content/biorxiv/early/2023/11/29/2023.11.28.568945.full.pdf,Background,,"In recent years, generative models have emerged as powerful tools for generating creative and diverse content, ranging from text [15]–[17], images [18], [19] to speech [20] and more",no,n/a,yes,LLM,"In particular, we introduce an optimization pipeline that utilizes Large Language Models (LLMs) to pinpoint the mutation hotspots in the sequence and then suggest replacements to improve the overall fitness."
Meta Pseudo Labels,Meta Pseudo Labels,https://arxiv.org/pdf/2208.12633,Differences,,"Other research fields include semi-supervised learning, where only parts of the data are annotated (Pham et al., 2021), and synthetic datasets, where more training data are generated (Xu et al., 2019).",no,n/a,yes,Extreme Gradient Boosting (XGBoost),The limitations can be overcome by proposing a pipeline to process remote sensing images into feature-based representations that allow the employment of Extreme Gradient Boosting (XGBoost) for yield prediction.
Deep LSTM for video classification,Beyond short snippets: Deep networks for video classification,https://www.nature.com/articles/s41598-023-35938-3.pdf,Uses,,The target model consists of an LSTM-CNN encoder [101] and a classifcation head Ht.,yes,n/a,yes,CT,"To address this problem, we develop a three-level optimization based method which leverages CT data from a source domain to mitigate the lack of labeled CT scans in a target domain. Our method automatically identifes and downweights low-quality source CT data examples which are noisy or have large domain discrepancy with target data, by minimizing the validation loss of a target model trained on reweighted source data."
RBM-tuning,A Practical Guide to Training Restricted Boltzmann Machines,https://arxiv.org/pdf/1812.05477,Motivation,,Replacing binary units with Gaussian units can be performed by modifying the energy function [12].,yes, GPDB,yes,ShapeOdd,In this paper we present a generative model of shapes which provides a low dimensional latent encoding which importantly resides on a smooth manifold with respect to the silhouette images.
VQGAN + CLIP,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/pdf/2308.15300,Differences,,"Unlike the widely-used generative models including GANs [48], [49] and VAEs [39], [50], which learn the data distribution by a proxy adversarial task or maximizing the ELBO, the flow models explicitly estimate arbitrary data distribution through the bijective invertible normalizing flows",yes,MSFlow,yes,Real-NVP,"o generalize the anomaly size variation, we propose a novel Multi-Scale Flow-based framework dubbed MSFlow composed of asymmetrical parallel flows followed by a fusion flow to exchange multi-scale perceptions."
XLM,Cross-lingual Language Model Pretraining,https://jurnal.stmikroyal.ac.id/index.php/jurteksi/article/download/2492/1253,Uses,,Can predict the next token in sequence because it is trained with Casual Language Modeling (CLM) goals[17].,no,n/a,yes,GPT-2,"This research proposes a Transformer model with an Attention mechanism that can fetch important information, solve parallelization problems, and summarize long texts. The Transformer model we propose is GPT-2."
Thumbs Up?,Thumbs up? Sentiment Classification using Machine Learning Techniques,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/743A9DD62DF3F2F448E199BDD1C37C8D/S1047198722000109a.pdf/div-class-title-sentiment-is-not-stance-target-aware-opinion-classification-for-political-text-analysis-div.pdf,Background,,,no,,no,,couldnt find citation used
Fractional Max-Pooling,Fractional Max-Pooling,http://manuscript.elsevier.com/S1361841520302048/pdf/S1361841520302048.pdf,,,,,,,,access denied
DALL·E 2,Hierarchical Text-Conditional Image Generation with CLIP Latents,https://arxiv.org/pdf/2310.02635,Background,,,yes,Foundation Reinforcement Learning,no,,couldnt find citation used
OpenAI Five,Dota 2 with Large Scale Deep Reinforcement Learning,https://arxiv.org/pdf/2104.07294,Background,,"Several methods have been proposed to try to either reduce the space of actions by re-using model outputs for different action types [2], [3], provide side information to facilitate the exploration of large numbers of possible actions [4]–[6], or simplify the manipulation of the action spaces through action embeddings via mechanisms such as attention and graphs networks [7]–[9]. I",yes,Conditional Action Trees,no,,
"Variational (untied weights, MC) LSTM (Large)",A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,https://aclanthology.org/2021.acl-long.101.pdf,Background,,"Moreover, we include variational dropout (Gal and Ghahramani, 2016) as a comparison since it was used in a previous work on zero-shot translation (Pham et al., 2019) instead of the standard element-wise dropout.",yes,"Transformer
(Vaswani et al., 2017) with 5 encoder and decoder
layers. For the Europarl datasets with more training data, we enlarge the model to 8 encoder and
decoder layers.",no,,"Found this very interesting, it involves translations"
Fast R-CNN,Fast R-CNN,https://wepub.org/index.php/TCSISR/article/download/369/341,Uses,,"In the development process of target detection algorithm, there are two branches, one is RCNN, fast RCNN and fast CNN, i.e. fastrcnn series [13-16], in which RCNN generates 1k~2k candidate boxes from a graph, extracts features from candidate regions and sends them to SVM classifier, and uses regression to refine the position.",yes,e BSSD algorithm based on SSD algorithm,yes,Fast R-CNN,
"MSRA (C, PReLU)",Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,http://www.cell.com/article/S2405844023052945/pdf,Background,,"It would be equally true that the DNN performance is heavily affected by the weight parameter initialization methods [55,56].",no,,no,,
GPipe (Transformer),GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://dl.acm.org/doi/pdf/10.1145/3470496.3527405,Uses,,"To efficiently train these models, a variety of techniques have been used to exploit both pipelined (model) parallelism [30][19] and data (mini-batch) parallelism.",yes,software-scheduled routing algorithm allows the automatic parallelizing compiler to load balance the global links of the Dragonfly network,yes,pipeline parallelism,
SVD in recommender systems,Application of Dimensionality Reduction in Recommender System - A Case Study,http://www.aaai.org/Papers/Workshops/2006/WS-06-10/WS06-10-005.pdf,Background,,"Collaborative filtering algorithms range from the simple nearest-neighbor methods (Breese, Heckerman, & Kadie 1998; Resnick et al. 1994; Sarwar et al. 2001) to more complex machine learning based methods such as graph based methods (Aggarwal et al. 1999; Huang, Chen, & Zeng 2004), linear algebra based methods (Billsus & Pazzani 1998; Sarwar et al. 2000; Goldberg et al. 2001; Marlin & Zemel 2004; Rennie & Srebro 2005; DeCoste 2006), and probabilistic methods (Hofmann & Puzicha 1999; Pennock et al. 2000; Popescul et al. 2001; Karypis 2001; Deshpande & Karypis 2004).",yes,MAD 6.00,no,,
AlphaFold 2,Single-sequence protein structure prediction using a language model and deep learning,https://www.biorxiv.org/content/biorxiv/early/2023/01/20/2023.01.18.524637.full.pdf,Background,,"The a-helical CTD conformation was also missed by RoseTTAfold3 and RGN256, an MSA-independent deep learning method that outperforms AlphaFold2 on orphan protein sequences (Extended Data Figure 3).",no,,no,,
NASv3 (CIFAR-10),Neural Architecture Search with Reinforcement Learning,http://arxiv.org/pdf/2301.01299,Background,,"On the other hand, NAS techniques [181] have been widely used to search a crafted architecture based on the data in each device, thus leading to the model heterogeneity situation.",no,,no,,
Bayesian automated hyperparameter tuning,Practical Bayesian Optimization of Machine Learning Algorithms,https://www.mdpi.com/2075-163X/12/12/1621/pdf?version=1671195392,Background,,"Contemporary HPO algorithms can be mainly divided into grid-based search (grid), Bayesian optimization (Bayesian), gradient-based optimization (gradient-based), and population-based optimization (evolutionary algorithm, genetic algorithm, etc.), among which the grid search and Bayes-based optimization are the most Minerals 2022, 12, 1621 3 of 25 popular [104–110]",yes,random search-GBDT,no,,
W2v-BERT,w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,https://arxiv.org/pdf/2303.01664,Uses,,"For the input feature, instead of a log-mel spectrogram used in conventional methods [1,4], we use a speech representation extracted from w2v-BERT [20], an SSL model trained on degraded speech samples.",yes,Miipher,yes,w2v-BERT,
DINOv2,DINOv2: Learning Robust Visual Features without Supervision,https://arxiv.org/pdf/2302.14051,Background,,"Concurrent work (Oquab et al., 2023) attempts to address this by adding a “onetime” automatic data curation step that keeps only the most relevant images from a static web crawl dataset",no,,no,,
NTM,Neural Turing Machines,https://arxiv.org/pdf/2211.06441,Background,,"Environment forcing also enables a neural network to access long-range, persistent memory in a
very different way than previous approaches, such as LSTM (Hochreiter & Schmidhuber, 1997),
Neural Turing Machines (Graves et al., 2014), or Memory Networks (Weston et al., 2014). These
methods can be thought of as improving long-range memory by alleviating difficulties associated
with back-propagating through many time-steps",no,,no,,
M6-T,M6-T: Exploring Sparse Expert Models and Beyond Anonymous ACL submission,https://aclanthology.org/2023.findings-eacl.180.pdf,Background,,"Following this, M6- T (Yang et al., 2021) splits experts into k prototypes (i.e., groups of experts).",no,,no,,
DeepNet,"DeepNet: Scaling Transformers to 1, 000 Layers",http://arxiv.org/pdf/2306.02701,Background,,"For example, ResNet101[8] has 101 layers, Swin-L[9] has 120 layers and DeepNet even has 1000 layers [10].",no,,no,,
Cross-Lingual POS Tagger,Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections,https://www.aclweb.org/anthology/E17-1084.pdf,Background,,"Crosslingual word embeddings can also be used in transfer learning, where the source model is trained on one language and applied directly to another language; this is suitable for the low-resource scenario (Yarowsky and Ngai, 2001; Duong et al., 2015b; Das and Petrov, 2011; Tackstr ¨ om et al., ¨ 2012).",yes,multilingual joint training model with explicit mapping,no,,
Universal approximation via Feedforward Networks,Multilayer feedforward networks are universal approximators,http://arxiv.org/pdf/2306.06913,Background,,"Relying on the degree characteristics and the universal approximation theorem of the MLP [51], The semantics of aggregation weights of both GCN and GraphSAGE can be approximated by GT, which
shows the aggregation of GCN and GraphSAGE are special
forms of the outer head of GT layer.",yes,NRL-GT,no,,
R-FCN,R-FCN: Object Detection via Region-based Fully Convolutional Networks,https://www.mdpi.com/2072-4292/14/4/950/pdf?version=1645156165,Background,,The convolutional neural network (CNN) model is the most widely used deep learning model [13–22]. CNN does not need to use artificially designed features and can learn and extract effective features of the image using massive images and annotations,yes,(CNN-AOOF),no,,
RNN-SpeedUp,Extensions of recurrent neural network language model,https://arxiv.org/pdf/1707.04242.pdf,,,,,,,,ccouldn't find 
Learning deep architectures,Learning Deep Architectures for AI,https://dergipark.org.tr/en/download/article-file/3271193,,,,,,,,
wave2vec 2.0 LARGE,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,https://arxiv.org/pdf/2306.10813,,,,,,,,
Chinchilla,Training Compute-Optimal Large Language Models,http://arxiv.org/pdf/2210.04909,Background,,"Anticipating this scaling trend to remain in vogue for the foreseeable future [6–8], it is imperative that we understand how to scale up models intelligently.",Yes,,Yes,,
DMN,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09541112.pdf,,,,,,,,no access
Naive Bayes,Pattern classification and scene analysis,https://strathprints.strath.ac.uk/76660/1/Venerandi_Fusco_ICCSA_2020_Describing_the_residential_valorisation_of_urban_space_at_the_street_level.pdf,Background,Uses,"Such variable is linked to each individual variable through oriented arcs, in what is called a naive Bayes classifier [16].",Yes,,Yes,Bayesian clustering,
MetaLM,Language Models are General-Purpose Interfaces,http://arxiv.org/pdf/2302.07842,Background,,"As recently demonstrated by Hao et al. (2022) and Alayrac et al. (2022), LMs can also be used as a general-purpose interface with models pre-trained on different modalities.",Yes,Augmented Language Models,Yes,Augmented Language Models,
,,,,,"For example, Hao et al. (2022) take a number of pre-trained encoders that can process diverse modalities such as vision and language, and connect them to a LM that serves as a universal task layer.",,,,,
BART-large,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",https://arxiv.org/pdf/2310.01558,Background,,"A dominant approach for combating this issue has been Retrieval Augmented Language Models (RALMs), which incorporate a retrieval mechanism to reduce the need for storing information in the LLM parameters (Guu et al., 2020; Lewis et al., 2020b; Izacard et al., 2022; Rubin & Berant, 2023).",Yes,fine tuned,Yes,RALM,
Transformer - LibriVox + Decoding/Rescoring,End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures,https://arxiv.org/pdf/2110.13900,,,,,,,,cited in a table (not sure how to quote the citation sentence)
Word Representations,Word Representations: A Simple and General Method for Semi-Supervised Learning,http://fulir.irb.hr/4124/7/Document-based%20topic%20coherence%20measures%20for%20news%20media%20text%20%5Bpostprint%20version%5D.pdf,Uses,,"This method of aggregation relies on pre-constructed word embeddings (Turian et al., 2010): low-dimensional, continuous-valued vectorial representations of words’ meanings derived from word co-occurrences in a large text corpus.",Yes,,Yes,Topic model,
InstructGPT,Training language models to follow instructions with human feedback,https://aclanthology.org/2023.findings-emnlp.983.pdf,Background,,"For nontoxic text generation, model retraining with auxiliary information is used, including control codes as in - CTRL (Keskar et al., 2019) or reinforcement learning from human feedback - InstructGPT (Ouyang et al., 2022).",Yes,Gated Toxicity Avoidance (GTA),Yes,Controllable Text Generation (CTG),
Base LM + kNN LM + Continuous Cache,Generalization through Memorization: Nearest Neighbor Language Models,https://arxiv.org/pdf/2308.04215,Background,,"Various methods have been proposed to integrate the retrieved data into the language model, including the use of prompts (Lewis et al., 2020; Guu et al., 2020; Shi et al., 2023), crossattention modules (Borgeaud et al., 2021), vector concatenation (Izacard and Grave, 2021; Fan et al., 2021), and output distribution adjustment at decoding (Khandelwal et al., 2020; Liu et al., 2022).",Yes,Hybrid RetrievalAugmented Generation (HybridRAG),Yes,Hybrid RetrievalAugmented Generation (HybridRAG),
EfficientZero,Mastering Atari Games with Limited Data,https://intellrobot.com/article/download/5115,Background,,"For sequential decision making problems, model-based planning is a powerful approach to improve sample efficiency and has achieved great success in applied domains such as game playing[79–81] and continuous control[82,83]",No,,No,,
Residual Dense Network,Residual Dense Network for Image Super-Resolution,https://www.mdpi.com/2079-9292/11/22/3809/pdf?version=1669116005,Background,Motivation,They are mainly PSNR-oriented and GAN-driven methods. PSNR-oriented methods [1–6] are trained with the MSE or L1 as loss functions and achieve excellent PSNR,Yes,epistemic-uncertainty-based divide-and-conquer network (EU-DC),Yes,epistemic-uncertainty-based divide-and-conquer network (EU-DC),
LRSO-GAN,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro,https://arxiv.org/pdf/2004.08787,Uses,,The experiments were conducted over two public datasets Market1501 [58] and DukeMTMC-ReID [36] [59] by using the evaluation metrics Cumulative Matching Characteristic (CMC) curve and mean average precision (mAP).,Yes,augmented discriminative clustering (AD-Cluster),Yes,"ResNet-50, ImageNet",
GoogLeNet / InceptionV1,Going deeper with convolutions,https://www.frontiersin.org/articles/10.3389/fpls.2023.1328952/pdf?isPublishedV2=False,Uses,Differences,"These experiments involved benchmarking the proposed approach against several state-of-the-art methods, namely AlexNet Krizhevsky et al. (2012), GoogleNet Szegedy et al. (2014), VGG Abas et al. (2018), ResNet101 Zhang (2021), EfficientNetB3 Singh et al. (2022), Inception V3 Jenipher and Radhika (2022), MobileNet V2140 Elfatimi et al. (2022), and vision transformer Dosovitskiy et al. (2020).",Yes,,Yes,GoogLeNet,An interesting paper that classifys leaf diseases in ligneous plants by proposing a new model & test against with the source paper
GenSLM,GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics,https://www.medrxiv.org/content/medrxiv/early/2022/11/15/2022.11.14.22282297.full.pdf,,,,,,,,failed to load paper
Diabetic Retinopathy Detection Net,Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.,https://www.mdpi.com/2079-9721/11/3/97/pdf?version=1689308882,Background,,"In general, for the diagnosis and treatment of diabetes, the clinical application of AI can be divided into four main sectors: (i) retinal screening; (ii) support for clinical diagnosis; (iii) management tools; and (iv) risk evaluation [31].",No,,No,commentary that briefly describes the evolution of AI and its applications in healthcare (focus on diabetes - related to the source paper),
,,,Background,,"The second group includes systems that send data collected from continuous glucose monitoring to a cloud server and utilizes AI to remotely decide and suggest the appropriate adjustment for insulin dose; the physicians can then evaluate the suggestion and, if necessary, alert patients [31].",,,,,
,,,Background,,"For example, in the Guardian Connect System, AI predicts hypoglycemia 1 h in advance on the basis of data received from continuous glucose monitoring and warning the patient who can take, for example, glucose tablets in time [31].",,,,,
,,,Background,,"Finally, the last group involves ML technologies that are able to identify subjects at high risk of developing diabetes, even if ML does not currently outperform the conventional statistical models already employed to calculate the risk of diabetes onset, so that more studies are needed in this regard [31].",,,,,
Photo-Geometric Autoencoder,Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild,https://www.mdpi.com/2079-9292/12/2/350/pdf?version=1673329193,Background,,"In recent years, the use of such architectures has demonstrated great success in both supervised and unsupervised learning for depth estimation problems [12,13].",Yes,HeightNet,Yes,"HeightNet, LapDepth",
GPU DBNs,Large-scale deep unsupervised learning using graphics processors,http://journals.pan.pl/Content/125559/PDF/BPASTS_2022_70_6_2977.pdf,,,,,,,,
BatchNorm,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://www.nature.com/articles/s41598-024-52802-0.pdf,,,,,,,,
Chinese - English translation,Achieving Human Parity on Automatic Chinese to English News Translation,https://arxiv.org/pdf/2309.15701,,,,,,,,
Greedy layer-wise DNN training,Greedy Layer-Wise Training of Deep Networks,https://www.frontiersin.org/articles/10.3389/fcvm.2022.772222/pdf,,,,,,,,
ShuffleNet v2,ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design,https://www.frontiersin.org/articles/10.3389/fpls.2023.1128399/pdf,,,,,,,,
GPT-4,GPT-4 Technical Report,https://www.biorxiv.org/content/biorxiv/early/2023/12/01/2023.11.29.569320.full.pdf,,,,,,,,
ViT-G (model soup),Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,https://arxiv.org/pdf/2309.08610,,,,,,,,
MobileNet,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,https://www.mdpi.com/1099-4300/25/11/1514/pdf?version=1699090952,,,,,,,,
XLNet,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://arxiv.org/pdf/2307.02269,,,,,,,,
Motion-Driven 3D Feature Tracking,A Combined Corner and Edge Detector,https://arxiv.org/pdf/2103.16792,,,,,,,,
DeepLab (2017),"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/hve2.12403,,,,,,,,
mT5-XXL,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,http://arxiv.org/pdf/2203.10753,,,,,,,,
CodeT5-base,CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation,http://arxiv.org/pdf/2305.11626,,,,,,,,
CodeT5-base,CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation,http://arxiv.org/pdf/2305.04207,,,,,,,,
Gradient Boosting Machine,Greedy function approximation: A gradient boosting machine.,https://www.medrxiv.org/content/medrxiv/early/2024/01/16/2024.01.14.24301299.full.pdf,,,,,,,,
Goat-7B,Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks,https://arxiv.org/pdf/2310.07488,,,,,,,,
NTM,Neural Turing Machines,https://arxiv.org/pdf/1911.03977,,,,,,,,
DeepSpeech2 (English),Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin,http://arxiv.org/pdf/2212.06828,,,,,,,,
Convolutional Pose Machines,Convolutional Pose Machines,https://arxiv.org/pdf/2208.11251,,,,,,,,
ProteinBERT,ProteinBERT: a universal deep-learning model of protein sequence and function,https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-023-00688-x,,,,,,,,
Cross-Lingual POS Tagger,Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections,https://direct.mit.edu/coli/article-pdf/45/3/559/1847397/coli_a_00357.pdf,,,,,,,,
GloVe (6B),GloVe: Global Vectors for Word Representation,https://arxiv.org/pdf/2308.12648,,,,,,,,
Stable Diffusion (LDM-KL-8-G),High-Resolution Image Synthesis with Latent Diffusion Models,https://www.nature.com/articles/s41597-024-02918-9.pdf,,,,,,,,
AlphaFold,Improved protein structure prediction using potentials from deep learning,http://www.cell.com/article/S2405844023048582/pdf,,,,,,,,
Chinchilla,Training Compute-Optimal Large Language Models,https://arxiv.org/pdf/2305.15011,,,,,,,,
Neocognitron,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,https://link.springer.com/content/pdf/10.1007/s11042-022-12153-2.pdf,,,,,,,,
RBM-tuning,A Practical Guide to Training Restricted Boltzmann Machines,https://ieeexplore.ieee.org/ielx7/9424/9264783/09110709.pdf,,,,,,,,
MusicLM,MusicLM: Generating Music From Text,https://arxiv.org/pdf/2307.11078,,,,,,,,
StarGAN v2,StarGAN v2: Diverse Image Synthesis for Multiple Domains,https://arxiv.org/pdf/2302.05496,,,,,,,,
ALBERT-xxlarge,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,http://arxiv.org/pdf/2305.16444,,,,,,,,
CRF-RNN,Conditional Random Fields as Recurrent Neural Networks,https://arxiv.org/pdf/2204.07243,,,,,,,,
Time-delay neural networks,Phoneme recognition using time-delay neural networks,http://arxiv.org/pdf/2106.03210,,,,,,,,
DLRM-2020,Deep Learning Recommendation Model for Personalization and Recommendation Systems,https://arxiv.org/pdf/2001.02772,,,,,,,,
CNN Best Practices,Best practices for convolutional neural networks applied to visual document analysis,https://www.ijcai.org/proceedings/2019/0636.pdf,,,,,,,,
BellKor 2007,The BellKor solution to the Netflix Prize,https://europepmc.org/articles/pmc4845762?pdf=render,,,,,,,,
Youtube recommendation model,Deep Neural Networks for YouTube Recommendations,https://infodemiology.jmir.org/2023/1/e42218/PDF,,,,,,,,
Boss (DARPA Urban Challenge),Autonomous driving in urban environments: Boss and the Urban Challenge,https://e-space.mmu.ac.uk/631072/1/Lidar%20point%20Cloud%20Compression.pdf,,,,,,,,
Phenaki,Phenaki: Variable Length Video Generation From Open Domain Textual Description,https://arxiv.org/pdf/2212.08045,Background,,"These broadly use discriminative tasks to learn representations for various downstream modalities; generative approaches to multimodal modelling have been scaled to billions of parameters, generating text [2, 10, 74, 82], images [58, 62, 83], videos [27, 72] or audio [5] from various modalities.",yes,CLIPPO,no,,
LaMDA,LaMDA: Language Models for Dialog Applications,https://arxiv.org/pdf/2306.08107,Background,,"Large Language Models (LLMs) (Zhao et al., 2023a) are currently on everybody’s lips due to the recent series of rapid breakthroughs achieved, such as self-attention (Vaswani et al., 2017), BERT (Devlin et al., 2019), several versions of GPT (Radford et al., 2018; 2019; Brown et al., 2020; OpenAI, 2022; 2023), LaMDA (Thoppilan et al., 2022), LLaMA (Touvron et al., 2023), or OpenAssistant (Köpf et al., 2023).",no,,no,,
Inception v3,Rethinking the Inception Architecture for Computer Vision,https://www.frontiersin.org/articles/10.3389/fncom.2023.1268116/pdf?isPublishedV2=False,Background,,"Specifically, we used three vision networks with pre-trained weights that are available in the TensorFlow’s model zoo: InceptionV3 (Szegedy et al., 2016), ResNet50 (He et al., 2016), and EfficientNetB4 (Tan and Le, 2019).",yes,e classical autoencoder framework and create two networks: a “forward network” and an “inverse network”.,no,,
ATLAS,UnifiedQA: Crossing Format Boundaries With a Single QA System,https://www.igi-global.com/ViewTitle.aspx?TitleId=328681&isxn=9781668479155,Background,,"In the relation extraction task, the verbalizer mainly aims to map the generated sequences to specific relation classifications, and some scholars have conducted answer engineering studies in classification tasks (Yin et al., 2019), relation extraction (Chen et al., 2022), named entity recognition (Cui et al., 2021), generation tasks (Alec Radford et al., 2019), and multiple choices tasks (Khashabi et al., 2020).",no,,no,,
GPU DBNs,Large-scale deep unsupervised learning using graphics processors,https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing/volume-11/issue-4/042609/Comprehensive-survey-of-deep-learning-in-remote-sensing--theories/10.1117/1.JRS.11.042609.pdf,Background,,"For example, Raina et al.67 put forth central processing unit (CPU) and GPU ideas to accelerate DBNs and sparse coding",no,,no,,
Naive Bayes,Pattern classification and scene analysis,https://www.mdpi.com/1424-8220/23/24/9811/pdf?version=1702535879,Uses,,Fourteen classifiers were tested: • K nearest neighbor methods with the selected metrics; • Naive Bayes classifier (NB) [33–35];,yes,MCA-8 device in the Styrofoam,yes,NB classifier. ,
OpenAI Five,Dota 2 with Large Scale Deep Reinforcement Learning,http://arxiv.org/pdf/2306.00975,Background,,"Although Reinforcement Learning (RL) has demonstrated success across challenging tasks and games in both simulated and real environments [2, 9, 11, 89, 97], the observation spaces for visual RL tasks are typically predefined to offer the most advantageous views based on prior knowledge and can not be actively adjusted by the agent itself.",yes,SUGARL,no,,
NPLM,A Neural Probabilistic Language Model,https://arxiv.org/pdf/2306.01941,Background,,"An LLM, like any language model, predicts the conditional probability of a token—which might be a character, word, or other string—given its preceding context and, in the case of bidirectional models, its surrounding context [17, 156].",no,,no,,
LeNet-5,Gradient-based learning applied to document recognition,https://plantmethods.biomedcentral.com/counter/pdf/10.1186/s13007-023-01119-6,Background,,"A typical Transformer is comprised of Attention modules focusing on vital information from global to local, and it often showed signifcant performance improvement when trained on large datasets [25].",yes,ESMAraPPI,no,,
NMT Transformer 437M,Massively Multilingual Neural Machine Translation,https://aclanthology.org/2021.eacl-main.30.pdf,Background,,"This is analogous to the techniques used for silver data pre-training (Konstas et al., 2017; van Noord and Bos, 2017) in AMR parsing and multi-lingual pre-training (Aharoni et al., 2019) in machine translation.",yes,"transformer-based
multilingual word embeddings for annotation projection of AMR annotations.",no,,
LSTM + dynamic eval,Dynamic Evaluation of Neural Sequence Models,https://aclanthology.org/2021.codi-main.14.pdf,Background,,"Running oracle self-training makes it similar to the dynamic evaluation approach introduced in language modeling (Mikolov, 2012; Graves, 2013; Krause et al., 145 2018), where input text to the language model is the target used to train the neural language model during evaluation.",yes,foreign-text-to-English AMR alignment,no,,
LSTM,Long Short-Term Memory,https://ijere.iaescore.com/index.php/IJERE/article/download/26024/13749,Background,,Long short-term memory (LSTM) is one of the powerful classification methods in deep learning. LSTM architecture has been developed as a solution to the problem of vanishing and exploding gradients encountered in recurring neural networks (RNNs) [24].,yes,"We integrated the RF model to
predict the sentiment analysis, and the LSTM model to predict the epistemic",no,,
OpenAI Five Rerun,Dota 2 with Large Scale Deep Reinforcement Learning,http://arxiv.org/pdf/2301.04299,Background,,"Deep Reinforcement Learning (DRL) has made significant progress over the past decade, from its start playing Atari games [1], to beating humans in board [2] and video games [3, 4], to now addressing important complex safetycritical challenges such as defending computer systems [5], managing power networks [6] and driving vehicles [7].",no,,no,,
T5-11B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/5E5A7C1272C234F77ECB4A354E6FB253/S1351324923000487a.pdf/div-class-title-data-to-text-generation-using-conditional-generative-adversarial-with-enhanced-transformer-div.pdf,Background,,"One of the most common deep learning methods is to use the maximum likelihood estimation (MLE) loss function.
D2T models trained with this method, which are generally based on RNN networks (Wen et al.
2015a, b, c, 2016; Dusek and Jurcicek 2016; Mei, Bansal, and Walter 2016; Nayak et al. 2017; Riou
et al. 2017; Gehrmann, Dai, and Elder 2018; Juraska et al. 2018; Liu et al. 2018; Oraby et al. 2018;
Sha et al. 2018) or transformers (Gehrmann et al. 2018; Gong 2018; Radford et al. 2019; Kasner
and Dušek 2020a; Peng et al. 2020; Harkous, Groves, and Saffari 2020; Chen et al. 2020; Kale and
Rastogi 2020a; Raffel et al. 2020; Chang et al. 2021; Lee 2021), auto-regressively generate each output token based on the sequence of previously generated tokens.",yes,"enhanced version of the vanilla transformer encoder
and decoder",no,,
GloVe (6B),GloVe: Global Vectors for Word Representation,https://arxiv.org/pdf/2310.07290,Background,,"ations. These models, such as Word2Vec [32], FastText [8], GloVe [44], and BERT [16], capture the semantic and syntactic meaning of words, sentences, or entire documents by mapping them to dense vecto",yes,G-CatA,yes,GloVe,
,,,Uses,,"Out of the word embedding models utilized by the authors, we specifically chose GloVe [44], as they demonstrated that it yielded the most favorable outcomes",yes,G-CatA,yes,GloVe,
Sparse coding model for V1 receptive fields,Sparse coding with an overcomplete basis set: A strategy employed by V1?,https://www.intechopen.com/citation-pdf-url/69022,Background,,"A significantly different approach to the sparse modelling, originally introduced by Olshausen and Field [1], consists of learning a dictionary from some training data.",no,,no,,
,,,Background,,"Olshausen and Field [1] proposed a significantly different approach for designing the dictionary using the training data, benefiting from modelling the receptive fields of simple cells in the mammalian primary visual cortex",no,,no,,
Time-delay neural networks,Phoneme recognition using time-delay neural networks,http://arxiv.org/pdf/2108.05679,Background,,"An encoder (or a frame processor) implemented with multiple layers of time-delay neural network (TDNN) [19], [20]. In [21], it’s shown that a TDNN could be implemented as a 1D-CNN with dilation. In [4], [22], [23], 2D-convolution has shown to be effective as well",yes,xi-vector,no,,
Walking Minotaur robot,Learning to Walk via Deep Reinforcement Learning,http://arxiv.org/pdf/2209.08381,Background,,"RL algorithms have seen impressive successes recently in a number of application such as playing games [14], [15] and robotics [16]–[19].",yes,"an algorithm for autonomous ship landing for VTOL capable UAVs in the presence of adversarial environmental
conditions such as wind gusts",no,,
ESRGAN,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,http://arxiv.org/pdf/2304.09414,Background,,Given the success of residual neural networks in the area of image reconstruction [56]–[58] we use one of such architectures as the backbone for our method,yes,no name,no,,
YOLOv2,"YOLO9000: Better, Faster, Stronger",https://arxiv.org/pdf/2308.11894,Background,,"The former, such as YOLO [29, 45, 46], usually has higher detection speed, while the latter, such as Faster R-CNN [47], usually has higher detection accuracy.",yes,SysAdv,no,,
DLRM-2020,Deep Learning Recommendation Model for Personalization and Recommendation Systems,https://link.springer.com/content/pdf/10.1007/s44196-022-00179-1.pdf,Background,,These standard scores will further be put into the whole ranking deep neural network to get the overall score ranking. Maxim et al. [28] designed a new parallel scheme that enabled efcient computation of fully connected layers in learning-based recommendation model.,yes,DRR-Max,no,,
Big Transfer (BiT-L),Large Scale Learning of General Visual Representations for Transfer,https://sciencescholar.us/journal/index.php/ijhs/article/download/9153/4985,Background,,,no,,,,couldnt find
DeepLabV3+,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,https://www.mdpi.com/2313-7673/8/4/356/pdf?version=1691641647,Background,,"Serveral key techniques have emerged, such as enlarging the receptive field with dilated or atrous convolution [18–20], refining the contextual information by multi-scale feature fusion [21–24], etc",yes,SCGNet-L,no,,
Cross-lingual alignment,"Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing",https://ojs.aaai.org/index.php/AAAI/article/download/17588/17395,Background,,"Recently, numerous cross-lingual adaptation methods have been applied to this data-scarcity scenario, where zero or very few target language training samples are utilized (Wisniewski et al. 2014; Schuster et al. 2019b; Artetxe and Schwenk 2019; Liu et al. 2019; Chen et al. 2019)",no,,no,,
"Mogrifier (d2, MoS2, MC) + dynamic eval",Remaining useful life prediction of lithium battery based on ACNN-Mogrifier LSTM-MMD,https://www.mdpi.com/1996-1944/16/22/7186/pdf?version=1700119067,Background,,"However, the type of life reported in these studies is the working life. Similar research on the fatigue life has rarely been reported [25,26].",no,,no,,
"MSRA (C, PReLU)",Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,https://arxiv.org/pdf/2310.01942,Background,,"Modern deep learning architectures have demonstrated great generalization performance, surpassing human baselines on different tasks [6, 19, 57].",yes,a new OODaware training regime tailored for representations trained with SupCo,no,,
Image Classification with the Fisher Vector: Theory and Practice,"Author manuscript, published in ""International Journal of Computer Vision (2013)"" International Journal of Computer Vision manuscript No. (will be inserted by the editor) Image Classification with the Fisher Vector: Theory and Practice",https://www.ijcai.org/proceedings/2017/0256.pdf,Background,,"It cannot work for those situations where we have limited resources which cannot meet the requirement, such as doing image labeling [Sanchez ´ et al., 2013] on smart phones",no,,no,,
MegaSyn,Dual use of artificial-intelligence-powered drug discovery,http://www.sciencepolicyjournal.org/uploads/5/4/3/4/5434385/cherney_etal_jspg_22-3.pdf,Background,,"In fact, a pharmaceuticals company found that toggling their drug discovery AI program to predict the most toxic compounds to humans could be accomplished in just six short hours (Urbina 2022).",no,,no,,
AbLang,AbLang: an antibody language model for completing antibody sequences,https://www.biorxiv.org/content/biorxiv/early/2023/01/31/2023.01.29.525793.full.pdf,Uses,,"Encouraged by the success of PLMs in protein representation learning, series work seeks to learn antibody representations based on sequences of antibodies. (Leem et al., 2021; Ruffolo et al., 2021; Olsen et al., 2022b; Prihoda et al., 2022; Li et al., 2022). AntiBERTy (Ruffolo et al., 2021) proposed the first antibody-specific language model, exploring a Transformer trained on 558M natural antibody sequences in the OAS database.",no,,yes,AbLang H,
BellKor 2009,The BellKor Solution to the Netflix Grand Prize,https://dl.acm.org/doi/pdf/10.1145/3383313.3412216,Background,,"Since temporal information hold contextual information, they can be very crucial for recommendation performance. TimeSVD++ and BPTF [12, 27] adopted time factor into the matrix factorization
method, and TimeSVD++ was one of the main contributions for
the winning of Netflix Grand Prize [11]",yes,MEANTIME,no,,
NLLB,No Language Left Behind: Scaling Human-Centered Machine Translation,https://arxiv.org/pdf/2307.15286,Uses,,"Inspired by one work [34], we use a paraphraser via multilingual Neural Machine Translation (NMT) system (NLLB) based on encoder-decoder framework[4], enabling high-quality zero-shot translations in 200 languages.",yes," new decoding
method that focuses on the lexical variations of the complex word.",yes,NLLB,
DeLight,DeLighT: Deep and Light-weight Transformer,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09984650.pdf,,,,,,,,not found
EfficientDet,EfficientDet: Scalable and Efficient Object Detection,https://www.mdpi.com/1424-8220/24/2/693/pdf?version=1705915610,,,,,,,,
Error Propagation,Learning internal representations by error propagation,https://www.mdpi.com/1424-8220/22/11/4058/pdf?version=1653640012,,,,,,,,
TSN,Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,http://arxiv.org/pdf/2207.03708,,,,,,,,
Whisper,Robust Speech Recognition via Large-Scale Weak Supervision,http://arxiv.org/pdf/2306.14310,,,,,,,,
LSTM + dynamic eval,Dynamic Evaluation of Neural Sequence Models,https://ojs.aaai.org/index.php/AAAI/article/download/4440/4318,,,,,,,,
Image Classification with the Fisher Vector: Theory and Practice,"Author manuscript, published in ""International Journal of Computer Vision (2013)"" International Journal of Computer Vision manuscript No. (will be inserted by the editor) Image Classification with the Fisher Vector: Theory and Practice",https://cea.hal.science/cea-01843176/file/Tran_Aggregating_Image_and_CVPR_2016_paper.pdf,,,,,,,,
VALL-E,Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers,https://arxiv.org/pdf/2308.16692,,,,,,,,
EfficientZero,Mastering Atari Games with Limited Data,https://link.springer.com/content/pdf/10.1007/s11467-023-1325-z.pdf,,,,,,,,
ERNIE 3.0,ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,https://www.nature.com/articles/s42256-023-00647-z.pdf,,,,,,,,
Youtube recommendation model,Deep Neural Networks for YouTube Recommendations,https://drpress.org/ojs/index.php/HSET/article/download/6752/6545,,,,,,,,
LSTM with forget gates,Learning to Forget: Continual Prediction with LSTM,https://www.nature.com/articles/s41598-023-35963-2.pdf,,,,,,,,
PSPNet,Pyramid Scene Parsing Network,http://arxiv.org/pdf/2306.12941,,,,,,,,
Image-to-image cGAN,Image-to-Image Translation with Conditional Adversarial Networks,https://www.nature.com/articles/s41598-023-41484-9.pdf,,,,,,,,
GPT-4,GPT-4 Technical Report,https://arxiv.org/pdf/2311.07592,,,,,,,,
mT0-13B,Crosslingual Generalization through Multitask Finetuning,http://arxiv.org/pdf/2306.10241,,,,,,,,
SqueezeNet,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size,https://www.mdpi.com/1424-8220/22/22/8870/pdf?version=1668603532,,,,,,,,
Conformer,Conformer: Convolution-augmented Transformer for Speech Recognition,https://arxiv.org/pdf/2310.07345,,,,,,,,
Textual Imager,Zero-Shot Learning Through Cross-Modal Transfer,http://arxiv.org/pdf/2203.15158,,,,,,,,
Transformer ELMo,Dissecting Contextual Word Embeddings: Architecture and Representation,http://arxiv.org/pdf/2301.02458,,,,,,,,
UnifiedQA,UnifiedQA: Crossing Format Boundaries With a Single QA System,http://arxiv.org/pdf/2303.09014,,,,,,,,
IBM-5,The Mathematics of Statistical Machine Translation: Parameter Estimation,https://www.aclweb.org/anthology/N16-1109.pdf,,,,,,,,
Wide & Deep,"STRING v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets",https://www.medrxiv.org/content/medrxiv/early/2023/08/21/2023.08.16.23294135.full.pdf,,,,,,,,
data2vec (speech),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://arxiv.org/pdf/2308.04666,,,,,,,,
GPT-NeoX-20B,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,https://aclanthology.org/2023.sigdial-1.21.pdf,,,,,,,,
MnasNet-A1 + SSDLite,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://arxiv.org/pdf/2201.10355,,,,,,,,
Transformer (Adaptive Input Embeddings),Adaptive Input Representations for Neural Language Modeling,https://arxiv.org/pdf/2111.15127,,,,,,,,
Domain Adaptation,Domain adaptation for object recognition: An unsupervised approach,https://arxiv.org/pdf/1812.05418,,,,,,,,
GPU DBNs,Large-scale deep unsupervised learning using graphics processors,https://eprints.whiterose.ac.uk/166897/8/37-csur2020.pdf,,,,,,,,
Inflated 3D ConvNet,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",https://arxiv.org/pdf/2308.05051,,,,,,,,
Decision tree (classification),Rapid object detection using a boosted cascade of simple features,https://www.mdpi.com/2071-1050/14/19/12274/pdf?version=1665223902,,,,,,,,
NASv3 (CIFAR-10),Neural Architecture Search with Reinforcement Learning,https://arxiv.org/pdf/2302.14838,,,,,,,,
XLM,Cross-lingual Language Model Pretraining,https://arxiv.org/pdf/2203.13339,,,,,,,,
LSTM + dynamic eval,Dynamic Evaluation of Neural Sequence Models,https://arxiv.org/pdf/2309.17167,,,,,,,,
ContextNet,ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context,https://arxiv.org/pdf/2101.06856,,,,,,,,
ERNIE 3.0,ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,https://arxiv.org/pdf/2308.09729,,,,,,,,
DLRM-2020,Deep Learning Recommendation Model for Personalization and Recommendation Systems,https://arxiv.org/pdf/2303.06455,,,,,,,,
Sparse coding model for V1 receptive fields,Sparse coding with an overcomplete basis set: A strategy employed by V1?,https://www.mdpi.com/1099-4300/21/9/862/pdf?version=1567678972,,,,,,,,
DALL·E 3,Improving Image Generation with Better Captions,https://arxiv.org/pdf/2307.14283,,,,,,,,
AltCLIP,AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities,https://arxiv.org/pdf/2308.06262,,,,,,,,
Sparse coding model for V1 receptive fields,Sparse coding with an overcomplete basis set: A strategy employed by V1?,https://www.mdpi.com/1099-4300/22/8/850/pdf?version=1596713752,,,,,,,,
Samuel Neural Checkers,Some Studies in Machine Learning Using the Game of Checkers,https://link.springer.com/content/pdf/10.1007/s11948-020-00259-5.pdf,,,,,,,,
LDM-1.45B,High-Resolution Image Synthesis with Latent Diffusion Models,https://www.biorxiv.org/content/biorxiv/early/2024/02/01/2024.02.01.578352.full.pdf,,,,,,,,
UL2,UL2: Unifying Language Learning Paradigms,http://arxiv.org/pdf/2210.00045,,,,,,,,
mT5-XXL,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,https://arxiv.org/pdf/2210.02643,,,,,,,,
REINFORCE in Stochastic Connectionism,Simple statistical gradient-following algorithms for connectionist reinforcement learning,https://arxiv.org/pdf/2308.00560,,,,,,,,
DOT(S)-RNN,How to Construct Deep Recurrent Neural Networks,https://www.jstage.jst.go.jp/article/transinf/E101.D/1/E101.D_2017EDL8155/_pdf,,,,,,,,
Whisper,Robust Speech Recognition via Large-Scale Weak Supervision,http://arxiv.org/pdf/2306.07967,,,,,,,,
Statistical Shape Constellations,Unsupervised Learning of Models for Recognition,https://arxiv.org/pdf/1505.00687,,,,,,,,
StarCoder,StarCoder: may the source be with you!,https://arxiv.org/pdf/2308.01861,,,,,,,,
Internal functionality of visual invariants,The internal representation of solid shape with respect to vision,http://www.cs.sunysb.edu/~mueller/v1109.pdf,,,,,,,,
ReLU (LFW),Rectified Linear Units Improve Restricted Boltzmann Machines,https://arxiv.org/pdf/2401.11156,,,,,,,,
Tensorized Transformer (257M),A Tensorized Transformer for Language Modeling,https://arxiv.org/pdf/2205.12961,,,,,,,,
Maxout Networks,Maxout Networks,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0254664&type=printable,,,,,,,,
TFE SVM,A trainable feature extractor for handwritten digit recognition,https://pure.ulster.ac.uk/files/11551018/paper_ijprai03.pdf,,,,,,,,
TD-Gammon,Practical issues in temporal difference learning,https://link.springer.com/content/pdf/10.1023/A:1007593124513.pdf,,,,,,,,
GOAT,Open-Ended Learning Leads to Generally Capable Agents,http://arxiv.org/pdf/2306.07372,,,,,,,,
BatchNorm,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://academic.oup.com/bjro/advance-article-pdf/doi/10.1093/bjro/tzad008/54974031/tzad008.pdf,,,,,,,,
RBM-tuning,A Practical Guide to Training Restricted Boltzmann Machines,https://arxiv.org/pdf/1905.06047,,,,,,,,
Neuro-Symbolic Concept Learner,"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",http://www.cell.com/article/S266638992030060X/pdf,,,,,,,,
Cross-lingual alignment,"Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing",https://arxiv.org/pdf/2310.04726,,,,,,,,
BigGAN-deep 512x512,Large Scale GAN Training for High Fidelity Natural Image Synthesis,https://arxiv.org/pdf/2308.02535,,,,,,,,
YOLO,"You Only Look Once: Unified, Real-Time Object Detection",https://www.tandfonline.com/doi/pdf/10.1080/21642583.2023.2300836?needAccess=true,,,,,,,,
Error Propagation,Learning internal representations by error propagation,https://arxiv.org/pdf/2309.06745,,,,,,,,
BigGAN-deep 512x512,Large Scale GAN Training for High Fidelity Natural Image Synthesis,http://arxiv.org/pdf/2303.16050,,,,,,,,
MnasNet-A3,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://arxiv.org/pdf/2206.04040,,,,,,,,
LDA,Latent Dirichlet Allocation,https://arxiv.org/pdf/2309.09658,,,,,,,,
ProxylessNAS,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,https://asp-eurasipjournals.springeropen.com/track/pdf/10.1186/s13634-022-00868-1,,,,,,,,
Hiero,A Hierarchical Phrase-Based Model for Statistical Machine Translation,http://dl.acm.org/ft_gateway.cfm?id=1690228&type=pdf,,,,,,,,
OverFeat,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",https://arxiv.org/pdf/2008.02107,,,,,,,,
SVM for face detection,Training support vector machines: an application to face detection,https://www.mdpi.com/2079-9292/8/7/801/pdf,,,,,,,,
MoCo,Momentum Contrast for Unsupervised Visual Representation Learning,https://cancerimagingjournal.biomedcentral.com/counter/pdf/10.1186/s40644-024-00654-2,,,,,,,,
GloVe (6B),GloVe: Global Vectors for Word Representation,https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1212793/pdf?isPublishedV2=False,,,,,,,,
NLLB,No Language Left Behind: Scaling Human-Centered Machine Translation,http://arxiv.org/pdf/2302.03528,,,,,,,,
Fisher-Boost,Improving the Fisher Kernel for Large-Scale Image Classification,https://arxiv.org/pdf/1705.09894,,,,,,,,
(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),Direct Output Connection for a High-Rank Language Model,https://aclanthology.org/2022.acl-long.554.pdf,,,,,,,,
3D city reconstruction,Building Rome in a day,https://arxiv.org/pdf/1704.02203,,,,,,,,
Deconvolutional Network,Deconvolutional networks,https://arxiv.org/pdf/1704.04133,,,,,,,,
DeepLab (2017),"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",https://iopscience.iop.org/article/10.1088/1742-6596/2644/1/012013/pdf,,,,,,,,
Textual Imager,Zero-Shot Learning Through Cross-Modal Transfer,https://arxiv.org/pdf/1808.01574,,,,,,,,
AltCLIP,AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities,https://arxiv.org/pdf/2309.04344,,,,,,,,
WizardLM-7B,WizardLM: Empowering Large Language Models to Follow Complex Instructions,https://arxiv.org/pdf/2310.05915,,,,,,,,
PDP model for serial order,Serial Order: A Parallel Distributed Processing Approach,https://arxiv.org/pdf/2201.12944,,,,,,,,
CNN Best Practices,Best practices for convolutional neural networks applied to visual document analysis,https://arxiv.org/pdf/2009.01417,,,,,,,,
GLM-130B,GLM-130B: An Open Bilingual Pre-trained Model,https://arxiv.org/pdf/2310.05242,,,,,,,,
LRSO-GAN,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro,https://air.uniud.it/bitstream/11390/1194960/1/ICIP2020_Multi%20Branch%20Siamese%20Network%20for%20Person%20Re-Identification.pdf,,,,,,,,
Minerva (540B),Solving Quantitative Reasoning Problems with Language Models,https://arxiv.org/pdf/2210.16859,,,,,,,,
GLIDE,GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,https://arxiv.org/pdf/2309.00398,,,,,,,,
SENet (ImageNet),Squeeze-and-Excitation Networks,https://dl.acm.org/doi/pdf/10.1145/3581783.3612234,,,,,,,,
ADM,Diffusion Models Beat GANs on Image Synthesis,https://arxiv.org/pdf/2310.08576,,,,,,,,
Megatron-Turing NLG 530B,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",http://arxiv.org/pdf/2204.06745,,,,,,,,
EfficientZero,Mastering Atari Games with Limited Data,https://arxiv.org/pdf/2308.04263,,,,,,,,
Character-enriched word2vec,Enriching Word Vectors with Subword Information,https://www.mdpi.com/2076-3417/13/14/8368/pdf?version=1689820968,,,,,,,,
Support Vector Machines,Support-Vector Networks,https://arxiv.org/pdf/2309.06219,,,,,,,,
VQGAN + CLIP,Taming Transformers for High-Resolution Image Synthesis,https://www.mdpi.com/1424-8220/23/3/1141/pdf?version=1674116892,,,,,,,,
LSTM-Char-Large,Character-Aware Neural Language Models,https://arxiv.org/pdf/2101.10382,,,,,,,,
Perceptron for Large Margin Classification,Large Margin Classification Using the Perceptron Algorithm,https://ojs.aaai.org/index.php/AAAI/article/download/8957/8816,,,,,,,,
Swift,Champion-level drone racing using deep reinforcement learning,https://arxiv.org/pdf/2309.10683,,,,,,,,
Denoising Autoencoders,Extracting and composing robust features with denoising autoencoders,https://www.mdpi.com/2079-9292/12/15/3361/pdf?version=1691314635,,,,,,,,
MADALINE III,"30 years of adaptive neural networks: perceptron, Madaline, and backpropagation",https://www.researchbank.ac.nz/bitstream/10652/2958/1/Security-of-Wireless-Devices-using-Biological-Inspired-RF-Fingerprinting-Technique.pdf,,,,,,,,
Optimized Multi-Scale Edge Detection,A Computational Approach to Edge Detection,https://www.biorxiv.org/content/biorxiv/early/2023/05/12/2023.05.11.540445.full.pdf,,,,,,,,
Learnability theory of language development,Language learnability and language development,http://arxiv.org/pdf/2306.00503,,,,,,,,
SPPNet,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,https://www.mdpi.com/2076-2615/13/18/2924/pdf?version=1694701244,,,,,,,,
Back-propagation,Learning representations by back-propagating errors,https://link.springer.com/content/pdf/10.1007/s10287-023-00491-x.pdf,,,,,,,,
BERT-Large,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://figshare.com/articles/preprint/Complaint_and_Severity_Identification_from_Online_Financial_Content/20087675/1/files/35945852.pdf,,,,,,,,
Random Decision Forests,New technique to alleviate the cold start problem in recommender systems using information from social media and random decision forests,https://www.mdpi.com/1999-4893/15/3/72/pdf?version=1645696086,,,,,,,,
Elastic weight consolidation,Overcoming catastrophic forgetting in neural networks,https://arxiv.org/pdf/2307.08208,,,,,,,,
AlphaFold,Improved protein structure prediction using potentials from deep learning,https://www.nature.com/articles/s41541-023-00795-8.pdf,,,,,,,,
Qwen-VL,"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",https://arxiv.org/pdf/2310.06627,,,,,,,,
TD-Gammon,Practical issues in temporal difference learning,https://ro.ecu.edu.au/cgi/viewcontent.cgi?article=1534&context=ecuworks2011,,,,,,,,
Stacked Denoising Autoencoders,Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion,https://assets.cureus.com/uploads/review_article/pdf/156879/20230617-22094-1ylkpny.pdf,,,,,,,,
GPT,Improving Language Understanding by Generative Pre-Training,https://arxiv.org/pdf/2310.02066,,,,,,,,
PaLM 2,PaLM 2 Technical Report,https://arxiv.org/pdf/2310.05492,,,,,,,,
ConvNet similarity metric,"Learning a similarity metric discriminatively, with application to face verification",http://arxiv.org/pdf/2303.13763,,,,,,,,
Diffractive Deep Neural Network,All-optical machine learning using diffractive deep neural networks,https://arxiv.org/pdf/2105.06296,,,,,,,,
Two-stream ConvNets for action recognition,Two-Stream Convolutional Networks for Action Recognition in Videos,https://link.springer.com/content/pdf/10.1007/s10514-022-10074-5.pdf,,,,,,,,
Fuzzy NN,"Multilayer perceptron, fuzzy sets, and classification",https://arxiv.org/pdf/2008.02196,,,,,,,,
SVM for face detection,Training support vector machines: an application to face detection,https://upcommons.upc.edu/bitstream/2117/341517/1/Improving_object_detection_in_paintings_based_ontime_contexts.pdf,,,,,,,,
AbLang,AbLang: an antibody language model for completing antibody sequences,https://arxiv.org/pdf/2209.12635,,,,,,,,
R-CNN (T-net),Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,https://www.mdpi.com/1424-8220/24/2/395/pdf?version=1704788520,,,,,,,,
DMN,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,https://arxiv.org/pdf/1603.03873,,,,,,,,
Transformer (Adaptive Input Embeddings),Adaptive Input Representations for Neural Language Modeling,https://www.aclweb.org/anthology/2020.emnlp-main.463.pdf,,,,,,,,
Refined Part Pooling,Beyond Part Models: Person Retrieval with Refined Part Pooling,https://www.mdpi.com/2313-433X/7/4/62/pdf?version=1616732131,,,,,,,,
FAIRSEQ Adaptive Inputs,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling",https://arxiv.org/pdf/2302.03162,,,,,,,,
ALBERT,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,https://arxiv.org/pdf/2303.10868,,,,,,,,
Parti,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,https://arxiv.org/pdf/2302.10893,,,,,,,,
Pointer Sentinel-LSTM (medium),Pointer Sentinel Mixture Models,https://aclanthology.org/2021.acl-long.70.pdf,,,,,,,,
ST-MoE,ST-MoE: Designing Stable and Transferable Sparse Expert Models,https://dl.acm.org/doi/pdf/10.1145/3580305.3599278,,,,,,,,
LaNet-L (CIFAR-10),Sample-Efficient Neural Architecture Search by Learning Action Space,https://arxiv.org/pdf/2110.10729,,,,,,,,
Part-of-sentence tagging model,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,https://www.nature.com/articles/s41597-023-02089-z.pdf,,,,,,,,
Convolutional Pose Machines,Convolutional Pose Machines,http://arxiv.org/pdf/2104.04721,,,,,,,,
Transformer-XL Large,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,https://arxiv.org/pdf/2205.11388,,,,,,,,
W2v-BERT,w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,http://arxiv.org/pdf/2305.11576,,,,,,,,
Discriminator Guidance,Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models,https://arxiv.org/pdf/2306.17046,,,,,,,,
SACHS,Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data,http://www.cell.com/article/S0006349514003282/pdf,,,,,,,,
EDSR,Enhanced Deep Residual Networks for Single Image Super-Resolution,http://arxiv.org/pdf/2212.09988,,,,,,,,
WGAN-GP,Improved Training of Wasserstein GANs,https://arxiv.org/pdf/2305.02164,,,,,,,,
Transformer (Adaptive Input Embeddings),Adaptive Input Representations for Neural Language Modeling,https://aclanthology.org/2022.emnlp-main.804.pdf,,,,,,,,
Deep Belief Nets,A Fast Learning Algorithm for Deep Belief Nets,https://www.mdpi.com/1099-4300/25/11/1477/pdf?version=1698213434,,,,,,,,
AudioGen,AudioGen: Textually Guided Audio Generation,http://arxiv.org/pdf/2304.13731,,,,,,,,
GNMT,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,https://arxiv.org/pdf/2210.15332,,,,,,,,
ESRGAN,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,https://arxiv.org/pdf/2307.07240,,,,,,,,
PSPNet,Pyramid Scene Parsing Network,https://www.ijcai.org/proceedings/2023/0075.pdf,,,,,,,,
Once for All,Once for All: Train One Network and Specialize it for Efficient Deployment,http://arxiv.org/pdf/2006.12986,,,,,,,,
OPT-175B,OPT: Open Pre-trained Transformer Language Models,http://arxiv.org/pdf/2305.19595,,,,,,,,
TensorReasoner,Reasoning With Neural Tensor Networks for Knowledge Base Completion,https://arxiv.org/pdf/2305.10531,,,,,,,,
mT5-XXL,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,http://arxiv.org/pdf/2301.10439,,,,,,,,
Fuzzy NN,"Multilayer perceptron, fuzzy sets, and classification",http://arxiv.org/pdf/2212.14695,,,,,,,,
GSM,Gated Self-Matching Networks for Reading Comprehension and Question Answering,https://dione.lib.unipi.gr/xmlui/bitstream/unipi/12958/1/Transfer%20Learning%20In%20Recommender%20Systems-Thesis.pdf,,,,,,,,
ProteinBERT,ProteinBERT: a universal deep-learning model of protein sequence and function,https://www.biorxiv.org/content/biorxiv/early/2023/04/07/2023.03.04.531015.full.pdf,,,,,,,,
FTW,Human-level performance in 3D multiplayer games with population-based reinforcement learning,https://arxiv.org/pdf/2301.12457,,,,,,,,
Unsupervised Scale-Invariant Learning,Object class recognition by unsupervised scale-invariant learning,https://www.pure.ed.ac.uk/ws/files/17737910/Ferrari_et_al_2010_From_images_to_shape_models.pdf,,,,,,,,
GloVe (32B),GloVe: Global Vectors for Word Representation,https://arxiv.org/pdf/2309.00550,,,,,,,,
GOAT,Open-Ended Learning Leads to Generally Capable Agents,http://arxiv.org/pdf/2211.13051,,,,,,,,
MLP as Bayesian Approximator,The multilayer perceptron as an approximation to a Bayes optimal discriminant function,https://digitalcommons.fiu.edu/cgi/viewcontent.cgi?article=3452&context=etd,,,,,,,,
Motion-Driven 3D Feature Tracking,A Combined Corner and Edge Detector,https://bioresources.cnr.ncsu.edu/wp-content/uploads/2022/07/BioRes_17_3_5532_Wang_ZWLF_Review_Vision_Raw_Material_Wood_Product_20245.pdf,,,,,,,,
Chinese - English translation,Achieving Human Parity on Automatic Chinese to English News Translation,https://www.astesj.com/?sdm_process_download=1&download_id=26112,,,,,,,,
ISS,Learning Intrinsic Sparse Structures within Long Short-term Memory,https://arxiv.org/pdf/2207.03523,,,,,,,,
Convolutional Pose Machines,Convolutional Pose Machines,https://arxiv.org/pdf/2007.10986,,,,,,,,
YouTube Video Recommendation System,The YouTube video recommendation system,https://arxiv.org/pdf/2211.05290,,,,,,,,
WizardLM-7B,WizardLM: Empowering Large Language Models to Follow Complex Instructions,https://arxiv.org/pdf/2310.05492,,,,,,,,
BASIC-L,Combined Scaling for Zero-shot Transfer Learning,http://arxiv.org/pdf/2211.13224,,,,,,,,
DensePhrases,Learning Dense Representations of Phrases at Scale,https://www.mdpi.com/2227-7390/11/7/1624/pdf?version=1681089185,,,,,,,,
R-FCN,R-FCN: Object Detection via Region-based Fully Convolutional Networks,https://www.mdpi.com/1424-8220/22/5/1737/pdf?version=1645612277,,,,,,,,
DiT-XL/2,Scalable Diffusion Models with Transformers,http://arxiv.org/pdf/2305.12347,,,,,,,,
PointNet,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,https://arxiv.org/pdf/2310.06232,,,,,,,,
Inception v3,Rethinking the Inception Architecture for Computer Vision,https://arxiv.org/pdf/2310.04901,,,,,,,,
Word Representations,Word Representations: A Simple and General Method for Semi-Supervised Learning,https://www.aclweb.org/anthology/W17-2503.pdf,,,,,,,,
RNN+LDA+KN5+cache,Context dependent recurrent neural network language model,http://www.iis.sinica.edu.tw/papers/hsu/17226-F.pdf,,,,,,,,
EfficientDet,EfficientDet: Scalable and Efficient Object Detection,https://downloads.hindawi.com/journals/cin/2023/2506274.pdf,,,,,,,,
GRUs,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,https://arxiv.org/pdf/2309.11247,,,,,,,,
GSM,Gated Self-Matching Networks for Reading Comprehension and Question Answering,https://www.ijcai.org/proceedings/2018/0110.pdf,,,,,,,,
AlphaGo Master,Mastering the game of Go without human knowledge,https://www.mdpi.com/2227-7390/12/2/214/pdf?version=1704790794,,,,,,,,
MCDNN (MNIST),Multi-column deep neural networks for image classification,https://jfin-swufe.springeropen.com/track/pdf/10.1186/s40854-020-00187-0,,,,,,,,
ReLU (NORB),Rectified Linear Units Improve Restricted Boltzmann Machines,https://www.mdpi.com/1424-8220/23/11/5341/pdf?version=1685950570,,,,,,,,
Unsupervised High-level Feature Learner,Building high-level features using large scale unsupervised learning,https://arxiv.org/pdf/1906.06298,,,,,,,,
GPT,Improving Language Understanding by Generative Pre-Training,https://pubs.aip.org/aip/aml/article-pdf/doi/10.1063/5.0174863/18931211/010901_1_5.0174863.pdf,,,,,,,,
Swift,Champion-level drone racing using deep reinforcement learning,http://arxiv.org/pdf/2305.01461,,,,,,,,
Hanabi 4 player,The Hanabi Challenge: A New Frontier for AI Research,https://arxiv.org/pdf/2211.10100,,,,,,,,
Culturome,Quantitative Analysis of Culture Using Millions of Digitized Books,https://scholarworks.iu.edu/bitstreams/e63e31d0-7a8c-4ecc-9d3c-4bf218929f89/download,,,,,,,,
CTC-Trained LSTM,Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,https://www.ijfmr.com/papers/2023/5/6305.pdf,,,,,,,,
BASIC-L,Combined Scaling for Zero-shot Transfer Learning,http://arxiv.org/pdf/2205.01397,,,,,,,,
XLM-RoBERTa,Unsupervised Cross-lingual Representation Learning at Scale,http://arxiv.org/pdf/2306.13888,,,,,,,,
MetaLM,Language Models are General-Purpose Interfaces,https://arxiv.org/pdf/2308.01907,,,,,,,,
LRSO-GAN,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro,https://arxiv.org/pdf/2008.04015,,,,,,,,
MLP as Bayesian Approximator,The multilayer perceptron as an approximation to a Bayes optimal discriminant function,http://www.disi.unige.it/person/RovettaS/research/publications/ieee-tnn99-weightsia/ieee-tnn99-weightsia.pdf,,,,,,,,
Visualizing CNNs,Visualizing and Understanding Convolutional Networks,http://arxiv.org/pdf/2303.11932,,,,,,,,
GLaM,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,https://arxiv.org/pdf/2302.07863,,,,,,,,
ENAS,Efficient Neural Architecture Search via Parameter Sharing,https://arxiv.org/pdf/2201.09032,,,,,,,,
ADAM (CIFAR-10),Adam: A Method for Stochastic Optimization,https://www.biorxiv.org/content/biorxiv/early/2023/11/16/2023.11.13.566879.full.pdf,,,,,,,,
Max-Margin Markov Networks,Max-Margin Markov Networks,http://www.cs.cornell.edu/~cnyu/papers/siso_workshop.pdf,,,,,,,,
ASE+ACE,Neuronlike adaptive elements that can solve difficult learning control problems,https://annals-csis.org/proceedings/2020/drp/pdf/141.pdf,,,,,,,,
GPT,Improving Language Understanding by Generative Pre-Training,https://www.mdpi.com/1424-8220/23/20/8425/pdf?version=1697116707,,,,,,,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,https://dl.acm.org/doi/pdf/10.1145/3580305.3599897,,,,,,,,
MatrixFac for Recommenders,Matrix Factorization Techniques for Recommender Systems,https://www.mdpi.com/2079-9292/12/4/809/pdf?version=1675669951,,,,,,,,
OR-WideResNet,Oriented Response Networks,https://arxiv.org/pdf/2204.08613,,,,,,,,
MSA Transformer,MSA Transformer,https://www.nature.com/articles/s42256-023-00721-6.pdf,,,,,,,,
Contriever,Unsupervised Dense Information Retrieval with Contrastive Learning,https://arxiv.org/pdf/2309.17078,,,,,,,,
Pattern recognition and reading by machine,Pattern recognition and reading by machine,https://kar.kent.ac.uk/38137/1/EST%202013%20Real-time%20Doorway%20Detection%20and%20Alignment%20Determination%20for%20Improved%20Trajectory%20Generation%20in%20Assistive%20Mobile%20Robotic%20Wheelchairs..pdf,,,,,,,,
MetaLM,Language Models are General-Purpose Interfaces,http://arxiv.org/pdf/2305.14057,,,,,,,,
NLLB,No Language Left Behind: Scaling Human-Centered Machine Translation,https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00631/2208870/tacl_a_00631.pdf,,,,,,,,
Chinchilla,Training Compute-Optimal Large Language Models,https://arxiv.org/pdf/2309.03241,,,,,,,,
ViT-G/14,Scaling Vision Transformers,https://arxiv.org/pdf/2310.05654,,,,,,,,
"Listen, Attend and Spell","Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",https://arxiv.org/pdf/2104.02775,,,,,,,,
SemVec,Linguistic Regularities in Continuous Space Word Representations,https://arxiv.org/pdf/1910.12740,,,,,,,,
FixRes ResNeXt-101 WSL,Fixing the train-test resolution discrepancy,http://arxiv.org/pdf/2210.06640,,,,,,,,
Hanabi 4 player,The Hanabi Challenge: A New Frontier for AI Research,https://arxiv.org/pdf/2307.09905,,,,,,,,
NTM,Neural Turing Machines,http://arxiv.org/pdf/2303.06841,,,,,,,,
BIDAF,Bidirectional Attention Flow for Machine Comprehension,https://doi.org/10.26615/978-954-452-072-4_029,,,,,,,,
MT-DNN,Multi-Task Deep Neural Networks for Natural Language Understanding,http://arxiv.org/pdf/2304.08109,,,,,,,,
data2vec (vision),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://arxiv.org/pdf/2310.05058,,,,,,,,
PyramidNet,Deep Pyramidal Residual Networks,https://arxiv.org/pdf/1811.04387,,,,,,,,
ALVINN,"ALVINN, an autonomous land vehicle in a neural network",https://www.mdpi.com/2624-8174/4/1/11/pdf?version=1644885409,,,,,,,,
DnCNN,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,http://arxiv.org/pdf/2304.08282,,,,,,,,
Theseus 6/768,BERT-of-Theseus: Compressing BERT by Progressive Module Replacing,https://aclanthology.org/2022.acl-long.11.pdf,,,,,,,,
Mnemonic Reader,Reinforced Mnemonic Reader for Machine Reading Comprehension,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09247161.pdf,,,,,,,,
BellKor 2007,The BellKor solution to the Netflix Prize,https://ojs.aaai.org/index.php/aimagazine/article/download/2361/2231,,,,,,,,
Optimized Single-layer Net,An Analysis of Single-Layer Networks in Unsupervised Feature Learning,http://arxiv.org/pdf/2306.07915,,,,,,,,
Adaptive Input Transformer + RD,R-Drop: Regularized Dropout for Neural Networks,http://arxiv.org/pdf/2203.07376,,,,,,,,
Cutout-regularized net,Improved Regularization of Convolutional Neural Networks with Cutout,https://arxiv.org/pdf/2307.13938,,,,,,,,
ResNeXt-101 Billion-scale,Billion-scale semi-supervised learning for image classification,https://arxiv.org/pdf/2008.01392,,,,,,,,
GPipe (Transformer),GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://dl.acm.org/doi/pdf/10.1145/3587135.3592200,,,,,,,,
DETR,End-to-End Object Detection with Transformers,https://figshare.com/articles/preprint/MCRformer_Morphological_Constraint_Reticular_Transformer_for_3D_Medical_Image_Segmentation/21280674/5/files/41234796.pdf,,,,,,,,
Wide & Deep,"STRING v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets",https://www.biorxiv.org/content/biorxiv/early/2023/08/08/2023.08.08.552342.full.pdf,,,,,,,,
Word2Vec (large),Distributed Representations of Words and Phrases and their Compositionality,https://arxiv.org/pdf/2309.11981,,,,,,,,
VGG19,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://bmcmedimaging.biomedcentral.com/counter/pdf/10.1186/s12880-023-01177-1,,,,,,,,
Incoder-6.7B,InCoder: A Generative Model for Code Infilling and Synthesis,http://arxiv.org/pdf/2306.01220,,,,,,,,
KN5 LM + RNN 400/10 (WSJ),Recurrent neural network based language model,https://www.mdpi.com/1424-8220/23/3/1145/pdf?version=1675218081,,,,,,,,
R-CNN (T-net),Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,https://link.springer.com/content/pdf/10.1007/s11554-023-01344-1.pdf,,,,,,,,
Llama 2-7B,Llama 2: Open Foundation and Fine-Tuned Chat Models,https://www.mdpi.com/2504-2289/7/4/182/pdf?version=1702541658,,,,,,,,
Advantage Learning,Increasing the Action Gap: New Operators for Reinforcement Learning,http://arxiv.org/pdf/2212.14164,,,,,,,,
Dropout (CIFAR),Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/pdf/2304.07501,,,,,,,,
DeepLab (2017),"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",https://www.mdpi.com/2078-2489/14/12/660/pdf?version=1702541369,,,,,,,,
ERNIE 3.0 Titan,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,http://arxiv.org/pdf/2208.04461,,,,,,,,
ASE+ACE,Neuronlike adaptive elements that can solve difficult learning control problems,https://mediatum.ub.tum.de/doc/1624259/document.pdf,,,,,,,,
Spectrally Normalized GAN,Spectral Normalization for Generative Adversarial Networks,http://arxiv.org/pdf/2306.10468,,,,,,,,
HyperNEAT,A Neuroevolution Approach to General Atari Game Playing,https://www.ijcai.org/proceedings/2018/0718.pdf,,,,,,,,
Semi-Supervised Embedding for DL,Deep learning via semi-supervised embedding,https://ieeexplore.ieee.org/ielx7/9607382/9607383/09607428.pdf,,,,,,,,
Word Representations,Word Representations: A Simple and General Method for Semi-Supervised Learning,https://www.aclweb.org/anthology/W17-5224.pdf,,,,,,,,
ADM,Diffusion Models Beat GANs on Image Synthesis,https://arxiv.org/pdf/2310.06228,,,,,,,,
SqueezeNet,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/aisy.202300042,,,,,,,,
VD-RHN,Recurrent Highway Networks,https://www.ijcai.org/proceedings/2018/0130.pdf,,,,,,,,
ShuffleNet v1,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,https://ieeexplore.ieee.org/ielx7/6221020/10063192/10021579.pdf,,,,,,,,
Dropout (MNIST),Improving neural networks by preventing co-adaptation of feature detectors,http://arxiv.org/pdf/2203.10006,,,,,,,,
GRUs,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,https://www.mdpi.com/2076-3417/13/18/10003/pdf?version=1693967516,,,,,,,,
AlphaFold 2,Single-sequence protein structure prediction using a language model and deep learning,https://www.nature.com/articles/s41467-023-38063-x.pdf,,,,,,,,
SemExp,Object Goal Navigation using Goal-Oriented Semantic Exploration,http://arxiv.org/pdf/2207.10761,,,,,,,,
TSN,Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,https://dl.acm.org/doi/pdf/10.1145/3595916.3626411,,,,,,,,
Megatron-BERT,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,http://arxiv.org/pdf/2303.10845,,,,,,,,
Recursive sentiment autoencoder,Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions,http://manuscript.elsevier.com/S0306457317306581/pdf/S0306457317306581.pdf,,,,,,,,
GAN-Advancer,Improved Techniques for Training GANs,https://arxiv.org/pdf/2306.12511,,,,,,,,
TD(0),An Adaptive Optimal Controller for Discrete-Time Markov Environments,https://ane.pl/index.php/ane/article/download/1862/1859,,,,,,,,
LeNet-5,Gradient-based learning applied to document recognition,https://www.mdpi.com/2076-3417/14/3/1257/pdf?version=1706884408,,,,,,,,
SimpleNet,"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",http://arxiv.org/pdf/2203.13453,,,,,,,,
AlphaFold-Multimer,Protein complex prediction with AlphaFold-Multimer,https://hal.science/hal-04130165/file/Perspective%20Structure_AI%20%283%29.pdf,,,,,,,,
DeBERTaV3-large + KEAR,Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention,https://aclanthology.org/2022.emnlp-main.79.pdf,,,,,,,,
Codex,Evaluating Large Language Models Trained on Code,http://arxiv.org/pdf/2306.00597,,,,,,,,
GPT-4,GPT-4 Technical Report,https://arxiv.org/pdf/2310.11597,,,,,,,,
GPT-4,GPT-4 Technical Report,https://arxiv.org/pdf/2310.07838,,,,,,,,
StarCoder,StarCoder: may the source be with you!,http://arxiv.org/pdf/2305.17145,,,,,,,,
NetTalk (dictionary),Parallel Networks that Learn to Pronounce English Text,https://drum.lib.umd.edu/bitstreams/80a432de-24c7-4dce-ad29-868b09634af0/download,,,,,,,,
NASNet-A,Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/pdf/2209.01688,,,,,,,,
ConSERT,ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer,https://aclanthology.org/2022.findings-acl.202.pdf,,,,,,,,
PaLM (540B),PaLM: Scaling Language Modeling with Pathways,https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1279317/pdf?isPublishedV2=False,,,,,,,,
3D city reconstruction,Building Rome in a day,https://www.mdpi.com/2072-4292/11/1/58/pdf?version=1546503292,,,,,,,,
AWD-LSTM,On the State of the Art of Evaluation in Neural Language Models,http://bphm.knu.ua/index.php/bphm/article/download/147/120,,,,,,,,
PaLI,PaLI: A Jointly-Scaled Multilingual Language-Image Model,http://arxiv.org/pdf/2303.03761,,,,,,,,
TransformerXL + spectrum control,Improving Neural Language Generation with Spectrum Control,https://aclanthology.org/2022.acl-long.3.pdf,,,,,,,,
fastText,Bag of Tricks for Efficient Text Classification,https://upjournals.up.ac.za/index.php/dhasa/article/download/4446/3858,,,,,,,,
Megatron-BERT,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://arxiv.org/pdf/2204.10562,,,,,,,,
RT-1,RT-1: Robotics Transformer for Real-World Control at Scale,https://arxiv.org/pdf/2310.05905,,,,,,,,
Spatiotemporal fusion ConvNet,Convolutional Two-Stream Network Fusion for Video Action Recognition,https://arxiv.org/pdf/2303.09941,,,,,,,,
CogVideo,CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,https://arxiv.org/pdf/2310.05922,,,,,,,,
DeLight,DeLighT: Deep and Light-weight Transformer,https://www.mdpi.com/2223-7747/12/8/1652/pdf?version=1681458291,,,,,,,,
Feedforward NN,Understanding the difficulty of training deep feedforward neural networks,https://www.nature.com/articles/s41598-023-42141-x.pdf,,,,,,,,
Dropout (ImageNet),Improving neural networks by preventing co-adaptation of feature detectors,http://arxiv.org/pdf/2207.02454,,,,,,,,
Restricted Bolzmann machines,Restricted Boltzmann machines for collaborative filtering,https://vfast.org/journals/index.php/VTSE/article/download/1178/1044,,,,,,,,
DQN,Playing Atari with Deep Reinforcement Learning,https://arxiv.org/pdf/2308.13246,,,,,,,,
"MSRA (C, PReLU)",Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,https://arxiv.org/pdf/2309.04437,,,,,,,,
Samuel Neural Checkers,Some Studies in Machine Learning Using the Game of Checkers,https://ieeexplore.ieee.org/ielx7/8964404/8966238/09241509.pdf,,,,,,,,
Pandemonium (morse),Pandemonium: a paradigm for learning,https://escholarship.org/content/qt2dr7d761/qt2dr7d761.pdf?t=q9htxe,,,,,,,,
Deep Multitask NLP Network,A unified architecture for natural language processing: deep neural networks with multitask learning,https://www.aclweb.org/anthology/2020.coling-main.225.pdf,,,,,,,,
BIDAF,Bidirectional Attention Flow for Machine Comprehension,https://aclanthology.org/2021.eacl-main.259.pdf,,,,,,,,
LaMDA,LaMDA: Language Models for Dialog Applications,https://arxiv.org/pdf/2308.05627,,,,,,,,
AlexaTM 20B,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,http://arxiv.org/pdf/2306.02349,,,,,,,,
Pointer Sentinel-LSTM (medium),Pointer Sentinel Mixture Models,http://arxiv.org/pdf/2306.10728,,,,,,,,
ResNet-50 Billion-scale,Billion-scale semi-supervised learning for image classification,https://arxiv.org/pdf/2106.11346,,,,,,,,
ISS,Learning Intrinsic Sparse Structures within Long Short-term Memory,https://www.zora.uzh.ch/id/eprint/219685/1/Lindmar_Gao_Liu_STD_AICAS_2022.pdf,,,,,,,,
Zip CNN,Backpropagation Applied to Handwritten Zip Code Recognition,https://dl.acm.org/doi/pdf/10.1145/3597212,,,,,,,,
T5-11B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://www.mdpi.com/2071-1050/16/2/781/pdf?version=1705417242,,,,,,,,
TensorReasoner,Reasoning With Neural Tensor Networks for Knowledge Base Completion,https://www.aclweb.org/anthology/2020.coling-main.489.pdf,,,,,,,,
VQGAN + CLIP,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/pdf/2310.00076,,,,,,,,
SimCLR,A Simple Framework for Contrastive Learning of Visual Representations,https://www.medrxiv.org/content/medrxiv/early/2024/01/26/2024.01.26.24301803.full.pdf,,,,,,,,
LSTM-Char-Large,Character-Aware Neural Language Models,https://www.biorxiv.org/content/biorxiv/early/2019/05/29/622803.full.pdf,,,,,,,,
T5-3B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/5E5A7C1272C234F77ECB4A354E6FB253/S1351324923000487a.pdf/div-class-title-data-to-text-generation-using-conditional-generative-adversarial-with-enhanced-transformer-div.pdf,,,,,,,,
BERT-Large,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://ijece.iaescore.com/index.php/IJECE/article/download/31253/17141,,,,,,,,
data2vec (vision),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",http://arxiv.org/pdf/2306.09361,,,,,,,,
ConSERT,ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer,https://ieeexplore.ieee.org/ielx7/6287639/10005208/10131937.pdf,,,,,,,,
NMT Transformer 437M,Massively Multilingual Neural Machine Translation,https://www.aclweb.org/anthology/D19-5610.pdf,,,,,,,,
Imagen,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,https://arxiv.org/pdf/2310.03104,,,,,,,,
Deep LSTM for video classification,Beyond short snippets: Deep networks for video classification,https://ojs.aaai.org/index.php/AAAI/article/download/6941/6795,,,,,,,,
ADAM (CIFAR-10),Adam: A Method for Stochastic Optimization,https://academic.oup.com/mnras/advance-article-pdf/doi/10.1093/mnras/stae421/56645091/stae421.pdf,,,,,,,,
BellKor 2007,The BellKor solution to the Netflix Prize,http://nlpr-web.ia.ac.cn/2009papers/gjhy/gh26.pdf,,,,,,,,
RBM-tuning,A Practical Guide to Training Restricted Boltzmann Machines,https://arxiv.org/pdf/2310.01762,,,,,,,,
GCNN-14,Language Modeling with Gated Convolutional Networks,http://arxiv.org/pdf/2304.12849,,,,,,,,
Fully Convolutional Networks,Fully convolutional networks for semantic segmentation,https://isprs-annals.copernicus.org/articles/X-1-W1-2023/153/2023/isprs-annals-X-1-W1-2023-153-2023.pdf,,,,,,,,
Once for All,Once for All: Train One Network and Specialize it for Efficient Deployment,http://arxiv.org/pdf/2204.03475,,,,,,,,
λ-WASP,Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus,https://repository.upenn.edu/bitstreams/4e67c4f2-934f-4f70-941c-31894e71f0f8/download,,,,,,,,
ProxylessNAS,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,https://arxiv.org/pdf/2108.07375,,,,,,,,
Trajectory-pooled conv nets,Action recognition with trajectory-pooled deep-convolutional descriptors,https://arxiv.org/pdf/2308.03908,,,,,,,,
Big-Little Net,Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition,https://ojs.aaai.org/index.php/AAAI/article/download/5886/5742,,,,,,,,
Residual Dense Network,Residual Dense Network for Image Super-Resolution,https://arxiv.org/pdf/2212.04457,,,,,,,,
Transformer-XL + RMS dynamic eval,Dynamic Evaluation of Transformer Language Models,https://arxiv.org/pdf/2011.09301,,,,,,,,
DITTO,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,https://arxiv.org/pdf/2310.00840,,,,,,,,
FAST,Machine Learning for High-Speed Corner Detection,https://authors.library.caltech.edu/records/yyq25-x9y16/files/2105.06464.pdf?download=1,,,,,,,,
Neocognitron,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,https://medcraveonline.com/MSEIJ/MSEIJ-06-00178.pdf,,,,,,,,
LSTM (2018),An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,http://arxiv.org/pdf/2306.06579,,,,,,,,
Spectrally Normalized GAN,Spectral Normalization for Generative Adversarial Networks,http://arxiv.org/pdf/2302.04763,,,,,,,,
BLSTM for handwriting (1),A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks,https://ieeexplore.ieee.org/ielx7/6221020/9497060/09305279.pdf,,,,,,,,
DQN-2015,Human-level control through deep reinforcement learning,https://www.frontiersin.org/articles/10.3389/fpubh.2023.1310016/pdf?isPublishedV2=False,,,,,,,,
Transformer (Adaptive Input Embeddings),Adaptive Input Representations for Neural Language Modeling,https://ojs.aaai.org/index.php/AAAI/article/download/16091/15898,,,,,,,,
LRCN,Long-term recurrent convolutional networks for visual recognition and description,https://www.nature.com/articles/s41599-022-01393-0.pdf,,,,,,,,
Flamingo,Flamingo: a Visual Language Model for Few-Shot Learning,http://arxiv.org/pdf/2306.16410,,,,,,,,
DeepFace,DeepFace: Closing the Gap to Human-Level Performance in Face Verification,https://www.nature.com/articles/s41599-021-00970-z.pdf,,,,,,,,
Stacked hourglass network,Stacked Hourglass Networks for Human Pose Estimation,http://arxiv.org/pdf/2208.10769,,,,,,,,
Large regularized LSTM,Recurrent Neural Network Regularization,https://akjournals.com/downloadpdf/journals/2062/69/4/article-p581.pdf,,,,,,,,
Restricted Bolzmann machines,Restricted Boltzmann machines for collaborative filtering,https://arxiv.org/pdf/1811.04343,,,,,,,,
Swift,Champion-level drone racing using deep reinforcement learning,https://arxiv.org/pdf/2310.00675,,,,,,,,
Diffractive Deep Neural Network,All-optical machine learning using diffractive deep neural networks,https://www.spiedigitallibrary.org/journals/advanced-photonics/volume-4/issue-2/026004/Optical-neural-network-quantum-state-tomography/10.1117/1.AP.4.2.026004.pdf,,,,,,,,
Zip CNN,Backpropagation Applied to Handwritten Zip Code Recognition,http://www.journal-cot.com/article/S0976566223002205/pdf,,,,,,,,
ERNIE-Doc (247M),ERNIE-Doc: A Retrospective Long-Document Modeling Transformer,https://arxiv.org/pdf/2210.10349,,,,,,,,
PSPNet,Pyramid Scene Parsing Network,https://arxiv.org/pdf/2308.06701,,,,,,,,
Deep Belief Nets,A Fast Learning Algorithm for Deep Belief Nets,https://link.springer.com/content/pdf/10.1007/s00371-023-02967-y.pdf,,,,,,,,
TFE SVM,A trainable feature extractor for handwritten digit recognition,http://pdf.aminer.org/000/346/439/recognition_of_unconstrained_on_line_devanagari_characters.pdf,,,,,,,,
LRCN,Long-term recurrent convolutional networks for visual recognition and description,https://link.springer.com/content/pdf/10.1007/s00236-021-00400-2.pdf,,,,,,,,
Spatially-Sparse CNN,Spatially-sparse convolutional neural networks,https://arxiv.org/pdf/2209.05324,,,,,,,,
ImageBind,ImageBind One Embedding Space to Bind Them All,http://arxiv.org/pdf/2306.13302,,,,,,,,
DeepLabV3,Rethinking Atrous Convolution for Semantic Image Segmentation,https://arxiv.org/pdf/2306.15442,,,,,,,,
Differentiable neural computer,Hybrid computing using a neural network with dynamic external memory,https://www.mdpi.com/2072-6694/14/15/3707/pdf?version=1659346029,,,,,,,,
Constituency-Tree LSTM,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,https://riuma.uma.es/xmlui/bitstream/10630/27636/5/JISBD2023%20.pdf,,,,,,,,
Transformer ELMo,Dissecting Contextual Word Embeddings: Architecture and Representation,https://arxiv.org/pdf/2007.12948,,,,,,,,
Fisher Kernel GMM,Fisher Kernels on Visual Vocabularies for Image Categorization,https://arxiv.org/pdf/1601.03295,,,,,,,,
DCN+,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,https://www.aclweb.org/anthology/2020.findings-emnlp.73.pdf,,,,,,,,
DeepLab (2017),"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",https://www.mdpi.com/2079-9292/12/24/4940/pdf?version=1702031138,,,,,,,,
Decoupled weight decay regularization,Decoupled Weight Decay Regularization,https://arxiv.org/pdf/2401.04119,,,,,,,,
Deep Boltzmann Machines,Deep Boltzmann Machines,https://arxiv.org/pdf/2103.16775,,,,,,,,
Large regularized LSTM,Recurrent Neural Network Regularization,https://dl.acm.org/doi/pdf/10.1145/3456529.3456560,,,,,,,,
MusicLM,MusicLM: Generating Music From Text,https://ieeexplore.ieee.org/ielx7/6221020/6363502/10261199.pdf,,,,,,,,
FAST,Machine Learning for High-Speed Corner Detection,https://isprs-archives.copernicus.org/articles/XLVIII-1-W2-2023/679/2023/isprs-archives-XLVIII-1-W2-2023-679-2023.pdf,,,,,,,,
Statement Curriculum Learning,Formal Mathematics Statement Curriculum Learning,http://arxiv.org/pdf/2209.02562,,,,,,,,
Hopfield Networks (2020),Hopfield Networks is All You Need,https://www.biorxiv.org/content/biorxiv/early/2022/12/10/2022.12.06.519318.1.full.pdf,,,,,,,,
CURL,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,http://arxiv.org/pdf/2306.08537,,,,,,,,
Bidirectional RNN,Bidirectional recurrent neural networks,https://link.springer.com/content/pdf/10.1007/s44196-023-00345-z.pdf,,,,,,,,
Contriever,Unsupervised Dense Information Retrieval with Contrastive Learning,http://arxiv.org/pdf/2210.05156,,,,,,,,
Pattern recognition and reading by machine,Pattern recognition and reading by machine,https://arxiv.org/pdf/1711.06583,,,,,,,,
LSTM with forget gates,Learning to Forget: Continual Prediction with LSTM,https://downloads.hindawi.com/journals/mpe/2022/2082708.pdf,,,,,,,,
SearchFusion,Selective Search for Object Recognition,https://www.nature.com/articles/s41598-021-04432-z.pdf,,,,,,,,
Statistical Shape Constellations,Unsupervised Learning of Models for Recognition,http://www.cs.toronto.edu/~sven/Papers/cvpr2001-ab.pdf,,,,,,,,
ProGen2-xlarge,ProGen2: Exploring the Boundaries of Protein Language Models,https://www.biorxiv.org/content/biorxiv/early/2022/12/02/2022.11.30.518466.full.pdf,,,,,,,,
ProGen2-xlarge,ProGen2: Exploring the Boundaries of Protein Language Models,http://arxiv.org/pdf/2306.00872,,,,,,,,
ERNIE-GEN (large),ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation,http://arxiv.org/pdf/2212.12192,,,,,,,,
CTC-Trained LSTM,Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,https://arxiv.org/pdf/2309.08023,,,,,,,,
Multiscale deformable part model,"A discriminatively trained, multiscale, deformable part model",http://downloads.hindawi.com/journals/js/2017/8241910.pdf,,,,,,,,
DBLSTM,Hybrid speech recognition with Deep Bidirectional LSTM,https://ojs.aaai.org/index.php/AAAI/article/download/4550/4428,,,,,,,,
PaLM (540B),PaLM: Scaling Language Modeling with Pathways,https://arxiv.org/pdf/2310.05209,,,,,,,,
Iterative Bootstrapping WSD,Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,https://ojs.aaai.org/index.php/AAAI/article/download/21420/21169,,,,,,,,
MLP as Bayesian Approximator,The multilayer perceptron as an approximation to a Bayes optimal discriminant function,https://ieeexplore.ieee.org/ielx7/6287639/9312710/09355141.pdf,,,,,,,,
Pandemonium (morse),Pandemonium: a paradigm for learning,http://www.dtic.mil/dtic/tr/fulltext/u2/a396346.pdf,,,,,,,,
MoCo,Momentum Contrast for Unsupervised Visual Representation Learning,https://www.nature.com/articles/s41598-023-48482-x.pdf,,,,,,,,
WizardLM-7B,WizardLM: Empowering Large Language Models to Follow Complex Instructions,https://arxiv.org/pdf/2308.07124,,,,,,,,
Learning deep architectures,Learning Deep Architectures for AI,https://www.mdpi.com/2227-7390/9/11/1180/pdf?version=1621836195,,,,,,,,
RNN 500/10 + RT09 LM (NIST RT05),Recurrent neural network based language model,https://ieeexplore.ieee.org/ielx7/4620076/10049222/09964407.pdf,,,,,,,,
OpenAI Five Rerun,Dota 2 with Large Scale Deep Reinforcement Learning,https://arxiv.org/pdf/2206.13353,,,,,,,,
DeepLab (2017),"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",https://arxiv.org/pdf/2309.16889,,,,,,,,
ESRGAN,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1029/2023EA002906,,,,,,,,
BIDAF,Bidirectional Attention Flow for Machine Comprehension,https://arxiv.org/pdf/2209.11407,,,,,,,,
Switch,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://dl.acm.org/doi/pdf/10.1145/3582016.3582047,,,,,,,,
Xception,Xception: Deep Learning with Depthwise Separable Convolutions,https://link.springer.com/content/pdf/10.1007/s00432-023-05339-0.pdf,,,,,,,,
AlexaTM 20B,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,https://arxiv.org/pdf/2303.18223,,,,,,,,
Parti,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,https://arxiv.org/pdf/2210.16056,,,,,,,,
DeepNash,Mastering the game of Stratego with model-free multiagent reinforcement learning,https://www.frontiersin.org/articles/10.3389/fnbot.2023.1243174/pdf?isPublishedV2=False,,,,,,,,
SemExp,Object Goal Navigation using Goal-Oriented Semantic Exploration,https://arxiv.org/pdf/2310.00887,,,,,,,,
MatrixFac for Recommenders,Matrix Factorization Techniques for Recommender Systems,http://arxiv.org/pdf/2302.00412,,,,,,,,
TFE SVM,A trainable feature extractor for handwritten digit recognition,http://hal.archives-ouvertes.fr/docs/00/14/30/34/PDF/LauerBlochNeurocomputing07.pdf,,,,,,,,
LSTM (2018),An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,https://arxiv.org/pdf/2303.04485,,,,,,,,
Stacked Denoising Autoencoders,Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion,http://arxiv.org/pdf/2205.04259,,,,,,,,
Diffractive Deep Neural Network,All-optical machine learning using diffractive deep neural networks,https://www.nature.com/articles/s41467-022-30901-8.pdf,,,,,,,,
KEPLER,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,https://vtechworks.lib.vt.edu/bitstream/10919/115958/1/3539618.3591781.pdf,,,,,,,,
AlphaGo Lee,Mastering the game of Go with deep neural networks and tree search,https://arxiv.org/pdf/2310.05876,,,,,,,,
LSTM-Char-Large,Character-Aware Neural Language Models,https://www.aclweb.org/anthology/N18-1091.pdf,,,,,,,,
BellKor 2008,The BellKor 2008 Solution to the Netflix Prize,https://www.mdpi.com/2076-3417/11/13/6108/pdf?version=1625191350,,,,,,,,
ENAS,Efficient Neural Architecture Search via Parameter Sharing,https://arxiv.org/pdf/2106.11346,,,,,,,,
Image-to-image cGAN,Image-to-Image Translation with Conditional Adversarial Networks,https://arxiv.org/pdf/2310.14197,,,,,,,,
2-layer-LSTM+Deep-Gradient-Compression,Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,https://arxiv.org/pdf/2102.09491,,,,,,,,
YOLO,"You Only Look Once: Unified, Real-Time Object Detection",https://www.nature.com/articles/s41598-023-48318-8.pdf,,,,,,,,
TransE,Translating Embeddings for Modeling Multi-relational Data,http://arxiv.org/pdf/2306.00088,,,,,,,,
RetinaNet-R101,Focal Loss for Dense Object Detection,https://ieeexplore.ieee.org/ielx7/7083369/7339444/10436339.pdf,,,,,,,,
Incoder-6.7B,InCoder: A Generative Model for Code Infilling and Synthesis,https://arxiv.org/pdf/2306.03324,,,,,,,,
GPT-NeoX-20B,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,http://arxiv.org/pdf/2208.14493,,,,,,,,
ALBERT,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,https://arxiv.org/pdf/2310.01424,,,,,,,,
Boss (DARPA Urban Challenge),Autonomous driving in urban environments: Boss and the Urban Challenge,https://www.mdpi.com/2079-9292/7/9/154/pdf?version=1534824006,,,,,,,,
Flan-PaLM 540B,Scaling Instruction-Finetuned Language Models,http://arxiv.org/pdf/2305.14232,,,,,,,,
CodeT5+,CodeT5+: Open Code Large Language Models for Code Understanding and Generation,https://arxiv.org/pdf/2307.14936,,,,,,,,
Megatron-LM (8.3B),Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,http://arxiv.org/pdf/2302.02599,,,,,,,,
"Listen, Attend and Spell","Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",https://www.mdpi.com/2079-9292/11/12/1831/pdf?version=1654767632,,,,,,,,
MobileNet,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,https://arxiv.org/pdf/2309.13744,,,,,,,,
Denoising Diffusion Probabilistic Models (LSUN Bedroom),Denoising Diffusion Probabilistic Models,https://www.mdpi.com/2079-9292/13/2/448/pdf?version=1705890422,,,,,,,,
CURL,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,http://arxiv.org/pdf/2205.11357,,,,,,,,
RNNsearch-50*,Neural Machine Translation by Jointly Learning to Align and Translate,https://arxiv.org/pdf/2308.13187,,,,,,,,
EMDR,End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering,http://arxiv.org/pdf/2303.17201,,,,,,,,
KN5 LM + RNN 400/10 (WSJ),Recurrent neural network based language model,https://arxiv.org/pdf/2307.06018,,,,,,,,
GloVe (32B),GloVe: Global Vectors for Word Representation,https://jurnal.polgan.ac.id/index.php/sinkron/article/download/13005/2156,,,,,,,,
FLAN 137B,Finetuned Language Models Are Zero-Shot Learners,https://aclanthology.org/2023.findings-emnlp.552.pdf,,,,,,,,
TriNet,In Defense of the Triplet Loss for Person Re-Identification,https://arxiv.org/pdf/2307.02753,,,,,,,,
Cascaded LNet-ANet,Deep Learning Face Attributes in the Wild,https://www.researchsquare.com/article/rs-1669440/latest.pdf,,,,,,,,
Florence,Florence: A New Foundation Model for Computer Vision,http://arxiv.org/pdf/2205.00538,,,,,,,,
RBM Image Classifier,Learning Multiple Layers of Features from Tiny Images,https://www.techrxiv.org/articles/preprint/SVFL_Efficient_Secure_Aggregation_and_Verification_for_Cross-Silo_Federated_Learning/19428755/1/files/34522637.pdf,,,,,,,,
GOAT,Open-Ended Learning Leads to Generally Capable Agents,https://www.frontiersin.org/articles/10.3389/fncom.2022.1060101/pdf,,,,,,,,
SRU++ Large,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,https://www.mdpi.com/2076-3417/13/23/12901/pdf?version=1701427146,,,,,,,,
Dropout (TIMIT),Improving neural networks by preventing co-adaptation of feature detectors,http://arxiv.org/pdf/2208.11508,,,,,,,,
PolyNet,PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,https://www.mdpi.com/2079-9292/10/4/396/pdf?version=1612865231,,,,,,,,
LongT5,LongT5: Efficient Text-To-Text Transformer for Long Sequences,http://arxiv.org/pdf/2305.03668,,,,,,,,
Word2Vec (large),Distributed Representations of Words and Phrases and their Compositionality,https://www.biorxiv.org/content/biorxiv/early/2022/05/20/2022.05.02.490310.full.pdf,,,,,,,,
AlphaCode,Competition-level code generation with AlphaCode,https://dl.acm.org/doi/pdf/10.1145/3586030,,,,,,,,
ProtT5-XXL,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,https://www.biorxiv.org/content/biorxiv/early/2022/05/28/2022.05.25.493516.full.pdf,,,,,,,,
RefineNet,RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation,http://arxiv.org/pdf/2306.05675,,,,,,,,
BigSSL,BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition,http://arxiv.org/pdf/2206.14716,,,,,,,,
RBM Image Classifier,Learning Multiple Layers of Features from Tiny Images,https://www.biorxiv.org/content/biorxiv/early/2023/12/21/2023.12.20.572123.full.pdf,,,,,,,,
ResNet-50 Billion-scale,Billion-scale semi-supervised learning for image classification,https://hal.inria.fr/hal-03530130/file/J31.pr21.nsod.pdf,,,,,,,,
EfficientNetV2,EfficientNetV2: Smaller Models and Faster Training,http://arxiv.org/pdf/2304.06502,,,,,,,,
ProgressiveGAN,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",http://arxiv.org/pdf/2306.08571,,,,,,,,
Relational Memory Core,Relational recurrent neural networks,https://arxiv.org/pdf/1908.09535,,,,,,,,
Dropout (TIMIT),Improving neural networks by preventing co-adaptation of feature detectors,https://www.mdpi.com/2072-4292/13/20/4065/pdf?version=1634289928,,,,,,,,
GPipe (Transformer),GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://arxiv.org/pdf/2004.10856,,,,,,,,
Local Binary Patterns for facial recognition,Face Description with Local Binary Patterns: Application to Face Recognition,https://arxiv.org/pdf/2105.10262,,,,,,,,
SACHS,Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data,https://www.intechopen.com/citation-pdf-url/58193,,,,,,,,
BellKor 2007,The BellKor solution to the Netflix Prize,https://eprints.soton.ac.uk/272686/2/MMMR_IS_Finalized.pdf,,,,,,,,
Contriever,Unsupervised Dense Information Retrieval with Contrastive Learning,https://arxiv.org/pdf/2310.07521,,,,,,,,
NLP from scratch,Natural Language Processing (Almost) from Scratch,https://www.frontiersin.org/articles/10.3389/fdgth.2022.833912/pdf,,,,,,,,
Llama 2-7B,Llama 2: Open Foundation and Fine-Tuned Chat Models,https://www.mdpi.com/1424-8220/24/2/347/pdf?version=1704531999,,,,,,,,
ERNIE-Doc (247M),ERNIE-Doc: A Retrospective Long-Document Modeling Transformer,http://arxiv.org/pdf/2204.06683,,,,,,,,
PDP model for serial order,Serial Order: A Parallel Distributed Processing Approach,http://arxiv.org/pdf/2212.10079,,,,,,,,
Pointer Sentinel-LSTM (medium),Pointer Sentinel Mixture Models,http://arxiv.org/pdf/2210.07566,,,,,,,,
KN5 LM + RNN 400/10 (WSJ),Recurrent neural network based language model,https://www.mdpi.com/1996-1073/15/19/7233/pdf?version=1664618818,,,,,,,,
CoAtNet,CoAtNet: Marrying Convolution and Attention for All Data Sizes,http://arxiv.org/pdf/2302.06675,,,,,,,,
MatrixFac for Recommenders,Matrix Factorization Techniques for Recommender Systems,http://arxiv.org/pdf/2306.00009,,,,,,,,
Differentiable neural computer,Hybrid computing using a neural network with dynamic external memory,https://arxiv.org/pdf/2105.06453,,,,,,,,
GPU DBNs,Large-scale deep unsupervised learning using graphics processors,https://www.mdpi.com/1099-4300/22/12/1429/pdf,,,,,,,,
GPT,Improving Language Understanding by Generative Pre-Training,https://arxiv.org/pdf/2310.03481,,,,,,,,
AlphaFold-Multimer,Protein complex prediction with AlphaFold-Multimer,https://www.biorxiv.org/content/biorxiv/early/2023/03/18/2023.03.17.533168.full.pdf,,,,,,,,
Transformer-XL + RMS dynamic eval,Dynamic Evaluation of Transformer Language Models,https://aclanthology.org/2021.findings-emnlp.206.pdf,,,,,,,,
GANs,A Style-Based Generator Architecture for Generative Adversarial Networks,https://www.mdpi.com/2079-9292/12/20/4235/pdf?version=1697186146,,,,,,,,
Transformer ELMo,Dissecting Contextual Word Embeddings: Architecture and Representation,https://aclanthology.org/2021.acl-srw.23.pdf,,,,,,,,
CapsNet (MNIST),Dynamic Routing Between Capsules,https://arxiv.org/pdf/2304.07911,,,,,,,,
Image Classification with the Fisher Vector: Theory and Practice,"Author manuscript, published in ""International Journal of Computer Vision (2013)"" International Journal of Computer Vision manuscript No. (will be inserted by the editor) Image Classification with the Fisher Vector: Theory and Practice",http://arxiv.org/pdf/2208.04201,,,,,,,,
Switch,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,https://arxiv.org/pdf/2107.11317,,,,,,,,
NEAT in neuroevolution,Evolving Neural Networks through Augmenting Topologies,https://osf.io/zk2dy/download,,,,,,,,
Florence,Florence: A New Foundation Model for Computer Vision,https://arxiv.org/pdf/2308.02487,,,,,,,,
Routing Transformer,Efficient Content-Based Sparse Attention with Routing Transformers,https://dl.acm.org/doi/pdf/10.1145/3534678.3539260,,,,,,,,
BLOOMZ-176B,Crosslingual Generalization through Multitask Finetuning,http://arxiv.org/pdf/2306.17806,,,,,,,,
GLEE,BOXES: AN EXPERIMENT IN ADAPTIVE CONTROL,https://intellrobot.com/article/download/4634,,,,,,,,
PaLM (540B),PaLM: Scaling Language Modeling with Pathways,https://arxiv.org/pdf/2310.05857,,,,,,,,
iGPT-XL,Generative Pretraining From Pixels,https://ojs.aaai.org/index.php/AAAI/article/download/20103/19862,,,,,,,,
KEPLER,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,https://aclanthology.org/2022.emnlp-main.650.pdf,,,,,,,,
Culturome,Quantitative Analysis of Culture Using Millions of Digitized Books,https://sas-space.sas.ac.uk/9202/1/Web%20archives%20and%20digital%20history.pdf,,,,,,,,
TD(0),An Adaptive Optimal Controller for Discrete-Time Markov Environments,https://dspace.mit.edu/bitstream/1721.1/7050/2/AITR-1424.pdf,,,,,,,,
RNN-SpeedUp,Extensions of recurrent neural network language model,http://www.jmis.org/download/download_pdf?pid=jmis-7-1-1,,,,,,,,
FTW,Human-level performance in 3D multiplayer games with population-based reinforcement learning,http://arxiv.org/pdf/2301.08030,,,,,,,,
ST-MoE,ST-MoE: Designing Stable and Transferable Sparse Expert Models,https://arxiv.org/pdf/2305.13230,,,,,,,,
WaveNet,WaveNet: A Generative Model for Raw Audio,https://arxiv.org/pdf/2305.19337,,,,,,,,
BLSTM for handwriting (1),A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks,https://doi.org/10.3906/elk-1801-234,,,,,,,,
Internal functionality of visual invariants,The internal representation of solid shape with respect to vision,http://cseweb.ucsd.edu/groups/hpcl/scg/papers/2012/vda12BestPaper.pdf,,,,,,,,
Zip CNN,Backpropagation Applied to Handwritten Zip Code Recognition,https://www.mdpi.com/2072-4292/15/10/2649/pdf?version=1684481589,,,,,,,,
LSTM-Char-Large,Character-Aware Neural Language Models,https://arxiv.org/pdf/1808.09891,,,,,,,,
PointNet++,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,https://arxiv.org/pdf/2308.09718,,,,,,,,
AlphaGo Fan,Mastering the game of Go with deep neural networks and tree search,https://downloads.hindawi.com/journals/cin/2023/2955442.pdf,,,,,,,,
Agent57,Agent57: Outperforming the Atari Human Benchmark,https://link.springer.com/content/pdf/10.1007/s10458-021-09497-8.pdf,,,,,,,,
PG-SWGAN,Sliced Wasserstein Generative Models,https://arxiv.org/pdf/1910.05425,,,,,,,,
T0-XXL,Multitask Prompted Training Enables Zero-Shot Task Generalization,http://arxiv.org/pdf/2209.12356,,,,,,,,
EMDR,End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering,http://arxiv.org/pdf/2205.12665,,,,,,,,
Spatiotemporal fusion ConvNet,Convolutional Two-Stream Network Fusion for Video Action Recognition,https://arxiv.org/pdf/2210.12686,,,,,,,,
Wide Residual Network,Wide Residual Networks,https://arxiv.org/pdf/2308.12577,,,,,,,,
Dropout-LSTM+Noise(Bernoulli) (WT2),Noisin: Unbiased Regularization for Recurrent Neural Networks,https://www.mdpi.com/2078-2489/11/11/536/pdf,,,,,,,,
SimCLR,A Simple Framework for Contrastive Learning of Visual Representations,https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1349204/pdf,,,,,,,,
LSTM-300units,LSTM Neural Networks for Language Modeling,https://www.mdpi.com/2227-7390/8/9/1558/pdf?version=1599793458,,,,,,,,
Q-learning,Learning from delayed rewards,http://arxiv.org/pdf/2303.06473,,,,,,,,
Deeply-supervised nets,Deeply-Supervised Nets,https://arxiv.org/pdf/2305.19480,,,,,,,,
Social and content-based classification,Recommendation as Classification: Using Social and Content-Based Information in Recommendation,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0235502&type=printable,,,,,,,,
DImensionality Reduction,Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks,https://arxiv.org/pdf/2309.13363,,,,,,,,
LeNet-5,Gradient-based learning applied to document recognition,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0296464&type=printable,,,,,,,,
Zip CNN,Backpropagation Applied to Handwritten Zip Code Recognition,https://jnanobiotechnology.biomedcentral.com/counter/pdf/10.1186/s12951-023-01864-9,,,,,,,,
CodeT5-large,CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning,https://arxiv.org/pdf/2309.11489,,,,,,,,
Word2Vec (large),Distributed Representations of Words and Phrases and their Compositionality,https://www.biorxiv.org/content/biorxiv/early/2022/05/16/2022.05.16.492029.full.pdf,,,,,,,,
CICERO,Human-level play in the game of Diplomacy by combining language models with strategic reasoning,http://arxiv.org/pdf/2306.09030,,,,,,,,
DeepSpeech2 (English),Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin,https://www.nature.com/articles/s41746-021-00555-9.pdf,,,,,,,,
MetaLM,Language Models are General-Purpose Interfaces,https://arxiv.org/pdf/2307.16376,,,,,,,,
LRSO-GAN,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro,https://arxiv.org/pdf/2112.01730,,,,,,,,
Pandemonium (morse),Pandemonium: a paradigm for learning,https://link.springer.com/content/pdf/10.3758/BF03210828.pdf,,,,,,,,
SRN-Encoded Grammatical Structures,"Distributed representations, simple recurrent networks, and grammatical structure",https://europepmc.org/articles/pmc2621112?pdf=render,,,,,,,,
Dropout (ImageNet),Improving neural networks by preventing co-adaptation of feature detectors,https://www.mdpi.com/2313-0105/8/3/21/pdf?version=1645864467,,,,,,,,
HMM Word Alignment,HMM-Based Word Alignment in Statistical Translation,https://journals.aiac.org.au/index.php/IJALEL/article/download/7304/4960,,,,,,,,
Temporal Convolutional Attention-based Network(TCAN) (WT2),Temporal Convolutional Attention-based Network For Sequence Modeling,https://bmcanesthesiol.biomedcentral.com/track/pdf/10.1186/s12871-022-01625-5,,,,,,,,
Flan-T5 11B,Scaling Instruction-Finetuned Language Models,https://arxiv.org/pdf/2310.00230,,,,,,,,
Continuous speech recognition by statistical methods,Continuous speech recognition by statistical methods,https://dl.acm.org/doi/pdf/10.3115/1075434.1075482,,,,,,,,
Neocognitron,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,https://beei.org/index.php/EEI/article/download/2802/2415,,,,,,,,
SRGAN,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/jsid.1266,,,,,,,,
GloVe (32B),GloVe: Global Vectors for Word Representation,https://arxiv.org/pdf/2310.00402,,,,,,,,
Sandwich Transformer,Improving Transformer Models by Reordering their Sublayers,https://www.aclweb.org/anthology/2020.emnlp-main.371.pdf,,,,,,,,
Codex,Evaluating Large Language Models Trained on Code,http://arxiv.org/pdf/2306.10518,,,,,,,,
ERNIE 3.0,ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,https://link.springer.com/content/pdf/10.1007/s10389-023-01921-5.pdf,,,,,,,,
Tranception,Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval,https://www.biorxiv.org/content/biorxiv/early/2023/09/30/2023.09.28.560044.full.pdf,,,,,,,,
Inflated 3D ConvNet,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",https://www.frontiersin.org/articles/10.3389/fpls.2023.1337467/pdf?isPublishedV2=False,,,,,,,,
ESM2-15B,Evolutionary-scale prediction of atomic level protein structure with a language model,http://www.csbj.org/article/S2001037023004129/pdf,,,,,,,,
Visualizing CNNs,Visualizing and Understanding Convolutional Networks,https://arxiv.org/pdf/2309.12805,,,,,,,,
Immediate trihead,Immediate-Head Parsing for Language Models,http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00171,,,,,,,,
data2vec (vision),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",http://arxiv.org/pdf/2301.11316,,,,,,,,
EDSR,Enhanced Deep Residual Networks for Single Image Super-Resolution,https://arxiv.org/pdf/2211.04687,,,,,,,,
Transformer,Attention is All you Need,https://www.nature.com/articles/s41598-024-52471-z.pdf,,,,,,,,
SRGAN,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,http://arxiv.org/pdf/2302.09346,,,,,,,,
Optimized Multi-Scale Edge Detection,A Computational Approach to Edge Detection,https://arxiv.org/pdf/2303.11546,,,,,,,,
MobileNet,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,https://www.mdpi.com/2073-4395/13/12/2943/pdf?version=1701252943,,,,,,,,
REINFORCE in Stochastic Connectionism,Simple statistical gradient-following algorithms for connectionist reinforcement learning,https://arxiv.org/pdf/2308.04844,,,,,,,,
ReLU (LFW),Rectified Linear Units Improve Restricted Boltzmann Machines,http://arxiv.org/pdf/2305.12398,,,,,,,,
Go-explore,"First return, then explore",https://arxiv.org/pdf/2104.08781,,,,,,,,
R-CNN (T-net),Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,https://drpress.org/ojs/index.php/fcis/article/download/12144/11827,,,,,,,,
LRSO-GAN,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro,https://arxiv.org/pdf/2007.08071,,,,,,,,
OverFeat,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",https://arxiv.org/pdf/2106.12966,,,,,,,,
W2v-BERT,w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,https://arxiv.org/pdf/2205.01086,,,,,,,,
Gradient Boosting Machine,Greedy function approximation: A gradient boosting machine.,http://tapchikttv.vn/article-file-download/3659,,,,,,,,
EfficientNet-L2,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://www.frontiersin.org/articles/10.3389/fnbot.2023.1291875/pdf?isPublishedV2=False,,,,,,,,
MnasNet-A1 + SSDLite,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://link.springer.com/content/pdf/10.1007/s42452-021-04897-7.pdf,,,,,,,,
ULM-FiT,Universal Language Model Fine-tuning for Text Classification,https://arxiv.org/pdf/2207.00748,,,,,,,,
LRSO-GAN,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro,https://arxiv.org/pdf/2009.08849,,,,,,,,
Stacked Denoising Autoencoders,Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion,http://arxiv.org/pdf/2203.12602,,,,,,,,
ProBERTa,Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks,https://dl.acm.org/doi/pdf/10.1145/3447548.3467163,,,,,,,,
Bayesian automated hyperparameter tuning,Practical Bayesian Optimization of Machine Learning Algorithms,https://arxiv.org/pdf/2308.09888,,,,,,,,
MuZero,"Mastering Atari, Go, chess and shogi by planning with a learned model",https://www.biorxiv.org/content/biorxiv/early/2023/01/19/2023.01.16.523429.full.pdf,,,,,,,,
S-Norm,Simple and Effective Multi-Paragraph Reading Comprehension,https://arxiv.org/pdf/2308.04566,,,,,,,,
GoogLeNet / InceptionV1,Going deeper with convolutions,https://arxiv.org/pdf/2310.02296,,,,,,,,
BloombergGPT,BloombergGPT: A Large Language Model for Finance,https://arxiv.org/pdf/2310.08419,,,,,,,,
BigGAN-deep 512x512,Large Scale GAN Training for High Fidelity Natural Image Synthesis,https://arxiv.org/pdf/2310.05590,,,,,,,,
IBM Model 4,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,https://arxiv.org/pdf/2309.15421,,,,,,,,
StarGAN v2,StarGAN v2: Diverse Image Synthesis for Multiple Domains,https://www.ijcai.org/proceedings/2021/0157.pdf,,,,,,,,
DeepNet,"DeepNet: Scaling Transformers to 1, 000 Layers",http://arxiv.org/pdf/2212.09097,,,,,,,,
ViT-G (model soup),Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,https://arxiv.org/pdf/2311.02236,,,,,,,,
PNASNet-5,Progressive Neural Architecture Search,http://arxiv.org/pdf/2203.14169,,,,,,,,
Hanabi 4 player,The Hanabi Challenge: A New Frontier for AI Research,https://arxiv.org/pdf/2308.09595,,,,,,,,
StarGAN v2,StarGAN v2: Diverse Image Synthesis for Multiple Domains,http://arxiv.org/pdf/2304.04960,,,,,,,,
Social and content-based classification,Recommendation as Classification: Using Social and Content-Based Information in Recommendation,https://link.springer.com/content/pdf/10.1007/978-3-540-73110-8_100.pdf,,,,,,,,
PG-SWGAN,Sliced Wasserstein Generative Models,http://arxiv.org/pdf/2209.13570,,,,,,,,
GCNN-14,Language Modeling with Gated Convolutional Networks,http://arxiv.org/pdf/2212.10544,,,,,,,,
ResNet-200,Identity Mappings in Deep Residual Networks,https://hal.science/hal-04203158/file/%5BBHCCM23%5D_Multi-flow_opt_greenh_sys_hierar_contr_approa_preprint.pdf,,,,,,,,
SimCLR,A Simple Framework for Contrastive Learning of Visual Representations,https://www.mdpi.com/1099-4300/25/12/1607/pdf?version=1701334396,,,,,,,,
MS-CNN,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,http://arxiv.org/pdf/2203.10939,,,,,,,,
AMDIM,Learning Representations by Maximizing Mutual Information Across Views,https://arxiv.org/pdf/2206.04726,,,,,,,,
EMDR,End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering,https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00473/2020706/tacl_a_00473.pdf,,,,,,,,
Transformer-XL + RMS dynamic eval,Dynamic Evaluation of Transformer Language Models,https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00371/1924150/tacl_a_00371.pdf,,,,,,,,
HuBERT,HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,http://arxiv.org/pdf/2304.14636,,,,,,,,
λ-WASP,Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus,http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00209,,,,,,,,
S-Norm,Simple and Effective Multi-Paragraph Reading Comprehension,https://aclanthology.org/2021.naacl-main.47.pdf,,,,,,,,
Empirical evaluation of deep architectures,An empirical evaluation of deep architectures on problems with many factors of variation,http://wrap.warwick.ac.uk/140052/1/WRAP-dense-steerable-filter-CNNs-exploiting-rotational-symmetry-histology-images-Rajpoot-2020.pdf,,,,,,,,
ALVINN,"ALVINN, an autonomous land vehicle in a neural network",https://arxiv.org/pdf/2006.03636,,,,,,,,
AlphaFold-Multimer,Protein complex prediction with AlphaFold-Multimer,http://www.jbc.org/article/S0021925823018707/pdf,,,,,,,,
MetNet,MetNet: A Neural Weather Model for Precipitation Forecasting,https://www.frontiersin.org/articles/10.3389/feart.2022.908869/pdf,,,,,,,,
Sparse coding model for V1 receptive fields,Sparse coding with an overcomplete basis set: A strategy employed by V1?,https://ojs.aaai.org/index.php/AAAI/article/download/5420/5276,,,,,,,,
Boss (DARPA Urban Challenge),Autonomous driving in urban environments: Boss and the Urban Challenge,https://arxiv.org/pdf/2107.06829,,,,,,,,
LDM-1.45B,High-Resolution Image Synthesis with Latent Diffusion Models,https://dl.acm.org/doi/pdf/10.1145/3595916.3626450,,,,,,,,
Character-enriched word2vec,Enriching Word Vectors with Subword Information,https://www.frontiersin.org/articles/10.3389/fnbot.2023.1193011/pdf,,,,,,,,
FAST,Machine Learning for High-Speed Corner Detection,https://www.mdpi.com/2072-4292/14/17/4228/pdf?version=1661597126,,,,,,,,
CodeT5-base,CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation,https://dl.acm.org/doi/pdf/10.1145/3641540,,,,,,,,
YOLOv3,YOLOv3: An Incremental Improvement,https://www.biorxiv.org/content/biorxiv/early/2023/11/27/2023.11.27.568889.full.pdf,,,,,,,,
Switch,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,http://arxiv.org/pdf/2305.12544,,,,,,,,
CLIP (ViT L/14@336px),Learning Transferable Visual Models From Natural Language Supervision,https://www.biorxiv.org/content/biorxiv/early/2024/01/26/2024.01.25.577219.full.pdf,,,,,,,,
AltCLIP,AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities,https://arxiv.org/pdf/2307.02971,,,,,,,,
CodeT5+,CodeT5+: Open Code Large Language Models for Code Understanding and Generation,https://arxiv.org/pdf/2308.04451,,,,,,,,
SimCLRv2,Big Self-Supervised Models are Strong Semi-Supervised Learners,http://arxiv.org/pdf/2306.12189,,,,,,,,
S4,Efficiently Modeling Long Sequences with Structured State Spaces,http://arxiv.org/pdf/2206.12037,,,,,,,,
AudioGen,AudioGen: Textually Guided Audio Generation,http://arxiv.org/pdf/2305.01319,,,,,,,,
Neocognitron,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,https://www.biorxiv.org/content/biorxiv/early/2023/03/16/2023.03.15.532836.full.pdf,,,,,,,,
MT-DNN,Multi-Task Deep Neural Networks for Natural Language Understanding,http://arxiv.org/pdf/2301.12618,,,,,,,,
Deep LSTM for video classification,Beyond short snippets: Deep networks for video classification,http://www.mecs-press.org/ijmsc/ijmsc-v6-n6/IJMSC-V6-N6-3.pdf,,,,,,,,
ProgressiveGAN,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",http://arxiv.org/pdf/2306.05182,,,,,,,,
Bayesian automated hyperparameter tuning,Practical Bayesian Optimization of Machine Learning Algorithms,http://arxiv.org/pdf/2302.04460,,,,,,,,
DALL-E,Zero-Shot Text-to-Image Generation,https://www.biorxiv.org/content/biorxiv/early/2023/10/07/2023.10.05.561066.full.pdf,,,,,,,,
Word Representations,Word Representations: A Simple and General Method for Semi-Supervised Learning,https://bmcmedinformdecismak.biomedcentral.com/track/pdf/10.1186/s12911-021-01662-z,,,,,,,,
TD(0),An Adaptive Optimal Controller for Discrete-Time Markov Environments,https://jisajournal.springeropen.com/track/pdf/10.1186/s13174-018-0087-2,,,,,,,,
Elastic weight consolidation,Overcoming catastrophic forgetting in neural networks,https://journals.flvc.org/FLAIRS/article/download/133322/137865,,,,,,,,
XLMR-XXL,Larger-Scale Transformers for Multilingual Masked Language Modeling,http://arxiv.org/pdf/2204.02601,,,,,,,,
Transformer + Simple Recurrent Unit,Simple Recurrent Units for Highly Parallelizable Recurrence,http://arxiv.org/pdf/2209.05943,,,,,,,,
"MSRA (C, PReLU)",Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,http://arxiv.org/pdf/2306.11250,,,,,,,,
Continuous speech recognition by statistical methods,Continuous speech recognition by statistical methods,http://arxiv.org/pdf/2203.15431,,,,,,,,
ERNIE 3.0 Titan,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,https://arxiv.org/pdf/2309.03852,,,,,,,,
Inceptionv4,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",https://www.nature.com/articles/s41598-023-43010-3.pdf,,,,,,,,
mT0-13B,Crosslingual Generalization through Multitask Finetuning,http://arxiv.org/pdf/2305.13707,,,,,,,,
UL2,UL2: Unifying Language Learning Paradigms,http://arxiv.org/pdf/2305.14763,,,,,,,,
Conformer,Conformer: Convolution-augmented Transformer for Speech Recognition,https://arxiv.org/pdf/2309.10707,,,,,,,,
Pattern recognition and reading by machine,Pattern recognition and reading by machine,https://eprints.whiterose.ac.uk/766/1/hodgevj3.pdf,,,,,,,,
LSTM (2018),An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,https://www.nature.com/articles/s41598-023-40922-y.pdf,,,,,,,,
REINFORCE in Stochastic Connectionism,Simple statistical gradient-following algorithms for connectionist reinforcement learning,http://arxiv.org/pdf/2306.06402,,,,,,,,
GLM-130B,GLM-130B: An Open Bilingual Pre-trained Model,https://arxiv.org/pdf/2308.04823,,,,,,,,
Transformer (Adaptive Input Embeddings),Adaptive Input Representations for Neural Language Modeling,https://www.mdpi.com/1424-8220/23/5/2612/pdf?version=1677637188,,,,,,,,
DQN,Playing Atari with Deep Reinforcement Learning,https://arxiv.org/pdf/2308.14991,,,,,,,,
ReLU (NORB),Rectified Linear Units Improve Restricted Boltzmann Machines,http://arxiv.org/pdf/2304.12703,,,,,,,,
Hanabi 4 player,The Hanabi Challenge: A New Frontier for AI Research,http://arxiv.org/pdf/2306.11551,,,,,,,,
Error Propagation,Learning internal representations by error propagation,https://sanscientific.com/journal/index.php/ise/article/download/148/263,,,,,,,,
Projected GAN,Projected GANs Converge Faster,https://arxiv.org/pdf/2303.05511,,,,,,,,
Maxout Networks,Maxout Networks,https://aclanthology.org/2022.findings-acl.71.pdf,,,,,,,,
LRSO-GAN,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro,https://arxiv.org/pdf/2109.03483,,,,,,,,
Libratus,Libratus: The Superhuman AI for No-Limit Poker,http://arxiv.org/pdf/2212.12669,,,,,,,,
Enhanced Neighborhood-Based Filtering,Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights,http://www0.cs.ucl.ac.uk/staff/l.capra/publications/seams11-vale.pdf,,,,,,,,
Image generation,Decision-Making with Auto-Encoding Variational Bayes,https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1011584&type=printable,,,,,,,,
A3C FF hs,Asynchronous Methods for Deep Reinforcement Learning,https://arxiv.org/pdf/2310.03195,,,,,,,,
MetNet,MetNet: A Neural Weather Model for Precipitation Forecasting,https://journals.ametsoc.org/downloadpdf/journals/hydr/23/3/JHM-D-21-0131.1.pdf,,,,,,,,
Dropout (ImageNet),Improving neural networks by preventing co-adaptation of feature detectors,https://www.mdpi.com/2313-433X/9/11/238/pdf?version=1698948986,,,,,,,,
Neuro-Symbolic Concept Learner,"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",http://arxiv.org/pdf/2305.02171,,,,,,,,
ObjectNet,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,http://arxiv.org/pdf/2204.04788,,,,,,,,
LUKE,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,https://ieeexplore.ieee.org/ielx7/6287639/6514899/10422739.pdf,,,,,,,,
Seq2Seq LSTM,Sequence to Sequence Learning with Neural Networks,https://arxiv.org/pdf/2307.07057,,,,,,,,
A3C FF hs,Asynchronous Methods for Deep Reinforcement Learning,https://doi.org/10.5121/csit.2023.131333,,,,,,,,
ProtT5-XXL,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1008736&type=printable,,,,,,,,
LSTM,Long Short-Term Memory,https://www.mdpi.com/2076-3417/14/3/1294/pdf?version=1707038969,,,,,,,,
Decoupled weight decay regularization,Decoupled Weight Decay Regularization,https://aclanthology.org/2023.findings-emnlp.797.pdf,,,,,,,,
PaLI,PaLI: A Jointly-Scaled Multilingual Language-Image Model,https://arxiv.org/pdf/2212.08045,,,,,,,,
Statistical Shape Constellations,Unsupervised Learning of Models for Recognition,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/iet-cvi.2013.0156,,,,,,,,
DeepNash,Mastering the game of Stratego with model-free multiagent reinforcement learning,https://arxiv.org/pdf/2210.12896,,,,,,,,
DQN,Playing Atari with Deep Reinforcement Learning,http://arxiv.org/pdf/2307.00593,,,,,,,,
REINFORCE in Stochastic Connectionism,Simple statistical gradient-following algorithms for connectionist reinforcement learning,http://arxiv.org/pdf/2302.14276,,,,,,,,
ESRGAN,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,https://arxiv.org/pdf/2212.09581,,,,,,,,
BigBiGAN,Large Scale Adversarial Representation Learning,https://link.springer.com/content/pdf/10.1007/s11633-021-1297-9.pdf,,,,,,,,
Population-based DRL,Human-level performance in 3D multiplayer games with population-based reinforcement learning,https://ora.ox.ac.uk/objects/uuid:706817c6-7ae3-4c43-9645-e65fa6b2de53/files/rfn106z61q,,,,,,,,
Relational Memory Core,Relational recurrent neural networks,https://arxiv.org/pdf/2302.11875,,,,,,,,
Cognitron,Cognitron: A self-organizing multilayered neural network,https://caiac.pubpub.org/pub/iooso56q/download/pdf,,,,,,,,
Conformer,Conformer: Convolution-augmented Transformer for Speech Recognition,https://arxiv.org/pdf/2305.05084,,,,,,,,
ADM,Diffusion Models Beat GANs on Image Synthesis,https://arxiv.org/pdf/2310.01251,,,,,,,,
DALL·E 2,Hierarchical Text-Conditional Image Generation with CLIP Latents,https://www.biorxiv.org/content/biorxiv/early/2023/10/23/2023.10.23.562918.full.pdf,,,,,,,,
Residual Dense Network,Residual Dense Network for Image Super-Resolution,https://arxiv.org/pdf/2112.12089,,,,,,,,
"Listen, Attend and Spell","Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",https://arxiv.org/pdf/2309.10926,,,,,,,,
MobileNet,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,https://arxiv.org/pdf/2309.08369,,,,,,,,
Zip CNN,Backpropagation Applied to Handwritten Zip Code Recognition,https://arxiv.org/pdf/2308.03741,,,,,,,,
TSN,Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,https://arxiv.org/pdf/2308.11070,,,,,,,,
XLNet,XLNet: Generalized Autoregressive Pretraining for Language Understanding,http://arxiv.org/pdf/2305.11840,,,,,,,,
Innervator,Designing Neural Networks using Genetic Algorithms,https://www.atlantis-press.com/article/125925606.pdf,,,,,,,,
Minerva (540B),Solving Quantitative Reasoning Problems with Language Models,http://arxiv.org/pdf/2303.03846,,,,,,,,
BigSSL,BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition,https://arxiv.org/pdf/2303.07533,,,,,,,,
AWD-LSTM,On the State of the Art of Evaluation in Neural Language Models,http://bphm.univ.kiev.ua/index.php/bphm/article/download/14/11,,,,,,,,
Gen-1,Structure and Content-Guided Video Synthesis with Diffusion Models,http://arxiv.org/pdf/2306.11503,,,,,,,,
DMN,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,https://ojs.aaai.org/index.php/AAAI/article/download/17488/17295,,,,,,,,
XLMR-XXL,Larger-Scale Transformers for Multilingual Masked Language Modeling,http://arxiv.org/pdf/2301.03728,,,,,,,,
Advantage Learning,Increasing the Action Gap: New Operators for Reinforcement Learning,https://ieeexplore.ieee.org/ielx7/5962385/6104215/09762369.pdf,,,,,,,,
DeepLab,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,https://arxiv.org/pdf/2212.03338,,,,,,,,
DistilBERT,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",http://arxiv.org/pdf/2305.17266,,,,,,,,
Empirical evaluation of deep architectures,An empirical evaluation of deep architectures on problems with many factors of variation,https://iopscience.iop.org/article/10.1088/2632-2153/ac8e4f/pdf,,,,,,,,
CogView,CogView: Mastering Text-to-Image Generation via Transformers,https://dl.acm.org/doi/pdf/10.1145/3618326,,,,,,,,
Xception,Xception: Deep Learning with Depthwise Separable Convolutions,https://arxiv.org/pdf/2309.17426,,,,,,,,
ResNeXt-101 Billion-scale,Billion-scale semi-supervised learning for image classification,https://zenodo.org/record/5006039/files/UPB-IJCV2021.pdf,,,,,,,,
GPipe (Amoeba),GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,http://arxiv.org/pdf/2304.11745,,,,,,,,
Rotation,Unsupervised Representation Learning by Predicting Image Rotations,http://arxiv.org/pdf/2206.00481,,,,,,,,
Diabetic Retinopathy Detection Net,Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/ddg.15194,,,,,,,,
DQN,Playing Atari with Deep Reinforcement Learning,https://arxiv.org/pdf/2308.10721,,,,,,,,
IBM Model 4,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,https://arxiv.org/pdf/2309.02010,,,,,,,,
RT-1,RT-1: Robotics Transformer for Real-World Control at Scale,http://arxiv.org/pdf/2304.08742,,,,,,,,
PaLM-E,PaLM-E: An Embodied Multimodal Language Model,http://arxiv.org/pdf/2305.19111,,,,,,,,
R-FCN,R-FCN: Object Detection via Region-based Fully Convolutional Networks,https://arxiv.org/pdf/2206.03017,,,,,,,,
Boss (DARPA Urban Challenge),Autonomous driving in urban environments: Boss and the Urban Challenge,https://hal.archives-ouvertes.fr/hal-01305749/file/Global_Navigation_ManagerITSC2015_aux.pdf,,,,,,,,
Fuzzy NN,"Multilayer perceptron, fuzzy sets, and classification",http://manuscript.elsevier.com/S0893608022001186/pdf/S0893608022001186.pdf,,,,,,,,
6-layer MLP (MNIST),"Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",https://www.frontiersin.org/articles/10.3389/fnins.2013.00178/pdf,,,,,,,,
ObjectNet,ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,http://arxiv.org/pdf/2207.00182,,,,,,,,
Sequence-based pattern recognition,Pattern recognition and modern computers,https://arxiv.org/pdf/1807.03165,,,,,,,,
YOLOv2,"YOLO9000: Better, Faster, Stronger",https://link.springer.com/content/pdf/10.1007/s11554-023-01353-0.pdf,,,,,,,,
Deeply-recursive ConvNet,Deeply-Recursive Convolutional Network for Image Super-Resolution,https://www.mdpi.com/1424-8220/23/1/370/pdf?version=1672749158,,,,,,,,
RNNsearch-50*,Neural Machine Translation by Jointly Learning to Align and Translate,https://link.springer.com/content/pdf/10.1007/s00521-023-08820-6.pdf,,,,,,,,
ADALINE,Adaptive switching circuits,https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1005796&type=printable,,,,,,,,
Diabetic Retinopathy Detection Net,Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.,https://www.medrxiv.org/content/medrxiv/early/2023/03/13/2023.03.09.23287058.full.pdf,,,,,,,,
ADALINE,Adaptive switching circuits,https://www.jneurosci.org/content/jneuro/28/42/10751.full.pdf,,,,,,,,
AlphaGo Master,Mastering the game of Go without human knowledge,https://arxiv.org/pdf/2301.13758,,,,,,,,
Stacked Denoising Autoencoders,Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion,https://ieeexplore.ieee.org/ielx7/6570655/6633080/10329432.pdf,,,,,,,,
XLNet,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://arxiv.org/pdf/2307.15920,,,,,,,,
Gradient Boosting Machine,Greedy function approximation: A gradient boosting machine.,https://www.mdpi.com/2227-9709/11/1/6/pdf?version=1706097033,,,,,,,,
PDP model for serial order,Serial Order: A Parallel Distributed Processing Approach,https://ieeexplore.ieee.org/ielx7/6287639/9312710/09395529.pdf,,,,,,,,
Diffractive Deep Neural Network,All-optical machine learning using diffractive deep neural networks,https://www.nature.com/articles/s41598-022-19973-0.pdf,,,,,,,,
T5-3B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://www.frontiersin.org/articles/10.3389/frai.2023.1350306/pdf?isPublishedV2=False,,,,,,,,
CLIP (ViT L/14@336px),Learning Transferable Visual Models From Natural Language Supervision,https://www.biorxiv.org/content/biorxiv/early/2024/01/26/2024.01.25.577209.full.pdf,,,,,,,,
SearchFusion,Selective Search for Object Recognition,http://arxiv.org/pdf/2208.04268,,,,,,,,
ALBERT,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,https://arxiv.org/pdf/2307.05567,,,,,,,,
DeBERTa,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,https://arxiv.org/pdf/2206.09676,,,,,,,,
HRA,Hybrid Reward Architecture for Reinforcement Learning,https://lib.dr.iastate.edu/cgi/viewcontent.cgi?article=8839&context=etd,,,,,,,,
Image-to-image cGAN,Image-to-Image Translation with Conditional Adversarial Networks,https://journals.itb.ac.id/index.php/jictra/article/download/19162/6296,,,,,,,,
Megatron-Turing NLG 530B,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2207.00032,,,,,,,,
Sparse Energy-Based Model,Efficient Learning of Sparse Representations with an Energy-Based Model,https://vtechworks.lib.vt.edu/bitstreams/c35aafdf-0277-40e7-bf61-a38557952e4f/download,,,,,,,,
ATLAS,UnifiedQA: Crossing Format Boundaries With a Single QA System,https://aclanthology.org/2021.naacl-main.45.pdf,,,,,,,,
StarGAN v2,StarGAN v2: Diverse Image Synthesis for Multiple Domains,https://arxiv.org/pdf/2303.09036,,,,,,,,
BigGAN-deep 512x512,Large Scale GAN Training for High Fidelity Natural Image Synthesis,http://arxiv.org/pdf/2212.05630,,,,,,,,
VGG16,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://csmj.mosuljournals.com/article_181628_3843cd8ea64c6b0428821be5dde8c2eb.pdf,,,,,,,,
Residual Dense Network,Residual Dense Network for Image Super-Resolution,https://www.mdpi.com/2079-9292/13/1/194/pdf?version=1704174929,,,,,,,,
PointNet++,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,https://link.springer.com/content/pdf/10.1007/s41095-023-0366-0.pdf,,,,,,,,
DiT-XL/2,Scalable Diffusion Models with Transformers,https://arxiv.org/pdf/2305.19066,,,,,,,,
TaLK Convolution,Time-aware Large Kernel Convolutions,http://arxiv.org/pdf/2306.01768,,,,,,,,
Learnability theory of language development,Language learnability and language development,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1207/s15516709cog0000_44,,,,,,,,
Projected GAN,Projected GANs Converge Faster,https://arxiv.org/pdf/2206.11404,,,,,,,,
GPT,Improving Language Understanding by Generative Pre-Training,https://arxiv.org/pdf/2310.06046,,,,,,,,
JFT,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,https://arxiv.org/pdf/2107.05617,,,,,,,,
JFT,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,https://arxiv.org/pdf/2212.02745,,,,,,,,
Naive Bayes,Pattern classification and scene analysis,https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLIII-B3-2020/605/2020/isprs-archives-XLIII-B3-2020-605-2020.pdf,,,,,,,,
RBM-tuning,A Practical Guide to Training Restricted Boltzmann Machines,https://aip.scitation.org/doi/pdf/10.1063/9.0000078,,,,,,,,
Base LM + kNN LM + Continuous Cache,Generalization through Memorization: Nearest Neighbor Language Models,http://arxiv.org/pdf/2303.01421,,,,,,,,
Multi-task Cascaded CNN,Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks,https://www.mdpi.com/1424-8220/23/22/9076/pdf?version=1699579815,,,,,,,,
NMT Transformer 437M,Massively Multilingual Neural Machine Translation,https://www.aclweb.org/anthology/W19-5319.pdf,,,,,,,,
CRF-RNN,Conditional Random Fields as Recurrent Neural Networks,https://digitalcollection.zhaw.ch/bitstream/11475/22256/3/2021_Simmler-etal_Learning-methods-labels-industrial-vision-applications_SDS.pdf,,,,,,,,
Megatron-LM (8.3B),Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,http://arxiv.org/pdf/2301.08984,,,,,,,,
data2vec (vision),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",http://arxiv.org/pdf/2305.17651,,,,,,,,
Neural Architecture Search with base 8 and shared embeddings,Neural Architecture Search with Reinforcement Learning,https://arxiv.org/pdf/2312.04918,,,,,,,,
Statistical Shape Constellations,Unsupervised Learning of Models for Recognition,http://www.cs.toronto.edu/~sven/Papers/pami09-skel.pdf,,,,,,,,
Dropout (ImageNet),Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/pdf/2211.13094,,,,,,,,
RCAN,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,https://arxiv.org/pdf/2303.08331,,,,,,,,
EfficientNet-L2,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://ieeexplore.ieee.org/ielx7/76/4358651/10102479.pdf,,,,,,,,
Decision tree (classification),Rapid object detection using a boosted cascade of simple features,https://www.mdpi.com/1424-8220/23/3/1505/pdf?version=1675756941,,,,,,,,
RETRO-7B,Improving language models by retrieving from trillions of tokens,http://arxiv.org/pdf/2304.05510,,,,,,,,
T5-3B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/pdf/2311.17786,,,,,,,,
Fractional Max-Pooling,Fractional Max-Pooling,https://arxiv.org/pdf/2010.01616,,,,,,,,
SpecAugment,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,https://arxiv.org/pdf/2207.05298,,,,,,,,
Hiero,A Hierarchical Phrase-Based Model for Statistical Machine Translation,https://escholarship.org/content/qt8kp924f2/qt8kp924f2.pdf?t=mtfqsy,,,,,,,,
NMT Transformer 437M,Massively Multilingual Neural Machine Translation,http://arxiv.org/pdf/2306.14913,,,,,,,,
Dropout (CIFAR),Improving neural networks by preventing co-adaptation of feature detectors,https://arxiv.org/pdf/2308.04391,,,,,,,,
GPipe (Amoeba),GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/ipr2.12853,,,,,,,,
ResNet-50 Billion-scale,Billion-scale semi-supervised learning for image classification,https://arxiv.org/pdf/2006.06666,,,,,,,,
IBM-5,The Mathematics of Statistical Machine Translation: Parameter Estimation,https://arxiv.org/pdf/1710.02086,,,,,,,,
CoAtNet,CoAtNet: Marrying Convolution and Attention for All Data Sizes,https://arxiv.org/pdf/2211.02292,,,,,,,,
Fast R-CNN,Fast R-CNN,https://www.mdpi.com/1424-8220/23/18/7685/pdf?version=1693987278,,,,,,,,
Transformer-XL Large,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,http://arxiv.org/pdf/2303.07305,,,,,,,,
LaMDA,LaMDA: Language Models for Dialog Applications,http://arxiv.org/pdf/2305.04008,,,,,,,,
YOLOv3,YOLOv3: An Incremental Improvement,https://dl.acm.org/doi/pdf/10.1145/3570361.3592531,,,,,,,,
NLP from scratch,Natural Language Processing (Almost) from Scratch,https://academiccommons.columbia.edu/doi/10.7916/0dsr-9w64/download,,,,,,,,
AlphaX-1,AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://arxiv.org/pdf/2202.05924,,,,,,,,
Codex,Evaluating Large Language Models Trained on Code,https://arxiv.org/pdf/2308.06261,,,,,,,,
Neocognitron,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,https://www.biorxiv.org/content/biorxiv/early/2023/10/23/2023.10.20.563031.full.pdf,,,,,,,,
Boss (DARPA Urban Challenge),Autonomous driving in urban environments: Boss and the Urban Challenge,https://www.emerald.com/insight/content/doi/10.1108/JICV-02-2018-0004/full/pdf?title=a-novel-intelligent-vehicle-risk-assessment-method-combined-with-multi-sensor-fusion-in-dense-traffic-environment,,,,,,,,
LUKE,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,https://aclanthology.org/2021.emnlp-main.211.pdf,,,,,,,,
GPT-3 175B (davinci),Language Models are Few-Shot Learners,https://aclanthology.org/2023.emnlp-industry.58.pdf,,,,,,,,
Adaptive Input Transformer + RD,R-Drop: Regularized Dropout for Neural Networks,http://arxiv.org/pdf/2203.11431,,,,,,,,
EfficientDet,EfficientDet: Scalable and Efficient Object Detection,http://www.scielo.cl/pdf/ingeniare/v31/0718-3305-ingeniare-31-12.pdf,,,,,,,,
Fuzzy NN,"Multilayer perceptron, fuzzy sets, and classification",https://www.mdpi.com/1999-4893/14/10/288/pdf?version=1634527261,,,,,,,,
DLRM-2020,Deep Learning Recommendation Model for Personalization and Recommendation Systems,http://arxiv.org/pdf/2301.02959,,,,,,,,
CoCa,CoCa: Contrastive Captioners are Image-Text Foundation Models,https://arxiv.org/pdf/2308.01907,,,,,,,,
Routing Transformer,Efficient Content-Based Sparse Attention with Routing Transformers,http://arxiv.org/pdf/2302.04542,,,,,,,,
Gopher (280B),"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",https://arxiv.org/pdf/2305.11391,,,,,,,,
Kohonen network,Self-organized formation of topologically correct feature maps,https://www.biorxiv.org/content/biorxiv/early/2021/03/24/2020.12.10.419549.full.pdf,,,,,,,,
AlphaGo Fan,Mastering the game of Go with deep neural networks and tree search,https://arxiv.org/pdf/2307.02991,,,,,,,,
NetTalk (transcription),Parallel Networks that Learn to Pronounce English Text,https://dspace.mit.edu/bitstream/1721.1/29142/2/42638539-MIT.pdf,,,,,,,,
LLaMA-7B,LLaMA: Open and Efficient Foundation Language Models,https://academic.oup.com/comjnl/advance-article-pdf/doi/10.1093/comjnl/bxad119/54705194/bxad119.pdf,,,,,,,,
HMM Word Alignment,HMM-Based Word Alignment in Statistical Translation,https://content.sciendo.com/downloadpdf/journals/pralin/93/1/article-p37.pdf,,,,,,,,
GLM-130B,GLM-130B: An Open Bilingual Pre-trained Model,https://arxiv.org/pdf/2308.05361,,,,,,,,
BPE,Neural Machine Translation of Rare Words with Subword Units,http://arxiv.org/pdf/2306.06620,,,,,,,,
SRN-Encoded Grammatical Structures,"Distributed representations, simple recurrent networks, and grammatical structure",https://ieeexplore.ieee.org/ielx7/6287639/6514899/09513285.pdf,,,,,,,,
ELMo,Deep Contextualized Word Representations,http://arxiv.org/pdf/2305.12816,,,,,,,,
Local Binary Patterns for facial recognition,Face Description with Local Binary Patterns: Application to Face Recognition,https://www.mdpi.com/2073-431X/10/9/113/pdf,,,,,,,,
Feedforward NN,Understanding the difficulty of training deep feedforward neural networks,https://www.igi-global.com/ViewTitle.aspx?TitleId=324086&isxn=9781668479711,,,,,,,,
Transformer-XL Large,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,https://arxiv.org/pdf/2208.10658,,,,,,,,
RetinaNet-R101,Focal Loss for Dense Object Detection,https://www.frontiersin.org/articles/10.3389/fphy.2023.1235856/pdf?isPublishedV2=False,,,,,,,,
CapsNet (MultiMNIST),Dynamic Routing Between Capsules,https://arxiv.org/pdf/2208.09203,,,,,,,,
CoCa,CoCa: Contrastive Captioners are Image-Text Foundation Models,http://arxiv.org/pdf/2304.06286,,,,,,,,
Spatially-Sparse CNN,Spatially-sparse convolutional neural networks,https://arxiv.org/pdf/2101.00443,,,,,,,,
SpecAugment,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,https://dl.acm.org/doi/pdf/10.1145/3597926.3598126,,,,,,,,
AlphaStar,Grandmaster level in StarCraft II using multi-agent reinforcement learning,http://arxiv.org/pdf/2303.04129,,,,,,,,
ProgressiveGAN,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",http://arxiv.org/pdf/2305.13625,,,,,,,,
GNMT,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,http://arxiv.org/pdf/2208.03473,,,,,,,,
DD-PPO,DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames,https://arxiv.org/pdf/2212.01186,,,,,,,,
Convolutional Pose Machines,Convolutional Pose Machines,https://arxiv.org/pdf/2012.07033,,,,,,,,
Maxout Networks,Maxout Networks,https://lirias.kuleuven.be/bitstream/123456789/654458/3/20-69-preprint.pdf,,,,,,,,
Adaptive Inputs + LayerDrop,Reducing Transformer Depth on Demand with Structured Dropout,https://aclanthology.org/2021.naacl-main.72.pdf,,,,,,,,
AR-LDM,Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models,http://arxiv.org/pdf/2306.00973,,,,,,,,
Stable Diffusion (LDM-KL-8-G),High-Resolution Image Synthesis with Latent Diffusion Models,https://www.biorxiv.org/content/biorxiv/early/2024/02/01/2024.02.01.578352.full.pdf,,,,,,,,
Phenaki,Phenaki: Variable Length Video Generation From Open Domain Textual Description,http://arxiv.org/pdf/2303.00750,,,,,,,,
Turing-NLG,DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters,http://arxiv.org/pdf/2305.01550,,,,,,,,
GloVe (6B),GloVe: Global Vectors for Word Representation,https://dl.acm.org/doi/pdf/10.1145/3604237.3626861,,,,,,,,
ERNIE 3.0 Titan,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,http://arxiv.org/pdf/2209.10372,,,,,,,,
SRN-Encoded Grammatical Structures,"Distributed representations, simple recurrent networks, and grammatical structure",https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1551-6709.2009.01076.x,,,,,,,,
Conv-DBN,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://figshare.com/articles/preprint/Sanare_Pluggable_Intrusion_Recovery_for_Web_Applications/13725991/1/files/26370301.pdf,,,,,,,,
RCAN,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,http://arxiv.org/pdf/2302.12831,,,,,,,,
Hopfield Networks (2020),Hopfield Networks is All You Need,http://arxiv.org/pdf/2210.08013,,,,,,,,
AlexNet,ImageNet classification with deep convolutional neural networks,https://www.mdpi.com/2079-9292/13/3/665/pdf?version=1707139703,,,,,,,,
Diffusion-GAN,Diffusion-GAN: Training GANs with Diffusion,http://arxiv.org/pdf/2302.10907,,,,,,,,
DiT-XL/2,Scalable Diffusion Models with Transformers,https://arxiv.org/pdf/2307.02347,,,,,,,,
Codex,Evaluating Large Language Models Trained on Code,https://arxiv.org/pdf/2308.11432,,,,,,,,
TD-Gammon,Practical issues in temporal difference learning,https://arxiv.org/pdf/0904.2595v1.pdf,,,,,,,,
LeNet-5,Gradient-based learning applied to document recognition,https://www.tandfonline.com/doi/pdf/10.1080/17538947.2024.2307999?needAccess=true,,,,,,,,
Decision tree (classification),Rapid object detection using a boosted cascade of simple features,http://arxiv.org/pdf/2304.07295,,,,,,,,
SPPNet,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,https://iopscience.iop.org/article/10.1088/1742-6596/2589/1/012003/pdf,,,,,,,,
SimpleNet,"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",https://kth.diva-portal.org/smash/get/diva2:1252156/FULLTEXT01,,,,,,,,
BLSTM for handwriting (1),A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks,https://arxiv.org/pdf/1502.07540,,,,,,,,
DDPM-IP (CelebA),Input Perturbation Reduces Exposure Bias in Diffusion Models,https://arxiv.org/pdf/2310.08442,,,,,,,,
Innervator,Designing Neural Networks using Genetic Algorithms,https://link.springer.com/content/pdf/bfm:978-3-030-61627-4/1?pdf=chapter%20toc,,,,,,,,
LRCN,Long-term recurrent convolutional networks for visual recognition and description,http://arxiv.org/pdf/2203.01289,,,,,,,,
DeepSpeech2 (English),Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin,https://arxiv.org/pdf/2203.07931,,,,,,,,
AWD-LSTM + MoS + Partial Shuffled,Improving Neural Language Modeling via Adversarial Training,https://aclanthology.org/2021.naacl-main.85.pdf,,,,,,,,
Youtube recommendation model,Deep Neural Networks for YouTube Recommendations,https://link.springer.com/content/pdf/10.1007/s00146-021-01289-8.pdf,,,,,,,,
PSPNet,Pyramid Scene Parsing Network,https://www.frontiersin.org/articles/10.3389/fnins.2023.1288366/pdf?isPublishedV2=False,,,,,,,,
DeLight,DeLighT: Deep and Light-weight Transformer,https://arxiv.org/pdf/2301.01146,,,,,,,,
HRA,Hybrid Reward Architecture for Reinforcement Learning,https://www.frontiersin.org/articles/10.3389/fnins.2019.00878/pdf,,,,,,,,
Error Propagation,Learning internal representations by error propagation,https://www.tandfonline.com/doi/pdf/10.1080/25765299.2022.2104224?needAccess=true&role=button,,,,,,,,
BLOOMZ-176B,Crosslingual Generalization through Multitask Finetuning,https://arxiv.org/pdf/2307.10928,,,,,,,,
EfficientNet-L2,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://www.mdpi.com/1424-8220/24/2/649/pdf?version=1705670401,,,,,,,,
Deep Boltzmann Machines,Deep Boltzmann Machines,https://e-archivo.uc3m.es/bitstream/10016/30787/2/evolutionary_N_2018_ps.pdf,,,,,,,,
T5-3B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://aclanthology.org/2023.findings-emnlp.611.pdf,,,,,,,,
ResNet-152 (ImageNet),Deep Residual Learning for Image Recognition,https://ijai.iaescore.com/index.php/IJAI/article/download/23339/13916,,,,,,,,
Diabetic Retinopathy Detection Net,Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.,https://www.mdpi.com/2075-4418/13/2/189/pdf?version=1673598307,,,,,,,,
Conformer,Conformer: Convolution-augmented Transformer for Speech Recognition,https://digibug.ugr.es/bitstream/10481/80609/1/IberSPEECH_2022_paper_29.pdf,,,,,,,,
Once for All,Once for All: Train One Network and Specialize it for Efficient Deployment,https://arxiv.org/pdf/2004.08955,,,,,,,,
MnasNet-A3,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://dr.ntu.edu.sg/bitstream/10356/155808/2/asp_dac2022.pdf,,,,,,,,
R-CNN (T-net),Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,https://www.nature.com/articles/s41598-023-38633-5.pdf,,,,,,,,
GShard (dense),GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,http://arxiv.org/pdf/2205.12904,,,,,,,,
NASv3 (CIFAR-10),Neural Architecture Search with Reinforcement Learning,https://arxiv.org/pdf/2206.01198,,,,,,,,
BigSSL,BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition,https://arxiv.org/pdf/2110.04621,,,,,,,,
YOLO,"You Only Look Once: Unified, Real-Time Object Detection",https://www.mdpi.com/2079-3197/11/12/244/pdf?version=1701676576,,,,,,,,
PaLM-E,PaLM-E: An Embodied Multimodal Language Model,http://arxiv.org/pdf/2306.02781,,,,,,,,
Unsupervised High-level Feature Learner,Building high-level features using large scale unsupervised learning,https://arxiv.org/pdf/2004.12725,,,,,,,,
OPT-175B,OPT: Open Pre-trained Transformer Language Models,http://arxiv.org/pdf/2306.09479,,,,,,,,
SVD in recommender systems,Application of Dimensionality Reduction in Recommender System - A Case Study,https://peerj.com/articles/cs-225.pdf,,,,,,,,
MV-RNN,Semantic Compositionality through Recursive Matrix-Vector Spaces,http://www.pertanika.upm.edu.my/resources/files/Pertanika PAPERS/inpress/jst/05 JST-4131-2022.pdf,,,,,,,,
LSTM with forget gates,Learning to Forget: Continual Prediction with LSTM,https://www.mdpi.com/1424-8220/24/1/226/pdf?version=1703938476,,,,,,,,
SimCSE,SimCSE: Simple Contrastive Learning of Sentence Embeddings,https://www.frontiersin.org/articles/10.3389/fnins.2023.1221970/pdf,,,,,,,,
Phenaki,Phenaki: Variable Length Video Generation From Open Domain Textual Description,http://arxiv.org/pdf/2303.04129,,,,,,,,
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),Regularizing and Optimizing LSTM Language Models,https://dl.acm.org/doi/pdf/10.1145/3491056,,,,,,,,
TD(0),An Adaptive Optimal Controller for Discrete-Time Markov Environments,https://arxiv.org/pdf/1811.00511,,,,,,,,
AlexNet,ImageNet classification with deep convolutional neural networks,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/adfm.202311821,,,,,,,,
Empirical evaluation of deep architectures,An empirical evaluation of deep architectures on problems with many factors of variation,https://library.oapen.org/bitstream/20.500.12657/23012/1/1007149.pdf,,,,,,,,
NLLB,No Language Left Behind: Scaling Human-Centered Machine Translation,http://arxiv.org/pdf/2302.05008,,,,,,,,
Inception-ResNet-V2,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cam4.6672,,,,,,,,
Conv-DBN,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,http://bonoi.org/index.php/si/article/download/113/89,,,,,,,,
Discriminator Guidance,Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models,https://arxiv.org/pdf/2307.02770,,,,,,,,
ViT-Base/32,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://downloads.hindawi.com/journals/jece/2024/9298478.pdf,,,,,,,,
FixRes ResNeXt-101 WSL,Fixing the train-test resolution discrepancy,https://arxiv.org/pdf/2210.06366,,,,,,,,
GroupLens,GroupLens: an open architecture for collaborative filtering of netnews,https://scientifica.umyu.edu.ng/index.php/scientifica/article/download/173/135,,,,,,,,
TrellisNet,Trellis Networks for Sequence Modeling,https://arxiv.org/pdf/2112.04709,,,,,,,,
Binarized Neural Network (MNIST),BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1,https://dl.acm.org/doi/pdf/10.1145/3613424.3614305,,,,,,,,
TFE SVM,A trainable feature extractor for handwritten digit recognition,https://www.mdpi.com/2073-8994/12/10/1742/pdf?version=1603276075,,,,,,,,
Gopher (280B),"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",http://arxiv.org/pdf/2205.11718,,,,,,,,
RNN+weight noise+dynamic eval,Generating Sequences With Recurrent Neural Networks,https://www.frontiersin.org/articles/10.3389/fspor.2021.682986/pdf,,,,,,,,
LSTM (2018),An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,http://arxiv.org/pdf/2301.13691,,,,,,,,
Statistical Shape Constellations,Unsupervised Learning of Models for Recognition,https://www.mdpi.com/2313-433X/6/7/61/pdf?version=1593671968,,,,,,,,
ViT-G/14 (LiT),LiT: Zero-Shot Transfer with Locked-image text Tuning,http://arxiv.org/pdf/2304.08386,,,,,,,,
TD-Gammon,Practical issues in temporal difference learning,https://www.annualreviews.org/doi/pdf/10.1146/annurev-statistics-010814-020120,,,,,,,,
CNN Best Practices,Best practices for convolutional neural networks applied to visual document analysis,https://arxiv.org/pdf/2203.15547,,,,,,,,
TrellisNet,Trellis Networks for Sequence Modeling,https://pubs.aip.org/asa/jasa/article-pdf/152/1/107/16525693/107_1_online.pdf,,,,,,,,
PSPNet,Pyramid Scene Parsing Network,https://www.mdpi.com/2306-5354/10/8/940/pdf?version=1691412367,,,,,,,,
Recursive sentiment autoencoder,Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions,https://direct.mit.edu/neco/article-pdf/34/4/991/2003085/neco_a_01483.pdf,,,,,,,,
iGPT-XL,Generative Pretraining From Pixels,http://arxiv.org/pdf/2210.10542,,,,,,,,
Refined Part Pooling,Beyond Part Models: Person Retrieval with Refined Part Pooling,https://www.mdpi.com/2076-3417/11/4/1775/pdf?version=1614059059,,,,,,,,
Base LM + kNN LM + Continuous Cache,Generalization through Memorization: Nearest Neighbor Language Models,http://arxiv.org/pdf/2305.06983,,,,,,,,
TensorReasoner,Reasoning With Neural Tensor Networks for Knowledge Base Completion,https://www.mdpi.com/1424-8220/23/22/9261/pdf?version=1700288266,,,,,,,,
Transformer-XL Large,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,https://arxiv.org/pdf/2308.09710,,,,,,,,
S4,Efficiently Modeling Long Sequences with Structured State Spaces,http://arxiv.org/pdf/2306.06635,,,,,,,,
Conv-DBN,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://www.mdpi.com/1999-4893/11/10/157/pdf?version=1540429441,,,,,,,,
Innervator,Designing Neural Networks using Genetic Algorithms,https://arxiv.org/pdf/2108.08474,,,,,,,,
BOXES,BOXES: AN EXPERIMENT IN ADAPTIVE CONTROL,https://arxiv.org/pdf/2304.06049,,,,,,,,
GPT-2 (1.5B),Language Models are Unsupervised Multitask Learners,https://www.mdpi.com/2079-9292/13/2/320/pdf?version=1704964811,,,,,,,,
RetinaNet-R101,Focal Loss for Dense Object Detection,https://www.mdpi.com/2075-4418/13/24/3604/pdf?version=1701773060,,,,,,,,
Rotation,Unsupervised Representation Learning by Predicting Image Rotations,https://arxiv.org/pdf/2205.14949,,,,,,,,
DnCNN,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,https://arxiv.org/pdf/2310.07887,,,,,,,,
GPT-3 175B (davinci),Language Models are Few-Shot Learners,https://www.medrxiv.org/content/medrxiv/early/2024/01/26/2024.01.26.24301810.full.pdf,,,,,,,,
Image-to-image cGAN,Image-to-Image Translation with Conditional Adversarial Networks,https://arxiv.org/pdf/2310.10414,,,,,,,,
YouTube Video Recommendation System,The YouTube video recommendation system,http://www.teses.usp.br/teses/disponiveis/3/3142/tde-06062012-104235/publico/TeseAislanFoinaRevFinal.pdf,,,,,,,,
PointNet++,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,https://arxiv.org/pdf/2308.13866,,,,,,,,
GloVe (6B),GloVe: Global Vectors for Word Representation,https://arxiv.org/pdf/2310.02229,,,,,,,,
MT-DNN,Multi-Task Deep Neural Networks for Natural Language Understanding,https://www.mdpi.com/2073-4441/14/4/674/pdf?version=1645445992,,,,,,,,
NÜWA,NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,https://arxiv.org/pdf/2310.07204,,,,,,,,
ViT-G (model soup),Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,https://arxiv.org/pdf/2204.08714,,,,,,,,
data2vec (vision),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",http://arxiv.org/pdf/2306.10058,,,,,,,,
MSA Transformer,MSA Transformer,http://arxiv.org/pdf/2306.12802,,,,,,,,
NetTalk (dictionary),Parallel Networks that Learn to Pronounce English Text,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09795013.pdf,,,,,,,,
BLOOMZ-176B,Crosslingual Generalization through Multitask Finetuning,https://arxiv.org/pdf/2307.07951,,,,,,,,
CLIP (ViT L/14@336px),Learning Transferable Visual Models From Natural Language Supervision,https://www.mdpi.com/1424-8220/24/1/126/pdf?version=1703576155,,,,,,,,
GLIDE,GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,https://arxiv.org/pdf/2310.06389,,,,,,,,
Deeply-supervised nets,Deeply-Supervised Nets,http://arxiv.org/pdf/2303.10396,,,,,,,,
Local Binary Patterns for facial recognition,Face Description with Local Binary Patterns: Application to Face Recognition,https://ieeexplore.ieee.org/ielx7/6221036/6352949/09961900.pdf,,,,,,,,
ALBERT,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,http://arxiv.org/pdf/2306.01108,,,,,,,,
PolyCoder,A systematic evaluation of large language models of code,http://arxiv.org/pdf/2303.18184,,,,,,,,
AlphaStar,Grandmaster level in StarCraft II using multi-agent reinforcement learning,http://arxiv.org/pdf/2211.11616,,,,,,,,
GShard (dense),GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,https://arxiv.org/pdf/2308.00951,,,,,,,,
FrameNet role labeling,Automatic Labeling of Semantic Roles,http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00183,,,,,,,,
ResNeXt-101 Billion-scale,Billion-scale semi-supervised learning for image classification,https://aclanthology.org/2021.emnlp-main.142.pdf,,,,,,,,
T0-XXL,Multitask Prompted Training Enables Zero-Shot Task Generalization,https://arxiv.org/pdf/2205.12113,,,,,,,,
InstructGPT,Training language models to follow instructions with human feedback,https://www.biorxiv.org/content/biorxiv/early/2024/01/24/2024.01.21.576542.full.pdf,,,,,,,,
Two-stream ConvNets for action recognition,Two-Stream Convolutional Networks for Action Recognition in Videos,https://dl.acm.org/doi/pdf/10.1145/3581783.3611868,,,,,,,,
DeepLabV3,Rethinking Atrous Convolution for Semantic Image Segmentation,https://iopscience.iop.org/article/10.1088/1742-6596/2562/1/012022/pdf,,,,,,,,
DeepSpeech2 (English),Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin,https://dl.acm.org/doi/pdf/10.1145/3474085.3475405,,,,,,,,
AlexNet,ImageNet classification with deep convolutional neural networks,https://dl.acm.org/doi/pdf/10.1145/3638554,,,,,,,,
MnasNet-A3,MnasNet: Platform-Aware Neural Architecture Search for Mobile,https://dl.acm.org/doi/pdf/10.1145/3533251,,,,,,,,
Symmetric Residual Encoder-Decoder Net,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,https://arxiv.org/pdf/1909.03573,,,,,,,,
BigGAN-deep 512x512,Large Scale GAN Training for High Fidelity Natural Image Synthesis,http://arxiv.org/pdf/2303.11649,,,,,,,,
IBM-5,The Mathematics of Statistical Machine Translation: Parameter Estimation,https://arxiv.org/pdf/1607.00578,,,,,,,,
LLaMA-65B,LLaMA: Open and Efficient Foundation Language Models,https://www.biorxiv.org/content/biorxiv/early/2023/11/15/2023.11.13.566825.full.pdf,,,,,,,,
Trajectory-pooled conv nets,Action recognition with trajectory-pooled deep-convolutional descriptors,https://arxiv.org/pdf/1711.11152,,,,,,,,
Big Transfer (BiT-L),Large Scale Learning of General Visual Representations for Transfer,https://arxiv.org/pdf/1905.10679,,,,,,,,
Adaptive Input Transformer + RD,R-Drop: Regularized Dropout for Neural Networks,https://aclanthology.org/2023.wmt-1.16.pdf,,,,,,,,
ERNIE 3.0 Titan,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,https://iopscience.iop.org/article/10.1088/1742-6596/2589/1/012020/pdf,,,,,,,,
Denoising Diffusion Probabilistic Models (LSUN Bedroom),Denoising Diffusion Probabilistic Models,https://www.mdpi.com/2313-433X/10/1/23/pdf?version=1705419895,,,,,,,,
Immediate trihead,Immediate-Head Parsing for Language Models,https://direct.mit.edu/coli/article-pdf/34/2/257/1798602/coli.2008.34.2.257.pdf,,,,,,,,
GloVe (32B),GloVe: Global Vectors for Word Representation,https://arxiv.org/pdf/2308.12612,,,,,,,,
Flamingo,Flamingo: a Visual Language Model for Few-Shot Learning,https://www.biorxiv.org/content/biorxiv/early/2023/12/23/2023.12.22.572990.full.pdf,,,,,,,,
Fisher-Boost,Improving the Fisher Kernel for Large-Scale Image Classification,https://www.mdpi.com/2072-4292/11/21/2525/pdf?version=1572328070,,,,,,,,
Youtube recommendation model,Deep Neural Networks for YouTube Recommendations,https://arxiv.org/pdf/2312.01760,,,,,,,,
Hiero,A Hierarchical Phrase-Based Model for Statistical Machine Translation,https://direct.mit.edu/coli/article-pdf/38/3/631/1810982/coli_a_00107.pdf,,,,,,,,
ResNet-50 Billion-scale,Billion-scale semi-supervised learning for image classification,https://arxiv.org/pdf/1911.04252,,,,,,,,
GPT-2 (1.5B),Language Models are Unsupervised Multitask Learners,https://www.ajol.info/index.php/jsasa/article/download/260311/245752,,,,,,,,
Wide Residual Network,Wide Residual Networks,http://arxiv.org/pdf/2303.10078,,,,,,,,
AlexaTM 20B,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,https://aclanthology.org/2023.findings-emnlp.117.pdf,,,,,,,,
Spatially-Sparse CNN,Spatially-sparse convolutional neural networks,https://ieeexplore.ieee.org/ielx7/6287639/8948470/08995603.pdf,,,,,,,,
Spatial Pyramid Matching,Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,https://www.mdpi.com/2072-4292/12/3/464/pdf?version=1581314199,,,,,,,,
DeepLabV3+,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,https://arxiv.org/pdf/2308.13767,,,,,,,,
ContextNet,ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context,https://arxiv.org/pdf/2203.00931,,,,,,,,
Maxout Networks,Maxout Networks,https://arxiv.org/pdf/2112.12089,,,,,,,,
Adaptive Input Transformer + RD,R-Drop: Regularized Dropout for Neural Networks,http://arxiv.org/pdf/2303.07665,,,,,,,,
PaLM-E,PaLM-E: An Embodied Multimodal Language Model,https://arxiv.org/pdf/2307.08775,,,,,,,,
GoogLeNet / InceptionV1,Going deeper with convolutions,https://arxiv.org/pdf/2310.16547,,,,,,,,
DLRM-2020,Deep Learning Recommendation Model for Personalization and Recommendation Systems,http://arxiv.org/pdf/2210.16691,,,,,,,,
CICERO,Human-level play in the game of Diplomacy by combining language models with strategic reasoning,http://arxiv.org/pdf/2302.10203,,,,,,,,
Mitosis,Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks,https://www.researchsquare.com/article/rs-1771334/latest.pdf,,,,,,,,
DensePhrases,Learning Dense Representations of Phrases at Scale,http://arxiv.org/pdf/2207.09068,,,,,,,,
IBM Model 4,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,https://downloads.hindawi.com/journals/ijis/2023/7434058.pdf,,,,,,,,
Hopfield Networks (2020),Hopfield Networks is All You Need,http://arxiv.org/pdf/2203.00936,,,,,,,,
ATLAS,UnifiedQA: Crossing Format Boundaries With a Single QA System,https://direct.mit.edu/daed/article-pdf/151/2/127/2060607/daed_a_01905.pdf,,,,,,,,
mT5-XXL,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,https://arxiv.org/pdf/2309.10539,,,,,,,,
GloVe (32B),GloVe: Global Vectors for Word Representation,https://dl.acm.org/doi/pdf/10.1145/3581783.3612523,,,,,,,,
mT0-13B,Crosslingual Generalization through Multitask Finetuning,https://aclanthology.org/2023.woah-1.6.pdf,,,,,,,,
GPT-2 (1.5B),Language Models are Unsupervised Multitask Learners,https://bmcbiol.biomedcentral.com/counter/pdf/10.1186/s12915-023-01803-y,,,,,,,,
YOLOv3,YOLOv3: An Incremental Improvement,https://www.biorxiv.org/content/biorxiv/early/2023/09/29/2023.09.27.559850.full.pdf,,,,,,,,
ProxylessNAS,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09766163.pdf,,,,,,,,
Whisper,Robust Speech Recognition via Large-Scale Weak Supervision,https://laccei.org/LACCEI2023-BuenosAires/papers/Contribution_1269_a.pdf,,,,,,,,
Convolutional Pose Machines,Convolutional Pose Machines,https://arxiv.org/pdf/2205.12066,,,,,,,,
DistilBERT,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",http://arxiv.org/pdf/2305.20045,,,,,,,,
Neural Architecture Search with base 8 and shared embeddings,Neural Architecture Search with Reinforcement Learning,https://arxiv.org/pdf/2208.10658,,,,,,,,
Recursive sentiment autoencoder,Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions,https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA210478,,,,,,,,
MADALINE III,"30 years of adaptive neural networks: perceptron, Madaline, and backpropagation",http://techlab.bu.edu/files/resources/articles_tt/Fuzzy ART neural network algorithm for classifying the power system faults.pdf,,,,,,,,
Base LM + kNN LM + Continuous Cache,Generalization through Memorization: Nearest Neighbor Language Models,https://arxiv.org/pdf/2210.15859,,,,,,,,
PhraseCond,Phase Conductor on Multi-layered Attentions for Machine Comprehension,https://onlinelibrary.wiley.com/doi/pdfdirect/10.4218/etrij.2018-0467,,,,,,,,
Cutout-regularized net,Improved Regularization of Convolutional Neural Networks with Cutout,http://arxiv.org/pdf/2208.13341,,,,,,,,
Stacked Denoising Autoencoders,Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion,https://www.mdpi.com/2079-9292/12/1/105/pdf?version=1672129602,,,,,,,,
Adversarial Joint Adaptation Network (ResNet),Deep Transfer Learning with Joint Adaptation Networks,https://www.mdpi.com/1424-8220/23/20/8406/pdf?version=1697091004,,,,,,,,
EMDR,End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering,https://arxiv.org/pdf/2205.09393,,,,,,,,
ShuffleNet v1,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,https://www.mdpi.com/2076-2615/13/23/3688/pdf?version=1701175655,,,,,,,,
Histograms of Oriented Gradients,Histograms of oriented gradients for human detection,https://link.springer.com/content/pdf/10.1007/s40747-023-01028-0.pdf,,,,,,,,
DensePhrases,Learning Dense Representations of Phrases at Scale,http://arxiv.org/pdf/2210.13678,,,,,,,,
ProtT5-XXL,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,https://www.biorxiv.org/content/biorxiv/early/2021/07/22/2021.07.21.453084.full.pdf,,,,,,,,
ResNet-50 Billion-scale,Billion-scale semi-supervised learning for image classification,http://arxiv.org/pdf/2205.05703,,,,,,,,
Recursive sentiment autoencoder,Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions,http://aclweb.org/anthology/P/P14/P14-1013.pdf,,,,,,,,
Mnemonic Reader,Reinforced Mnemonic Reader for Machine Reading Comprehension,https://ieeexplore.ieee.org/ielx7/5962385/9106620/08807370.pdf,,,,,,,,
LSTM-300units,LSTM Neural Networks for Language Modeling,http://arxiv.org/pdf/2303.05759,,,,,,,,
VD-LSTM+REAL Large,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,https://ieeexplore.ieee.org/ielx7/9670/9779590/09546691.pdf,,,,,,,,
BLOOMZ-176B,Crosslingual Generalization through Multitask Finetuning,https://arxiv.org/pdf/2310.01444,,,,,,,,
StarCoder,StarCoder: may the source be with you!,http://arxiv.org/pdf/2305.14775,,,,,,,,
HRA,Hybrid Reward Architecture for Reinforcement Learning,https://dl.acm.org/doi/pdf/10.1145/3449726.3459563,,,,,,,,
TD-Gammon,Practical issues in temporal difference learning,https://link.springer.com/content/pdf/10.1007/s10994-006-6888-8.pdf,,,,,,,,
Spatial Pyramid Matching,Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,https://eprints.lancs.ac.uk/id/eprint/133186/1/NN_FUFE.pdf,,,,,,,,
RetinaNet-R50,Focal Loss for Dense Object Detection,https://www.mdpi.com/1424-8220/24/2/317/pdf?version=1704440405,,,,,,,,
BigSSL,BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition,http://arxiv.org/pdf/2208.03067,,,,,,,,
Stacked hourglass network,Stacked Hourglass Networks for Human Pose Estimation,https://arxiv.org/pdf/2301.00695,,,,,,,,
MT-DNN,Multi-Task Deep Neural Networks for Natural Language Understanding,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/6D848C753AAA7DD978217645283F9DFE/S1351324921000371a.pdf/div-class-title-joint-learning-of-morphology-and-syntax-with-cross-level-contextual-information-flow-div.pdf,,,,,,,,
PreTrans-3L-250H,Speech recognition with deep recurrent neural networks,https://www.mdpi.com/2076-3417/12/18/8998/pdf?version=1662609256,,,,,,,,
Fully Convolutional Networks,Fully convolutional networks for semantic segmentation,https://arxiv.org/pdf/2310.01164,,,,,,,,
Seq2Seq LSTM,Sequence to Sequence Learning with Neural Networks,https://link.springer.com/content/pdf/10.1007/s10579-023-09671-2.pdf,,,,,,,,
Fractional Max-Pooling,Fractional Max-Pooling,https://arxiv.org/pdf/1706.05157,,,,,,,,
DiT-XL/2,Scalable Diffusion Models with Transformers,https://arxiv.org/pdf/2304.11118,,,,,,,,
PermuteFormer,PermuteFormer: Efficient Relative Position Encoding for Long Sequences,http://arxiv.org/pdf/2305.00355,,,,,,,,
2-layer-LSTM+Deep-Gradient-Compression,Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,http://arxiv.org/pdf/2301.00767,,,,,,,,
Bayesian automated hyperparameter tuning,Practical Bayesian Optimization of Machine Learning Algorithms,https://www.frontiersin.org/articles/10.3389/fpsyt.2023.1104222/pdf,,,,,,,,
FrameNet role labeling,Automatic Labeling of Semantic Roles,https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00201,,,,,,,,
ELMo,Deep Contextualized Word Representations,https://dl.acm.org/doi/pdf/10.1145/3630108,,,,,,,,
Decoupled weight decay regularization,Decoupled Weight Decay Regularization,https://www.nature.com/articles/s41598-023-50334-7.pdf,,,,,,,,
DensePhrases,Learning Dense Representations of Phrases at Scale,https://aclanthology.org/2022.trustnlp-1.1.pdf,,,,,,,,
DQN,Playing Atari with Deep Reinforcement Learning,http://arxiv.org/pdf/2306.03552,,,,,,,,
Kohonen network,Self-organized formation of topologically correct feature maps,https://www.mdpi.com/2076-3417/12/5/2679/pdf?version=1646639217,,,,,,,,
Phenaki,Phenaki: Variable Length Video Generation From Open Domain Textual Description,https://arxiv.org/pdf/2308.08089,,,,,,,,
Big-Little Net (speech),Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition,https://www.melba-journal.org/pdf/2022:026.pdf,,,,,,,,
DETR,End-to-End Object Detection with Transformers,https://dl.acm.org/doi/pdf/10.1145/3581783.3613795,,,,,,,,
YOLOv3,YOLOv3: An Incremental Improvement,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/ipr2.12940,,,,,,,,
ContextNet + Noisy Student,Improved Noisy Student Training for Automatic Speech Recognition,http://arxiv.org/pdf/2306.03773,,,,,,,,
Part-of-sentence tagging model,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,http://arxiv.org/pdf/2203.13920,,,,,,,,
SRU++ Large,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,https://arxiv.org/pdf/2205.12674,,,,,,,,
BASIC-L + Lion,Symbolic Discovery of Optimization Algorithms,https://arxiv.org/pdf/2304.07580,,,,,,,,
MobileNet,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,https://strathprints.strath.ac.uk/82846/1/Chen_etal_ToR_2022_Data_augmentation_and_intelligent_fault_diagnosis_of_planetary_gearbox_using_ILoFGAN.pdf,,,,,,,,
6-layer MLP (MNIST),"Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",https://www.mdpi.com/2227-7080/9/1/20/pdf?version=1615797891,,,,,,,,
AmoebaNet-A (F=448),Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/pdf/2312.10560,,,,,,,,
Q-learning,Learning from delayed rewards,https://ieeexplore.ieee.org/ielx7/6287639/9668973/09665743.pdf,,,,,,,,
DINOv2,DINOv2: Learning Robust Visual Features without Supervision,http://arxiv.org/pdf/2304.12210,,,,,,,,
MetNet,MetNet: A Neural Weather Model for Precipitation Forecasting,https://www.mdpi.com/2072-4292/14/7/1750/pdf?version=1649222137,,,,,,,,
ERNIE-GEN (large),ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation,https://dl.acm.org/doi/pdf/10.1145/3411763.3450391,,,,,,,,
Sparse coding model for V1 receptive fields,Sparse coding with an overcomplete basis set: A strategy employed by V1?,http://arxiv.org/pdf/2205.00920,,,,,,,,
TD(0),An Adaptive Optimal Controller for Discrete-Time Markov Environments,https://europepmc.org/articles/pmc3808183?pdf=render,,,,,,,,
Constituency-Tree LSTM,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,https://arxiv.org/pdf/2101.10642,,,,,,,,
Mnemonic Reader,Reinforced Mnemonic Reader for Machine Reading Comprehension,http://arxiv.org/pdf/2210.02621,,,,,,,,
GNMT,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,https://www.mdpi.com/2076-3417/12/19/10074/pdf?version=1665297910,,,,,,,,
Gradient Boosting Machine,Greedy function approximation: A gradient boosting machine.,https://www.mdpi.com/2072-4292/15/20/5021/pdf?version=1697700766,,,,,,,,
λ-WASP,Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus,https://www.aclweb.org/anthology/D17-1125.pdf,,,,,,,,
Cutout-regularized net,Improved Regularization of Convolutional Neural Networks with Cutout,http://arxiv.org/pdf/2211.00113,,,,,,,,
FAST,Machine Learning for High-Speed Corner Detection,https://www.tandfonline.com/doi/pdf/10.1080/10095020.2023.2224837?needAccess=true&role=button,,,,,,,,
Gradient Boosting Machine,Greedy function approximation: A gradient boosting machine.,https://www.nature.com/articles/s41598-023-44859-0.pdf,,,,,,,,
Denoising Autoencoders,Extracting and composing robust features with denoising autoencoders,http://arxiv.org/pdf/2306.04600,,,,,,,,
ResNet-110 (CIFAR-10),Deep Residual Learning for Image Recognition,https://www.biorxiv.org/content/biorxiv/early/2024/02/07/2024.02.06.579067.full.pdf,,,,,,,,
MuZero,"Mastering Atari, Go, chess and shogi by planning with a learned model",https://arxiv.org/pdf/2309.05582,,,,,,,,
ConvNet similarity metric,"Learning a similarity metric discriminatively, with application to face verification",https://arxiv.org/pdf/2301.09164,,,,,,,,
KEPLER,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,http://arxiv.org/pdf/2301.01172,,,,,,,,
FixRes ResNeXt-101 WSL,Fixing the train-test resolution discrepancy,https://link.springer.com/content/pdf/10.1007/s42600-021-00151-6.pdf,,,,,,,,
Domain Adaptation,Domain adaptation for object recognition: An unsupervised approach,https://ojs.aaai.org/index.php/AAAI/article/download/12313/12172,,,,,,,,
HuBERT,HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,https://arxiv.org/pdf/2305.05084,,,,,,,,
Retrieval-Augmented Generator,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,https://arxiv.org/pdf/2206.13163,,,,,,,,
GPT-3 175B (davinci),Language Models are Few-Shot Learners,https://www.igi-global.com/ViewTitle.aspx?TitleId=338222&isxn=9798369324578,,,,,,,,
PDP model for serial order,Serial Order: A Parallel Distributed Processing Approach,https://www.mdpi.com/2079-9292/10/1/67/pdf?version=1609501173,,,,,,,,
NASv3 (CIFAR-10),Neural Architecture Search with Reinforcement Learning,https://arxiv.org/pdf/2309.14293,,,,,,,,
AlphaStar,Grandmaster level in StarCraft II using multi-agent reinforcement learning,https://arxiv.org/pdf/2303.03395,,,,,,,,
HyperNEAT,A Neuroevolution Approach to General Atari Game Playing,https://research.aalto.fi/files/44627999/CHEM_Ikonen_et_al_Reinforcement_learning_Computers_and_Chemical_Engineering.pdf,,,,,,,,
TriNet,In Defense of the Triplet Loss for Person Re-Identification,http://arxiv.org/pdf/2204.07442,,,,,,,,
GOAT,Open-Ended Learning Leads to Generally Capable Agents,http://arxiv.org/pdf/2306.03034,,,,,,,,
ShuffleNet v1,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,https://iopscience.iop.org/article/10.1088/1361-6501/acb2e1/pdf,,,,,,,,
AlphaCode,Competition-level code generation with AlphaCode,https://link.springer.com/content/pdf/10.1007/s10489-022-04078-y.pdf,,,,,,,,
Fully Convolutional Networks,Fully convolutional networks for semantic segmentation,http://journals.pan.pl/Content/112085/PDF/21_363-376_00946_Bpast.No.67-2_06.02.20.pdf,,,,,,,,
Unsupervised High-level Feature Learner,Building high-level features using large scale unsupervised learning,https://arxiv.org/pdf/2201.07383,,,,,,,,
BellKor 2008,The BellKor 2008 Solution to the Netflix Prize,https://arxiv.org/pdf/2006.11011,,,,,,,,
Make-A-Video,Make-A-Video: Text-to-Video Generation without Text-Video Data,https://arxiv.org/pdf/2308.09710,,,,,,,,
IMPALA,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://arxiv.org/pdf/2308.13661,,,,,,,,
Feedforward NN,Understanding the difficulty of training deep feedforward neural networks,https://arxiv.org/pdf/2310.07194,,,,,,,,
DeBERTa,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,http://arxiv.org/pdf/2204.03324,,,,,,,,
MCDNN (MNIST),Multi-column deep neural networks for image classification,https://hal.inria.fr/hal-03136537/document,,,,,,,,
Optimized Multi-Scale Edge Detection,A Computational Approach to Edge Detection,https://www.mdpi.com/2076-3263/14/2/29/pdf?version=1706265711,,,,,,,,
OPT-175B,OPT: Open Pre-trained Transformer Language Models,https://arxiv.org/pdf/2308.15851,,,,,,,,
Transformer-XL Large,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,https://arxiv.org/pdf/2310.06851,,,,,,,,
LLaMA-65B,LLaMA: Open and Efficient Foundation Language Models,https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00808-1,,,,,,,,
Character-enriched word2vec,Enriching Word Vectors with Subword Information,https://www.ijcai.org/proceedings/2023/0570.pdf,,,,,,,,
Mitosis,Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks,https://dl.acm.org/doi/pdf/10.1145/3296957.3173212,,,,,,,,
2-layer-LSTM+Deep-Gradient-Compression,Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,http://arxiv.org/pdf/2207.03481,,,,,,,,
JFT,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,https://www.mdpi.com/1424-8220/22/11/4011/pdf?version=1653554430,,,,,,,,
RT-2,RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control,https://arxiv.org/pdf/2309.12089,,,,,,,,
AlphaGo Lee,Mastering the game of Go with deep neural networks and tree search,https://arxiv.org/pdf/2310.02782,,,,,,,,
LongT5,LongT5: Efficient Text-To-Text Transformer for Long Sequences,http://arxiv.org/pdf/2301.06627,,,,,,,,
wave2vec 2.0 LARGE,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,https://arxiv.org/pdf/2309.00169,,,,,,,,
CoCa,CoCa: Contrastive Captioners are Image-Text Foundation Models,https://arxiv.org/pdf/2305.16556,,,,,,,,
ViT-G (model soup),Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,https://arxiv.org/pdf/2302.11075,,,,,,,,
Optimized Multi-Scale Edge Detection,A Computational Approach to Edge Detection,http://arxiv.org/pdf/2306.12155,,,,,,,,
MS-CNN,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,http://repository.essex.ac.uk/28844/1/itis17_cardetect_cmr.pdf,,,,,,,,
SimpleNet,"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",https://www.mdpi.com/1424-8220/21/23/8077/pdf?version=1638870013,,,,,,,,
Internal functionality of visual invariants,The internal representation of solid shape with respect to vision,https://arxiv.org/pdf/1012.2491,,,,,,,,
6-layer MLP (MNIST),"Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",https://link.springer.com/content/pdf/10.1007%2F978-3-030-30493-5_74.pdf,,,,,,,,
Relational Memory Core,Relational recurrent neural networks,https://doi.org/10.3906/elk-2008-94,,,,,,,,
Gopher (280B),"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",http://arxiv.org/pdf/2302.00871,,,,,,,,
AmoebaNet-A (F=448),Regularized Evolution for Image Classifier Architecture Search,https://dl.acm.org/doi/pdf/10.1145/3539597.3570407,,,,,,,,
RBM-tuning,A Practical Guide to Training Restricted Boltzmann Machines,https://www.frontiersin.org/articles/10.3389/frai.2020.00004/pdf,,,,,,,,
Tranception,Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval,https://arxiv.org/pdf/2206.13517,,,,,,,,
Parti,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,https://arxiv.org/pdf/2211.02408,,,,,,,,
GPT-3 175B (davinci),Language Models are Few-Shot Learners,https://www.biorxiv.org/content/biorxiv/early/2024/01/26/2024.01.23.576654.full.pdf,,,,,,,,
FTW,Human-level performance in 3D multiplayer games with population-based reinforcement learning,http://arxiv.org/pdf/2301.07608,,,,,,,,
Deeply-supervised nets,Deeply-Supervised Nets,http://arxiv.org/pdf/2003.07326,,,,,,,,
Spatial Pyramid Matching,Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,https://bmcmedimaging.biomedcentral.com/track/pdf/10.1186/s12880-021-00694-1,,,,,,,,
λ-WASP,Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus,https://aclanthology.org/2021.naacl-main.220.pdf,,,,,,,,
DenseNet-264,Densely Connected Convolutional Networks,https://link.springer.com/content/pdf/10.1007/s10278-023-00926-6.pdf,,,,,,,,
ALBERT-xxlarge,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,https://arxiv.org/pdf/2304.06947,,,,,,,,
Residual Dense Network,Residual Dense Network for Image Super-Resolution,https://arxiv.org/pdf/2310.19288,,,,,,,,
Enhanced Neighborhood-Based Filtering,Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09270002.pdf,,,,,,,,
Q-learning,Learning from delayed rewards,https://downloads.hindawi.com/journals/mse/2021/4641450.pdf,,,,,,,,
TD-Gammon,Practical issues in temporal difference learning,https://dspace.mit.edu/bitstream/1721.1/64641/1/Bertsekas_Pathologies%20of.pdf,,,,,,,,
Imagen,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,https://arxiv.org/pdf/2310.02596,,,,,,,,
Conformer,Conformer: Convolution-augmented Transformer for Speech Recognition,https://arxiv.org/pdf/2312.09760,,,,,,,,
MegaSyn,Dual use of artificial-intelligence-powered drug discovery,https://arxiv.org/pdf/2206.11267,,,,,,,,
Iterative Bootstrapping WSD,Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,https://arxiv.org/pdf/1811.03402,,,,,,,,
Dropout-LSTM+Noise(Bernoulli) (WT2),Noisin: Unbiased Regularization for Recurrent Neural Networks,https://research-repository.griffith.edu.au/bitstream/10072/400664/2/Ma456654-Accepted.pdf,,,,,,,,
Retrieval-Augmented Generator,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,https://arxiv.org/pdf/2306.11182,,,,,,,,
Wide Residual Network,Wide Residual Networks,https://www.mdpi.com/2072-6643/15/7/1728/pdf?version=1680263206,,,,,,,,
Minerva (540B),Solving Quantitative Reasoning Problems with Language Models,https://arxiv.org/pdf/2310.00656,,,,,,,,
DnCNN,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,https://www.nature.com/articles/s42256-023-00689-3.pdf,,,,,,,,
ConSERT,ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer,https://arxiv.org/pdf/2310.04456,,,,,,,,
RNN 500/10 + RT09 LM (NIST RT05),Recurrent neural network based language model,https://arxiv.org/pdf/2109.09317,,,,,,,,
MatrixFac for Recommenders,Matrix Factorization Techniques for Recommender Systems,https://www.mdpi.com/2227-7390/12/2/320/pdf?version=1705589951,,,,,,,,
KEPLER,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,http://arxiv.org/pdf/2305.12092,,,,,,,,
YOLO,"You Only Look Once: Unified, Real-Time Object Detection",https://bmcgastroenterol.biomedcentral.com/counter/pdf/10.1186/s12876-023-03067-w,,,,,,,,
EfficientDet,EfficientDet: Scalable and Efficient Object Detection,https://arxiv.org/pdf/2307.16751,,,,,,,,
Make-A-Video,Make-A-Video: Text-to-Video Generation without Text-Video Data,http://arxiv.org/pdf/2305.12940,,,,,,,,
Deeply-recursive ConvNet,Deeply-Recursive Convolutional Network for Image Super-Resolution,https://www.mdpi.com/1424-8220/21/20/6870/pdf?version=1634698743,,,,,,,,
Gradient Boosting Machine,Greedy function approximation: A gradient boosting machine.,https://www.nature.com/articles/s41370-023-00624-z.pdf,,,,,,,,
BERT-Large-CAS (PTB+WT2+WT103),Language Models with Transformers,https://ojs.aaai.org/index.php/AAAI/article/download/6451/6307,,,,,,,,
HOGWILD!,Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent,https://hal.archives-ouvertes.fr/hal-02373338v6/file/article.pdf,,,,,,,,
GOAT,Open-Ended Learning Leads to Generally Capable Agents,http://arxiv.org/pdf/2304.13174,,,,,,,,
DCN+,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,https://www.aclweb.org/anthology/D19-1284.pdf,,,,,,,,
Cross-lingual alignment,"Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing",https://www.mdpi.com/2076-3417/11/5/1974/pdf?version=1614411414,,,,,,,,
LSTM with forget gates,Learning to Forget: Continual Prediction with LSTM,http://arxiv.org/pdf/2210.04615,,,,,,,,
Flan-PaLM 540B,Scaling Instruction-Finetuned Language Models,https://arxiv.org/pdf/2310.09107,,,,,,,,
Optimized Multi-Scale Edge Detection,A Computational Approach to Edge Detection,http://arxiv.org/pdf/2301.10165,,,,,,,,
ULM-FiT,Universal Language Model Fine-tuning for Text Classification,https://arxiv.org/pdf/2211.01979,,,,,,,,
Rotation,Unsupervised Representation Learning by Predicting Image Rotations,https://arxiv.org/pdf/2301.06957,,,,,,,,
Part-of-sentence tagging model,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,https://ojs.aaai.org/index.php/AAAI/article/download/17773/17580,,,,,,,,
SPN-4+KN5,Language modeling with sum-product networks,https://download.atlantis-press.com/article/25882991.pdf,,,,,,,,
CogView,CogView: Mastering Text-to-Image Generation via Transformers,https://downloads.hindawi.com/journals/scn/2023/9597905.pdf,,,,,,,,
DeLight,DeLighT: Deep and Light-weight Transformer,https://arxiv.org/pdf/2303.00448,,,,,,,,
Transformer + Simple Recurrent Unit,Simple Recurrent Units for Highly Parallelizable Recurrence,https://research.edgehill.ac.uk/files/68212565/TIM.pdf,,,,,,,,
GPipe (Transformer),GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://arxiv.org/pdf/2203.02839,,,,,,,,
Denoising Autoencoders,Extracting and composing robust features with denoising autoencoders,https://arxiv.org/pdf/2206.03066,,,,,,,,
Deep Deterministic Policy Gradients,Continuous control with deep reinforcement learning,https://arxiv.org/pdf/2310.13809,,,,,,,,
LLaMA-65B,LLaMA: Open and Efficient Foundation Language Models,https://aclanthology.org/2023.arabicnlp-1.9.pdf,,,,,,,,
Error Propagation,Learning internal representations by error propagation,https://www.mdpi.com/1099-4300/25/5/831/pdf?version=1684832871,,,,,,,,
Cross-lingual alignment,"Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing",https://arxiv.org/pdf/2211.08547,,,,,,,,
ByT5-XXL,ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models,https://arxiv.org/pdf/2309.11549,,,,,,,,
iGPT-L,Generative Pretraining From Pixels,https://arxiv.org/pdf/2210.16870,,,,,,,,
ENAS,Efficient Neural Architecture Search via Parameter Sharing,https://arxiv.org/pdf/2210.09084,,,,,,,,
Spatially-Sparse CNN,Spatially-sparse convolutional neural networks,https://arxiv.org/pdf/2305.09504,,,,,,,,
RCAN,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,http://arxiv.org/pdf/2302.11184,,,,,,,,
FTW,Human-level performance in 3D multiplayer games with population-based reinforcement learning,https://arxiv.org/pdf/2011.12582,,,,,,,,
S-Norm,Simple and Effective Multi-Paragraph Reading Comprehension,https://www.aclweb.org/anthology/2020.nuse-1.13.pdf,,,,,,,,
LUKE,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,https://arxiv.org/pdf/2305.05862,,,,,,,,
TCN (P-MNIST),Convolutional Sequence Modeling Revisited,https://dergipark.org.tr/tr/download/article-file/1474355,,,,,,,,
Deep LSTM for video classification,Beyond short snippets: Deep networks for video classification,https://figshare.com/articles/conference_contribution/A_Lightweight_Action_Recognition_Method_for_Unmanned-Aerial-Vehicle_video/14170880/1/files/26707469.pdf,,,,,,,,
AlphaFold-Multimer,Protein complex prediction with AlphaFold-Multimer,https://academic.oup.com/genetics/advance-article-pdf/doi/10.1093/genetics/iyad175/51863217/iyad175.pdf,,,,,,,,
RNNsearch-50*,Neural Machine Translation by Jointly Learning to Align and Translate,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/6F66EDB58D240DF00F6324C8B6F3B9B9/S2634460223000262a.pdf/div-class-title-leveraging-spatiotemporal-information-in-meteorological-image-sequences-from-feature-engineering-to-neural-networks-div.pdf,,,,,,,,
Deep Deterministic Policy Gradients,Continuous control with deep reinforcement learning,https://downloads.hindawi.com/journals/ijis/2024/6740701.pdf,,,,,,,,
FrameNet role labeling,Automatic Labeling of Semantic Roles,http://dl.acm.org/ft_gateway.cfm?id=1626500&type=pdf,,,,,,,,
ST-MoE,ST-MoE: Designing Stable and Transferable Sparse Expert Models,http://arxiv.org/pdf/2302.11529,,,,,,,,
ViT-G (model soup),Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,https://arxiv.org/pdf/2308.01175,,,,,,,,
StarGAN v2,StarGAN v2: Diverse Image Synthesis for Multiple Domains,https://arxiv.org/pdf/2110.02711,,,,,,,,
Inception-ResNet-V2,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",https://arxiv.org/pdf/2310.05768,,,,,,,,
ViT-G/14,Scaling Vision Transformers,http://arxiv.org/pdf/2304.03589,,,,,,,,
VD-RHN,Recurrent Highway Networks,https://www.aclweb.org/anthology/W17-4710.pdf,,,,,,,,
InstructGPT,Training language models to follow instructions with human feedback,https://www.mdpi.com/2076-3417/14/2/898/pdf?version=1705890862,,,,,,,,
Fuzzy NN,"Multilayer perceptron, fuzzy sets, and classification",https://www.i-jmr.org/2022/1/e28366/PDF,,,,,,,,
Relational Memory Core,Relational recurrent neural networks,https://www.biorxiv.org/content/biorxiv/early/2019/08/01/561761.full.pdf,,,,,,,,
AMDIM,Learning Representations by Maximizing Mutual Information Across Views,https://arxiv.org/pdf/2103.11568,,,,,,,,
Hybrid H3-2.7B,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,http://arxiv.org/pdf/2304.12776,,,,,,,,
Gen-1,Structure and Content-Guided Video Synthesis with Diffusion Models,http://arxiv.org/pdf/2305.18670,,,,,,,,
Deep Multitask NLP Network,A unified architecture for natural language processing: deep neural networks with multitask learning,https://arxiv.org/pdf/2212.00105,,,,,,,,
VGG19,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://ijeecs.iaescore.com/index.php/IJEECS/article/download/30029/17866,,,,,,,,
DeepFace,DeepFace: Closing the Gap to Human-Level Performance in Face Verification,https://acaa-p.com/index.php/acj/article/download/47/36,,,,,,,,
Prototypical networks,Prototypical Networks for Few-shot Learning,https://www.frontiersin.org/articles/10.3389/fmolb.2023.1147514/pdf,,,,,,,,
MoCo,Momentum Contrast for Unsupervised Visual Representation Learning,https://arxiv.org/pdf/2311.02378,,,,,,,,
LSTM-300units,LSTM Neural Networks for Language Modeling,https://ojs.aaai.org/index.php/AAAI/article/download/5363/5219,,,,,,,,
Multi-scale Dilated CNN,Multi-Scale Context Aggregation by Dilated Convolutions,https://www.mdpi.com/1424-8220/23/2/841/pdf?version=1673430706,,,,,,,,
SVD in recommender systems,Application of Dimensionality Reduction in Recommender System - A Case Study,https://www.atlantis-press.com/article/2052.pdf,,,,,,,,
Neuro-Symbolic Concept Learner,"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",http://arxiv.org/pdf/2209.13517,,,,,,,,
GPipe (Transformer),GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://repository.hkust.edu.hk/ir/bitstream/1783.1-125989/1/125989-1.pdf,,,,,,,,
Pandemonium (morse),Pandemonium: a paradigm for learning,https://link.springer.com/content/pdf/10.1007/978-0-387-35318-0_9.pdf,,,,,,,,
GroupLens,GroupLens: an open architecture for collaborative filtering of netnews,https://arxiv.org/pdf/1710.07072,,,,,,,,
NÜWA,NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,http://arxiv.org/pdf/2304.06818,,,,,,,,
SRGAN,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,http://arxiv.org/pdf/2302.10341,,,,,,,,
DistilBERT,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",https://dl.acm.org/doi/pdf/10.1145/3539618.3592063,,,,,,,,
Word2Vec (small),Distributed Representations of Words and Phrases and their Compositionality,https://www.mdpi.com/1424-8220/23/24/9881/pdf?version=1702804291,,,,,,,,
Trajectory-pooled conv nets,Action recognition with trajectory-pooled deep-convolutional descriptors,http://arxiv.org/pdf/2208.03444,,,,,,,,
Sparse coding model for V1 receptive fields,Sparse coding with an overcomplete basis set: A strategy employed by V1?,https://www.mdpi.com/1099-4300/23/10/1251/pdf?version=1634553907,,,,,,,,
Word2Vec (large),Distributed Representations of Words and Phrases and their Compositionality,https://jwcn-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13638-023-02283-y,,,,,,,,
Social and content-based classification,Recommendation as Classification: Using Social and Content-Based Information in Recommendation,https://riunet.upv.es/bitstream/10251/35736/8/INS.pdf,,,,,,,,
SemVec,Linguistic Regularities in Continuous Space Word Representations,https://www.tandfonline.com/doi/pdf/10.1080/01615440.2020.1760157?needAccess=true,,,,,,,,
PaLI,PaLI: A Jointly-Scaled Multilingual Language-Image Model,https://arxiv.org/pdf/2209.05534,,,,,,,,
BIDAF,Bidirectional Attention Flow for Machine Comprehension,https://www.aclweb.org/anthology/D19-1418.pdf,,,,,,,,
Empirical evaluation of deep architectures,An empirical evaluation of deep architectures on problems with many factors of variation,http://openaccess.ihu.edu.tr/xmlui/bitstream/20.500.12154/1698/6/delen_d.pdf,,,,,,,,
Tagging via Viterbi Decoding,Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms,https://www.aclweb.org/anthology/P16-1088.pdf,,,,,,,,
SENet (ImageNet),Squeeze-and-Excitation Networks,https://www.nature.com/articles/s41598-024-54124-7.pdf,,,,,,,,
CTC-Trained LSTM,Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,https://arxiv.org/pdf/2305.12493,,,,,,,,
Bidirectional RNN,Bidirectional recurrent neural networks,https://www.frontiersin.org/articles/10.3389/fninf.2023.1067095/pdf,,,,,,,,
LeNet-5,Gradient-based learning applied to document recognition,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/bies.202300114,,,,,,,,
Megatron-LM (8.3B),Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://dl.acm.org/doi/pdf/10.1145/3567955.3567959,,,,,,,,
DeepLabV3+,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,https://arxiv.org/pdf/2308.10488,,,,,,,,
Zip CNN,Backpropagation Applied to Handwritten Zip Code Recognition,https://arxiv.org/pdf/2311.11847,,,,,,,,
PointNet,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,https://www.mdpi.com/2076-3417/13/17/9636/pdf?version=1692968873,,,,,,,,
Dropout (ImageNet),Improving neural networks by preventing co-adaptation of feature detectors,https://www.biorxiv.org/content/biorxiv/early/2023/08/29/2023.08.28.555020.full.pdf,,,,,,,,
W2v-BERT,w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,https://arxiv.org/pdf/2302.09908,,,,,,,,
VD-RHN,Recurrent Highway Networks,https://discovery.ucl.ac.uk/10127962/1/Nature_Protocols%20accepted.pdf,,,,,,,,
T5-11B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://aclanthology.org/2023.arabicnlp-1.23.pdf,,,,,,,,
GShard (dense),GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,https://arxiv.org/pdf/2301.13310,,,,,,,,
Statistical Shape Constellations,Unsupervised Learning of Models for Recognition,http://vision.ucla.edu/papers/vedaldiS08.pdf,,,,,,,,
DeepLab (2017),"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",https://arxiv.org/pdf/2309.01207,,,,,,,,
Xception,Xception: Deep Learning with Depthwise Separable Convolutions,https://www.mdpi.com/1424-8220/23/21/8809/pdf?version=1698735915,,,,,,,,
SimCLRv2,Big Self-Supervised Models are Strong Semi-Supervised Learners,https://arxiv.org/pdf/2303.15180,,,,,,,,
ERNIE-ViLG,ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation,http://arxiv.org/pdf/2302.12172,,,,,,,,
LLaMA-7B,LLaMA: Open and Efficient Foundation Language Models,https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00808-1,,,,,,,,
DrLIM,Dimensionality Reduction by Learning an Invariant Mapping,https://arxiv.org/pdf/2209.07819,,,,,,,,
Minerva (540B),Solving Quantitative Reasoning Problems with Language Models,http://arxiv.org/pdf/2210.00720,,,,,,,,
DeLight,DeLighT: Deep and Light-weight Transformer,http://arxiv.org/pdf/2306.04235,,,,,,,,
BellKor 2008,The BellKor 2008 Solution to the Netflix Prize,http://dl.acm.org/ft_gateway.cfm?id=3186102&type=pdf,,,,,,,,
BASIC-L + Lion,Symbolic Discovery of Optimization Algorithms,https://arxiv.org/pdf/2307.06440,,,,,,,,
RT-2,RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control,https://arxiv.org/pdf/2309.09969,,,,,,,,
W2v-BERT,w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,https://arxiv.org/pdf/2211.00115,,,,,,,,
CTC-Trained LSTM,Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,http://arxiv.org/pdf/2210.01512,,,,,,,,
AlexaTM 20B,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,https://arxiv.org/pdf/2210.14169,,,,,,,,
WizardCoder-15.5B,WizardCoder: Empowering Code Large Language Models with Evol-Instruct,https://arxiv.org/pdf/2309.16609,,,,,,,,
Mask R-CNN,Mask R-CNN,https://www.biorxiv.org/content/biorxiv/early/2023/12/01/2023.11.29.568996.full.pdf,,,,,,,,
ALBERT-xxlarge,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,http://arxiv.org/pdf/2304.06861,,,,,,,,
Tranception,Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval,http://arxiv.org/pdf/2203.06125,,,,,,,,
GoogLeNet / InceptionV1,Going deeper with convolutions,https://www.frontiersin.org/articles/10.3389/fpls.2023.1281386/pdf?isPublishedV2=False,,,,,,,,
GPT-2 (1.5B),Language Models are Unsupervised Multitask Learners,https://aclanthology.org/2023.emnlp-main.881.pdf,,,,,,,,
TensorReasoner,Reasoning With Neural Tensor Networks for Knowledge Base Completion,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09037292.pdf,,,,,,,,
GAN-Advancer,Improved Techniques for Training GANs,https://www.nature.com/articles/s41598-023-38322-3.pdf,,,,,,,,
AlphaStar,Grandmaster level in StarCraft II using multi-agent reinforcement learning,http://arxiv.org/pdf/2302.01399,,,,,,,,
RBM-tuning,A Practical Guide to Training Restricted Boltzmann Machines,https://www.mdpi.com/2079-9292/8/5/535/pdf?version=1557736555,,,,,,,,
Regularized SVD for Collaborative Filtering,Improving regularized singular value decomposition for collaborative filtering,https://bmcmedgenomics.biomedcentral.com/track/pdf/10.1186/s12920-017-0313-y,,,,,,,,
YOLOv3,YOLOv3: An Incremental Improvement,https://www.mdpi.com/1424-8220/24/1/212/pdf?version=1703865876,,,,,,,,
AlphaGo Master,Mastering the game of Go without human knowledge,https://arxiv.org/pdf/2307.16212,,,,,,,,
Sandwich Transformer,Improving Transformer Models by Reordering their Sublayers,https://aclanthology.org/2021.naacl-main.407.pdf,,,,,,,,
3D city reconstruction,Building Rome in a day,https://arxiv.org/pdf/1708.04672,,,,,,,,
CNN Best Practices,Best practices for convolutional neural networks applied to visual document analysis,https://www.mdpi.com/1424-8220/22/1/292/pdf?version=1640947677,,,,,,,,
MetNet,MetNet: A Neural Weather Model for Precipitation Forecasting,https://www.frontiersin.org/articles/10.3389/fclim.2022.1022624/pdf,,,,,,,,
Image Classification with the Fisher Vector: Theory and Practice,"Author manuscript, published in ""International Journal of Computer Vision (2013)"" International Journal of Computer Vision manuscript No. (will be inserted by the editor) Image Classification with the Fisher Vector: Theory and Practice",https://www.frontiersin.org/articles/10.3389/fnagi.2016.00077/pdf,,,,,,,,
Big-Little Net (speech),Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition,https://www.frontiersin.org/articles/10.3389/fonc.2022.925903/pdf,,,,,,,,
LSTM (2018),An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,https://www.frontiersin.org/articles/10.3389/fnagi.2022.1027224/pdf,,,,,,,,
LeNet-5,Gradient-based learning applied to document recognition,https://arxiv.org/pdf/2312.04862,,,,,,,,
Maxout Networks,Maxout Networks,http://arxiv.org/pdf/2301.11663,,,,,,,,
BiLSTM for Speech,Framewise phoneme classification with bidirectional LSTM and other neural network architectures,http://arxiv.org/pdf/2305.12640,,,,,,,,
ContextNet + Noisy Student,Improved Noisy Student Training for Automatic Speech Recognition,http://arxiv.org/pdf/2210.07677,,,,,,,,
Large regularized LSTM,Recurrent Neural Network Regularization,https://www.mdpi.com/1424-8220/23/18/7869/pdf?version=1694616214,,,,,,,,
BERT-Large-CAS (PTB+WT2+WT103),Language Models with Transformers,https://aclanthology.org/2021.emnlp-main.133.pdf,,,,,,,,
AlphaFold 2,Single-sequence protein structure prediction using a language model and deep learning,https://arxiv.org/pdf/2212.03456,,,,,,,,
ResNeXt-101 32x48d,Exploring the Limits of Weakly Supervised Pretraining,https://arxiv.org/pdf/2106.03149,,,,,,,,
MLP as Bayesian Approximator,The multilayer perceptron as an approximation to a Bayes optimal discriminant function,https://arxiv.org/pdf/1711.08752,,,,,,,,
TrellisNet,Trellis Networks for Sequence Modeling,http://arxiv.org/pdf/2209.12951,,,,,,,,
NASNet-A,Learning Transferable Architectures for Scalable Image Recognition,https://link.springer.com/content/pdf/10.1007/s44267-023-00006-x.pdf,,,,,,,,
DETR,End-to-End Object Detection with Transformers,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/13E80EA6A425B6AC3D91D06FEC7E7C99/S1323358023000644a.pdf/div-class-title-radiogalaxynet-dataset-and-novel-computer-vision-algorithms-for-the-detection-of-extended-radio-galaxies-and-infrared-hosts-div.pdf,,,,,,,,
FixRes ResNeXt-101 WSL,Fixing the train-test resolution discrepancy,https://arxiv.org/pdf/2110.14819,,,,,,,,
ADALINE,Adaptive switching circuits,https://www.nature.com/articles/srep14730.pdf,,,,,,,,
AlexNet,ImageNet classification with deep convolutional neural networks,https://www.nature.com/articles/s41598-024-51687-3.pdf,,,,,,,,
OPT-175B,OPT: Open Pre-trained Transformer Language Models,http://arxiv.org/pdf/2306.04441,,,,,,,,
DINOv2,DINOv2: Learning Robust Visual Features without Supervision,http://arxiv.org/pdf/2306.13731,,,,,,,,
Visualizing CNNs,Visualizing and Understanding Convolutional Networks,https://www.mdpi.com/2075-4418/13/19/3063/pdf?version=1695741728,,,,,,,,
Generative BST,Recipes for Building an Open-Domain Chatbot,http://arxiv.org/pdf/2212.09588,,,,,,,,
NetTalk (transcription),Parallel Networks that Learn to Pronounce English Text,https://openresearch.surrey.ac.uk/view/delivery/44SUR_INST/12139326630002346/13140595080002346,,,,,,,,
Inceptionv4,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",https://www.mdpi.com/2227-9032/11/14/2068/pdf?version=1689768797,,,,,,,,
YOLOv3,YOLOv3: An Incremental Improvement,https://ctujs.ctu.edu.vn/index.php/ctujs/article/download/685/652,,,,,,,,
ResNeXt-50,Aggregated Residual Transformations for Deep Neural Networks,https://lirias.kuleuven.be/bitstream/20.500.12942/709285/2/finalManuscript_xpress.pdf,,,,,,,,
ViT-Base/32,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://www.mdpi.com/2079-9292/13/3/471/pdf?version=1706007406,,,,,,,,
TransE,Translating Embeddings for Modeling Multi-relational Data,https://www.ijcai.org/proceedings/2023/0259.pdf,,,,,,,,
ProGen2-xlarge,ProGen2: Exploring the Boundaries of Protein Language Models,https://www.biorxiv.org/content/biorxiv/early/2022/12/27/2022.12.07.519495.full.pdf,,,,,,,,
Bidirectional RNN,Bidirectional recurrent neural networks,https://dl.acm.org/doi/pdf/10.1145/3643893,,,,,,,,
Residual Dense Network,Residual Dense Network for Image Super-Resolution,https://dl.acm.org/doi/pdf/10.1145/3510548.3519378,,,,,,,,
Two-stream ConvNets for action recognition,Two-Stream Convolutional Networks for Action Recognition in Videos,https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-023-00779-4,,,,,,,,
iGPT-XL,Generative Pretraining From Pixels,https://arxiv.org/pdf/2204.13101,,,,,,,,
PolyCoder,A systematic evaluation of large language models of code,http://arxiv.org/pdf/2212.09248,,,,,,,,
MoE,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,http://arxiv.org/pdf/2305.13999,,,,,,,,
Deeply-recursive ConvNet,Deeply-Recursive Convolutional Network for Image Super-Resolution,https://arxiv.org/pdf/2207.12941,,,,,,,,
NÜWA,NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,https://arxiv.org/pdf/2309.14623,,,,,,,,
VD-LSTM+REAL Large,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,https://www.aclweb.org/anthology/P18-1085.pdf,,,,,,,,
CapsNet (MNIST),Dynamic Routing Between Capsules,https://www.mdpi.com/2072-4292/15/19/4804/pdf?version=1696242390,,,,,,,,
Histograms of Oriented Gradients,Histograms of oriented gradients for human detection,https://www.mdpi.com/1424-8220/23/15/6774/pdf?version=1690783788,,,,,,,,
PaLI,PaLI: A Jointly-Scaled Multilingual Language-Image Model,https://arxiv.org/pdf/2310.04869,,,,,,,,
NPLM,A Neural Probabilistic Language Model,https://www.mdpi.com/2073-8994/14/4/703/pdf?version=1648633629,,,,,,,,
BART-large,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",https://arxiv.org/pdf/2310.00741,,,,,,,,
Diabetic Retinopathy Detection Net,Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.,https://www.nature.com/articles/s41598-022-18785-6.pdf,,,,,,,,
DOT(S)-RNN,How to Construct Deep Recurrent Neural Networks,https://www.aclweb.org/anthology/K18-1015.pdf,,,,,,,,
ESM1-670M (UR50/D),Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,https://www.biorxiv.org/content/biorxiv/early/2023/05/05/2023.05.04.539497.full.pdf,,,,,,,,
NÜWA,NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,https://arxiv.org/pdf/2205.04421,,,,,,,,
PolyCoder,A systematic evaluation of large language models of code,https://arxiv.org/pdf/2307.05950,,,,,,,,
Immediate trihead,Immediate-Head Parsing for Language Models,http://www.cs.pitt.edu/~hwa/nle04draft.pdf,,,,,,,,
Sandwich Transformer,Improving Transformer Models by Reordering their Sublayers,https://aclanthology.org/2021.emnlp-main.446.pdf,,,,,,,,
EMDR,End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering,https://arxiv.org/pdf/2206.14989,,,,,,,,
Neural Architecture Search with base 8 and shared embeddings,Neural Architecture Search with Reinforcement Learning,https://downloads.hindawi.com/journals/jhe/2023/4597445.pdf,,,,,,,,
OPT-175B,OPT: Open Pre-trained Transformer Language Models,http://arxiv.org/pdf/2305.15594,,,,,,,,
LUKE,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,https://arxiv.org/pdf/2310.00299,,,,,,,,
Culturome,Quantitative Analysis of Culture Using Millions of Digitized Books,https://www.aclweb.org/anthology/2020.findings-emnlp.259.pdf,,,,,,,,
BatchNorm,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,http://apjcriweb.org/content/vol9no11/9.pdf,,,,,,,,
Residual Dense Network,Residual Dense Network for Image Super-Resolution,http://arxiv.org/pdf/2207.01831,,,,,,,,
ProGen2-xlarge,ProGen2: Exploring the Boundaries of Protein Language Models,https://arxiv.org/pdf/2308.16259,,,,,,,,
BART-large,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",https://arxiv.org/pdf/2310.05280,,,,,,,,
Greedy layer-wise DNN training,Greedy Layer-Wise Training of Deep Networks,https://www.mdpi.com/1424-8220/22/18/6887/pdf?version=1663038263,,,,,,,,
iGPT-L,Generative Pretraining From Pixels,https://arxiv.org/pdf/2111.11418,,,,,,,,
Pluribus,Superhuman AI for multiplayer poker,https://ojs.aaai.org/index.php/AAAI/article/download/17350/17157,,,,,,,,
GLIDE,GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,https://arxiv.org/pdf/2305.04063,,,,,,,,
Fully Convolutional Networks,Fully convolutional networks for semantic segmentation,https://www.research-collection.ethz.ch/bitstream/20.500.11850/641570/1/IEEE_SENSORS_2023_Armasuisse_Sentinel.pdf,,,,,,,,
GPT-NeoX-20B,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,https://arxiv.org/pdf/2309.13734,,,,,,,,
PaLM-E,PaLM-E: An Embodied Multimodal Language Model,https://arxiv.org/pdf/2305.10912,,,,,,,,
Sparse coding model for V1 receptive fields,Sparse coding with an overcomplete basis set: A strategy employed by V1?,https://www.mdpi.com/1424-8220/19/14/3216/pdf?version=1563768688,,,,,,,,
Chinchilla,Training Compute-Optimal Large Language Models,https://www.biorxiv.org/content/biorxiv/early/2023/07/08/2023.07.06.547963.full.pdf,,,,,,,,
Flan-PaLM 540B,Scaling Instruction-Finetuned Language Models,https://arxiv.org/pdf/2309.12616,,,,,,,,
AlphaFold,Improved protein structure prediction using potentials from deep learning,https://bmcbiol.biomedcentral.com/counter/pdf/10.1186/s12915-023-01651-w,,,,,,,,
SENet (ImageNet),Squeeze-and-Excitation Networks,https://www.mdpi.com/2076-2615/14/3/458/pdf?version=1706629250,,,,,,,,
ResNet-1001,Identity Mappings in Deep Residual Networks,https://www.grapeinsight.in/index.php/gi/article/download/10/8,,,,,,,,
Iterative Bootstrapping WSD,Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,https://arxiv.org/pdf/2307.12576,,,,,,,,
Tagging via Viterbi Decoding,Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms,https://doi.org/10.3115/v1/p15-2076,,,,,,,,
Context-dependent RNN,Context dependent recurrent neural network language model,https://aclanthology.org/2021.acl-long.474.pdf,,,,,,,,
Population-based DRL,Human-level performance in 3D multiplayer games with population-based reinforcement learning,https://www.frontiersin.org/articles/10.3389/frai.2023.804682/pdf,,,,,,,,
ProxylessNAS,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,http://arxiv.org/pdf/2203.08734,,,,,,,,
BigChaos 2008,The BigChaos Solution to the Netflix Prize 2008,https://peerj.com/articles/3644.pdf,,,,,,,,
DistilBERT,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",https://arxiv.org/pdf/2309.08351,,,,,,,,
Two-stream ConvNets for action recognition,Two-Stream Convolutional Networks for Action Recognition in Videos,http://arxiv.org/pdf/2301.10492,,,,,,,,
Prototypical networks,Prototypical Networks for Few-shot Learning,https://arxiv.org/pdf/2309.08944,,,,,,,,
Elastic weight consolidation,Overcoming catastrophic forgetting in neural networks,https://arxiv.org/pdf/2309.03964,,,,,,,,
ResNet-50 Billion-scale,Billion-scale semi-supervised learning for image classification,https://www.nature.com/articles/s41593-023-01442-0.pdf,,,,,,,,
Error Propagation,Learning internal representations by error propagation,https://www.mdpi.com/2306-5354/10/10/1209/pdf?version=1697515953,,,,,,,,
GNMT,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,https://ojs.aaai.org/index.php/AAAI/article/download/25222/24994,,,,,,,,
ViT-G/14 (LiT),LiT: Zero-Shot Transfer with Locked-image text Tuning,https://arxiv.org/pdf/2310.08255,,,,,,,,
SimCLR,A Simple Framework for Contrastive Learning of Visual Representations,https://www.biorxiv.org/content/biorxiv/early/2023/04/29/2023.04.28.538691.full.pdf,,,,,,,,
ERNIE-GEN (large),ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation,https://ojs.aaai.org/index.php/AAAI/article/download/17544/17351,,,,,,,,
NoisyNet-Dueling,Noisy Networks for Exploration,http://arxiv.org/pdf/2210.01620,,,,,,,,
Advantage Learning,Increasing the Action Gap: New Operators for Reinforcement Learning,https://ojs.aaai.org/index.php/AAAI/article/download/10887/10746,,,,,,,,
DD-PPO,DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames,https://arxiv.org/pdf/2206.06994,,,,,,,,
MobileNetV2,MobileNetV2: Inverted Residuals and Linear Bottlenecks,https://www.mdpi.com/1424-8220/23/24/9681/pdf?version=1701955361,,,,,,,,
Adversarial Joint Adaptation Network (ResNet),Deep Transfer Learning with Joint Adaptation Networks,https://link.springer.com/content/pdf/10.1007/s40747-023-01094-4.pdf,,,,,,,,
Word2Vec (small),Distributed Representations of Words and Phrases and their Compositionality,https://arxiv.org/pdf/2310.08954,,,,,,,,
GPT-4,GPT-4 Technical Report,https://www.medrxiv.org/content/medrxiv/early/2024/02/08/2024.02.08.24302376.full.pdf,,,,,,,,
StarGAN v2,StarGAN v2: Diverse Image Synthesis for Multiple Domains,https://arxiv.org/pdf/2111.13105,,,,,,,,
BASIC-L + Lion,Symbolic Discovery of Optimization Algorithms,https://arxiv.org/pdf/2309.02373,,,,,,,,
Transformer local-attention (NesT-B),"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",https://ieeexplore.ieee.org/ielx7/7433213/10169864/10098158.pdf,,,,,,,,
OpenAI Five,Dota 2 with Large Scale Deep Reinforcement Learning,http://arxiv.org/pdf/2205.12258,,,,,,,,
Constituency-Tree LSTM,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,https://downloads.hindawi.com/journals/mpe/2021/2390958.pdf,,,,,,,,
ADM,Diffusion Models Beat GANs on Image Synthesis,https://arxiv.org/pdf/2309.16750,,,,,,,,
GPipe (Amoeba),GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://www.mdpi.com/2079-9292/11/10/1525/pdf?version=1652928553,,,,,,,,
LSTM-Char-Large,Character-Aware Neural Language Models,https://www.aclweb.org/anthology/D19-1279.pdf,,,,,,,,
BellKor 2007,The BellKor solution to the Netflix Prize,https://www.frontiersin.org/articles/10.3389/fdata.2022.923397/pdf,,,,,,,,
Switch,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,http://arxiv.org/pdf/2303.11934,,,,,,,,
BellKor 2007,The BellKor solution to the Netflix Prize,http://ceur-ws.org/Vol-720/Airoldi.pdf,,,,,,,,
MS-CNN,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,https://www.mdpi.com/2220-9964/8/9/421/pdf?version=1568801034,,,,,,,,
Heuristic problem solving for AI,Steps toward Artificial Intelligence,https://www.mdpi.com/2227-7390/12/2/214/pdf?version=1704790794,,,,,,,,
Photo-Geometric Autoencoder,Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild,http://arxiv.org/pdf/2104.03515,,,,,,,,
AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),Improved Language Modeling by Decoding the Past,https://ojs.aaai.org/index.php/AAAI/article/download/5958/5814,,,,,,,,
base LM+GNN+kNN,GNN-LM: Language Modeling based on Global Contexts via GNN,http://arxiv.org/pdf/2304.09058,,,,,,,,
BIDAF,Bidirectional Attention Flow for Machine Comprehension,https://arxiv.org/pdf/2202.06080,,,,,,,,
Transformer ELMo,Dissecting Contextual Word Embeddings: Architecture and Representation,http://repositori.upf.edu/bitstream/10230/45237/1/accuosto_DKE_min.pdf,,,,,,,,
ESM1-670M (UR50/D),Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,https://www.frontiersin.org/articles/10.3389/fgene.2022.935351/pdf,,,,,,,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",FRAGE: Frequency-Agnostic Word Representation,https://arxiv.org/pdf/2210.05098,,,,,,,,
GPipe (Amoeba),GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,https://arxiv.org/pdf/2202.01306,,,,,,,,
Back-propagation,Learning representations by back-propagating errors,https://storage.googleapis.com/jnl-sljo-j-sljas1-files/journals/1/articles/8098/651ba49edc58d.pdf,,,,,,,,
Megatron-BERT,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://aclanthology.org/2022.findings-emnlp.545.pdf,,,,,,,,
SENet (ImageNet),Squeeze-and-Excitation Networks,https://www.mdpi.com/2071-1050/15/22/15857/pdf?version=1699691079,,,,,,,,
T5-11B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/pdf/2312.17349,,,,,,,,
"Listen, Attend and Spell","Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",https://jcsce.vnu.edu.vn/index.php/jcsce/article/download/321/134,,,,,,,,
PSPNet,Pyramid Scene Parsing Network,https://ieeexplore.ieee.org/ielx7/76/4358651/10102479.pdf,,,,,,,,
ShuffleNet v1,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,http://arxiv.org/pdf/2303.16900,,,,,,,,
Context-dependent RNN,Context dependent recurrent neural network language model,https://ojs.aaai.org/index.php/AAAI/article/download/11967/11826,,,,,,,,
ProgressiveGAN,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",https://www.ijcai.org/proceedings/2023/0038.pdf,,,,,,,,
ProteinBERT,ProteinBERT: a universal deep-learning model of protein sequence and function,http://manuscript.elsevier.com/S0959440X2200197X/pdf/S0959440X2200197X.pdf,,,,,,,,
RETRO-7B,Improving language models by retrieving from trillions of tokens,https://ieeexplore.ieee.org/ielx7/6287639/6514899/10380590.pdf,,,,,,,,
Maximum Entropy Models for machine translation,Discriminative Training and Maximum Entropy Models for Statistical Machine Translation,http://transducens.dlsi.ua.es/repositori/transducens/pubs/214/sanchez06a.pdf,,,,,,,,
AlexNet,ImageNet classification with deep convolutional neural networks,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0297655&type=printable,,,,,,,,
Segment Anything Model,Segment Anything,https://arxiv.org/pdf/2309.11745,,,,,,,,
Binarized Neural Network (MNIST),BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1,https://www.ijcai.org/proceedings/2018/0336.pdf,,,,,,,,
FAIRSEQ Adaptive Inputs,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling",https://aclanthology.org/2022.iwslt-1.5.pdf,,,,,,,,
SVD in recommender systems,Application of Dimensionality Reduction in Recommender System - A Case Study,http://www0.cs.ucl.ac.uk/staff/l.capra/publications/PID1165631.pdf,,,,,,,,
Elastic weight consolidation,Overcoming catastrophic forgetting in neural networks,https://arxiv.org/pdf/2308.16567,,,,,,,,
Back-propagation,Learning representations by back-propagating errors,https://arxiv.org/pdf/2308.09728,,,,,,,,
GPT,Improving Language Understanding by Generative Pre-Training,https://arxiv.org/pdf/2310.05035,,,,,,,,
BigBiGAN,Large Scale Adversarial Representation Learning,http://arxiv.org/pdf/2208.08056,,,,,,,,
RNNsearch-50*,Neural Machine Translation by Jointly Learning to Align and Translate,https://arxiv.org/pdf/2308.03051,,,,,,,,
Q-learning,Learning from delayed rewards,http://arxiv.org/pdf/2301.13799,,,,,,,,
RBM-tuning,A Practical Guide to Training Restricted Boltzmann Machines,https://www.mdpi.com/2076-3417/10/6/1994/pdf,,,,,,,,
Visualizing CNNs,Visualizing and Understanding Convolutional Networks,https://www.mdpi.com/2072-4292/15/21/5204/pdf?version=1698890237,,,,,,,,
SpecAugment,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,https://arxiv.org/pdf/2304.00173,,,,,,,,
BPE,Neural Machine Translation of Rare Words with Subword Units,https://www.biorxiv.org/content/biorxiv/early/2023/06/13/2023.06.12.544594.full.pdf,,,,,,,,
Part-of-sentence tagging model,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,https://www.aclweb.org/anthology/2020.acl-main.750.pdf,,,,,,,,
Sparse digit recognition SVM,Simple Method for High-Performance Digit Recognition Based on Sparse Coding,https://arxiv.org/pdf/1509.09187,,,,,,,,
BellKor 2008,The BellKor 2008 Solution to the Netflix Prize,https://www.mdpi.com/2072-6694/14/5/1291/pdf?version=1646217040,,,,,,,,
Stable Diffusion XL,SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis,https://arxiv.org/pdf/2310.08579,,,,,,,,
ViT-Huge/14,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://www.frontiersin.org/articles/10.3389/fpls.2023.1328952/pdf?isPublishedV2=False,,,,,,,,
ViT-Base/32,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://vciba.springeropen.com/counter/pdf/10.1186/s42492-024-00155-w,,,,,,,,
AudioLM,AudioLM: A Language Modeling Approach to Audio Generation,https://arxiv.org/pdf/2306.10521,,,,,,,,
DeepNet,"DeepNet: Scaling Transformers to 1, 000 Layers",http://arxiv.org/pdf/2305.15781,,,,,,,,
NeuMF (Pinterest),Neural Collaborative Filtering,http://arxiv.org/pdf/2303.04561,,,,,,,,
Visualizing CNNs,Visualizing and Understanding Convolutional Networks,http://arxiv.org/pdf/2306.03745,,,,,,,,
CTC-Trained LSTM,Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,https://arxiv.org/pdf/2309.13102,,,,,,,,
DNABERT,DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome,https://www.biorxiv.org/content/biorxiv/early/2023/03/10/2023.03.10.532047.full.pdf,,,,,,,,
CoAtNet,CoAtNet: Marrying Convolution and Attention for All Data Sizes,https://arxiv.org/pdf/2301.10038,,,,,,,,
Generative BST,Recipes for Building an Open-Domain Chatbot,https://dl.acm.org/doi/pdf/10.1145/3552310,,,,,,,,
Error Propagation,Learning internal representations by error propagation,https://olj.onlinelearningconsortium.org/index.php/olj/article/download/4055/1308,,,,,,,,
Go-explore,"First return, then explore",https://arxiv.org/pdf/2211.07627,,,,,,,,
ISS,Learning Intrinsic Sparse Structures within Long Short-term Memory,https://arxiv.org/pdf/1805.08941,,,,,,,,
MCDNN (MNIST),Multi-column deep neural networks for image classification,https://www.mdpi.com/2227-9717/10/7/1406/pdf?version=1658223870,,,,,,,,
EDSR,Enhanced Deep Residual Networks for Single Image Super-Resolution,https://arxiv.org/pdf/2307.07240,,,,,,,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,https://ojs.bonviewpress.com/index.php/jdsis/article/download/1131/568,,,,,,,,
EfficientNet-L2,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://www.frontiersin.org/articles/10.3389/fpls.2023.1324491/pdf?isPublishedV2=False,,,,,,,,
ProGen2-xlarge,ProGen2: Exploring the Boundaries of Protein Language Models,https://www.biorxiv.org/content/biorxiv/early/2022/12/19/2022.12.15.519894.full.pdf,,,,,,,,
DBLSTM,Hybrid speech recognition with Deep Bidirectional LSTM,https://www.techrxiv.org/articles/preprint/Deep_Neural_Networks_for_Rapid_Simulation_of_Planar_Microwave_Circuits_Based_on_their_Layouts/19372862/1/files/34406264.pdf,,,,,,,,
VGG19,Very Deep Convolutional Networks for Large-Scale Image Recognition,https://jurnal.iaii.or.id/index.php/RESTI/article/download/5376/876,,,,,,,,
RBM Image Classifier,Learning Multiple Layers of Features from Tiny Images,https://arxiv.org/pdf/2312.16475,,,,,,,,
BLOOMZ-176B,Crosslingual Generalization through Multitask Finetuning,http://arxiv.org/pdf/2305.13707,,,,,,,,
Decoupled weight decay regularization,Decoupled Weight Decay Regularization,https://www.mdpi.com/1424-8220/24/3/777/pdf?version=1706160514,,,,,,,,
Unsupervised Scale-Invariant Learning,Object class recognition by unsupervised scale-invariant learning,https://arxiv.org/pdf/1411.5268,,,,,,,,
Deep Belief Nets,A Fast Learning Algorithm for Deep Belief Nets,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-023-01092-1,,,,,,,,
iGPT-XL,Generative Pretraining From Pixels,https://arxiv.org/pdf/2105.02358,,,,,,,,
NASv3 (CIFAR-10),Neural Architecture Search with Reinforcement Learning,https://link.springer.com/content/pdf/10.1007/s44163-022-00035-3.pdf,,,,,,,,
data2vec (vision),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://arxiv.org/pdf/2310.08225,,,,,,,,
BART-large,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",https://ieeexplore.ieee.org/ielx7/7333/4359219/10248031.pdf,,,,,,,,
BPL,Human-level concept learning through probabilistic program induction,https://arxiv.org/pdf/2201.05151,,,,,,,,
MV-RNN,Semantic Compositionality through Recursive Matrix-Vector Spaces,https://www.aclweb.org/anthology/D19-1503.pdf,,,,,,,,
DeBERTa,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,http://arxiv.org/pdf/2304.04746,,,,,,,,
TD(0),An Adaptive Optimal Controller for Discrete-Time Markov Environments,https://ediss.uni-goettingen.de/bitstream/11858/00-1735-0000-000D-F171-5/1/kolodziejski.pdf,,,,,,,,
Unsupervised High-level Feature Learner,Building high-level features using large scale unsupervised learning,https://www.frontiersin.org/articles/10.3389/fdata.2020.579774/pdf,,,,,,,,
MS-CNN,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,https://arxiv.org/pdf/1903.01864,,,,,,,,
Fisher Kernel GMM,Fisher Kernels on Visual Vocabularies for Image Categorization,http://pure.aber.ac.uk/ws/files/25248304/Lanihun_Olalejan_Abebayo.pdf,,,,,,,,
HuBERT,HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,https://arxiv.org/pdf/2309.17395,,,,,,,,
DeepLabV3,Rethinking Atrous Convolution for Semantic Image Segmentation,https://arxiv.org/pdf/2308.02287,,,,,,,,
ConvNet similarity metric,"Learning a similarity metric discriminatively, with application to face verification",https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/s12859-022-04752-5,,,,,,,,
Gato,A Generalist Agent,https://arxiv.org/pdf/2309.03895,,,,,,,,
DCN+,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,https://www.aclweb.org/anthology/P19-1469.pdf,,,,,,,,
Image generation,Decision-Making with Auto-Encoding Variational Bayes,https://www.mdpi.com/1099-4300/25/12/1649/pdf?version=1702385581,,,,,,,,
FAIRSEQ Adaptive Inputs,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling",http://arxiv.org/pdf/2305.15997,,,,,,,,
ProteinBERT,ProteinBERT: a universal deep-learning model of protein sequence and function,https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/s12859-023-05164-9,,,,,,,,
ProxylessNAS,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,http://arxiv.org/pdf/2302.05433,,,,,,,,
RNN+weight noise+dynamic eval,Generating Sequences With Recurrent Neural Networks,https://arxiv.org/pdf/2310.02619,,,,,,,,
Deep Belief Nets,A Fast Learning Algorithm for Deep Belief Nets,https://www.mdpi.com/1424-8220/23/14/6486/pdf?version=1689671256,,,,,,,,
CPM-Large,CPM: A Large-scale Generative Chinese Pre-trained Language Model,https://arxiv.org/pdf/2206.06315,,,,,,,,
Ankh_base,Ankh ☥: Optimized Protein Language Model Unlocks General-Purpose Modelling,https://www.biorxiv.org/content/biorxiv/early/2023/07/14/2023.07.05.547496.full.pdf,,,,,,,,
A3C FF hs,Asynchronous Methods for Deep Reinforcement Learning,https://www.mdpi.com/2076-3417/13/13/7594/pdf?version=1688027230,,,,,,,,
data2vec (vision),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",http://arxiv.org/pdf/2207.01039,,,,,,,,
TrOCR,TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models,https://arxiv.org/pdf/2208.07682,,,,,,,,
Big Transfer (BiT-L),Large Scale Learning of General Visual Representations for Transfer,https://arxiv.org/pdf/2006.06666,,,,,,,,
Word Representations,Word Representations: A Simple and General Method for Semi-Supervised Learning,https://www.aclweb.org/anthology/P17-2029.pdf,,,,,,,,
GShard (dense),GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,https://aclanthology.org/2022.findings-emnlp.484.pdf,,,,,,,,
Youtube recommendation model,Deep Neural Networks for YouTube Recommendations,https://www.tandfonline.com/doi/pdf/10.1080/21670811.2023.2209153?needAccess=true&role=button,,,,,,,,
WizardCoder-15.5B,WizardCoder: Empowering Code Large Language Models with Evol-Instruct,https://arxiv.org/pdf/2309.07254,,,,,,,,
Greedy layer-wise DNN training,Greedy Layer-Wise Training of Deep Networks,https://www.frontiersin.org/articles/10.3389/fgene.2020.00328/pdf,,,,,,,,
VQGAN + CLIP,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/pdf/2211.11319,,,,,,,,
KataGo,Accelerating Self-Play Learning in Go,https://arxiv.org/pdf/2102.03467,,,,,,,,
XLNet,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://arxiv.org/pdf/2307.05605,,,,,,,,
NLLB,No Language Left Behind: Scaling Human-Centered Machine Translation,https://arxiv.org/pdf/2206.11249,,,,,,,,
DrLIM,Dimensionality Reduction by Learning an Invariant Mapping,https://arxiv.org/pdf/2304.05824,,,,,,,,
Cross-lingual alignment,"Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing",https://arxiv.org/pdf/2202.05924,,,,,,,,
Deep LSTM for video classification,Beyond short snippets: Deep networks for video classification,http://www.cell.com/article/S2405844022026895/pdf,,,,,,,,
Ankh_large,Ankh ☥: Optimized Protein Language Model Unlocks General-Purpose Modelling,https://arxiv.org/pdf/2306.13952,,,,,,,,
Transformer (Adaptive Input Embeddings),Adaptive Input Representations for Neural Language Modeling,https://www.biorxiv.org/content/biorxiv/early/2022/07/22/2022.07.21.500999.full.pdf,,,,,,,,
Xception,Xception: Deep Learning with Depthwise Separable Convolutions,https://arxiv.org/pdf/2308.15502,,,,,,,,
Agile Soccer Robot,Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning,https://arxiv.org/pdf/2307.14732,,,,,,,,
FAIRSEQ Adaptive Inputs,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling",https://arxiv.org/pdf/2205.10835,,,,,,,,
Meta Pseudo Labels,Meta Pseudo Labels,https://arxiv.org/pdf/2211.13050,,,,,,,,
GloVe (6B),GloVe: Global Vectors for Word Representation,https://journalofcloudcomputing.springeropen.com/counter/pdf/10.1186/s13677-023-00585-6,,,,,,,,
LUKE,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,https://ojs.aaai.org/index.php/AAAI/article/download/21441/21190,,,,,,,,
Routing Transformer,Efficient Content-Based Sparse Attention with Routing Transformers,https://aclanthology.org/2023.eacl-main.273.pdf,,,,,,,,
Elastic weight consolidation,Overcoming catastrophic forgetting in neural networks,https://arxiv.org/pdf/2307.05014,,,,,,,,
Multi-scale Dilated CNN,Multi-Scale Context Aggregation by Dilated Convolutions,https://iopscience.iop.org/article/10.1088/1742-6596/2387/1/012029/pdf,,,,,,,,
CoAtNet,CoAtNet: Marrying Convolution and Attention for All Data Sizes,https://downloads.hindawi.com/journals/cin/2022/2661231.pdf,,,,,,,,
IMPALA,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,http://arxiv.org/pdf/2007.08082,,,,,,,,
Spatial Pyramid Matching,Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,https://www.mdpi.com/1424-8220/20/8/2436/pdf?version=1587892092,,,,,,,,
FixRes ResNeXt-101 WSL,Fixing the train-test resolution discrepancy,http://arxiv.org/pdf/2210.03347,,,,,,,,
GNMT,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,http://arxiv.org/pdf/2207.01262,,,,,,,,
data2vec (language),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",http://arxiv.org/pdf/2306.11297,,,,,,,,
DeLight,DeLighT: Deep and Light-weight Transformer,https://arxiv.org/pdf/2202.06263,,,,,,,,
Deep LSTM for video classification,Beyond short snippets: Deep networks for video classification,https://arxiv.org/pdf/2109.00829,,,,,,,,
ERNIE-GEN (large),ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation,https://dl.acm.org/doi/pdf/10.1145/3580305.3599903,,,,,,,,
RNN-SpeedUp,Extensions of recurrent neural network language model,https://www.aclweb.org/anthology/K19-1073.pdf,,,,,,,,
Bidirectional RNN,Bidirectional recurrent neural networks,https://arxiv.org/pdf/2308.06866,,,,,,,,
DMN,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09090974.pdf,,,,,,,,
AMDIM,Learning Representations by Maximizing Mutual Information Across Views,https://dl.acm.org/doi/pdf/10.1145/3477495.3531918,,,,,,,,
FixRes ResNeXt-101 WSL,Fixing the train-test resolution discrepancy,http://arxiv.org/pdf/2306.11339,,,,,,,,
Boss (DARPA Urban Challenge),Autonomous driving in urban environments: Boss and the Urban Challenge,https://repository.upenn.edu/bitstreams/ba20af5c-86b0-4e40-b8b7-c0584875d8dc/download,,,,,,,,
SENet (ImageNet),Squeeze-and-Excitation Networks,https://www.mdpi.com/1424-8220/23/23/9390/pdf?version=1700833738,,,,,,,,
Inception v3,Rethinking the Inception Architecture for Computer Vision,https://www.mdpi.com/2075-4418/13/22/3461/pdf?version=1700202257,,,,,,,,
Iterative Bootstrapping WSD,Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,https://ojs.aaai.org/index.php/AAAI/article/download/8597/8456,,,,,,,,
MT-DNN,Multi-Task Deep Neural Networks for Natural Language Understanding,http://arxiv.org/pdf/2204.04541,,,,,,,,
AlexNet,ImageNet classification with deep convolutional neural networks,https://www.extrica.com/article/23786/pdf,,,,,,,,
Transformer + Simple Recurrent Unit,Simple Recurrent Units for Highly Parallelizable Recurrence,https://arxiv.org/pdf/2110.05571,,,,,,,,
AlexNet,ImageNet classification with deep convolutional neural networks,https://link.springer.com/content/pdf/10.1007/s11119-023-10096-8.pdf,,,,,,,,
AWD-LSTM + MoS + Partial Shuffled,Improving Neural Language Modeling via Adversarial Training,https://arxiv.org/pdf/1911.12562,,,,,,,,
Universal approximation via Feedforward Networks,Multilayer feedforward networks are universal approximators,https://link.springer.com/content/pdf/10.1007/s10928-023-09886-4.pdf,,,,,,,,
Xception,Xception: Deep Learning with Depthwise Separable Convolutions,https://arxiv.org/pdf/2309.05590,,,,,,,,
WizardLM-7B,WizardLM: Empowering Large Language Models to Follow Complex Instructions,https://arxiv.org/pdf/2309.15025,,,,,,,,
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),Regularizing and Optimizing LSTM Language Models,http://arxiv.org/pdf/2205.10733,,,,,,,,
BellKor 2007,The BellKor solution to the Netflix Prize,http://repositorio.inesctec.pt/bitstreams/8d930925-9114-46ac-8e2d-ad8f8a44f836/download,,,,,,,,
GenSLM,GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics,https://www.biorxiv.org/content/biorxiv/early/2023/03/09/2023.01.11.523679.full.pdf,,,,,,,,
Contriever,Unsupervised Dense Information Retrieval with Contrastive Learning,https://arxiv.org/pdf/2309.08960,,,,,,,,
T5-3B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://aclanthology.org/2023.arabicnlp-1.23.pdf,,,,,,,,
Perceptron for Large Margin Classification,Large Margin Classification Using the Perceptron Algorithm,https://www.mdpi.com/1424-8220/20/11/3236/pdf?version=1591438869,,,,,,,,
LSTM (2018),An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,https://www.mdpi.com/1424-8220/23/15/6950/pdf?version=1691157177,,,,,,,,
DNABERT,DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome,https://www.mdpi.com/2073-8994/15/3/731/pdf?version=1678862914,,,,,,,,
Generative BST,Recipes for Building an Open-Domain Chatbot,https://arxiv.org/pdf/2211.15731,,,,,,,,
DeBERTa,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,http://arxiv.org/pdf/2304.06861,,,,,,,,
BIDAF,Bidirectional Attention Flow for Machine Comprehension,https://dl.acm.org/doi/pdf/10.1145/3404835.3463099,,,,,,,,
SRU++ Large,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,http://arxiv.org/pdf/2306.02870,,,,,,,,
Conformer,Conformer: Convolution-augmented Transformer for Speech Recognition,https://arxiv.org/pdf/2309.13029,,,,,,,,
OpenAI Five,Dota 2 with Large Scale Deep Reinforcement Learning,https://arxiv.org/pdf/2202.10134,,,,,,,,
Symmetric Residual Encoder-Decoder Net,Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections,https://arxiv.org/pdf/1909.10774,,,,,,,,
Diffractive Deep Neural Network,All-optical machine learning using diffractive deep neural networks,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/cit2.12114,,,,,,,,
Generative BST,Recipes for Building an Open-Domain Chatbot,https://aclanthology.org/2022.findings-acl.308.pdf,,,,,,,,
Dropout (ImageNet),Improving neural networks by preventing co-adaptation of feature detectors,https://www.researchsquare.com/article/rs-2006370/latest.pdf,,,,,,,,
MT-DNN,Multi-Task Deep Neural Networks for Natural Language Understanding,https://ojs.aaai.org/index.php/AAAI/article/download/6245/6101,,,,,,,,
"Listen, Attend and Spell","Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",https://arxiv.org/pdf/2102.10905,,,,,,,,
OPT-175B,OPT: Open Pre-trained Transformer Language Models,https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00086/2138378/opmi_a_00086.pdf,,,,,,,,
AudioLM,AudioLM: A Language Modeling Approach to Audio Generation,https://ieeexplore.ieee.org/ielx7/6221020/6363502/10261199.pdf,,,,,,,,
ALIGN,Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision,https://arxiv.org/pdf/2306.08789,,,,,,,,
Differentiable neural computer,Hybrid computing using a neural network with dynamic external memory,https://arxiv.org/pdf/2307.15679,,,,,,,,
Spatial Pyramid Matching,Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,https://hal.archives-ouvertes.fr/hal-01835074/file/LTBMpaper.pdf,,,,,,,,
ResNet-1001,Identity Mappings in Deep Residual Networks,http://arxiv.org/pdf/2303.16861,,,,,,,,
ResNet-50 Billion-scale,Billion-scale semi-supervised learning for image classification,https://arxiv.org/pdf/2109.01903,,,,,,,,
ShuffleNet v2,ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design,https://www.mdpi.com/2076-2615/13/2/264/pdf?version=1673522167,,,,,,,,
Stable Diffusion (LDM-KL-8-G),High-Resolution Image Synthesis with Latent Diffusion Models,https://www.biorxiv.org/content/biorxiv/early/2024/01/25/2024.01.23.576647.full.pdf,,,,,,,,
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),Regularizing and Optimizing LSTM Language Models,https://www.aclweb.org/anthology/2020.acl-main.355.pdf,,,,,,,,
DITTO,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,http://arxiv.org/pdf/2305.17325,,,,,,,,
Hanabi 4 player,The Hanabi Challenge: A New Frontier for AI Research,https://link.springer.com/content/pdf/10.1007/s12369-022-00944-4.pdf,,,,,,,,
WeNet (Penn Treebank),WeNet: Weighted Networks for Recurrent Network Architecture Search,https://www.aclweb.org/anthology/D19-1367.pdf,,,,,,,,
EnCodec,High Fidelity Neural Audio Compression,https://arxiv.org/pdf/2308.16692,,,,,,,,
Make-A-Video,Make-A-Video: Text-to-Video Generation without Text-Video Data,http://arxiv.org/pdf/2306.04321,,,,,,,,
Flan-PaLM 540B,Scaling Instruction-Finetuned Language Models,http://arxiv.org/pdf/2305.14847,,,,,,,,
GNMT,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,http://arxiv.org/pdf/2210.06340,,,,,,,,
MV-RNN,Semantic Compositionality through Recursive Matrix-Vector Spaces,https://www.aclweb.org/anthology/W15-2712.pdf,,,,,,,,
ResNeXt-101 32x48d,Exploring the Limits of Weakly Supervised Pretraining,https://arxiv.org/pdf/2206.11795,,,,,,,,
BERT-Large,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://dl.acm.org/doi/pdf/10.1145/3597503.3623332,,,,,,,,
CICERO,Human-level play in the game of Diplomacy by combining language models with strategic reasoning,http://arxiv.org/pdf/2304.13004,,,,,,,,
DLRM-2020,Deep Learning Recommendation Model for Personalization and Recommendation Systems,https://ojs.aaai.org/index.php/AAAI/article/download/17275/17082,,,,,,,,
Inception v3,Rethinking the Inception Architecture for Computer Vision,https://arxiv.org/pdf/2310.06234,,,,,,,,
EnCodec,High Fidelity Neural Audio Compression,http://arxiv.org/pdf/2305.16107,,,,,,,,
SRGAN,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,https://www.mdpi.com/1424-8220/23/17/7338/pdf?version=1692762949,,,,,,,,
OR-WideResNet,Oriented Response Networks,https://edepot.wur.nl/466814,,,,,,,,
EDSR,Enhanced Deep Residual Networks for Single Image Super-Resolution,https://arxiv.org/pdf/2309.03020,,,,,,,,
Inception v3,Rethinking the Inception Architecture for Computer Vision,http://www.cell.com/article/S2405844023083056/pdf,,,,,,,,
Meta Pseudo Labels,Meta Pseudo Labels,https://ieeexplore.ieee.org/ielx7/6287639/10005208/10024297.pdf,,,,,,,,
Go-explore,"First return, then explore",http://arxiv.org/pdf/2306.10944,,,,,,,,
Residual Dense Network,Residual Dense Network for Image Super-Resolution,https://arxiv.org/pdf/2201.09314,,,,,,,,
Hanabi 4 player,The Hanabi Challenge: A New Frontier for AI Research,https://ojs.aaai.org/index.php/AIIDE/article/download/7404/7333,,,,,,,,
Multiscale deformable part model,"A discriminatively trained, multiscale, deformable part model",https://downloads.hindawi.com/journals/wcmc/2021/7978644.pdf,,,,,,,,
T5-3B,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://dl.acm.org/doi/pdf/10.1145/3597503.3623326,,,,,,,,
Search-Proven Best LSTM,An Empirical Exploration of Recurrent Network Architectures,http://manuscript.elsevier.com/S0893608020300708/pdf/S0893608020300708.pdf,,,,,,,,
SpecAugment,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,https://hal.science/hal-04149763/file/ICASSP23_final.pdf,,,,,,,,
CogVideo,CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,https://arxiv.org/pdf/2303.16541,,,,,,,,
DensePhrases,Learning Dense Representations of Phrases at Scale,https://aclanthology.org/2023.findings-acl.577.pdf,,,,,,,,
ShuffleNet v2,ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design,https://www.techscience.com/CMES/online/detail/19017/pdf,,,,,,,,
Contriever,Unsupervised Dense Information Retrieval with Contrastive Learning,https://annals-csis.org/proceedings/2023/drp/pdf/5627.pdf,,,,,,,,
DARTS,DARTS: Differentiable Architecture Search,http://arxiv.org/pdf/2301.08727,,,,,,,,
Multi-scale Dilated CNN,Multi-Scale Context Aggregation by Dilated Convolutions,https://www.mdpi.com/2306-5354/10/9/1060/pdf?version=1694179102,,,,,,,,
Flan-PaLM 540B,Scaling Instruction-Finetuned Language Models,http://arxiv.org/pdf/2305.15023,,,,,,,,
DQN,Playing Atari with Deep Reinforcement Learning,https://arxiv.org/pdf/2310.06147,,,,,,,,
Universal approximation via Feedforward Networks,Multilayer feedforward networks are universal approximators,http://arxiv.org/pdf/2306.06281,,,,,,,,
Contriever,Unsupervised Dense Information Retrieval with Contrastive Learning,https://arxiv.org/pdf/2308.04711,,,,,,,,
ADALINE,Adaptive switching circuits,https://openresearch.surrey.ac.uk/view/delivery/44SUR_INST/12139506540002346/13140457760002346,,,,,,,,
Faster R-CNN,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,https://asp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13634-023-01095-y,,,,,,,,
GloVe (32B),GloVe: Global Vectors for Word Representation,https://arxiv.org/pdf/2309.15086,,,,,,,,
Cascaded LNet-ANet,Deep Learning Face Attributes in the Wild,https://www.mdpi.com/2076-3417/13/14/8110/pdf?version=1689229147,,,,,,,,
PNAS-net,Progressive Neural Architecture Search,http://arxiv.org/pdf/2208.13909,,,,,,,,
GANs,A Style-Based Generator Architecture for Generative Adversarial Networks,https://arxiv.org/pdf/2308.06701,,,,,,,,
T0-XXL,Multitask Prompted Training Enables Zero-Shot Task Generalization,https://aclanthology.org/2022.emnlp-main.481.pdf,,,,,,,,
Mnemonic Reader,Reinforced Mnemonic Reader for Machine Reading Comprehension,https://arxiv.org/pdf/1912.11589,,,,,,,,
Dropout (CIFAR),Improving neural networks by preventing co-adaptation of feature detectors,https://www.tandfonline.com/doi/pdf/10.1080/08839514.2021.2011688?needAccess=true,,,,,,,,
Semi-Supervised Embedding for DL,Deep learning via semi-supervised embedding,https://ojs.aaai.org/index.php/AAAI/article/download/17203/17010,,,,,,,,
Support Vector Machines,Support-Vector Networks,https://iieta.org/download/file/fid/103014,,,,,,,,
RNN-WER,Towards End-To-End Speech Recognition with Recurrent Neural Networks,https://arxiv.org/pdf/2107.05382,,,,,,,,
Learnability theory of language development,Language learnability and language development,https://www.scielo.br/j/prc/a/my9Y86Fj8vW6HmW4RSz5Bdb/?lang=pt&format=pdf,,,,,,,,
Social and content-based classification,Recommendation as Classification: Using Social and Content-Based Information in Recommendation,https://vfast.org/journals/index.php/VTSE/article/download/340/408,,,,,,,,
HuBERT,HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,http://arxiv.org/pdf/2306.00789,,,,,,,,
JFT,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,https://arxiv.org/pdf/2310.06993,,,,,,,,
PointNet,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,https://onlinelibrary.wiley.com/doi/pdfdirect/10.4218/etrij.2023-0109,,,,,,,,
BloombergGPT,BloombergGPT: A Large Language Model for Finance,https://dl.acm.org/doi/pdf/10.1145/3604237.3626867,,,,,,,,
GSM,Gated Self-Matching Networks for Reading Comprehension and Question Answering,http://arxiv.org/pdf/2203.06667,,,,,,,,
SimCLRv2,Big Self-Supervised Models are Strong Semi-Supervised Learners,http://arxiv.org/pdf/2203.08717,,,,,,,,
NoisyNet-Dueling,Noisy Networks for Exploration,https://arxiv.org/pdf/2007.12817,,,,,,,,
XGLM,Few-shot Learning with Multilingual Language Models,https://arxiv.org/pdf/2201.09012,,,,,,,,
Word2Vec (large),Distributed Representations of Words and Phrases and their Compositionality,https://arxiv.org/pdf/2308.00189,,,,,,,,
DensePhrases,Learning Dense Representations of Phrases at Scale,http://arxiv.org/pdf/2210.05758,,,,,,,,
Multi-scale Dilated CNN,Multi-Scale Context Aggregation by Dilated Convolutions,https://www.eneuro.org/content/eneuro/early/2023/11/01/ENEURO.0111-23.2023.full.pdf,,,,,,,,
Spatial Pyramid Matching,Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,https://www.mdpi.com/2073-8994/12/8/1230/pdf,,,,,,,,
GOAT,Open-Ended Learning Leads to Generally Capable Agents,http://arxiv.org/pdf/2210.03649,,,,,,,,
CoAtNet,CoAtNet: Marrying Convolution and Attention for All Data Sizes,https://ieeexplore.ieee.org/ielx7/6287639/6514899/10264866.pdf,,,,,,,,
Residual Dense Network,Residual Dense Network for Image Super-Resolution,https://arxiv.org/pdf/2207.13670,,,,,,,,
Greedy layer-wise DNN training,Greedy Layer-Wise Training of Deep Networks,https://www.mdpi.com/1424-8220/21/2/603/pdf?version=1610979102,,,,,,,,
Named Entity Recognition model,Layer Normalization,https://iopscience.iop.org/article/10.1088/1742-6596/2637/1/012010/pdf,,,,,,,,
MatrixFac for Recommenders,Matrix Factorization Techniques for Recommender Systems,https://link.springer.com/content/pdf/10.1007/s10115-023-01903-9.pdf,,,,,,,,
Adaptive Inputs + LayerDrop,Reducing Transformer Depth on Demand with Structured Dropout,https://arxiv.org/pdf/2206.02976,,,,,,,,
S-Norm,Simple and Effective Multi-Paragraph Reading Comprehension,https://arxiv.org/pdf/2112.03572,,,,,,,,
Inception v3,Rethinking the Inception Architecture for Computer Vision,https://dr.ntu.edu.sg/bitstream/10356/171623/2/EdgeCompress_Coupling_Multi-Dimensional_Model_Compression_and_Dynamic_Inference_for_EdgeAI.pdf,,,,,,,,
RT-2,RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control,https://arxiv.org/pdf/2311.11183,,,,,,,,
AWD-LSTM + MoS + Partial Shuffled,Improving Neural Language Modeling via Adversarial Training,https://aclanthology.org/2021.emnlp-main.527.pdf,,,,,,,,
Megatron-Turing NLG 530B,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2309.10706,,,,,,,,
mT5-XXL,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,http://arxiv.org/pdf/2210.03057,,,,,,,,
Faster R-CNN,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,https://arxiv.org/pdf/2401.07589,,,,,,,,
PNAS-net,Progressive Neural Architecture Search,https://arxiv.org/pdf/2204.12726,,,,,,,,
GLEE,BOXES: AN EXPERIMENT IN ADAPTIVE CONTROL,http://www.jart.icat.unam.mx/index.php/jart/article/download/390/386,,,,,,,,
Elastic weight consolidation,Overcoming catastrophic forgetting in neural networks,https://arxiv.org/pdf/2308.02537,,,,,,,,
MetNet,MetNet: A Neural Weather Model for Precipitation Forecasting,https://arxiv.org/pdf/2309.10808,,,,,,,,
EnCodec,High Fidelity Neural Audio Compression,http://arxiv.org/pdf/2301.02111,,,,,,,,
MatrixFac for Recommenders,Matrix Factorization Techniques for Recommender Systems,http://arxiv.org/pdf/2210.05801,,,,,,,,
OpenAI Five,Dota 2 with Large Scale Deep Reinforcement Learning,https://quantum-journal.org/papers/q-2022-05-24-720/pdf/,,,,,,,,
SRU++ Large,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,http://arxiv.org/pdf/2207.07061,,,,,,,,
RETRO-7B,Improving language models by retrieving from trillions of tokens,http://arxiv.org/pdf/2210.01738,,,,,,,,
DETR,End-to-End Object Detection with Transformers,https://www.mdpi.com/2072-4292/15/21/5130/pdf?version=1698331882,,,,,,,,
Thumbs Up?,Thumbs up? Sentiment Classification using Machine Learning Techniques,https://aclanthology.org/2023.emnlp-main.869.pdf,,,,,,,,
VD-RHN,Recurrent Highway Networks,https://arxiv.org/pdf/2207.03523,,,,,,,,
Fisher-Boost,Improving the Fisher Kernel for Large-Scale Image Classification,https://arxiv.org/pdf/1804.04527,,,,,,,,
Pandemonium (morse),Pandemonium: a paradigm for learning,https://link.springer.com/content/pdf/10.3758/BF03200876.pdf,,,,,,,,
DINOv2,DINOv2: Learning Robust Visual Features without Supervision,https://arxiv.org/pdf/2310.06313,,,,,,,,
Advantage Learning,Increasing the Action Gap: New Operators for Reinforcement Learning,https://arxiv.org/pdf/2309.07548,,,,,,,,
ProteinBERT,ProteinBERT: a universal deep-learning model of protein sequence and function,https://arxiv.org/pdf/2110.05006,,,,,,,,
WizardCoder-15.5B,WizardCoder: Empowering Code Large Language Models with Evol-Instruct,http://arxiv.org/pdf/2305.14879,,,,,,,,
GLaM,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/pdf/2208.01448,,,,,,,,
BLSTM for handwriting (2),Unconstrained Online Handwriting Recognition with Recurrent Neural Networks,https://www.frontiersin.org/articles/10.3389/fenrg.2023.1128201/pdf,,,,,,,,
Thumbs Up?,Thumbs up? Sentiment Classification using Machine Learning Techniques,https://downloads.hindawi.com/journals/ietsfw/2023/5566781.pdf,,,,,,,,
DETR,End-to-End Object Detection with Transformers,https://arxiv.org/pdf/2312.10529,,,,,,,,
"Listen, Attend and Spell","Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",https://limits.pubpub.org/pub/wm1lwjce/download/pdf,,,,,,,,
DrLIM,Dimensionality Reduction by Learning an Invariant Mapping,https://www.mdpi.com/2079-9292/12/11/2489/pdf?version=1685543267,,,,,,,,
Hiero,A Hierarchical Phrase-Based Model for Statistical Machine Translation,https://content.sciendo.com/downloadpdf/journals/cait/17/2/article-p28.pdf,,,,,,,,
CogVideo,CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,http://arxiv.org/pdf/2204.03638,,,,,,,,
Diffractive Deep Neural Network,All-optical machine learning using diffractive deep neural networks,https://www.mdpi.com/1424-8220/22/20/7754/pdf?version=1666667704,,,,,,,,
SemVec,Linguistic Regularities in Continuous Space Word Representations,https://www.jair.org/index.php/jair/article/download/13353/26748,,,,,,,,
BOXES,BOXES: AN EXPERIMENT IN ADAPTIVE CONTROL,https://arxiv.org/pdf/2301.03403,,,,,,,,
PSPNet,Pyramid Scene Parsing Network,https://www.frontiersin.org/articles/10.3389/fphys.2023.1166061/pdf,,,,,,,,
Q-learning,Learning from delayed rewards,https://journals.plos.org/plosbiology/article/file?id=10.1371/journal.pbio.3001985&type=printable,,,,,,,,
CoCa,CoCa: Contrastive Captioners are Image-Text Foundation Models,https://arxiv.org/pdf/2309.05950,,,,,,,,
Word2Vec (large),Distributed Representations of Words and Phrases and their Compositionality,https://arxiv.org/pdf/2307.02443,,,,,,,,
Mnemonic Reader,Reinforced Mnemonic Reader for Machine Reading Comprehension,https://www.aclweb.org/anthology/D19-1170.pdf,,,,,,,,
BigGAN-deep 512x512,Large Scale GAN Training for High Fidelity Natural Image Synthesis,http://arxiv.org/pdf/2306.01902,,,,,,,,
Conv-DBN,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,https://eprints.qut.edu.au/130461/1/33281336.pdf,,,,,,,,
NeuMF (Pinterest),Neural Collaborative Filtering,https://dl.acm.org/doi/pdf/10.1145/3583780.3614801,,,,,,,,
BigSSL,BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition,http://arxiv.org/pdf/2306.15265,,,,,,,,
DeepNet,"DeepNet: Scaling Transformers to 1, 000 Layers",http://arxiv.org/pdf/2209.01530,,,,,,,,
Gopher (280B),"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",https://www.nature.com/articles/s41586-023-06647-8.pdf,,,,,,,,
DOT(S)-RNN,How to Construct Deep Recurrent Neural Networks,https://www.ijcai.org/proceedings/2020/0570.pdf,,,,,,,,
ProgressiveGAN,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",https://arxiv.org/pdf/2307.01091,,,,,,,,
DNABERT,DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome,http://arxiv.org/pdf/2108.10808,,,,,,,,
Binarized Neural Network (MNIST),BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1,https://arxiv.org/pdf/1802.03494,,,,,,,,
Constituency-Tree LSTM,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-020-00383-w,,,,,,,,
Learning deep architectures,Learning Deep Architectures for AI,https://www.mdpi.com/2227-7390/11/12/2674/pdf?version=1686581685,,,,,,,,
SACHS,Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data,https://link.springer.com/content/pdf/10.1007%2F978-3-030-50153-2_44.pdf,,,,,,,,
IMPALA,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://arxiv.org/pdf/2106.10566,,,,,,,,
Hopfield Networks (2020),Hopfield Networks is All You Need,http://arxiv.org/pdf/2305.16338,,,,,,,,
CoCa,CoCa: Contrastive Captioners are Image-Text Foundation Models,https://arxiv.org/pdf/2211.10590,,,,,,,,
data2vec (language),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",http://www.cell.com/article/S2666389922002410/pdf,,,,,,,,
ConSERT,ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer,http://arxiv.org/pdf/2212.08802,,,,,,,,
Restricted Bolzmann machines,Restricted Boltzmann machines for collaborative filtering,https://figshare.com/articles/journal_contribution/A_Group-Specific_Recommender_System/3803748/1/files/5921967.pdf,,,,,,,,
Decoupled weight decay regularization,Decoupled Weight Decay Regularization,https://genomebiology.biomedcentral.com/counter/pdf/10.1186/s13059-023-03153-y,,,,,,,,
CoCa,CoCa: Contrastive Captioners are Image-Text Foundation Models,http://arxiv.org/pdf/2303.02141,,,,,,,,
REINFORCE in Stochastic Connectionism,Simple statistical gradient-following algorithms for connectionist reinforcement learning,https://aclanthology.org/2023.findings-emnlp.356.pdf,,,,,,,,
Once for All,Once for All: Train One Network and Specialize it for Efficient Deployment,https://arxiv.org/pdf/2004.14525,,,,,,,,
PreTrans-3L-250H,Speech recognition with deep recurrent neural networks,https://petsymposium.org/popets/2023/popets-2023-0007.pdf,,,,,,,,
PolyCoder,A systematic evaluation of large language models of code,http://arxiv.org/pdf/2306.09523,,,,,,,,
Motion-Driven 3D Feature Tracking,A Combined Corner and Edge Detector,https://www.mdpi.com/1424-8220/21/4/1313/pdf?version=1613981079,,,,,,,,
YouTube Video Recommendation System,The YouTube video recommendation system,https://iopscience.iop.org/article/10.1088/1742-6596/2310/1/012084/pdf,,,,,,,,
Genetic algorithm,Numerical testing of evolution theories,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/stc.1898,,,,,,,,
DeLight,DeLighT: Deep and Light-weight Transformer,https://aclanthology.org/2022.emnlp-main.475.pdf,,,,,,,,
RefineNet,RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation,https://journalajrcos.com/index.php/AJRCOS/article/download/260/519,,,,,,,,
Semi-Supervised Embedding for DL,Deep learning via semi-supervised embedding,https://link.springer.com/content/pdf/10.1007/s10994-019-05855-6.pdf,,,,,,,,
Cutout-regularized net,Improved Regularization of Convolutional Neural Networks with Cutout,http://arxiv.org/pdf/2209.09236,,,,,,,,
DMN,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,https://www.aclweb.org/anthology/N18-1114.pdf,,,,,,,,
Projected GAN,Projected GANs Converge Faster,https://arxiv.org/pdf/2208.13753,,,,,,,,
ReLU (LFW),Rectified Linear Units Improve Restricted Boltzmann Machines,http://arxiv.org/pdf/2306.10171,,,,,,,,
TransE,Translating Embeddings for Modeling Multi-relational Data,https://heritagesciencejournal.springeropen.com/counter/pdf/10.1186/s40494-023-01042-y,,,,,,,,
Image-to-image cGAN,Image-to-Image Translation with Conditional Adversarial Networks,https://www.tandfonline.com/doi/pdf/10.1080/13658816.2023.2262550?needAccess=true,,,,,,,,
data2vec (speech),"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",https://arxiv.org/pdf/2304.05919,,,,,,,,
MetaLM,Language Models are General-Purpose Interfaces,http://arxiv.org/pdf/2306.06687,,,,,,,,
BERT-Large-CAS (PTB+WT2+WT103),Language Models with Transformers,https://ojs.aaai.org/index.php/AAAI/article/download/5874/5730,,,,,,,,
Retrieval-Augmented Generator,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,http://arxiv.org/pdf/2302.07842,,,,,,,,
Naive Bayes,Pattern classification and scene analysis,https://www.mdpi.com/1424-8220/21/8/2873/pdf?version=1619138353,,,,,,,,
TSN,Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,https://arxiv.org/pdf/2202.07925,,,,,,,,
Enhanced Neighborhood-Based Filtering,Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights,https://eng.ucmerced.edu/people/wwang5/papers/slt14a.pdf,,,,,,,,