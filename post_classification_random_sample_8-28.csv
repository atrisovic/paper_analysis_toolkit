multisentence,classification,classification_order,labels,modelKey,modelTitle,modelId,modelYear,paperId,paperYear,strippedModelKey,answer_vector,answer_string
"the severity of blackleg in the plant at maturity was related to the inoculum later produced from the stubble (lo-pelzer et al, 2009b; mcgee and emmett, 1977). the formation of pseudothecia and hence the discharge of primary inoculum was influenced by the genotype of the infected stubble (marcroft et al, 2004b) and by chemical treatment (<cite>wherrett et al, 2003</cite>). the quantification of inoculum was achieved either by direct counting under the microscope (lo-pelzer et al, 2009b) or by visually recording pseudothecial density in classes from 0 to 100% (by 10% increment) of the stubble surface area infested with pseudothecia (marcroft et al, 2004b; wherrett et al, 2003).",context,,set(),533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,e55ead6b1179070814a6404de960ceeb8b04f37e,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"in our experiments, this is represented by interactions that are not included in the training set but in the test set. therefore, we introduced the ""unknown"" token which is one way to deal with the out-of-vocabulary problem [51],<cite>[58]</cite>. note that the created negative examples of the amazon training set are not included in the trigrams.",context,,{'method'},871_seq2seq_lstm,sequence to sequence learning with neural networks,cea967b59209c6be22829699f05b8b1ac4dc092d,2014.0,35fe3c2cfa6f29eb986c3875b0cccfe9dedddefb,2024.0,seq2seq_lstm,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"this provides an estimate of the recommendation performance for each dataset, achievable when no payload optimization is performed in federated mode. ### hyper-parameters

to ensure the fair treatment of all three methods, we adapted the same hyper-parameter settings for fcf (as shown in table 3) that were found to be optimal from the previous studies ammad-ud-din et al (2019), <cite>flanagan et al (2021)</cite>. the fcf-bts specific hyper-parameters of the prior were set as \((\mu_{\theta},\tau_{\theta})=(0,10000)\) and the regularization of the reward was set as \(\gamma=0.999\).",context,,{'method'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,06366ddd943e9a894d97cb01138b99ef82c47162,,ase,"[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"this means that similar concepts in the space of human cognition are also similar in the spaces of both multisensory and text-derived representations (2) the results of

figure 1: a demo representation of the concept “fromey” in the concept representation dataset mentioned in this article. the circular bar of the same concept “noney” is shown here in lcb23(lynott and cornell, 2009, 2013), lancaster40k(lynott et al, 2019), bbsr (binder et al, 2016), word2vec (<cite>mikolov et al, 2013</cite>), and glove (pennington et al, 2014). it is obvious that the multisensory vectors have good interpretability, as each dimension has clear information referring to it, whereas we are unsure what each dimension in the text-derived vectors represents.",context,,{'result'},432_word2vec_(small),distributed representations of words and phrases and their compositionality,87f40e6f3022adbc1f1905e3e506abad05a9964f,2013.0,102946a292c31bf1516ef3054591b82118dbe9f5,,word2vec_(small),"[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"except for polygames, the used adaptive evaluation function (learned by each learning process) is a neural network of the following form: a residual network (he _et al_, 2016) with a convolutional layer with 83 filters, followed by 4 residual blocks (2 convolutions per block with 83 filters each), and followed by two fully connected hidden layers (with 425 neurons each). the activation function used is the relu (<cite>glorot _et al_, 2011</cite>). the network architecture used for polygames is an adaptations of the architecture being used with descent framework-based algorithms, in order to add a policy while keeping an analogous number of weights in the neural network.",context,,{'appendix'},106_deep_rectifier_networks,deep sparse rectifier neural networks,67107f78a84bdb2411053cb54e94fa226eea6d8e,2011.0,9be2bf970a8c4a1b948c77b63f1a72c70b596302,2023.0,deep_rectifier_networks,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"unified i/o (lu et al, 2022a) further introduces more tasks like segmentation and depth estimation into a unified framework. another category of research focuses on building vision-language representation models (radford et al, 2021; jia et al, 2021; zhai et al, 2022; <cite>yuan et al, 2021</cite>; yang et al, 2022a). clip (radford et al, 2021) leverages contrastive learning and large amounts of data to align images and language in a semantic space, resulting in strong generalization capabilities across a wide range of downstream tasks.",context,,{'background'},178_florence,florence: a new foundation model for computer vision,21ec90872abd986c12afe39bebe807732ffa70c9,2021.0,5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a,2023.0,florence,"[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"wang et al [25] described the underlying reasoning behavior through bayesian networks, resisting attackers' reasoning attacks on sensitive information. petrolini et al <cite>[26]</cite> developed a classifier that can monitor documents containing sensitive data, making it easier to identify and protect sensitive information. bracamonte et al [27] studied users' perceptions of monitoring sensitive information tools, quantitatively and qualitatively applying their reactions.",context,,{'background'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,ae2beff7e012c1d491103316779daea5263fba80,,ase,"[0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"##  1 introduction

object detection is a fundamental task in computer vision, which aims to locate and categorize semantic regions with bounding boxes. traditionally, there are two-stage [13],[19],<cite>[33]</cite> methods based on the densely tiled anchors or one-stage [25],[28],[31],[32],[41] methods built on either anchors or grid points. however, they are both complained for handcrafted designs, _e.g._ the anchor shapes, the standard for positive and negative training samples assignment, and the extra post processing step, like non-maximum suppression (nms).",context,,{'introduction'},603_faster_r-cnn,faster r-cnn: towards real-time object detection with region proposal networks,424561d8585ff8ebce7d5d07de8dbf7aae5e7270,2015.0,1788fa6daa8c85d35fad424e8d51c0503d55e733,2022.0,faster_r-cnn,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"in comparison to proxylessnas [4] for imagenet-1k, dca-nas yields 32% smaller model in terms of model parameters for similar accuracy. in comparison to dnas methods [25],[45] for each of the three datasets, we observe that the performance of the dca-nas searched models is retained to a certain extent as resources are further limited

\begin{table}
\begin{tabular}{l c c c c c} \hline
**method** & \multicolumn{3}{c}{**test error (\%) parameters flops search cost search**} \\  & **top-1** & **top-5** & **(mil)** & **(mil)** & **(gpu days)** & **strategy** \\ \hline inception-v1 (2015) [35] & 30.2 & 10.1 & 6.6 & 1448 & - & manual \\ mobilenet,v1 (2017) [17] & 29.4 & 10.5 & 4.2 & 569 & - & manual \\ mobilenet,v2 (2018) [33] & 72.0 & 91.0 & 3.4 & 300 & - & manual \\ shufflenet 2\(\times\) (v2) (2018) <cite>[28]</cite> & 25.1 & - & 5 & 591 & - & manual \\ \hline masenet-92 (2020) [14] & 25.2 & 8.0 & 4.4 & 388 & - & rl \\ ameonet-c (2019) [31] & 24.3 & 7.6 & 6.4 & 570 & 3150 & evolution \\ \hline darts+cutout (2018) [25] & 26.7 & 8.7 & 4.7 & 574 & 1.0 & gradient \\ snas (2018) [43] & 27.3 & 9.2 & 4.3 & 522 & 1.5 & gradient \\ gdas (2019) [10] & 26.0 & 8.5 & 5.3 & 545 & 0.3 & gradient \\ bayesnas (2019) [49] & 26.5 & 8.9 & 3.9 & - & 0.2 & gradient \\ p-darts (2018) [30] & 24.4 & 7.4 & 4.9 & 557 & 0.3 & gradient \\ sgas (cf1.1 best) (2020) [23] & **24.2** & **7.2** & 5.3 & 585 & 0.25 & gradient \\ sdarts-adr (2020) [5] & 25.2 & 7.8 & 6.1 & - & 0.4 & gradient \\ shapley-nas (2022) [42] & 24.3 & - & 5.1 & 566 & 0.3 & gradient \\ \hline rc-darts (2019) [20] & 25.1 & 7.8 & 4.9 & 590 & 1 & rcas \\ \hline
**dca-nas** & **25.1** & 8.1 & **5.1** & 578 & **0.06** & rcas \\ \hline proxylessnas (gpu) (2019) [4](imagenet) & 24.9 & 7.5 & 7.1 & 465 & 8.3 & gradient \\ pc-darts (2019) [45] (imagenet) & 24.2 & 7.3 & 5.3 & 597 & 3.8 & gradient \\ dfnas (2020) [6] (imagenet) & 24.2 & 7.3 & 5.2 & 644 & 3.9 & gradient \\ darts+pt (2021) [38] (imagenet) & 25.5 & - & 4.7 & 538 & 3.4 & gradient \\ shapley-nas (2022) [42] (imagenet) & 23.9 & - & 5.4 & 582 & 4.2 & gradient \\ \hline rcnet-b (2019) [44] (imagenet) & 25.3 & 8.0 & 4.7 & 471 & 9 & rcas \\ \hline
**dca-nas-5.5** & **mil(imagenet)** & **24.4** & **7.2** & **5.3** & **597** & **1.9** & rcas \\ \hline \end{tabular}
\end{table}
table 2: performance and comparison of architectures evaluated on imagenet-1k. the label ”(imagenet)” indicates that the architecture has been searched and evaluated on imagenet-1k.",context,,"{'method', 'result'}",218_shufflenet_v2,shufflenet v2: practical guidelines for efficient cnn architecture design,c02b909a514af6b9255315e2d50112845ca5ed0e,2018.0,53ed7feda1c5ac16db789a7a96e4411c960711b5,2023.0,shufflenet_v2,"[0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"we used eeglab software embedded topoplot function to construct beam images from eeg signals with the independent components of eeg signal after ica processing. with the obtained beam images, a set of highly acknowledged cnn models were deployed in the cnn module, including vgg [49], resnet [50], inception <cite>[51]</cite>, densenet [52], and xception [53]. this study selects the most widely used classifiers from different research domains for comprehensive analysis, including vgg-19, resnet50, inceptionv3, densenet121, and xception.",context,,{'method'},672_googlenet___inceptionv1,going deeper with convolutions,e15cf50aa89fee8535703b9f9512fca5bfc43327,2014.0,07438ac539478419fbf6c8a9c29cc47deb31af93,2024.0,googlenet___inceptionv1,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"the geoeye panchromatic channel has a 0.4 m spatial resolution and allows rgb to be enhanced. worldview-2 provides eight spectral bands with a spatial resolution of 2 m. an additional source of very high remote sensing data is basemaps, with rgb bands such as those provided by maxar one <cite>[38]</cite>. nevertheless, the majority of works focus on using only the wide multispectral range (more than eight bands), sacrificing the spatial resolution.",context,,{'background'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,f6f0b1ffbcf5162363e087fe394fea6f5261e150,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"herein, the behaviors of users based on the data collected in the past are analyzed to determine the connection between users and their items of interest, which can aid in recommending items to the users with similar preferences and support considering the opinions of different users. a hybrid recommendation approach that combines user-based and item-based cf techniques was proposed for mobile product and service recommendation [28],<cite>[29]</cite>,[30]. although cf can achieve efficiency based on similar measurements of users' interests and recommended items, it is difficult to exploit the cross features completely owing to the lack of deep and effective feature extraction.",context,,"{'background', 'method'}",533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,54a86eed40ab1ad03b13b221192e8a2623f5094d,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"while contrastive pretraining has been widely applied to classification problems [7],[13],[15],[16], there have been few works about segmentation [2],[6]. recent works [1],<cite>[6]</cite>,[35] propose to include pseudo labels at the pixel (decoder) level and not after the encoder, but, due to high computational burden, they can only consider 2d images and not whole 3d volumes. furthermore, many datasets contain several weak metadata at the exam level (e.g., pi-rads score) obtained by different annotators.",context,,{'introduction'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,14821371d35e817ff2c140cf0c3a434b1f3a83fe,,ase,"[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"islam _et al_. [19] find padding implicitly injects positional information in resnet [15] and vgg <cite>[47]</cite>, verified with an auxiliary positional encoding module. the experiments in [24] further show that the effects of padding vary among different architectures [4],[17].",context,,{'background'},597_vgg19,very deep convolutional networks for large-scale image recognition,eb42cf88027de515750f230b23b1a057dc782108,2014.0,19a34d0a79dd2eda93a5c013cb7cdd5f4638c57f,,vgg19,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
the hifi-gan vocoder for both the hubert and mel-spectrogram model is based on the original implementation [27]. the hubert hifi-gan is trained on the librispeech train-clean-100 subset <cite>[32]</cite> to vocode activations from layer 6 of the hubert base model [18]. the mel-spectrogram hifi-gan is trained on the google speech commands dataset.,context,,{'method'},1034_imagen,photorealistic text-to-image diffusion models with deep language understanding,9695824d7a01fad57ba9c01d7d76a519d78d65e7,2022.0,7393c61f5dbfa5e53004cc0d31f7c5b23846a23b,2022.0,imagen,"[0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0
 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"if the training data used to build models is biased or of poor quality, it can lead to biased predictions and inaccurate results when deployed as a service. pre-trained large models may inherit biases present in the training data, leading to biased or unfair outcomes <cite>[71]</cite>,[72]. careful evaluation and mitigation of biases are necessary to ensure the ethical and fair application of the models.",context,,set(),533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,71263bbf0253e7e4301babd96edb4883f9a8388a,2023.0,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"assuming that input objects will have different sizes in practice, we cannot obtain optimal packing with the fixed output image size or width-to-height ratio. to calculate the hard height limit, we use equation <cite>(2)</cite>. \[\hat{h}=max\bigg{(}maxh,\theta\frac{\sum_{t=1}^{n}\widehat{h}_{t}}{|\sqrt{n}| }\bigg{)} \tag{2}\]

the fraction in equation (2) estimates the required value of height to make a square scene.",context,,"{'method', 'background'}",533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,0815a43e8868136db3c20d81c2713a1a0a4eb28c,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"further, these models require minimal inductive bias by design which enables them to effectively model complex functions and capture relationships from large scale datasets [28]. these salient features allow vision transformers to perform exceptionally for computer vision tasks and better tackle nuances like occlusions, adversarial perturbations<cite>[55]</cite>,[56]. the capacity of transformer architectures to learn from large scale pre-training and their ability to capture long range content dependent interactions has lead to progress towards utilising these models for processing multiple modalities for vision tasks like object detection [23],[27],[47] and classification [61],[37],[43].",context,,{'background'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,b46065f3e4453939c87dd0b10f3e22449d6d68c0,2022.0,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"our method is implemented with pytorch library [34]. the densenet-121 network <cite>[11]</cite> used as the backbone of our aesthetics feature encoding fcn is pretrained on imagenet [36]. after assembling our rgnet architecture, we fine-tune the entire network end-to-end with images in the domain of aesthetics prediction.",context,,set(),322_densenet-264,densely connected convolutional networks,5694e46284460a648fe29117cbc55f6c9be3fa3c,2016.0,cf220d3415f5a48305e693a58130821224a7d46f,,densenet-264,"[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"this is an attention-based process. however, unlike traditional self-attention <cite>[18]</cite>,[76],[80] whose query, key, and value come from the same input, we propose to use a _non-self attention_ mechanism, where the query is from the target frame and the key and value are from neighbouring frames. besides, we only update the query during the iterative running of non-self attention, but we keep the context tokens unchanged.",context,,{'method'},214_vit-base_32,an image is worth 16x16 words: transformers for image recognition at scale,268d347e8a55b5eb82fb5e7d2f800e33c75ab18a,2020.0,4b0da4745b04b3a89bf80575d45e4e0590ec25ec,,vit-base_32,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"the markov blanket algorithm has been used to built the causal graphs. the markov blanket (mb) algorithm theoretically selects a optimal set of features considering their performance both individually and in group to predict the variable target \(t\) (see <cite>[28]</cite>). therefore, with mb(\(t\)) it is enough to know the distribution of \(t\) and all the values of the other variables that do not belong to mb(\(t\)) become superfluous.",context,,set(),533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,aff33da68cc8b67f5ae6a49bf09727252c2ee06c,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"moreover, the provided feature vectors have been considered in this experiment: de in five frequency bands (\(\delta,\theta,\alpha,\beta\) and \(\gamma\) bands) in [25],[26]. for <cite>[21]</cite>,[8] psd has been considered with the same frequency bands limit and methodologies as in the original paper [8]. ### _domain classification_

it has been noted that the recent advance in dl leads to novel training methodology increasing estimation accuracy.",context,,{'method'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,d687eda8623744daf0643fe66a428ce01f7c926f,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]",
"(1)) favors transferable representations based on global feature vector \(\mathbf{h}_{i}\), it might ignore some local discriminative information in feature map \(\hat{\mathbf{x}}_{i}\) which could be beneficial in meta-testing. inspired by [1],<cite>[3]</cite>,[16],[32], we compute self-supervised contrastive loss at the local level. unlike previous approaches, we leverage map-map and vector-map modules to boost the robustness and generalizability of the representations.",context,,"{'method', 'background'}",300_simclr,a simple framework for contrastive learning of visual representations,34733eaf66007516347a40ad5d9bbe1cc9dacb6b,2020.0,d227a95f86781c0395da37c248b2c14e42b447f3,,simclr,"[0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"this section does a holistic study of two mid-scale yfcc15m versions. v1 from clip <cite>[18]</cite> and v2 from declip [9]. data statisticswe present statistics of two versions yfcc15m on examples number, mean/std of caption length, mean english word ratio, and the vocabulary size (unique tokens) in table 2. the v2 consists of 15.4m image-text pairs, 0.6m(3%) more than v1.",context,,set(),1042_clip_(vit_l_14@336px),learning transferable visual models from natural language supervision,6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4,2021.0,a4b61fa58ab3df61be36600f5a6647f958d3766b,2022.0,clip_(vit_l_14@336px),"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"supervised feature learning has also been used in many biqa methods, where the feature extractor and regression model are learned jointly. recently, deep learning has made a great progress in a variety of vision tasks <cite>[30]</cite>,[31],[32],[33]. researchers have shown an increasing interest in applying deep neural networks to biqa [34],[35],[36],[37],[38],[39].",context,,{'introduction'},844_vgg16,very deep convolutional networks for large-scale image recognition,eb42cf88027de515750f230b23b1a057dc782108,2014.0,3bdbbaf05123db27b11d8bc8b7c339dfe1f21a51,,vgg16,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"the rnn and gp-based models were _not_ given direct information about the difficulty and group of the tasks since this information is typically not known at test time. instead, each task was associated with a 50-dimensional glove word vector <cite>[25]</cite> computed from the task descriptions in fig. 2 (the average of all the word vectors in each description).",context,,{'method'},693_glove_(32b),glove: global vectors for word representation,f37e1b62a767a307c046404ca96bc140b3e68cb5,2014.0,56f22faffc8ec8fcf8f077be964360936d3e3f21,2018.0,glove_(32b),"[0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]",
"they directly predict object categories and bounding boxes at each location of feature maps generated by a single cnn backbone. note that most state-of-the-art object detectors (including both one-stage detectors <cite>[12]</cite>,[16],[19] and two-stage detectors [20]) make predictions based on anchor boxes of different scales and aspect ratios at each convolutional feature map location. however, the usage of anchor boxes may lead to high imbalance between object and non-object examples and introduce extra hyper-parameters.",context,,{'background'},945_retinanet-r101,focal loss for dense object detection,79cfb51a51fc093f66aac8e858afe2e14d4a1f20,2017.0,18a9d1f9e27369d8edf12717ca003b861095823e,,retinanet-r101,"[0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"parameter-sharing based nas methods aim to reduce the typically high training cost (pham et al, 2018; liu et al, 2019; kokiopoulou et al, 2019). optimization for multi-factor quality/cost trade-offs have been explored (<cite>tan et al, 2019</cite>). the proposed method is capable to dynamically extend the system, adding capacity or novel structures in an unconstrained fashion.",context,,set(),655_mnasnet-a1_+_ssdlite,mnasnet: platform-aware neural architecture search for mobile,693c97ecedb0a84539b7162c95e89fa3cd84ca73,2018.0,8f61a198bf9f97e445267f8b48d459e4bac9a14c,2022.0,mnasnet-a1_+_ssdlite,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"in particular, non-diacritic and diacritic texts are considered as source and target languages respectively, and machine translation models are trained to learn how to restore diacritics. ### _phrase-based machine translation_

phrase-based machine translation is one type of statistical machine translation that translate phrases in source language to phrases in target language <cite>[8]</cite>,[9]. the main idea of this approach is an input sentence is segmented into a number of sequences of consecutive words (phrases).",context,,{'method'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,2d4f27c6a963acf2630bd58890c4405876244e02,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"as semantic spaces, we used the original word2vec as also enriched semantic spaces, namely: glove, fasttext, dependency, bert and t5. glove (<cite>pennington et al, 2014</cite>) is a count-based method that trains on aggregated global word-word co-occurrence statistics from a corpus. the fasttext (joulin et al, 2016) model uses morphological information.",context,,{'method'},1094_glove_(6b),glove: global vectors for word representation,f37e1b62a767a307c046404ca96bc140b3e68cb5,2014.0,c1c29622ffe0c25930123901111ed7ef59dc7dc3,,glove_(6b),"[0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0
 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1]",
"\(08\) in semantickitti [1]) as the validation set. in summary, we choose datasets with different numbers of laser beams (_i.e._, \(32\) for nuscenes [12] and \(64\) for semantickitti <cite>[1]</cite> and scribblekitti [51]), different inclination ranges (_i.e._, \([10^{\circ},-30^{\circ}]\) for nuscenes [12] and \([3^{\circ},-25^{\circ}]\) for semantickitti <cite>[1]</cite> and scribblekitti [51]), and different annotation proportions (_i.e._, \(100\%\) for nuscenes [12] and semantickitti <cite>[1]</cite> and \(8.06\%\) for scribblekitti [51]). our proposed ssl framework exhibit constant and evident improvements on all three datasets, which further verifies the scalability of our approaches.",context,,set(),533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,8c976863f941219f0d2b70f51c1048a9dee98780,,ase,"[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"we therefore consider ch14isolsq-it to be the best current analytic approach to assess the potential of our solution. additionally, we also compare against a deep network baseline, consisting of a resnet-50 v2 architecture <cite>[22]</cite> directly inferring 3d mesh coordinates. in the following, we will report the reconstruction error, computed as the l2 distance between the estimated and the ground truth shapes (dimensionless for the synthetic results and in mm for the real ones).",context,,"{'method', 'conclusion'}",191_resnet-1001,identity mappings in deep residual networks,77f0a39b8e02686fd85b01971f8feb7f60971f80,2016.0,2e92be75cc8a6ab82b2c2ec244b47645b9c6df3b,,resnet-1001,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"modelnet40 [43], shapenet [4] and three real-scanned dataset i.e. scanobjectnn <cite>[38]</cite>, common objects in 3d (co3d) [30] with our proposed two different experimental setups. ### within-dataset experiment

we design within-dataset experimental setups by ordering all classes in descending order for a dataset based on sample frequency.",context,,set(),533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,364401b21802ebbdbae674ac51c6b73265c5926d,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"the actions consider the moving direction, translation and scaling of the 3d bb. s. iyer et al (2018, 2020) <cite>[7]</cite>,[12] employ two 3d cnns, one for learning the navigation in the coordinate directions and the other to predict the size of the bb dimensions. ### _anchor based approaches_

another often seen approach is using anchor boxes, which are predefined bb guesses of certain scales and aspect ratios.",context,,{'method'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,482a879ccd8f73b8c707d176407248f4442fd266,2020.0,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"(**q\({}_{3}\)**) could a system trained on docnli work well on sentence-level nli? ### the docnli task is challenging

the state-of-the-art systems on sentence-level nli problems are largely based on transformers (vaswani et al, 2017), such as bert, roberta (<cite>liu et al, 2019</cite>), etc. however, they can only handle maximal 512 tokens preprocessed by the wordpiece tokenizer (wu et al, 2016).",context,,{'method'},521_roberta_large,roberta: a robustly optimized bert pretraining approach,077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,2019.0,42f8a3da7021bc725fa14fdb63fa9c7c9fc934f6,,roberta_large,"[0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"many auxiliary loss functions are added to the ce loss to address the feature interpretability issues. for example in <cite>[7]</cite>, angular margin contrastive loss, a geometric constraint, was added to help separate the class features by a predefined margin. the quality of the features slightly improves from these constraints in supervised setups, giving a little improvement over ce loss.",context,,{'introduction'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,b7c2e4d1fcf33f6b4758a45e3f38c29d1fa9afe2,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"extensive experimental results illustrate that rpje outperforms other state-of-the-art baselines on kg completion task, which also demonstrate the superiority of utilizing logic rules as well as paths for improving the accuracy and explainability of representation learning. ##  1 introduction

knowledge graphs (kgs) such as freebase <cite>[1]</cite>, dbpedia <cite>[1]</cite> and nell [15] are knowledge bases which store factual triples consisting of entities with their relations. they have achieved rapid development and extensive applications for various research fields, such as zero-shot recognition [16], question answering [17] and recommender systems [18].",context,,{'introduction'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,ebcaee68bdbad2758fcf498b23ded232c1e3df57,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"##  experimental results

appendix, supplementary figures, and the code used for experiments presented here are available from the github repository of the project - [https://github.com/chiffa/alife2023_goea-sgd](https://github.com/chiffa/alife2023_goea-sgd). ### model used

in order to perform numerical experiments, we used a convolutional neural network (convnet) learning to recognize digits in the mnist dataset <cite>lecun et al (1998)</cite>. it is a well-established model and a textbook use case for sgd, chosen to minimize the chance of unexpected edge cases interfering with our experimental results.",context,,{'method'},211_lenet-5,gradient-based learning applied to document recognition,162d958ff885f1462aeda91cd72582323fd6a1f4,1998.0,6bf10cf55ed638f218c9cd7f4d459e8d35be776b,2023.0,lenet-5,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]",
"##  2 related work

pre-training by masked modeling.compared to contrastive learning methods [21],[8],[9] that learn from inter-sample relations, self-supervised pre-training by masked autoencoding builds the pretext tasks to predict the masked parts of the input signals. the series of gpt [38],<cite>[39]</cite>,[5] and bert [12] apply masked modeling to natural language processing and achieve extraordinary performance boost on downstream tasks with fine-tuning. inspired by this, beit [4] proposes to match image patches with discrete tokens via dvae [40] and pre-train a standard vision transformer [14],[60] by masked image modeling.",context,,{'background'},428_gpt-2_(1.5b),language models are unsupervised multitask learners,9405cc0d6169988371b2755e573cc28650d14dfe,2019.0,34103f1294844c447fe8872bf5c3ab1c7ce32103,2022.0,gpt-2_(1.5b),"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"in section vii, we give some discussions to conclude this paper. ##  ii clonal selection algorithm with immunological memory

this section describes our proposed ais model called clonal selection algorithm with immunological memory (csaim) [5],<cite>[6]</cite>. the area of artificial immune system (ais) has been an ever-increasing interested in not only theoretical works but applications in pattern recognition, network security, and optimization [11],[12],[13].",context,,set(),533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,44b4629126574107778a6d8fb71a14f68090ac3d,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"to solve the imbalance of the number of positive and negative samples in a one-stage network, kaiming he proposed the focal loss method, which can reduce the weight of large sample losses and increases the weight of small samples in total loss. however, focal loss is not effective in practical applications, and evenly reduces map (mean average precision) <cite>[24]</cite>. therefore, the precision of a two-stage network is generally higher than its one-stage network counterpart.",context,,{'background'},573_yolov3,yolov3: an incremental improvement,ebc96892b9bcbf007be9a1d7844e4b09fde9d961,2018.0,ca052c254fef9d7121638daf5fe6df582f451d19,2023.0,yolov3,"[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"or, within a more careful, but also more laborious approach, they average over sufficiently many such maxima. for the studied model these maxima are given by <cite>(45)</cite>, and the comparison will show that maximizing \(l_{\beta\lesssim 1}\) is superior with respect to such random selection methods. let us assume that we know the true joint probability \(\pi_{k}(x,y)\) of \(x\) and \(y\) (the meaning of an integer \(k\) is specified below).",context,,set(),533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,5ae08df2e62014db8225d91b5229125db77452b9,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"* we empirically demonstrate that lwr significantly improves accuracies, robustness, and calibration of various modern networks, and outperforms the state-of-the-art label smoothing based approaches. ##  related work

### label smoothing

<cite>szegedy et al (2016)</cite> propose the label smoothing regularizer (lsr) that utilizes the weighted average of one-hot labels and the uniform distribution as soft labels, and successfully uses it to improve the performance of the inception architecture on image classification. ever since then, many advanced image classification approaches zoph et al (2018); real et al (2019); huang et al (2019) have incorporated lsr into training procedures.",context,,{'background'},609_inception_v3,rethinking the inception architecture for computer vision,23ffaa0fe06eae05817f527a47ac3291077f9e58,2015.0,ca73f80bd7f2865cbd63f7345efd24f8d0d0a29a,,inception_v3,"[0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"\(\mathbf{i}_{h}^{(0)}=\mathbf{f}_{d}\), then, the spatial details are obtained through

\[\mathbf{d}_{h}^{(s)}=g_{s}\big{(}\big{[}\mathbf{i}_{h}^{(s-1)},\mathbf{f}_{d}^{( s-1)}\big{]}_{dim}\big{)}, \tag{10}\]

in which \(\left[\mathbf{a},\mathbf{b}\right]_{dim}\) means the concatenation of \(\mathbf{a}\) and \(\mathbf{b}\) in the spectral dimension and \(g_{s}(\cdot)\) is the generator at the scale \(s\). we can obtain \(\{\mathbf{i}_{h}^{<cite>(1)</cite>},...,\mathbf{i}_{h}^{(s)},...,\mathbf{i}_{h}^{(r)}\}\) by

\[\mathbf{i}_{h}^{(s)}=\frac{u_{2}(\mathbf{i}_{h}^{(s-1)})+\mathbf{d}_{h}^{(s)} }{2} \tag{11}\]

after a cascade of cbs, the refined fusion result is

\[\mathbf{f}_{f}=\mathbf{i}_{h}^{(r)}. \tag{12}\]

similarly, for fbs, the input images are \(\{\mathbf{r}^{(0)},...,\mathbf{r}^{(r-1)}\}\), and we use \(\{\mathbf{d}_{l}^{(1)},...,\mathbf{d}_{l}^{(r)}\}\) and \(\{\mathbf{i}_{l}^{(0)},...,\mathbf{i}_{l}^{(r)}\}\) to denote the detail images and output images.",context,,{'background'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,8ee9c5db8a4c787e83524c4611712754df9799b5,2022.0,ase,"[0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"figure 1: overview of the semi-parametric multitask prompted training. each training and evaluation instance is formatted with unified text-to-text prompt templates <cite>sanh et al (2021)</cite>; bach et al (2022). in this work, we further augment the prompted instances with retrieved passages from a large-scale task-agnostic corpus, c4 sanh et al (2021), which is the same unlabeled pretraining corpus used in t5 raffel et al (2020) and t0 sanh et al (2021).",context,,{'introduction'},427_t0-xxl,multitask prompted training enables zero-shot task generalization,17dd3555fd1ccf1141cf984347fa1b3fd6b009ca,2021.0,ec97c3248537bb0b455b3fe9bc341110cfceffde,2022.0,t0-xxl,"[0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0]",
ca-net [19] proposes a coarse-to-fine strategy to predict gaze direction from face images. gaze360 <cite>[20]</cite> presents a large number of diverse annotated data for robust 3d gaze estimation in an unconstrained environment. it further proposes a 3d gaze model to extend existing models to include temporal information.,context,,{'background'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,e81e48607c7341a01b5f6d5b4bd09e2168154085,2023.0,ase,"[0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"that is, the similarity between a visual feature vector \(\mathbf{u}\) and a textual feature vector \(\mathbf{v}\) is calculated as \(\mathbf{u}^{t}w\mathbf{v}\), where \(w\) are the learnable weight parameters. thanks to the revolutionary advances in computational power, we can now achieve this in a more effective and practical approach termed contrastive learning, where we align quantities of positive samples and push their negative samples away simultaneously in a large mini-batch (<cite>radford et al, 2021</cite>; singh et al, 2022; jia et al, 2021; pham et al, 2021; yuan et al, 2021). **cosine similarity:** the common choice of the distance measure between an image-text pair for the contrastive learning algorithm is the cosine similarity (in both uni-modal chen et al (2020a); caron et al (2020); chen et al (2020b) and cross-modal radford et al (2021); jia et al (2021); singh et al (2022) scenarios).",context,,{'introduction'},1042_clip_(vit_l_14@336px),learning transferable visual models from natural language supervision,6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4,2021.0,12c679ac59289ad8f65fd645211897a1a3f8e0d0,2022.0,clip_(vit_l_14@336px),"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"##  8 related work

language models are increasingly used to power task-oriented dialogue systems, like chatgpt openai (2022) and google's bard (pichai, 2023). they are used as personal assistants and customer support in different domains rastogi et al (2020); <cite>eric et al (2019)</cite>. with this increase in language model ability, there has been an increased focus on ensuring that generated text does not contain harmful content weidinger et al (2021); bender et al (2021); nair et al (2023) for large language models.",context,,{'background'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,d665588837309ee0956daba3a80027bdd87cfeff,2023.0,ase,"[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"\(\upsilon^{\rm jr}\) is the path loss exponent and \(\kappa=\left(\frac{\lambda}{4\pi}\right)^{2}\) with \(\lambda\) is the wavelength. in the same way, the direct link signals from the transmitter received at the \(m\)-th antenna of the receiver are also expressed as follows:

\[d_{mn}^{\rm t}=f_{m}^{\rm t}\sqrt{p^{\rm tr}}s_{n}^{\rm t}, \tag{4}\]

where \(f_{m}^{\rm t}\) denotes the rayleigh fading <cite>[26]</cite> from the transmitter to the receiver. without loss of generality, let \(\mathbb{e}[|f_{m}^{\rm t}|^{2}]=1\).",context,,set(),533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,c44e490e506056d6a58d8336e2b07c3bc9095d04,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"we may categorize the existing works based on their utilized sources of information for scientific impact prediction. in the first category, the graph of the scientific papers is used as the main source of information (sarigol et al, 2014; pobiedina and ichise, 2016; daud et al, 2017; butun et al, 2017; mcnamara et al, 2013; <cite>klimek et al, 2016</cite>). sarigol et al use the co-authorship network of the scientists and the author centrality measures for predicting highly cited papers (sarigol et al, 2014).",context,,{'background'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,0e500beae47864b21c579765a0f9773a171675df,,ase,"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"text perturbation described in section 4.1 is conducted on the samsum dataset for training and validation. ### summarization model configuration

for controllable dialogue summarization, the language backbone _bart_<cite>lewis et al (2020)</cite> is applied. the number of encoder layers, decoder layers, input and hidden dimension are \(6/6/768\) for the _'bart-base'_, and \(12/12/1024\) for the _'bart-large'_ and _'ctrlsum'_.",uses,,{'method'},582_bart-large,"bart: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",395de0bd3837fdf4b4b5e5f04835bcc69c279481,2019.0,766107f5e59590cadb6555cbf864ee0e53971b77,,bart-large,"[1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"igpt [43] firstly mentions masked image modeling (mim). beit [20] propose visual token prediction with the pretrained tokenizer [44], maskfeat [6] predicts the hand-crafted image descriptor, and mae <cite>[25]</cite> directly reconstructs the raw pixels. for spatiotemporal representation learning, videomae [23] and bevt [45] respectively extend mae and beit to spatiotemporal space.",uses,,{'background'},1249_masked_autoencoders,masked autoencoders are scalable vision learners,6351ebb4a3287f5f3e1273464b3b91e5df5a16d7,2021.0,325d8e9501af05e594bd668b6cd6d43ed42c8b4d,2022.0,masked_autoencoders,"[1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0
 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0]",
the representations held the association of word linkage on multi-dimensional level. later these representations were replaced by glove <cite>[21]</cite> which is considered as global vectors for word representations which used context of latent semantic analysis [22] at a very different level which falls under the category of topic modeling [23]. these embeddings were related to word level embeddings and later the concept of character level embedding [24] also started to gain popularity which states that learning characters and making a random word out of context can also generate some semantics of the words respectively.,uses,,{'introduction'},693_glove_(32b),glove: global vectors for word representation,f37e1b62a767a307c046404ca96bc140b3e68cb5,2014.0,7679c6c15c07f3920b43e25de192ec76d5ddaa0b,2022.0,glove_(32b),"[1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"therefore, we define the edge-promoting adversarial loss as:

\[\begin{split}\mathcal{l}_{adv}(g,d)&=\mathbb{e}_{c _{i}\sim s_{data}(c)}[\log d(c_{i})]\\ &+\mathbb{e}_{e_{j}\sim s_{data}(e)}[\log(1-d(e_{j}))]\\ &+\mathbb{e}_{p_{k}\sim s_{data}(p)}[\log(1-d(g(p_{k})))].\end{split} \tag{3}\]

#### 3.2.2 content loss \(\mathcal{l}_{con}(g,d)\)

in addition to transformation between correct manifolds, one more important goal in cartoon stylization is to ensure the resulting cartoon images retain semantic content of the input photos. in cartoongan, we adopt the high-level feature maps in the vgg network <cite>[30]</cite> pre-trained by [27], which has been demonstrated to have good object preservation ability. accordingly, we define the content loss as:

\[\begin{split}\mathcal{l}_{con}(g,d)=\\ &\quad\mathbb{e}_{p_{i}\sim s_{data}(p)}[||vgg_{l}(g(p_{i}))-vgg _{l}(p_{i})||_{1}]\end{split} \tag{4}\]

where \(l\) refers to the feature maps of a specific vgg layer.",uses,,set(),844_vgg16,very deep convolutional networks for large-scale image recognition,eb42cf88027de515750f230b23b1a057dc782108,2014.0,6394a5b15281180591bdf7e6fbdf704bccc899ed,,vgg16,"[0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0
 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1]",
"1 and 2). on motsynth, we focus on three trackers (_i.e._, sort [9], bytetrack [88], and oc-sort [11]) and adhere to the following common evaluation pipeline: _i)_ we compute predicted bounding boxes through yolox <cite>[27]</cite>; _ii)_ as our approach requires an estimate of per-pedestrian camera distances, we exploit yolox bounding boxes by providing them to the distance estimator distsynth (sec. 3.1); _iii)_ we finally integrate our density estimator trackflow into the pipeline of each tracker, applying the normalization described in sec.",uses,,{'method'},1158_yolox-x,yolox: exceeding yolo series in 2021,c01b385205e488a731c8c8c11c0c494d426beb03,2021.0,3aa164d5c40fc5fb9035c41049259c47474531e8,2023.0,yolox-x,"[1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"while for the ml-1m dataset, the logarithmic growth family (represented by seagull and llu) performed the best, see figure 3-right. \begin{table}
\begin{tabular}{l c c} \hline model & movielens 100k & movielens 1m \\ \hline glocal-k [12] & 0.890 & 0.822 \\ sparse fc [22] & - & 0.824 \\ cf-nade [38] & - & 0.829 \\ i-autorec [29] & - & 0.831 \\ gc-mc [4] & 0.910 & 0.832 \\ i-cfn <cite>[31]</cite> & - & 0.8321 \\ bst [5] & - & 0.8401 \\ nnmf [8] & - & 0.843 \\ graphrec [25] & 0.901 & - \\ igmc [37] & 0.905 & - \\ self-supervised exchangeable [14] & 0.910 & - \\ factorized eae [14] & 0.920 & - \\ ae\({}^{2}\)i (current) & **0.887** & **0.817** \\ \hline \end{tabular}
\end{table}
table 1: comparison of rmse test results. the best results are highlighted in bold.",uses,,{'result'},533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,b27f7cfbb0a9977a818dd97faab2886687b1e77c,2023.0,ase,"[1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0
 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]",
"**fasttext**[3] is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. also, fasttext provides an implementation of \(hierarchical\)\(softmax\) based activation function for text classification purposes <cite>[4]</cite>. we used implementation of fasttext for conducting the experiments in this paper.",uses,,set(),944_fasttext,bag of tricks for efficient text classification,892e53fe5cd39f037cb2a961499f42f3002595dd,2016.0,431f46b08ec9f3983baeed4ea1909fa15cd3685b,,fasttext,"[1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
": _a large language model (llm) parameterized by \(\theta\) is a transformer-based model with an autoregressive, autoencoding, or encoder-decoder architecture that has been trained on a large corpus of hundreds of millions to trillions of tokens. llms encompass pre-trained models._

examples of autoregressive models include gpt (radford et al, 2018), gpt-2 (radford et al, 2019), gpt-3 (brown et al, 2020), and gpt-4 (openai, 2023); autoencoding models include bert (<cite>devlin et al, 2019</cite>), roberta (liu et al, 2019), and xlm-r (conneau et al, 2020); and encoder-decoder models include bart (lewis et al, 2020) and t5 (raffel et al, 2020). llms are commonly adapted for a specific task, such as text generation, sequence classification, or question-answering, typically via fine-tuning.",uses,,{'background'},1064_bert-large,bert: pre-training of deep bidirectional transformers for language understanding,df2b0e26d0599ce3e70df8a9da02e51594e0e992,2019.0,bcfa73aedf1b2d1ee4f168e21298a37ac55a37f7,2023.0,bert-large,"[0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0
 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]",
"in case spatial and channel dimensions are preserved, a stride value \(s=1\) is used in the depthwise convolution layer and residual connection is still applied. this small difference between the two types of blocks is further explained in figure 2. bottleneck residual blocks are the building blocks in mobilenetv2 <cite>[31]</cite> which by default uses 17 consecutive layers of this type in its architecture (in addition to the initial convolution layer of 32 filters). table 1 shows more detailed specifications where \(h\) and \(w\) are the input spatial dimensions, \(k\) and \(k^{\prime}\) are the numbers of input and output channels, respectively, \(s\) is the stride value, and \(t\) is the expansion factor.",uses,,set(),505_mobilenetv2,mobilenetv2: inverted residuals and linear bottlenecks,dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4,2018.0,1cbc63a5aebfd3daf5e71b8cb47d2d82a9944f63,,mobilenetv2,"[1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"for the sequence-to-sequence model, mbart liu et al (2020) pre-trains a transformer model by denoising training objective in multiple languages. mt5 <cite>xue et al (2021)</cite> extends the span corruption task for multilingual training and mt6 chi et al (2021) amplify generation task by introducing a partially non-autoregressive objective. along the line of research, different multilingual pre-trained models ma et al (2020); chi et al (2020) are proposed to solve downstream cross-lingual generation tasks.",uses,,{'background'},635_mt5-xxl,mt5: a massively multilingual pre-trained text-to-text transformer,74276a37bfa50f90dfae37f767b2b67784bd402a,2020.0,46d64582890afc296c134c0ba69137bb1d7fbba7,2022.0,mt5-xxl,"[1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0
 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1]",
"it divides the image into a grid predicting bounding boxes and class probabilities for each grid cell. yolo processes the image in one go delivering accurate results, in object detection, <cite>[19]</cite>. it is applied to crowd management for counting and estimating the density of crowds and detecting anomalies in areas to identify suspicious activities or behavior.",uses,,set(),805_yolo,"you only look once: unified, real-time object detection",f8e79ac0ea341056ef20f2616628b3e964764cfd,2015.0,19a3920799e69086d50b9da842042f0a3c3eb263,,yolo,"[1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"the resulting terms are then mapped into a vector space using word or graph embeddings, and the similarity between keyword-dataset pairs is used to calculate the ranking. semantic matching solutions can also apply deep contextualized language models, such as bert <cite>[27]</cite>, to match keywords with datasets on the basis of their semantics rather than their syntax. in **natural language processing (nlp)**, bert has achieved impressive results on a wide variety of tasks (e.g., [2]).",uses,,set(),1064_bert-large,bert: pre-training of deep bidirectional transformers for language understanding,df2b0e26d0599ce3e70df8a9da02e51594e0e992,2019.0,62d73ff4302304027621e86a31acd6c0c2d33401,,bert-large,"[1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]",
"to perform object detection, an input image is abstracted into a set of features. this can include low-level visual properties, such as colour or texture captured by sift (lowe, 1999) or hog (<cite>dalal and triggs, 2005</cite>), to more sophisticated mid-level representations such as cnn (krizhevsky et al, 2012). detection is achieved by comparing features of the input image to object models that are learned from sets of labeled training images (object vs. non-object).",uses,,set(),535_histograms_of_oriented_gradients,histograms of oriented gradients for human detection,e8b12467bdc20bde976750b8a28decdb33246d1d,2005.0,163d95ce550e58843cc5129972ba048214f4f7cf,,histograms_of_oriented_gradients,"[0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0
 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]",
"we sample 5,000 questions for each type (see examples in table 1) in our training set. #### 6.2.2 context-dependent qa

we started with unifiedqa (<cite>khashabi et al, 2020</cite>), a pre-trained language model for robust cross-domain qa, and observed two issues in real user conversations: (1) it often fails to detect _unanswerable_ questions; and (2) it sometimes generates unrelated hallucinated answers. for example, when

\begin{table}
\begin{tabular}{l|l|l} \hline question type & example & context \\ \hline mrc & use what tool to blend?",uses,,set(),645_unifiedqa,unifiedqa: crossing format boundaries with a single qa system,ad5970584754cc7a1d91c95ab84a1e210258183a,2020.0,e2c1b21537708034c6ce32245136e3e965da6634,2022.0,unifiedqa,"[0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0
 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]",
"subsequently, we also set bounds on the caption length between 4 to 70 words. any captions lesser than 4 words would not be informative enough for extracting usable features and captions larger than 70 words cannot be handled by the clip-based text encoder <cite>[13]</cite> that we employ in our experiments. following caption-based filtering, we also remove all images where human faces are clearly visible in the foreground.",uses,,set(),1104_clip_(resnet-50),learning transferable visual models from natural language supervision,6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4,2021.0,0b5b54b7326eff71505a8ff8f3dd587d9b9ac530,,clip_(resnet-50),"[0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"genre-label count. \begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline model & \(\mathcal{p}\) & \(\mathcal{r}\) & \(sp\) & \(\mathcal{ba}\) & \(\mathcal{fm}\) & \(\mathcal{hl}\) \\ \hline r & 52.04 & 50.44 & 85.36 & 67.90 & 48.97 & 0.18524 \\ rt & 52.54 & 55.53 & 86.67 & 71.10 & 53.57 & 0.17833 \\ rdt & 55.01 & 57.25 & 87.08 & 72.16 & 55.69 & 0.17069 \\ r + rt & 54.76 & 56.28 & 87.04 & 71.66 & 54.69 & 0.16902 \\ r + rdt & 55.35 & 56.46 & 86.97 & 71.72 & 55.20 & 0.16954 \\ rt + rdt & 54.96 & 57.44 & 87.28 & 72.36 & 55.70 & 0.16755 \\ erdt & **55.95** & **57.88** & **87.35** & **72.61** & **56.40** & **0.16546** \\ \hline \end{tabular}
\end{table} table iv: performance by various models for ensemble analysis

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline  & method & \(\mathcal{p}\) & \(\mathcal{r}\) & \(sp\) & \(\mathcal{ba}\) & \(\mathcal{fm}\) & \(\mathcal{hl}\) \\ \hline \multirow{4}{*}{baseline} & resnetov2 [30] & **52.84** & 50.44 & 83.56 & 67.90 & 48.97 & 0.18524 \\  & densenet21 [59] & 26.10 & 27.18 & 76.82 & 53.09 & **22.63** & 0.30120 \\  & efficientnetb2 <cite>[40]</cite> & 51.37 & **52.53** & **85.95** & **69.24** & **51.40** & **0.18503** \\  & wt [11] & 29.11 & 27.40 & 78.61 & 53.01 & 22.58 & 0.25243 \\  & inceptionv3 [41] & 21.03 & 28.80 & 79.12 & 33.96 & 22.43 & 0.25599 \\  & mobilenetv2 & 37.34 & 33.22 & 80.57 & 56.90 & 30.37 & 0.23799 \\ \hline improvement of bdt & 2.97 & 4.72 & 1.13 & 2.93 & 4.29 & 7.55s \\ improvement of erdt & 3.91 & 3.55 & 1.40 & 3.38 & 5.00 & 10.58\% \\ improvement of prerdt & 5.73 & 2.12 & 2.76 & 2.44 & 3.86 & 12.36\% \\ \hline \hline \multirow{4}{*}{sota} & chu et al [19] & 19.73 & 27.32 & — & 20.89 & \\  & gourasick et al [26] & 36.76 & 35.12 & **80.91** & **58.02** & 33.49 & **0.2429** \\  & pohur et al [21] & 28.76 & 47.76 & 68.18 & 57.97 & 34.72 & 0.34688 \\  & wi et al [41] & **52.89** & **51.18** & **-** & **-** & **-** & \\ \hline improvement of rdt & 2.12 & 6.07 & 6.17 & 4.15 & 6.08 & 2.79s \\ improvement of erdt & 3.06 & 6.70 & 6.44 & 14.60 & 6.79 & 31.88\% \\ improvement of prerdt & 4.88 & 3.47 & 7.80 & 13.66 & 5.65 & 33.29\% \\ \hline \multirow{4}{*}{proposed} & rdt & 55.01 & 57.25 & 57.08 & 72.16 & 55.69 & 0.17069 \\  & erdt & 55.95 & **57.88** & 87.35 & **72.61** & **56.40** & 0.16546 \\ \cline{1-1}  & prerdt & **57.77** & 54.65 & **88.71** & 71.68 & 55.26 & **0.16216** \\ \hline \end{tabular}
\end{table} table iii: comparison with baseline and sota models

[missing_page_fail:7]

_(i) ablation study for rdt:_ as discussed earlier, rdt is the composition of a residual network and dense transformer. therefore, here, we first compare the performance of rdt with other component models, such as the residual network (r), transformer network (t), dense transformer (dt), and residual transformer network (rt).",uses,,{'method'},377_efficientnet-l2,efficientnet: rethinking model scaling for convolutional neural networks,4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9,2019.0,33fcb4173ffb1e3381b8ac6bd079d63640447fb8,2023.0,efficientnet-l2,"[1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0]",
"** for mobilenetv2 we consider a lighter detection head, by decreasing the number of proposals for both dpp and baselines. similar to resnet,

\begin{table}
\begin{tabular}{l|c|c|c c c c c c|c} \hline \hline method & \(\bar{n}\) & epochs & ap & ap\({}_{50}\) & ap\({}_{75}\) & ap\({}_{s}\) & ap\({}_{m}\) & ap\({}_{l}\) & gflops \\ \hline faster rcnn-fpn [25] & 2000 & 36 & 40.2 & 43.8 & 43.8 & 24.2 & 43.5 & 52.0 & 14 \\ retinanet <cite>[19]</cite> & - & 36 & 38.7 & 58.0 & 41.5 & 23.3 & 42.3 & 50.3 & 90 \\ detr-dc5 [3] & 100 & 500 & 43.3 & 63.1 & 45.9 & 22.5 & 47.3 & **61.1** & 76 \\ deformable detr [34] & 300 & 50 & 43.8 & 62.6 & 44.2 & 20.5 & 47.1 & 58.0 & 98 \\ sparse r-cnn [27] & 300 & 36 & **45.0** & **64.1** & **49.0** & 27.8 & **47.6** & 59.7 & 25 \\ \hline dpp-xl (ours) & 182.3 & 36 & **45.0** & 63.8 & 48.8 & **28.2** & 47.4 & 59.9 & 15 \\ dpp-l (ours) & 82.6 & 36 & 43.7 & 62.4 & 47.5 & 27.2 & 46.0 & 59.1 & 6.8 \\ dpp-m (ours) & 38.8 & 36 & 42.2 & 60.6 & 45.5 & 23.9 & 44.6 & 58.5 & 3.2 \\ dpp-s (ours) & 25.5 & 36 & 40.4 & 58.2 & 43.4 & 22.0 & 42.8 & 57.0 & 2.1 \\ \hline \hline \end{tabular}
\end{table}
table 2: **comparison to state-of-the-art object detectors on coco validation set with resnet-50. ** four variants of dpp with various sizes are shown, based on fpn.",uses,,{'method'},945_retinanet-r101,focal loss for dense object detection,79cfb51a51fc093f66aac8e858afe2e14d4a1f20,2017.0,1c8b59a2fda46c6aed94ef46f1dc789102502d1f,,retinanet-r101,"[1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"the obtained attention maps and predicted scores are highly correlated with the actual location of infection. in addition, the predicted ge scores

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**model** & **mae \(\downarrow\)** & **pc \(\uparrow\)** & **number of parameters** & **training time** \\ \hline _covid-net_[59] & 4.563 & 0.545 & 12 m & 40 min \\ _covid-net-s_[57] & 4.746 & 0.581 & 12 m & 40 min \\ _resnet50_[20] & 1.107 & 0.684 & 23 m & 1.5 hr \\ _swin transformer_[38] & 0.927 & 0.819 & 29 m & 2 hr \\ _xceptionnet_<cite>[5]</cite> & 0.864 & 0.802 & 23 m & 1.5 hr \\ _inceptionnet_[52] & 0.717 & 0.881 & 24 m & 1.5 hr \\ _feature extraction_[6] & 0.981 & 0.741 & 20 m & 1 hr \\ _mobilenetv3_[21] & 0.864 & 0.822 & 4.2 m & 40 min \\ _vitreg-ip (ours)_ & **0.569** & **0.923** & **5.5 m** & **20 min** \\ \hline \hline \end{tabular}
\end{table}
table 2: geographic extent score prediction results. \begin{table}
\begin{tabular}{l c c c} \hline \hline
**model** & **mae \(\downarrow\)** & **pc \(\uparrow\)** & **number of parameters** & **training time** \\ \hline _covid-net_[59] & 2.249 & 0.531 & 12 m & 40 min \\ _covid-net-s_[57] & 2.227 & 0.525 & 12 m & 40 min \\ _resnet50_[20] & 1.082 & 0.427 & 23 m & 1.5 hr \\ _swin transformer_[38] & 0.811 & 0.692 & 29 m & 2 hr \\ _xceptionnet_[5] & 0.771 & 0.696 & 23 m & 1.5 hr \\ _inceptionnet_[52] & 0.614 & 0.825 & 24 m & 1.5 hr \\ _feature extraction_[6] & 0.881 & 0.701 & 20 m & 1 hr \\ _mobilenetv3_[21] & 0.741 & 0.731 & 4.2 m & 40 min \\ _vitreg-ip (ours)_ & **0.512** & **0.855** & **5.5 m** & **20 min** \\ \hline \hline \end{tabular}
\end{table}
table 3: lung opacity score prediction results.",uses,,{'method'},1014_xception,xception: deep learning with depthwise separable convolutions,5b6ec746d309b165f9f9def873a2375b6fb40f3d,2016.0,f49843338f1ffff7feee1b44ab4e59015c03c1c6,2023.0,xception,"[0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]",
"we include results for the aorta and heart classes in the supplementary material. \begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**class** & **model** & **dice\(\uparrow\)** & **hd\(\downarrow\)** & **assd\(\downarrow\)** & **\% violations\(\downarrow\)** \\ \hline \hline \multicolumn{6}{c}{**aorta dataset**} \\ \hline \multirow{6}{*}{**auc**} & unet [7] & 0.900 \(\pm\) 0.016 & 64.392 \(\pm\) 16.874 & 9.315 \(\pm\) 1.749 & 13.994 \(\pm\) 1.809 \\  & fcn <cite>[30]</cite> & 0.894 \(\pm\) 0.013 & 57.974 \(\pm\) 19.756 & 9.77 \(\pm\) 1.421 & 15.675 \(\pm\) 2.409 \\  & nnunet [21] & 0.906 \(\pm\) 0.020 & 36.368 \(\pm\) 12.559 & 4.563 \(\pm\) 0.675 & 5.424 \(\pm\) 2.461 \\  & topo-crf [2] & 0.897 \(\pm\) 0.057 & 40.162 \(\pm\) 18.687 & 5.952 \(\pm\) 0.999 & 8.358 \(\pm\) 2.151 \\  & midl [33] & 0.912 \(\pm\) 0.008 & 32.157 \(\pm\) 16.270 & 6.405 \(\pm\) 0.524 & 6.377 \(\pm\) 1.661 \\  & nonadj [32] & 0.916 \(\pm\) 0.030 & 32.465 \(\pm\) 18.848 & 4.771 \(\pm\) 1.129 & 4.932 \(\pm\) 1.479 \\  & ours & **0.922 \(\pm\) 0.009** & **25.959 \(\pm\) 13.574** & **3.920 \(\pm\) 0.765** & **3.526 \(\pm\) 1.244** \\ \hline \multirow{6}{*}{**auc**} & unet [7] & 0.677 \(\pm\) 0.015 & 71.109 \(\pm\) 24.653 & 12.497 \(\pm\) 1.372 & / \\  & fcn <cite>[30]</cite> & 0.651 \(\pm\) 0.015 & 66.059 \(\pm\) 17.188 & 12.339 \(\pm\) 0.959 & / \\  & nnunet [21] & 0.741 \(\pm\) 0.026 & 42.486 \(\pm\) 15.139 & 8.005 \(\pm\) 0.811 & / \\  & topo-crf [2] & 0.739 \(\pm\) 0.010 & 46.873 \(\pm\) 17.636 & 7.914 \(\pm\) 0.877 & / \\  & midl [33] & 0.742 \(\pm\) 0.028 & 43.132 \(\pm\) 15.624 & 6.420 \(\pm\) 1.242 & / \\  & nonadj [32] & 0.748 \(\pm\) 0.017 & 38.197 \(\pm\) 19.598 & 4.887 \(\pm\) 0.702 & / \\  & ours & **0.758 \(\pm\) 0.017** & **31.137 \(\pm\) 17.772** & **5.799 \(\pm\) 0.737** & / \\ \hline \multicolumn{6}{c}{**ivus dataset**} \\ \hline \multirow{6}{*}{**auc**} & unet [34] & 0.786 \(\pm\) 0.144 & 6.643 \(\pm\) 1.936 & 30.944 \(\pm\) 11.631 & 5.970 \(\pm\) 2.141 \\  & fcn <cite>[30]</cite> & 0.824 \(\pm\) 0.071 & 5.319 \(\pm\) 1.519 & 22.551 \(\pm\) 7.882 & 3.766 \(\pm\) 1.444 \\  & nnunet [21] & 0.893 \(\pm\) 0.066 & 3.464 \(\pm\) 0.917 & 11.152 \(\pm\) 3.954 & 2.708 \(\pm\) 1.032 \\  & topo-crf [2] & 0.887 \(\pm\) 0.096 & 4.138 \(\pm\) 1.454 & 10.497 \(\pm\) 2.487 & 2.371 \(\pm\) 0.960 \\  & midl [33] & 0.891 \(\pm\) 0.073 & 4.226 \(\pm\) 1.390 & 10.641 \(\pm\) 2.322 & 2.394 \(\pm\) 0.918 \\  & nonadj [32] & 0.897 \(\pm\) 0.081 & 3.140 \(\pm\) 1.154 & 9.628 \(\pm\) 3.221 & 2.173 \(\pm\) 0.994 \\  & ours & **0.949 \(\pm\) 0.070** & **2.046 \(\pm\) 1.079** & **6.057 \(\pm\) 2.746** & **0.157 \(\pm\) 0.808** \\ \hline \multirow{6}{*}{**auc**} & unet [34] & 0.651 \(\pm\) 0.130 & 7.391 \(\pm\) 1.072 & 21.984 \(\pm\) 6.634 & / \\  & fcn <cite>[30]</cite> & 0.782 \(\pm\) 0.144 & 6.806 \(\pm\) 1.147 & 13.863 \(\pm\) 4.511 & / \\ \cline{1-1}  & nnunet [21] & 0.856 \(\pm\) 0.090 & 5.646 \(\pm\) 1.228 & 6.491 \(\pm\) 2.314 & / \\ \cline{1-1}  & topo-crf [2] & 0.843 \(\pm\) 0.106 & 5.409 \(\pm\) 1.166 & 5.929 \(\pm\) 1.785 & / \\ \cline{1-1}  & midl [33] & 0.841 \(\pm\) 0.121 & 5.461 \(\pm\) 1.214 & 6.071 \(\pm\) 1.837 & / \\ \cline{1-1}  & nonadj [32] & 0.848 \(\pm\) 0.117 & 5.983 \(\pm\) 1.342 & 6.615 \(\pm\) 1.937 & / \\ \cline{1-1}  & ours & **0.910 \(\pm\) 0.089** & **3.873 \(\pm\) 0.933** & **3.171 \(\pm\) 1.871** & / \\ \hline \hline \end{tabular}
\end{table}
table 1: quantitative comparison for containment constraint 

### ablation studies

to further demonstrate the efficacy of the proposed method, we conduct several ablation studies. the following ablation studies have been performed on the ivus dataset (containment constraint).",uses,,"{'method', 'result'}",711_fully_convolutional_networks,fully convolutional networks for semantic segmentation,6fc6803df5f9ae505cae5b2f178ade4062c768d0,2014.0,ba34c9c30080d78def94afee783c9c63ca12edd6,,fully_convolutional_networks,"[1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]",
"to tackle these issues, we propose caption anything (cat), a zero-shot controllable image captioning framework augmented by pre-trained foundation models. specifically, cat integrates pre-trained image captioners [18],[17],[38] with sam <cite>[14]</cite> and an instruction-tuned llm. the image and the visual controls are first processed by sam, which generates a pixel-level mask that corresponds to the selected region, thereby facilitating the perception centered on user-interested objects.",uses,,{'introduction'},770_segment_anything_model,segment anything,7470a1702c8c86e6f28d32cfa315381150102f5b,2023.0,6f8b9192b1f215254ee7625d752710182c05d2f9,,segment_anything_model,"[1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"the table shows the domain, tasks and whether are use for training, evaluation or both. ### language models and baselines

backbone llms: **q** gollie is a fine-tuned version of code-llama <cite>roziere et al (2023)</cite>. other backbone llms, such as llama (touvron et al, 2023a), llama-2 touvron et al (2023b) or falcon penedo et al (2023) were considered during the development, however, as our approach uses code to represent the input and output, code-llama model worked better on the preliminary experiments.",uses,,{'method'},865_code_llama_70b,code llama: open foundation models for code,0b0debb710366cdff461938c80763eace1651af6,2023.0,3f40edfcafc018b2cb54612a9aaa9d6b43a11a26,2023.0,code_llama_70b,"[1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0
 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0]",
"footnote 2: in this paper, we performed the stratified shuffle split using python’s scikit-learn module

### baselines

we use the following baselines in our experiments:

**bert****ft**devlin et al (2018) is a fine-tuned version of bert\({}_{base}\) model on the input tweets with and without considering the discussion context. **rober****a****ft**<cite>liu et al (2019)</cite> is a fine-tuned version of rober****a__base model on the input tweets conditioning on the discussion context tweets. **l****a****fro**wang et al (2020) leverages the declarative knowledge expressed in both first-order logic and text.",uses,,"{'method', 'result'}",521_roberta_large,roberta: a robustly optimized bert pretraining approach,077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,2019.0,33e220c400da5889f7943ac2ca43d2d80265ae11,,roberta_large,"[1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0]",
"llama adapter v2 [42] model shows more proficiency in action recognition. what's more, multimodal gpt [12], otter <cite>[11]</cite>, openflamingo [41], gvt [33], and the three videollms [15],[16],[17] exhibit balanced strength across various evaluation dimensions. ### analysis

through the comprehension and objective evaluation of various models on seed-bench, we have observed a number of findings that can bring insights for future work.",uses,,{'result'},1223_otter,otter: a multi-modal model with in-context instruction tuning,d6d3604f369bb0415cbe814e43ca3131323b03e2,2023.0,4309d572a37d655779f9dce6a2c98c66334132de,,otter,"[1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"to obviate the limitations of phygeonet [14], kashefi and mukerji [26] introduced physics-informed pointnet (pipn). in pipn, pointnet <cite>[43]</cite> carries out the role of the ""neural network"" in pinns. similar to cnns, pointnet [43] is able to also encode the geometric features of input computational domains; however, using a different mathematical methodology.",uses,,{'introduction'},981_pointnet,pointnet: deep learning on point sets for 3d classification and segmentation,d997beefc0922d97202789d2ac307c55c2c52fba,2016.0,9a3d65e8ece09aec486e2b6bf8d340aea613441e,2023.0,pointnet,"[1 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"_sst_[26] proposes a deep affinity network, which models the appearance of the objects and does the data association by computing the affinity between the existing tracks and the detected object and then matching them using the hungarian algorithm [24]. _c-biou_[5] uses detections from _yolox_<cite>[11]</cite> then performs cascade matching between existing tracklets and the detections. the size of the bounding boxes is enlarged by a constant to allow for the tracking of hard examples where the position of the object has changed a lot.",uses,,{'background'},1158_yolox-x,yolox: exceeding yolo series in 2021,c01b385205e488a731c8c8c11c0c494d426beb03,2021.0,116dec794516c1155d27ae42f12cb63784fd466e,2023.0,yolox-x,"[0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0]",
"##  appendix g code, license, assets and computation requirements

### code and licenses of assets

in this work, we use the open source implementation of fixmatch [35]3 in pytorch, which is licensed under mit license for educational purpose. also for nlp experiments we make use of distillbert <cite>[33]</cite> pretrained model available in the huggingface [41] library. the code to reproduce the main experiments results can be found at [https://github.com/val-iisc/costsensitiveselftraining](https://github.com/val-iisc/costsensitiveselftraining).",uses,,{'appendix'},628_distilbert,"distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",a54b56af24bb4873ed0163b77df63b92bd018ddc,2019.0,086662434ccc5a6ce43d49edc5f3d1682c18d607,2023.0,distilbert,"[1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"** we feed the \(n\) trajectories into agentic transformer, this results in a total of \(5\times n\times t\) tokens, with one token for each of the five modalities: returns-to-go, state, action, reward, and completion. to create the token embeddings, a linear layer is trained for each modality which transforms the raw inputs into the desired embedding dimension, followed by layer normalization (<cite>ba et al, 2016</cite>). in addition to this, an embedding for each time step is also learned and added to the tokens, which is distinct from the standard positional embedding used in transformers where one time step is represented by five tokens.",uses,,{'method'},1263_layer_normalization:_order_embeddings_of_images_and_language,layer normalization,97fb4e3d45bb098e27e0071448b6152217bd35a5,2016.0,f11044596cf2eaf59f83d82b8167b16ba6a08617,2023.0,layer_normalization:_order_embeddings_of_images_and_language,"[1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0
 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0]",
"the mnist dataset was processed by letnet (mnist et al, 2016), which is shown in figure 3. the cifar dataset is processed by alexnet (kingma et al, 2014), which has 5 convolutional layers and 3 fully-connected layers. imagenet dataset is processed by googlenet (<cite>krizhevsky et al, 2012</cite>) and vgg (<cite>krizhevsky et al, 2012</cite>). googlenet has 22 layers and vgg has 19 layers.",uses,,{'method'},867_alexnet,imagenet classification with deep convolutional neural networks,abd1c342495432171beb7ca8fd9551ef13cbd0ff,2012.0,32426a7765aa376350b70095fe6ca32623fb433e,,alexnet,"[0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0
 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]",
"### stem and dense-branched modules (dbm)

in our network, we use two parts for feature extraction. the first part is the pre-trained stem of inception-v3 <cite>[14]</cite>, which produces the low-level feature maps. the stem consists of convolutional, activation, and pooling

layers before the inception modules.",uses,,{'introduction'},609_inception_v3,rethinking the inception architecture for computer vision,23ffaa0fe06eae05817f527a47ac3291077f9e58,2015.0,b7333790fa5854b869d0db24c69159f9a54e1414,2023.0,inception_v3,"[1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"* alpaca (taori et al, 2023) is based on llama and finetuned on an instruction-following dataset. * llama2-chat (<cite>touvron et al, 2023</cite>) is a large language model that is optimized for dialogue purposes. * chatgpt8 is a powerful closed-source llm that could follow instructions to conduct complex tasks9.",uses,,{'appendix'},448_llama_2-7b,llama 2: open foundation and fine-tuned chat models,104b0bb1da562d53cbda87aec79ef6a2827d191a,2023.0,b47e96762351b2dbf7e863ece4640df6194bcc0c,2023.0,llama_2-7b,"[0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"therefore, the proposed model name is multi-edge optimized lstm rnn for vs(molrvs). a combination of a tensor train embedding layer and a multi lstm (two-layer) forms tensor-train hierarchical recurrent neural network for

\begin{table}
\begin{tabular}{l l l l l} \hline model name & parameters & depth & size (mb) & top 5 error rate \\  & (in millions) & & & \\ \hline lenet (adom et al, 2019) & 0.060 & 005 & – & 00.95 (mnist) \\ alexnet (adom et al, 2019) & 060 & 008 & – & 15.30 (imagenet) \\ vgg16 (<cite>simonyan and zisserman, 2014</cite>) & 138 & 016 & 528 & 08.80 (imagenet) \\ vgg19 (<cite>simonyan and zisserman, 2014</cite>) & 144 & 019 & 549 & 09.00 (imagenet) \\ google v3 (szegedy et al, 2016) & 23.6 & 159 & 92 & 03.50 (imagenet) \\ resnet (he et al, 2016) & 25.6 & 152 & 98 & 03.60 (imagenet) \\  & 25.6 & 190 & – & 3.46 (cifar10+) \\ densenet (adom et al, 2019) & 25.6 & 190 & – & 17.18 (cifar100+) \\ (huang et al, 2017) & 15.3 & 250 & – & 05.19 (cifar10) \\  & 15.3 & 250 & – & 19.64 (cifar100) \\ xception (chollet, 2017) & 23.00 & 126 & 088 & 0.055 (imagenet) \\ mobilenetv2 (howard et al, 2017) & 03.50 & 88 & 14 & – \\ \hline \end{tabular}
\end{table}
table 2: comparison of cnn models used for vs

figure 5: basic architecture of cnn

video summarization (tth-rnn) model (zhao et al, 2020). tensor train embedding layer prevents the significant feature to hidden mapping matrices caused by the high-dimensional video features.",uses,,set(),844_vgg16,very deep convolutional networks for large-scale image recognition,eb42cf88027de515750f230b23b1a057dc782108,2014.0,4b5e595806cdfcef0ca07316e6f5c44152f9edd6,2023.0,vgg16,"[0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"_rotation_[53] is a self-supervised pre-training approach by predicting the image rotations. _simclr_<cite>[16]</cite>, _swav_[52], _moco_[17], and _byol_[49] are the sota self-supervised learning approaches for pre-training. we combine these three self-supervised approaches with _fedavg_[18] as their federated variants _fedrotation_, _fedsimclr_, _fedswav_, _fedmoco_, and _fedbyol_ for pre-training the encoder.",uses,,{'method'},300_simclr,a simple framework for contrastive learning of visual representations,34733eaf66007516347a40ad5d9bbe1cc9dacb6b,2020.0,0609ec2bbf13489b9dd01dc1d6ebfc642d0a97a6,,simclr,"[1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0]",
"popular single stage detectors are _""you only look once""_ (yolo) [36][50][51], the _single shot multibox detector_ (ssd) [52], cornernet [37] and refinenet [38]. double stage detectors, such as rcnn [53], faster-rcnn <cite>[54]</cite>, or r-fcn [41], split the object detection process into two parts: region of interest candidates proposals and bounding boxes classification. in general, single stage detectors do not provide the same performances as double stage detectors, but are significantly faster.",uses,,set(),603_faster_r-cnn,faster r-cnn: towards real-time object detection with region proposal networks,424561d8585ff8ebce7d5d07de8dbf7aae5e7270,2015.0,a595767fa35bcc84362f629fbc4d2d9b05d7342a,,faster_r-cnn,"[0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0
 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"although many llms can be used here, we adopt the llama 65b (touvron et al, 2023) due to its performances, availability, costs and privacy concerns. #### 3.2.3 bronze data (label-to-data)

we used gpt-4, a state-of-the-art llm that has been shown to generate coherent and diverse texts across various domains and tasks (<cite>openai, 2023</cite>). gpt-4 is a transformer-based model with billions of parameters, trained on a large corpus of web texts.",uses,,{'method'},566_gpt-4,gpt-4 technical report,a8e1f42412639275fd59e7ac9b702787ab59016a,2023.0,75b874acc4eb463e177e88bb73c668ccc1aae0a2,2023.0,gpt-4,"[1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0]",
"sam has made it incredibly easy, fast, and highly effective to generate high-quality pseudo-labels [126][128][181], thus facilitating new breakthroughs in the field of computer vision research and applications. he _et al_<cite>[126]</cite> propose a new method for weakly-supervised multi-object semantic segmentation. the ws-sam framework utilizes the recently proposed vision foundation model, sam, to generate segmentation masks, and proposes several techniques, including multi-augmentation result fusion, pixel-level uncertainty weighting, and image-level uncertainty filtration, to obtain reliable pseudo labels for training a segmentation model.",uses,,set(),533_ase,ase,4ba973b38e448b2060bd6e2cbc0255d767ddaf98,2022.0,1856bebc4cb35e68368d9c83bd2ac2d26cd4bcfa,2023.0,ase,"[0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0
 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"_normalisation_

we use normalisation in two places. 1) in em models, after retrieving a memory i.e., before applying \(\mathbf{w}_{out}^{em}\) we perform a layer normalisation<cite>[54]</cite>: layernorm(\(\mathbf{v}_{t-1}\text{softmax}(\mathbf{k}_{t-1}^{t}\mathbf{w}_{q}f(\mathbf{r}_{t}))\)). this is so all retrieved memories have the same scaling.",uses,,{'appendix'},1265_layer_normalization:_skip_thoughts,layer normalization,97fb4e3d45bb098e27e0071448b6152217bd35a5,2016.0,733ded5b1f8ab236165a4645142681c832c86ca1,2024.0,layer_normalization:_skip_thoughts,"[0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]",
"we propose an augmentation of three popular embedding methods (word2vec skip-gram, glove, and fasttext). word2vec skip-gram (mikolov et al, 2013a) is a neural word context predictor, glove (<cite>pennington et al, 2014</cite>) is a log-bilinear model that includes global context information with a co-occurrence matrix, and fasttext (bojanowski et al, 2017) incorporates sub-word information via character n-grams with a skip-gram objective to expedite training and handle unseen words. more details about these algorithms are in ss2.",uses,,{'introduction'},693_glove_(32b),glove: global vectors for word representation,f37e1b62a767a307c046404ca96bc140b3e68cb5,2014.0,512ac8a70be56404aa45ce1968ee36b134a48c82,2022.0,glove_(32b),"[0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"ground truth sketch responses were then constructed by replacing all rare word instances in ground truth responses with @persona tags. language model pre-trainingsketch-fill-a-r uses a transformer-based gpt (<cite>radford et al, 2018</cite>) pre-trained on the books text corpus (zhu et al, 2015) to rank candidate responses with filled @persona slots according to their lm-perplexity scores. for model details, see the appendix.",uses,,set(),18_gpt,improving language understanding by generative pre-training,cd18800a0fe0b668a1cc19f2ec95b5003d0a5035,2018.0,78945210cee2356b28393a4f2fdebd219d1c1aeb,,gpt,"[1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]",
"the obtained representations are projected with \(n\) weight matrices \(\mathbf{w}_{t}\in\mathbb{r}^{d\times 1}\). we also explore a cnn based model as another neural baseline and report its performance and architectural design in appendix c.

pre-trained language modelswe fine-tune base bert <cite>devlin et al (2019)</cite> and scibert beltagy et al (2019) using the huggingface9 transformers library. we use the ""bert-base-uncased"" and ""scibert-scivocab-uncased"" variants of bert and scibert, respectively.",uses,,{'method'},1064_bert-large,bert: pre-training of deep bidirectional transformers for language understanding,df2b0e26d0599ce3e70df8a9da02e51594e0e992,2019.0,8f066cdf7a5a37834b147dfa8e21e3275008ae8c,,bert-large,"[1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"but are mlms really doing better _because_ they have access to full word sequences? to assess this question, we first compare the internal representations of bert and roberta<cite>liu et al (2019)</cite> when the sequence of unigrams is not available.1 we do this by using the bag-of-words counts of an input to generate a random ordering of the unigrams, i.e., ""shuffling"" the input. for example, in a sentiment classification corpus, if an intact input was ""the movie was great!",uses,,{'introduction'},521_roberta_large,roberta: a robustly optimized bert pretraining approach,077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,2019.0,3757de51963e31d0d88dc7a0fbffb8e5d6ba0a79,,roberta_large,"[1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"fig. 4 shows a detailed visualisation of our mfsr based on residual dense blocks (rdb) <cite>[17]</cite>. in the shared network, the image sr model consists of four parts: shallow feature extraction, basic blocks, dense feature fusion, and finally upsampling.",uses,,set(),668_residual_dense_network,residual dense network for image super-resolution,4ef1476dec02c62227187edbba88615278b3edba,2018.0,1fe5aaa15886639089e53903215561fad718b6e7,,residual_dense_network,"[1 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"to achieve this abstraction, we leverage the relevancy maps extracted from 2d vlms and used them as the ""abstracted object"" representation that is agnostic to their semantic labels. the semabs module factorizes into two submodules: 1) a semantic-aware wrapper that takes an input rgb-d image and object category label and outputs the relevancy map of a pre-trained 2d vlm model (i.e., clip <cite>[7]</cite>) for that label, and 2) a semantic-abstracted 3d module that uses the relevancy map to predict 3d occupancy. this 3d occupancy can either represent the 3d geometry of a partially observed object or the possible 3d locations of a hidden object (_e.g._ mask in the trash) conditioned on the language input.",uses,,{'introduction'},1104_clip_(resnet-50),learning transferable visual models from natural language supervision,6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4,2021.0,d5c4550d285b57111e52c5956dbc40942d36b117,2022.0,clip_(resnet-50),"[1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"##  iv evaluation

this section summarizes the empirical results of csq. we evaluate csq using resnet-20 [18] and vgg19bn <cite>[19]</cite> on cifar-10 [20], and using resnet-18 and resnet-50 [18] on imagenet [21]. we compare the results of our method with existing uniform [3],[4],[5] and mix-precision [7],[8],[9],[22] quantization methods.",uses,,set(),597_vgg19,very deep convolutional networks for large-scale image recognition,eb42cf88027de515750f230b23b1a057dc782108,2014.0,dde94755516328baa2d764aa0e5845dbdb66695e,2022.0,vgg19,"[0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0]",
"in this way, we enhance the semantic representation ability of mid-level features and boost semantic segmentation performance. based on the proposed semantic-affine transformation, we design a semantic-aware network named **semaffinet** and introduce transformer <cite>[58]</cite> to manage semantic information both implicitly and explicitly. the transformer encoder implicitly communicates geometric information across modalities via the self-attention technique, while the special design of class queries in the transformer decoder performs explicit semantic-aware reasoning to predict semantic-affine parameters via the cross-attention mechanism.",uses,,{'introduction'},1059_transformer,attention is all you need,204e3073870fae3d05bcbc2f6a8e263d9b72e776,2017.0,36ff709a11dad3c4e388345342d1a56e619c67fa,,transformer,"[1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0
 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0]",
"### architecture

#### 3.3.1 encoder

our encoder aims to catch the multimodal interaction between video \(v\) and query \(q\). initially, the pre-trained feature extractor (_e.g._, clip <cite>[32]</cite>) is employed to convert each input into multi-dimensional features and normalize them. we utilize two projection layers to convert input features into the same hidden dimension \(d\).",uses,,"{'background', 'method'}",1104_clip_(resnet-50),learning transferable visual models from natural language supervision,6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4,2021.0,e9b7fbbb072e2155b498f604b8f3b16c72183735,2023.0,clip_(resnet-50),"[0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"in our experiments, we sample 9 pairs of \((w,h)\) to estimate \(\text{viou}\) during training. specifically, we adopt the 9 pairs of different scales and aspect ratios used in retinanet<cite>[19]</cite>. theoretically, the more pairs we sample, the more accurate the approximation is, while the computational cost is heavier.",uses,,set(),266_retinanet-r50,focal loss for dense object detection,79cfb51a51fc093f66aac8e858afe2e14d4a1f20,2017.0,830918ec06f22c0de40884ccfcb5705823ac9a3f,,retinanet-r50,"[0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0
 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]",
"the choice of the width of the hidden layer of the spns allows strassennets to precisely control over the computational cost and the precision of the approximate matrix multiplication, and, in turn, the predictive performance of the dnn architecture. the significant compression achieved by strassennets for \(3\times 3\) convolutions (tschannen et al (2018)) and increasing visibility of ds convolution layers in compute-efficient networks (<cite>howard et al (2017)</cite>; sandler et al (2018); zhang et al (2018b); chollet (2017)) motivated us to apply strassennets over mobilenets architecture dominated with ds layers to reduce its computational complexity and model size even further. further compression of mobilenets-like already compute-efficient networks will not only improve their energy- and runtime-efficiency leading to longer battery life, but also will create opportunities for more complex applications with stringent real-time requirements to fit in the limited memory budget and to run in the limited silicon area of emergent dnn hardware accelerators.",uses,,{'introduction'},120_mobilenet,mobilenets: efficient convolutional neural networks for mobile vision applications,3647d6d0f151dc05626449ee09cc7bce55be497e,2017.0,7b1c8d75fd39915caf03860bcc2c29d400d6eaa5,2019.0,mobilenet,"[0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0
 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0]",
"pre-trained classifier.after applying the prompts to the resized image through the preceding stages, the prompted image is subsequently fed into the pre-trained model, which serves as a feature extractor to generate predictions in the source domain. we include four representative pre-trained models in our autovp framework: resnet18 (he et al, 2016), resnext101-ig (mahajan et al, 2018), swin-t (liu et al, 2021), and a vision-language multi-modal model, clip (<cite>radford et al, 2021</cite>) with the vit-b/32 vision encoder backbone. note that in autovp, the weights of the pre-trained classifiers are frozen and kept unchanged.",uses,,set(),1042_clip_(vit_l_14@336px),learning transferable visual models from natural language supervision,6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4,2021.0,12cef36ff033fe4e5c3ea6797b74ed8b27fa738e,,clip_(vit_l_14@336px),"[0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]",
"finally, multi-source information is aggregated for the next iteration. ### shared encoder

we apply two modules to extract sentence features, 1) we adopt convolutional neural network (_cnn_kim (2014) as the feature extractor kalchbrenner et al (2014); 2) we investigate a more powerful encoder (_i.e._, bert <cite>devlin et al (2018)</cite>) as the backbone. the encoder is shared by the three aspect-level tasks and the two document-level tasks for providing common features.",uses,,set(),1064_bert-large,bert: pre-training of deep bidirectional transformers for language understanding,df2b0e26d0599ce3e70df8a9da02e51594e0e992,2019.0,74a0041d5fb6d33ea28346d5ea77f85e6d6ddbdc,,bert-large,"[1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]",
"hog, gaze vector) are each modeled by random forests [5] and mapped to the classification label. the pixel-space features (cropped face and eye regions) are each processed by inceptionv3 networks <cite>[38]</cite>. each model in the ensemble receives a weight depending on their accuracy during the training phase, makingthe accuracy values a parameter.",uses,,{'method'},609_inception_v3,rethinking the inception architecture for computer vision,23ffaa0fe06eae05817f527a47ac3291077f9e58,2015.0,0919b2c56a5cfebc5118f9081bfdcd01a75d87fd,,inception_v3,"[1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]",
"**imagenet**: vint initialized with the efficientnet-b0 weights pre-trained on imagenet, other parameters initialized from scratch, and fine-tuned with the carla on-task dataset. **simclr**: vint initialized with the efficientnet-b0 weights pre-trained with simclr <cite>[7]</cite> on the training data described in section c, other parameters initialized from scratch, and fine-tuned with the carla on-task dataset. **vc-1**: vint initialized with a pre-trained vit-b model checkpoint from the authors of vc-1 [44]_and frozen_, other parameters initialized from scratch, and fine-tunedwith the carla on-task dataset.",extends,,{'appendix'},300_simclr,a simple framework for contrastive learning of visual representations,34733eaf66007516347a40ad5d9bbe1cc9dacb6b,2020.0,d77e806cd177a162fd20445ed6df566e08d58ced,2023.0,simclr,"[1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0
 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0]",
"[32] employs contrastive supervision to learn segmentation masks from text. concurrently, recent works [6],[33],[27],[20],[9] follow the ""frozen clip"" paradigm for zero-shot segmentation, they first generate a series of mask proposals and then utilize clip <cite>[28]</cite> or align [17] to classify them. zsseg and ovseg [33],[20] train clip adapters to boost performance.",extends,,{'introduction'},1104_clip_(resnet-50),learning transferable visual models from natural language supervision,6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4,2021.0,6a627c258084054b2648058a78c579539d7f7bc3,2023.0,clip_(resnet-50),"[1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0
 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0]",
"dropout layers with probability 0.1 are added between each pair of hidden layers. adam <cite>[62]</cite> is used for optimization with hyperparameters: \(\text{lr}=0.001\), \(\beta_{1}=0.8\)/\(\beta_{2}=0.99\), decay \(=5\times 10^{-4}\). the model is trained with batch size of 5000, the large number being chosen to increase the chance of true signal events being in each batch.",extends,,set(),1093_adam_(cifar-10),adam: a method for stochastic optimization,a6cb366736791bcccc5c8639de5a8f9636bf87e8,2014.0,1022f834bfed10c20b9bd2f43d44ad8a87b1291b,,adam_(cifar-10),"[1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1
 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]",
"specifically, we include natural language inference (williams et al, 2018; wang et al, 2019), question answering (rajpurkar et al, 2016), ner (tjong kim sang and de meulder, 2003), and sentiment classification (socher et al, 2013; agirre et al, 2013)). to build our metric evaluation dataset, we train and run llama-7b models (<cite>touvron et al, 2023</cite>) on varied data mixtures and target tasks. for rigor, we also report scores on the summarization with human feedback dataset from (stiennon et al, 2022) (sum).5

footnote 5: more details in sec.",extends,,{'method'},956_llama-7b,llama: open and efficient foundation language models,57e849d0de13ed5f91d086936296721d4ff75a75,2023.0,5ee2d80a3b0e7b0a4c7d9418d7c3f2fee9b6011a,2023.0,llama-7b,"[0 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0
 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0]",
"this paper offers a thorough summary of frequently utilized datasets and metrics across diverse downstream tasks, as presented in table 5 and, table 6. \begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{model} & \multirow{2}{*}{reference} & \multirow{2}{*}{task (metric)} & pre-training & \multicolumn{2}{c}{dataset} \\ \cline{4-6}  & & & dataset (hours) & training & test \\ \hline \multirow{4}{*}{best-rq} & \multirow{2}{*}{[78]} & \multirow{2}{*}{asr} & \multirow{2}{*}{ll (60000h)} & \multirow{2}{*}{ls (960h)} & ls (test) \\  & & & & ls (test-other) \\  & & & & ls (dev) \\ \cline{3-6}  & & & ll (60000h) & & ls (dev-other) \\ \cline{3-6}  & & asr-multi & & \begin{tabular}{c} ll (60000h) \\ gigaspeech (10000h) \\ vp (24000h) \\ \end{tabular} & superb & superb \\ \hline data2vec & <cite>[24]</cite> & asr & ls (960h) & ls (10m, 1h, 100h, 960h) & ls (960h) \\ \hline discrete bert & [23] & asr & ls (960h) & ls (100h) & ls (test) \\ \hline hubert & [625] & asr & 
\begin{tabular}{c} ls (960h) \\ ll (60000h) \\ \end{tabular} & ls (960h) & ls (test) \\ \hline wavlm & [71] & asr & ll (60000h) & superb & superb \\ \hline \hline \end{tabular}
\end{table}
table 4. summary of _predictive self-supervised_ approaches and proposed models for speech processing with associated metrics and training data. **pc**: phoneme classification, **sr**: speaker recognition, **ll**: librilight, **ls**: librispeech.",extends,,set(),99_data2vec_(vision),"data2vec: a general framework for self-supervised learning in speech, vision and language",8f2bca9d684005675e294b33c26481e36f528cdb,2022.0,16c4f8bd3b485950c4b553bec4f431566a7dda0e,2023.0,data2vec_(vision),"[0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0
 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0]",
"since the ground-truth label denotes as input when pcl is adopted, which is unknown during inference, we use the softmax for multi-classification as a surrogate. ##  4 experiments

### experiment settings

#### 4.1.1 implementation details

to keep consistency with previous works, we adopt the faster-rcnn (<cite>ren et al, 2016</cite>) as object detector, pre-trained on imagenet (russakovsky et al, 2015) and refined on vg150 (krishna et al, 2017), with vgg16 (simonyan and zisserman, 2015) and resnext-101-fpn (lin et al, 2017; xie et al, 2017) being the backbone to generate region proposals. when motif and vctree are adopted as the baseline, the initial learning rate is 0.01. when transformer is adopted as the baseline, the initial learning rate is 0.001. all the experiments are implemented with pytorch and conducted with nvidia 1080 gpus.",extends,,{'method'},603_faster_r-cnn,faster r-cnn: towards real-time object detection with region proposal networks,424561d8585ff8ebce7d5d07de8dbf7aae5e7270,2015.0,483c29fd15d40b6aee40582a1a35541d88f0d364,,faster_r-cnn,"[1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0
 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0]",
"the second stage bootstraps vision-to-language generative learning from a frozen llm flant5-xl [82], which enables zero-shot instructed image-to-text generation. * **llava**[9] connects the visual encoder vit-l/14 of clip <cite>[83]</cite> with the language decoder llama [1] by a lightweight fully-connected (fc) layer. llava first trains the fc layer with 595k image-text pairs while freezing the visual encoder and llm and then fine-tunes the fc layer and llm on 158k instructional vision-language data.",extends,,{'appendix'},1042_clip_(vit_l_14@336px),learning transferable visual models from natural language supervision,6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4,2021.0,a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9,,clip_(vit_l_14@336px),"[0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1
 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0]",
"in the field of natural language processing, self-attention-based deep learning architectures, such as transformer and bidirectional encoder representations from transformer (bert), have achieved state-of-the-art performance in various tasks (devlin _et al_, 2019; vaswani _et al_, 2017). additionally, bert, which essentially consists of stacked transformer encoder layers, shows enhanced performance in downstream task-specific predictions after pretraining on a massive dataset (<cite>devlin _et al_, 2019</cite>). in the field of bioinformatics, several bert architectures pretrained on a massive corpus of protein sequences have been recently proposed, demonstrating their capability to decode the context of biological sequences (elmaggar _et al_, 2021; luchi _et al_, 2021; rao _et al_, 2019; rives _et al_, 2021).",extends,,{'introduction'},1064_bert-large,bert: pre-training of deep bidirectional transformers for language understanding,df2b0e26d0599ce3e70df8a9da02e51594e0e992,2019.0,a76c11fd3119c44520f7487b4141ff7e833bc5d0,2021.0,bert-large,"[0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 0 0 1 1 0
 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0]",
"### network architecture for gsn and lrn

we adopt the same backbone for gsn and lrn. our model is simply built on the fcn architecture with the pre-trained 16-layer vgg network <cite>[30]</cite>. the original vgg-16 network [30] is trained for image classification task while our model is trained for saliency detection, a pixel-wise prediction task.",extends,,set(),844_vgg16,very deep convolutional networks for large-scale image recognition,eb42cf88027de515750f230b23b1a057dc782108,2014.0,d08c7e556071e87eea263abe9393405501457ea4,,vgg16,"[1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0]",
"both show that the information loss is more significant in standard quantization. ### object detection

we apply our method to efficientdet <cite>[54]</cite> using the efficientdet-d1 variant. we train and evaluate the networks on the ms coco 2017 [39] object detection dataset.",extends,,{'result'},1139_efficientdet,efficientdet: scalable and efficient object detection,41c67d04be2d1632c0d3b0880c21c9fe797cdab8,2019.0,1c859c104b954ab0fc9e6e0d0cb43ee8e9030f7d,,efficientdet,"[1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0
 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0]",
"its outputs are then fed to a 600m parameter super-resolution model to upsample from 64\(\times\)64 to 256\(\times\)256, followed by a 400m parameter model to upsample from 256\(\times\)256 to 1024\(\times\)1024. the base 64\(\times\)64 model is conditioned on text embeddings via a pooled embedding vector added to the diffusion time-step embedding, like previous class-conditional diffusion models [26]. all three stages of the diffusion cascade include text cross-attention layers <cite>[50]</cite>. given the relative paucity of high resolution images in imagenet, we fine-tune only the 64\(\times\)64 base model and 64\(\times\)64\(\rightarrow\)256\(\times\)256 super-resolution model on the imagenet-1k train split, keeping the final super-resolution module and text-encoder unchanged.",extends,,set(),1034_imagen,photorealistic text-to-image diffusion models with deep language understanding,9695824d7a01fad57ba9c01d7d76a519d78d65e7,2022.0,4538e353dd98f396c8facc29ebb72e9b1ba5f7c2,2023.0,imagen,"[1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1
 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0]",
"for example, vdsr [35] proposes a very deep cnn to improve the sr results and espcn [34] designs a simple yet efficient strategy, called pixel-shuffle, for real-time feature upsampling. edsr <cite>[26]</cite> proposes to enhance the cnn-based sr network by removing the batch normalization layer of all residual blocks. moreover, to improve the perceptual visual quality, recent works [24],[33],[38] propose to employ some advanced losses such as the vgg loss [35], perceptual loss [20], and gan loss [11] to help the network to learn realistic image details.",extends,,{'background'},63_edsr,enhanced deep residual networks for single image super-resolution,7ba5d3808e117e7a68dc40331ce1d483ceeedcb2,2017.0,fea9136bea3780973736f2d6a7ebb33af78197a5,2022.0,edsr,"[0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0
 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0]",
"the machine has intel core-i7-5930k cpu@ 3.50ghz x 12 processors with 64gb of memory. during training, we adopted inception v3 <cite>[26]</cite> as the backbone network. we employed ws-dan [12] technique to perform experiments to demonstrate the effectiveness of transfer learning.",extends,,{'method'},609_inception_v3,rethinking the inception architecture for computer vision,23ffaa0fe06eae05817f527a47ac3291077f9e58,2015.0,0db9cd6929c9c4a0e7f89f3251ab288861c5e5b3,,inception_v3,"[1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0
 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0]",
"these vary from simple data augmentation methods to semi-supervised learning methods [6],[7]. data augmentation techniques, such as filter augmentation [8] and specaugment <cite>[9]</cite>, have been successful in improving the generalization capability of sed models. filter augmentation weights frequency bands to extract useful information from a wider frequency range, while specaugment masks blocks of consecutive frequency channels and time frames in the spectrogram.",extends,,{'introduction'},166_specaugment,specaugment: a simple data augmentation method for automatic speech recognition,b0fae9fbb4e580d92395eabafe73e317ae6510e3,2019.0,3296c89f3338e5ff53097366dacfe44768450c4e,2023.0,specaugment,"[1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0
 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1]",
"therefore, the search cost of entrannas increases sub-linearly as the search space is enlarged. \begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{methods} & test error & params & search cost & \multirow{2}{*}{search method} \\  & (\%) & (m) & (gpu-day) & \\ \hline \hline densenet-bc [22] & 3.46 & 25.6 & - & manual \\ \hline nasnet-a + cutout [57] & 2.65 & 3.3 & 1800 & rl \\ enas + cutout [39] & 2.89 & 4.6 & 0.5 & rl \\ amoebanet-b +cutout [40] & 2.55\(\pm\)0.05 & 2.8 & 3150 & evolution \\ hierarchical evolution [31] & 3.75\(\pm\)0.12 & 15.7 & 300 & evolution \\ \hline darts (2nd order) + cutout [32] & 2.76\(\pm\)0.09 & 3.3 & 4.0 & gradient \\ snas (moderate) + cutout [48] & 2.85\(\pm\)0.02 & 2.8 & 1.5 & gradient \\ proxylessnas+cutout <cite>[7]</cite> & 2.08\({}^{\dagger}\) & 5.7 & 4.0 & gradient \\ pc-darts + cutout [49] & 2.57\(\pm\)0.07 & 3.6 & 0.1 & gradient \\ nasp + cutout [51] & 2.83\(\pm\)0.09 & 3.3 & 0.1 & gradient \\ milenas + cutout [18] & **2.51\(\pm\)0.11** & 3.87 & 0.3 & gradient \\ \hline entrannas + cutout & 2.53\(\pm\)0.06 & 3.45 & **0.06** & gradient \\ entrannas-dst + cutout & **2.48\(\pm\)0.08** & 3.20 & **0.10** & gradient \\ \hline \hline nasp (12 operations) + cutout [51] & 2.44\(\pm\)0.04 & 7.4 & 0.2 & gradient \\ entrannas (12 operations) + cutout & **2.22\(\pm\)0.05** & 7.68 & **0.07** & gradient \\ \hline \hline \end{tabular}
\end{table}
table 6: search results on cifar-10 and comparison with state-of-the-art methods. search cost is tested on a single nvidia gtx 1080 ti gpu.",extends,,"{'result', 'method'}",480_proxylessnas,proxylessnas: direct neural architecture search on target task and hardware,f323407464c4cd492d3fc1afd7170eab08f44d9b,2018.0,289ac3c18b4d5282440643a78c8e595256b14cd4,,proxylessnas,"[0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0
 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]",
"mask r-cnn has been used extensively for multi-task segmentation models for a wide range of application areas (abdulla, 2017), such as adding sports fields to openstreetmap (remillard, 2018), detection and segmentation for surgery robots (suyegit, 2018), understanding climate change patterns from aerial imagery of the arctic (zhang et al, 2018), converting satellite imagery to maps (mohanty, 2018), detecting image forgeries (wang et al, 2019), and segmenting tree canopy (zhao et al, 2018). ### multi-task models applied to medical images

chaichulee et al (2017) extended the vgg16 architecture (<cite>simonyan and zisserman, 2014</cite>) to include a global average pooling layer for patient detection and a fully convolutional network for skin segmentation. the proposed model was evaluated on images from a clinical study conducted at a neonatal intensive care unit, and was robust to changes in lighting, skin tone, and pose.",extends,,set(),844_vgg16,very deep convolutional networks for large-scale image recognition,eb42cf88027de515750f230b23b1a057dc782108,2014.0,b0968cfeb5f3ae01da5bd04cdabd018fd0829966,2019.0,vgg16,"[1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0
 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0]",
"the findings demonstrated that by refining the cnn model and incorporating self-learning softmax, a commendable level of accuracy was achieved, signifying its potential in recognizing pedestrians promptly and accurately in real-world scenarios. to enhance automatic person detection from thermal images, researchers utilized you only look once (yolo) <cite>[10]</cite>. ivasic-kos _et al_[11] conducted experiments using the yolo neural network in its default configuration and also after training it on a subset of their custom dataset of thermal videos.",extends,,{'introduction'},805_yolo,"you only look once: unified, real-time object detection",f8e79ac0ea341056ef20f2616628b3e964764cfd,2015.0,66375c547ba723eafea48ddcb870d4ec30a7398f,2023.0,yolo,"[1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0
 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0]",
"our approach is even more widely applicable in that we give the model concatenated utterances and labels and allow it to transduce to variable-length label sequences, rather than reformatting the evaluation data into a cloze format where the label must be one word/token. sequence-to-sequence (seq2seq) approaches based on pointer/copy mechanisms rongali et al (2020) or t5 <cite>raffel et al (2020)</cite> tend to be effective and more data-efficient for text and token classification in semantic parsing tasks, including slot labeling athiwaratkun et al (2020) and entity-relation extraction paolini et al (2021). in this method, one trains or fine-tunes a seq2seq model to transduce from an unlabeled text sequence to a sequence with labeled spans (or to just the label).",extends,,{'background'},852_t5-3b,exploring the limits of transfer learning with a unified text-to-text transformer,6c4b76232bb72897685d19b3d264c6ee3005bc2b,2019.0,17ae9c4297e0feb23b2ef84a406d76dc7033c98c,2022.0,t5-3b,"[0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0
 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0]",
"for the training stage we use 20 randomly-drawn images from the imagenet training set

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline  & & \multicolumn{3}{c}{distortion} & \multicolumn{3}{c}{perception} \\ \cline{3-8}  & signal & psnr \(\uparrow\) & ssim \(\uparrow\) & lpips \(\downarrow\) & fid \(\downarrow\) & is \(\uparrow\) & kid\(\times 10^{3}\downarrow\) \\ \cline{2-8} task & \(\mathbf{x}\) & \(∞\) & 1 & 0 & 0 & \(240.53\pm 4.42\) & 0 \\  & \(\mathbf{d}(\mathbf{e}(\mathbf{x}))\) & 27.10 & 0.81 & 0.13 & 0.24 & \(234.71\pm 4.04\) & 0.02\(\pm 0.07\) \\ \hline \multirow{2}{*}{\(\text{sisr}_{\times 4}\)} & swinir [11] & 28.10 & **0.84** & **0.24** & 2.54 & \(201.52\pm 4.85\) & 1.24\(\pm 0.24\) \\  & \(\mathbf{\hat{x}}_{0.9}\) & **28.15** & **0.84** & **0.24** & 2.80 & \(198.69\pm 2.97\) & 1.38\(\pm 0.24\) \\  & \(\mathbf{\hat{x}}_{-0.2}\) & 25.08 & 0.77 & 0.25 & **1.19** & **216.74\(\pm 4.26\)** & **0.38\(\pm 0.89\)** \\ \hline \multirow{2}{*}{\(\text{jpeg}_{q=10}\)} & swinir [11] & **29.68** & **0.86** & **0.30** & 8.95 & \(161.73\pm 3.36\) & \(6.52\pm 0.77\) \\  & \(\mathbf{\hat{x}}_{1.1}\) & 29.58 & **0.86** & **0.30** & 8.36 & \(166.50\pm 3.12\) & \(6.08\pm 0.75\) \\  & \(\mathbf{\hat{x}}_{-0.2}\) & 23.74 & 0.76 & 0.31 & **7.56** & \(166.65\pm 3.58\) & **5.68\(\pm 0.83\)** \\ \hline \multirow{2}{*}{\(\text{awgn}_{\sigma=50}\)} & restormer [12] & **30.18** & **0.86** & 0.26 & 5.21 & \(178.62\pm 2.83\) & \(3.29\pm 0.56\) \\  & \(\mathbf{\hat{x}}_{1.1}\) & 30.09 & **0.86** & **0.25** & 4.63 & \(183.36\pm 3.20\) & \(2.61\pm 1.53\) \\  & \(\mathbf{\hat{x}}_{1.7}\) & 27.26 & 0.82 & **0.25** & **2.73** & **198.93\(\pm 5.13\)** & **1.76\(\pm 1.58\)** \\ \hline \multirow{2}{*}{\(\text{sr}_{\times 4}\text{jpeg}_{q=10}\)} & swin2sr [13] & 19.75 & **0.55** & 0.53 & \(205.00\) & \(5.95\pm 0.49\) & \(40.68\pm 3.34\) \\  & \(\mathbf{\hat{x}}_{0.8}\) & **19.81** & **0.55** & 0.53 & \(209.82\) & \(5.91\pm 0.69\) & \(43.28\pm 3.86\) \\  & \(\mathbf{\hat{x}}_{1.9}\) & 18.44 & 0.49 & **0.51** & **168.12** & \(\mathbf{6.36}\pm 0.69\) & **19.95\(\pm 2.84\)** \\ \hline \multirow{2}{*}{\(\text{sisr}_{\times 4}\)} & esrgan [16] & 26.77 & 0.80 & **0.21** & 1.06 & \(221.68\pm 3.06\) & \(0.43\pm 0.14\) \\  & \(\mathbf{\hat{x}}_{0.7}\) & **27.00** & **0.81** & **0.21** & 1.51 & \(215.87\pm 3.64\) & \(0.56\pm 0.21\) \\  & \(\mathbf{\hat{x}}_{-0.2}\) & 24.84 & 0.74 & 0.23 & **0.80** & \(\mathbf{221.89}\pm 2.53\) & \(\mathbf{0.30}\pm 0.20\) \\ \hline \multirow{2}{*}{\(\text{sr}_{\times 4}\text{awgn}_{\sigma=50}\)} & ddrm [3] & **26.10** & **0.75** & 0.34 & \(36.44\) & \(43.52\pm 3.33\) & \(5.09\) \\  & \(\mathbf{\hat{x}}_{1.2}\) & 25.91 & **0.75** & **0.33** & \(33.68\) & \(44.90\pm 4.06\) & \(3.88\) \\  & \(\mathbf{\hat{x}}_{1.7}\) & 24.48 & 0.70 & 0.35 & **29.05** & **47.91\(\pm 2.69\)** & **1.47** \\ \hline \multirow{2}{*}{\(\text{awgn}_{\sigma=50}\)} & nlm [15] & 26.09 & 0.71 & 0.44 & \(12.84\) & \(148.71\pm 3.75\) & \(8.73\pm 0.91\) \\  & \(\mathbf{\hat{x}}_{0.8}\) & **26.24** & **0.72** & **0.43** & **12.46** & \(148.78\pm 2.49\) & \(\mathbf{8.60}\pm 0.95\) \\ \hline \hline \end{tabular}
\end{table}
table 1: using eq. (4), our algorithm can trade-off perception and distortion at inference time on any predictor [3],[11],[12],[13],[15],<cite>[16]</cite> and image restoration task. for each task, we report choices of \(\alpha\) that optimize perception and distortion.",extends,,{'method'},512_esrgan,esrgan: enhanced super-resolution generative adversarial networks,1bdd30a8acc75c58a1bdd4daa4545d5f3971a826,2018.0,e0c1ad4a0335682ccf71426225d80e4aa800e47e,2023.0,esrgan,"[1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0
 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0]",
"first, we automatically identify a limited set of potential pieces of evidence in a document collection, typically represented as relevant paragraphs corresponding to each question. second, we employ a text generation model based on a bart encoding-decoding neural architecture <cite>[10]</cite>. we fine-tune this model using an expectation-maximization (em) approach.",extends,,{'method'},582_bart-large,"bart: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",395de0bd3837fdf4b4b5e5f04835bcc69c279481,2019.0,9dd3fdc257f1bb897522e9c39d0204c8289296de,,bart-large,"[0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0
 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]",
"defensive distillation and adversarial training were applied by papernot et al [65]. recently, zhang et al proposed yolov2 <cite>[58]</cite> enhanced with adversarial patches during training [66]. furthermore, an attempt to apply provable defenses was performed by croce et al [67].",extends,,{'conclusion'},696_yolov2,"yolo9000: better, faster, stronger",7d39d69b23424446f0400ef603b2e3e22d0309d6,2016.0,2c86af0ce87696533f5e41243331a9af2dd707df,2023.0,yolov2,"[0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1
 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0]",
"** ideally, we want the machine annotator to provide precise labels on training images. for this we consider relabel generated by a few state-of-the-art classifiers efficientnet-{b1,b3,b5,b7,b8}<cite>[49]</cite>, efficientnet-l2 [56] trained with jft-300m [46], and resnext-101_32x{32d,48d}[57] trained with instagramnet-1b [35]. we train resnet-50 with the above label maps from diverse classifiers.",extends,,"{'method', 'conclusion'}",377_efficientnet-l2,efficientnet: rethinking model scaling for convolutional neural networks,4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9,2019.0,08bbe9ee1271ff8e0fd7bdbb1d87a995cc4509c6,2021.0,efficientnet-l2,"[0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 1 1 0
 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0]",
"as a speech encoder, we consider wav2vec 2.0 (baevski et al, 2020) and hubert (hsu et al, 2021), both already fine-tuned on english asr data. as a text decoder, we use an mbart50 (<cite>tang et al, 2020</cite>) fine-tuned on multilingual mt (one-to-many). these two modules are coupled with a _length adaptor_ block, that reduces the length discrepancy.",extends,,{'introduction'},327_mbart-50,multilingual translation with extensible multilingual pretraining and finetuning,1f58c42f44113f1c3c8a97c538e78f37f839f4b8,2020.0,d86f5b7094aa93cc449c271ea95eba04bb0a86d7,2022.0,mbart-50,"[1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0
 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0]",
"\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**models**} & \multicolumn{4}{c|}{**msvd-qa**} & \multicolumn{4}{c|}{**activitynet-qa**} & \multicolumn{4}{c}{**tgif-qa**} & \multicolumn{4}{c}{**msrvtt-qa**} \\  & **b** & **c** & **r** & **u** & **t** & **m** & **b** & **c** & **r** & **u** & **t** & **m** & **b** & **c** & **r** & **u** & **t** & **m** & **b** & **c** & **r** & **u** & **t** & **m** \\ \hline \hline \multicolumn{12}{l}{**cvqa**} \\ hcrn [6] & – & - & - & - & 36.8 & - & - & - & - & - & - & - & - & - & 57.9 & - & - & - & - & - & - & 35.4 & - & - \\ clipbert [1] & – & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 60.3 & - & - & - & - & - & 37.4 & - \\ siasanlena [44] & – & - & - & - & 45.5 & - & - & - & - & - & - & 39.8 & - & - & - & - & - & 60.2 & - & - & - & - & - & - & 41.6 & - \\ merlot [5] & - & - & - & - & - & - & - & - & - & - & - & 41.4 & - & - & - & - & - & **69.5** & - & - & - & - & - & - & - & - & - \\ all-ine [2] & 62.6 & 31.5 & 4.5 & 0.0 & 42.8 & 7.9 & 65.1 & 34.1 & 6.9 & 0.0 & 39.5 & 5.3 & 79.4 & 34.5 & 5.7 & 0.0 & 65.6 & 10.1 & 50.4 & 12.3 & 0.8 & 0.0 & 39.5 & 3.9 \\ justask [45] & 65.9 & 37.8 & 13.6 & 0.0 & 47.5 & 12.6 & 60.5 & 37.1 & 16.9 & 0.0 & 39.0 & 8.2 & 68 & 0.31 & 1.3 & 11.4 & 0.0 & 56.9 & 11.7 & 51.7 & 18.5 & 6.0 & 0.0 & 41.8 & 7.0 \\ violet [47] & **77.5** & 10.5 & 0.0 & 0.0 & 43.6 & 2.7 & 63.5 & 32.2 & 0.5 & 0.0 & 37.6 & 3.7 & **38.9** & 14.3 & 0.0 & 0.0 & 68.0 & 4.5 & 55.0 & 0.6 & 0.0 & 0.0 & 0.0 & 40.9 & 1.4 \\ frozenbilm [7] & 72.7 & **48.3** & 18.9 & 0.0 & 54.9 & 17.2 & 68.1 & **40.8** & 16.4 & 0.0 & 43.5 & 7.9 & 77.9 & 51.8 & 24.7 & 0.0 & 68.6 & 23.5 & **57.0** & 25.5 & 0.0 & 0.0 & 46.6 & 6.7 \\ \hline \multicolumn{12}{l}{_ovqa_} \\
**all-in-on+** & 62.8 & 34.0 & 6.3 & 0.4 & 43.8 & 9.4 & 64.9 & 35.9 & 9.8 & 0.5 & 40.2 & 6.8 & 78.3 & 39.3 & 10.2 & 0.4 & 66.0 & 13.2 & 49.8 & 14.6 & 16.0 & 0.0 & 39.5 & 4.7 \\
**justask+** & 65.6 & 37.9 & 13.6 & 6.3 & 47.7 & 14.5 & 60.6 & 37.1 & 16.7 & 4.8 & 40.0 & 11.5 & 68.0 & 32.1 & 12.4 & 9.8 & 57.4 & 14.4 & 51.5 & 18.4 & 6.0 & 26.1 & 41.8 & 7.6 \\
**violet+** & 70.6 & 38.8 & 6.7 & 0.1 & 49.5 & 10.7 & 63.4 & 37.1 & 9.2 & 0.6 & 39.7 & 6.1 & 77.3 & 38.9 & 10.8 & 2.0 & 65.3 & 14.3 & 53.8 & 14.7 & 0.9 & 0.0 & 42.4 & 4.5 \\
**frozenbilm+** & 72.2 & 48.2 & **21.6** & **16.1** & **55.8** & **21.7** & **68.8** & **39.9** & **17.3** & **5.8** & **44.8** & **12.4** & 77.7 & **52.1** & **28** & **21.3** & 69 & 0.0 & **30.2** & **5 

### evaluation on ovqa

we first evaluate the open-ended videoqa baseline models under both settings of cvqa and ovqa. in ovqa, we additionally introduce an answer encoder, deberta <cite>[43]</cite> tokenizer, to extract the answer embeddings. in tab.",extends,,{'method'},1062_deberta,deberta: decoding-enhanced bert with disentangled attention,05f5f8b2065a520846d89771ebaea2bb1534e9c6,2020.0,a97e10228662b0159692a64edc69e2acf5394318,2023.0,deberta,"[1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0
 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0]",
"61772330), science and technology commission of shanghai municipality under grant 2021shzdzx0102. \begin{table}
\begin{tabular}{l|c|c|c} \hline \hline method & \begin{tabular}{c} pascal voc \\ miou(\%) \\ \end{tabular} & \begin{tabular}{c} ade20k \\ miou(\%) \\ \end{tabular} & 
\begin{tabular}{c} params \\ (m) \\ \end{tabular} \\ \hline fcn [1] & 69.6 & 39.91 & 134.5 \\ \hline refinenet [51] & 82.4 & 40.7 & 118.1 \\ \hline deeplab v3 [23] & 77.9 & 44.99 & 87.1 \\ \hline psanet [52] & 77.9 & 43.47 & 78.13 \\ \hline ocrnet [6] & 80.3 & 43.7 & 70.37 \\ \hline \hline \multicolumn{4}{c}{results w/ and w/o distillation schemes} \\ \hline \hline t: pspnet-r101 <cite>[3]</cite> & 78.52 & 44.39 & 70.43 \\ \hline s: pspnet-r18 & 65.42 & 24.65 & 13.07 \\ +skds [11] & 67.73 & 25.11 & 13.07 \\ +ifvd [35] & 68.04 & 25.72 & 13.07 \\ +cwd [12] & 69.25 & 26.80 & 13.07 \\ +sstkd & 70.98 & 29.19 & 13.07 \\ \hline s: deeplab-r18 & 66.81 & 24.89 & 12.62 \\ +skds [11] & 68.13 & 25.52 & 12.62 \\ +ifvd [35] & 68.42 & 26.53 & 12.62 \\ +cwd [12] & 69.97 & 27.37 & 12.62 \\ +sstkd & 71.45 & 29.79 & 12.62 \\ \hline \hline \end{tabular}
\end{table}
table 6: quantitative results on pascal voc 2012 and ade20k. “r18”(“r101”) means resnet-18(resnet-101)

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline \multirow{2}{*}{method} & \multicolumn{2}{c|}{\begin{tabular}{c} cityscapes \\ miou(\%) \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} params \\ (m) \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} flops \\ (g) \\ \end{tabular} } \\ \cline{2-2} \cline{4-5}  & _val_ & & & \\ \hline enet [50] & - & 58.3 & 0.358 & 3.612 \\ \hline icnet [33] & - & 69.5 & 26.50 & 28.30 \\ \hline fcn [1] & - & 62.7 & 134.5 & 333.9 \\ \hline refinenet [51] & - & 73.6 & 118.1 & 525.7 \\ \hline ocnet [4] & - & 80.1 & 62.58 & 548.5 \\ \hline stlnet [7] & 82.3 & 82.3 & 81.39 & 293.03 \\ \hline \hline \multicolumn{5}{c}{results w/ and w/o distillation schemes} \\ \hline \hline t: pspnet-r101 [3] & 78.56 & 78.4 & 70.43 & 574.9 \\ \hline s: pspnet-r18 & 69.10 & 67.60 & 13.07 & 125.8 \\ + skds [11] & 72.70 & 71.40 & 13.07 & 125.8 \\ + skdd [12] & 74.08 & - & 13.07 & 125.8 \\ + ifvd [35] & 74.54 & 72.74 & 13.07 & 125.8 \\ + cwd [13] & 74.87 & - & 13.07 & 125.8 \\ + sstkd & 75.15 & 74.39 & 13.07 & 125.8 \\ \hline \hline s: deeplab-r18 & 73.37 & 72.39 & 12.62 & 123.9 \\ + skds [11] & 73.87 & 72.63 & 12.62 & 123.9 \\ + ifvd [35] & 74.09 & 72.97 & 12.62 & 123.9 \\ + cwd [13] & 75.91 & 74.32 & 12.62 & 123.9 \\ + sstkd & 76.13 & 75.01 & 12.62 & 123.9 \\ \hline \hline s: efficientnet-b1 & 60.40 & 59.91 & 6.70 & 9.896 \\ + skds [11] & 63.13 & 62.59 & 6.70 & 9.896 \\ + ifvd [35] & 66.50 & 64.42 & 6.70 & 9.896 \\ + cwd [13] & - & - & 6.70 & 9.896 \\ + sstkd & 68.26 & 65.77 & 6.70 & 9.896 \\ \hline \hline \end{tabular}
\end{table}
table 5: quantitative results on cityscapes.",extends,,set(),478_pspnet,pyramid scene parsing network,1031a69923b80ad01cf3fbb703d10757a80e699b,2016.0,0493a062269004c354c2b74a115ea8b53692b79a,,pspnet,"[0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0
 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0]",
"the whole brain age is then calculated as the median across slices. in this paper, we instead used the actual (i.e., not transformed by synthsr) clinical-grade mprages and the synthetic mprages predicted from all clinical-grade mris (i.e., actual mprages and other modalities) of 71% of the participants of our sample (i.e., the ""training set"", 1109 participants, 4770 mris) to retrain two cnns of the deepbrainnet model: the ""inceptionresnetv2"" and the ""visual geometry"" network with 16 layers (vgg16)<cite>[30]</cite>. this retraining was also performed under several configurations of hyper-parameters.",extends,,{'first'},844_vgg16,very deep convolutional networks for large-scale image recognition,eb42cf88027de515750f230b23b1a057dc782108,2014.0,91c5e7ddf73ec85c1d0f84071400bc28d1b7f343,2023.0,vgg16,"[0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1
 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0]",
densenet was developed to improve the vanishing gradient issues in deep convolutional neural networks. we fine-tune the densenet model <cite>[65]</cite> for 5 bravais lattice classes using the stem image dataset. we find that densenet provided an accuracy of 83.0 %.,extends,,"{'result', 'method'}",322_densenet-264,densely connected convolutional networks,5694e46284460a648fe29117cbc55f6c9be3fa3c,2016.0,e9fe4144776b6c2daa1982668970aa531065c53f,2022.0,densenet-264,"[1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0
 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0]",
"additionally, we test the scalability of our method by further conducting experiments on opus-100, which contains english-centric bilingual data from 100 language pairs (zhang et al, 2020). in the extraction process, we run our extraction code on the cpu with 24 cores and 200g memory.2 in the generation process, we take transformer-big (<cite>vaswani et al, 2017</cite>) as the configuration for \(m\), and \(m\) is trained with the self-constructed examples mentioned in section 3.2 on eight v100 gpu cards.3

footnote 2: intel(r) xeon(r) platinum 8255c cpu @ 2.50ghz

footnote 3: detailed training process for \(m\) can be found in the appendix a.1. we choose transformer as the basic structure for our model and conduct experiments on two standard configurations, i.e, transformer-base and transformer-big.",extends,,{'method'},1059_transformer,attention is all you need,204e3073870fae3d05bcbc2f6a8e263d9b72e776,2017.0,2fb284c49f8edfe67e149f883fef9c72392823ee,2022.0,transformer,"[1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0
 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0]",
"##  3 experimental setup

**multilingual sentence encoders. ** we probe two widely used multilingual ses: **1)** language-agnostic bert sentence embedding (**labse**) (feng et al, 2022) which adapts pretrained multilingual bert (**mbert**) (devlin et al, 2019) into a multilingual se; **2)** multilingual **xmp-net** is a distillation-based adaptation (reimers and gurevych, 2020) of **xlm-r**(<cite>conneau et al, 2020</cite>) as the student model into a multilingual se, based on the monolingual english mpnet encoder (song et al, 2020) as the teacher model. labse is the current state-of-the-art multilingual se and supports 109 languages, while xmpnet is the best-performing multilingual se in the sentence-bert repository (reimers and gurevych, 2019): for further technical details regarding the models in our comparison, we refer to the original papers.",extends,,{'method'},131_xlm-roberta,unsupervised cross-lingual representation learning at scale,6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6,2019.0,3301604a6b138fb8cadb37df2859fdee52ec5a2f,2022.0,xlm-roberta,"[1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0
 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0]",
"we follow [42],[41] to construct a deep model trained via multi-layer pixel-wise contrastive loss function as our feature extractor. according to [42], we use vgg <cite>[30]</cite> as the backbone, followed with 5 blocks to reduce the dimension. this model is trained with a pixel-wise matching proxy task for over 500 epochs.",extends,,{'method'},597_vgg19,very deep convolutional networks for large-scale image recognition,eb42cf88027de515750f230b23b1a057dc782108,2014.0,292036c25bcc86aaf0e713d332c931433629ec09,,vgg19,"[1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0
 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1]",
"finally, key vision-language tasks are translated into 80 languages with a strong translation system, to support multilingual studies. to evaluate the effectiveness of the proposed dataset, we develop a vision-language model, ying-vlm, by integrating a strong vision encoder, blip-2 [23] with a large language model, ziya-13b [61], derived from llama <cite>[49]</cite>. building on the successful approach of incorporating visual tokens as textual prompts in llms [7],[63],[28], we employ a two-stage training process: (1) the initial stage aligns vision features with text embeddings through image captioning on laion400m [41], and (2) the second stage enhances the model by conducting instruction tuning on selected tasks of our dataset.",extends,,{'introduction'},64_llama-65b,llama: open and efficient foundation language models,57e849d0de13ed5f91d086936296721d4ff75a75,2023.0,6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8,2023.0,llama-65b,"[1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0
 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0]",
"our augmentation method, named clip-paste (clip-level copy-paste), randomly pastes either 'thing' or'stuff' (or both) region tubes from a video clip to the target video clip. we use clip-paste with a probability of 0.5.

depth prediction branch.to grant tubeformer-deeplab the ability to perform monocular depth estimation, we add a small depth prediction module (_i.e_., aspp [14] and deeplab3+ lightweight decoder <cite>[17]</cite>) on top of the cnn _backbone_ features \(x^{v}\). note that we found the performance slightly degrades if we add the depth prediction to the decoded video pixel features \(x^{v^{\prime}}\), indicating that it is not beneficial to share depth estimation with segmentation prediction in our case.",extends,,{'method'},161_deeplabv3+,encoder-decoder with atrous separable convolution for semantic image segmentation,9217e28b2273eb3b26e4e9b7b498b4661e6e09f5,2018.0,ef8f5acfc5d25368feac205b7a7e03b54032d267,,deeplabv3+,"[1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0
 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]",
"##  appendix c efficientdet training & hyperparameters

the input resolution of the efficientdet model was 512x512 and it was trained on the heridal dataset[28]. the tf_efficientnetv2_l model <cite>[46]</cite> was used as the backbone which was then fine tuned using heridal data. the heridal has a published train and test set.",extends,,{'appendix'},681_efficientnetv2,efficientnetv2: smaller models and faster training,8f8f73f0f208302546c825ed474432389ed63be4,2021.0,67b0eb1052c09a89ace6c4b7f9e592c3f1e2c45f,,efficientnetv2,"[1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0
 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0]",
"on this basis, the submitted ape system was built by training the multilingual m2m100-418m model (fan et al, 2021). huawei translation service center and xiamen university school of informatics (hw_tsc).late submission - this team participated with a transformer-based system pre-trained on the provided synthetic ape data and then fine-tuned on the real ape data augmented via automatic translation (with google translate run on the post-edits in the training set) and by integrating en-mr parallel sentences from flores-200 (nllb <cite>team et al, 2022</cite>). r-drop (liang et al, 2021), which regularizes the training inconsistency induced by dropout, is used to mitigate overfitting during the training phase.",extends,,set(),1065_nllb,no language left behind: scaling human-centered machine translation,e19b54ad4c1c8af045069e9cac350ffc2ce60e1a,2022.0,62eca143712e286ccabca137327e3e1570dc3a05,2023.0,nllb,"[0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0
 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0]",
"we conduct experi ments on voc07+12 [9] and coco [24] datasets for downstream detection tasks, where the coco dataset is also used for the downstream instance segmentation task. for the voc07+12 dataset, we adopt the faster r-cnn <cite>[31]</cite> model with c4 backbone which is finetuned for 24k iterations; while for the coco dataset, we adopt the mask r-cnn [19] model with c4 backbone which is finetuned for 180k iterations (using 1\(\times\) learning rate schedule). **mocov2 results.",extends,,{'conclusion'},603_faster_r-cnn,faster r-cnn: towards real-time object detection with region proposal networks,424561d8585ff8ebce7d5d07de8dbf7aae5e7270,2015.0,2b182248a3b2af496caba031f6209c20378b649a,2023.0,faster_r-cnn,"[1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0
 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1]",
"in this part, cross-entropy loss was used. also, this work uses vit <cite>[77]</cite> to extract visual features pre-trained and fine-tuning on imagenet [78], and profes that are better than frcnn [43] in a scalable way. the datasets used in this work were textvqa [6] to fine-tuning in the language module, st-vqa [7] to fine-tuning and the newest the paper presents, the pre-train into scanned documents, are used idl.",extends,,set(),214_vit-base_32,an image is worth 16x16 words: transformers for image recognition at scale,268d347e8a55b5eb82fb5e7d2f800e33c75ab18a,2020.0,385d3b5ba6faa8ad360df0c79da04985c7968e00,2023.0,vit-base_32,"[1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0
 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]",
"it then analyses the claim verbalisation module's execution on the wtr evaluation dataset, looking at the quality of its outputs. #### model validation

prove's verbalisation module consists of a pre-trained t5 model <cite>[35]</cite> fine-tuned on the webnlg dataset [13]. to confirm that fine-tuning was properly carried, the authors measure the bleu [32] scores of its verbalisations on webnlg data.",extends,,{'result'},942_t5-11b,exploring the limits of transfer learning with a unified text-to-text transformer,6c4b76232bb72897685d19b3d264c6ee3005bc2b,2019.0,e470bfdcdaa1458ea9e2a149d4ddbea460ec197b,,t5-11b,"[1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0
 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0]",
"\(\mathcal{v}\)oltron uses a combination of different language objectives on top of the standard mae pipeline, adding complexity. to help ensure stable and reliable training, we follow best practices from the nlp community and make a series of small changes to the transformer architecture including: 1) switching the default layernorm to root-mean square normalization (<cite>zhang and sennrich, 2019</cite>; narang et al, 2021) (stability, no learned parameters), 2) switching from the default glu to the more powerful swishglu activation (shazer, 2020; chowdhury et al, 2022) (performance), and 3) adopting layerscale for scaling down the magnitude of each residual connection (touvron et al, 2021; karamcheti et al, 2021) (prevents overflow). to ensure that any gains in evaluation performance stem from our insights around language-driven learning rather than this modified architecture, we run an ablation experiment in ss6.",extends,,set(),1263_layer_normalization:_order_embeddings_of_images_and_language,layer normalization,97fb4e3d45bb098e27e0071448b6152217bd35a5,2016.0,3396609b96dd24cac3b1542aec686ce362f32fe2,,layer_normalization:_order_embeddings_of_images_and_language,"[1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0
 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0]",
"they also altered the way residual blocks [38] work by introducing a term consisting of the square-root of the element-wise product between the block's input and output. moreover, they applied these alterations on well-known neural network architectures such as lenet [50], vggnet <cite>[46]</cite>, resnet [38] and wrnet [51], and trained them to perform various image recognition tasks, namely object recognition on cifar10 and cifar100 datasets [52] and digit recognition on the svhn dataset [53]. in addition, they compared the prediction accuracy of the altered variants of the networks to their unaltered counterparts and they observed a consistent improvement in all the altered networks on all tasks with an extra computation overhead of less than 5%.",extends,,{'background'},597_vgg19,very deep convolutional networks for large-scale image recognition,eb42cf88027de515750f230b23b1a057dc782108,2014.0,38f479e186a15fffbf77a181bf5e0b792f16480d,,vgg19,"[1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0
 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0]",
"nevertheless, the plm in our method can be replaced by other models. ### idea verbalization

in our public beta system, we employ t5 (<cite>raffel et al, 2020</cite>), a large pretrained sequence-to-sequence model for idea verbalization. we select 2m highly-cited papers for unsupervised denoising training with the language model loss:

\[\mathcal{l}_{lm}=\mathbb{e}_{p}[-\log p(p|\tilde{p};\theta)], \tag{6}\]

where \(\tilde{p}\) represent the corrupted sentence of paper \(p\).",extends,,set(),852_t5-3b,exploring the limits of transfer learning with a unified text-to-text transformer,6c4b76232bb72897685d19b3d264c6ee3005bc2b,2019.0,13bfdd6d1b2cf7222b4ea8bf1b99aefa7243cf53,,t5-3b,"[1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0
 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]",
"rate \(\in\){1e\({}^{-4}\),5e\({}^{-4}\),1e\({}^{-3}\),5e\({}^{-3}\)}, dropout \(\in\){0,0.1,0.2,0.3,0.4,0.5}, batch_size \(\in\){8,16}, lstm_lave \(\in\){1,2}, lstm_hidden_size \(\in\){64,128}, transformer_layer \(\in\){1,2,3,5}, transformer_head \(\in\){1,2,3,4,6,8,12}. the hyper-parameters used in this paper are listed in table 3. the standard back-propagation was used to update all parameters and adam algorithm <cite>[41]</cite> was employed to optimize the objective function. to avoid overfitting problem, an early stopping strategy [42] was employed on the validation set.",extends,,"{'background', 'method'}",1093_adam_(cifar-10),adam: a method for stochastic optimization,a6cb366736791bcccc5c8639de5a8f9636bf87e8,2014.0,cf328824a5a003fd24438b95907e338f46177009,2021.0,adam_(cifar-10),"[1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0
 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1]",
"this issue has been researched in directions such as zero-shot learning, few-shot learning, and continual learning [224]. alternatively, fine-tuning the downstream task head can 

**cnn**: vggnet [48], inception [49], resnet [24], efficientnet [50], c3d [55], i3d [56], s3d [57], x3d [58]

**transformer**: vit <cite>[63]</cite>, doit [64], tnt [65], tzt [66], pvt [67], swin-vit [68], video swin transformer [69], cpvt [70]

**cnn and transformer**: shuffle [72], cmt [73], volo [74]

**vision-driven prompt**: vpt [82], s-prompting[83], dept [84], zegclip [85], act [86], pvit [87], tecoa [88], evp [89], prosfda [90], apt [91], pat [92], lpt [93], pointclip [94], p2p [95], promptgen [96], noair [97], pgn [98], fptrans [99], frpt [100], repro [101], vilo [102], lion [103]

**language-driven prompt**: coop [104], subpt [105], mpa [106], zegot [107], x-clip [108], prograd [109], berg et. al [110], ptp [111], lanit [112], sgva-clip [113], lasp [114], dualcoop [115], plot [116], cpl [117], dego [118], galip [119], cocoop [120], pointclip [121]

**vision-language prompt**: upt [122], dpt [123], maple [124], mvlpt [125], metaprompt [126], tppt [127]

**sequential adapter**: res-adapt [128], epm [129], dan [130], lst [17], conv-adapter [131], polyhistoric [132], pro-tuning [133], amixer [134], fit [135], tina [136], repadapter [137], bdtl [138], vitdet [139], florence [78], snd [140], mk-adapter [141], ada [142], aim [143], st-adapter [144], pea [145], caoa [146], ha [147], clip-adapter [148], tip-adapter [149], balad [150], magma [151], vl-adapter [15], hierarchical3d [152], hyperpelt [153], svl-adapter [154], lavish [155], crossmodal-adapter [156], mv-adapter [157]

**parallel adapter**: vit-adapter [158], pesf-kd [159], adaptmlp [160], convpass [161], ama [162], uniadapter [163]

**mix adapter**: consolidator [164], ett [165], patt [166], palit [167], tvg [168], vqt [169]

**bias part**: bitfit [170], side adapter [171], adapterbias [172], dp-bitfit [173]

**weight part**: lora [174], mosa [175], dylora [167] dna [176], compacter [177], kadaptation [166], phm [178], phnns [179], tarp [180], fact [181], krona [182], dldr [183]

**weight and bias**: ssf [184]

**knowledge distillation**: kd [185], fitnet [186], student [187], dfa [188], adain [189], normalized kd [190], heterogeneous kd [191], deit [64], manifold kd [192], paraphrasing kd [193], rkd [194], akdnet [195], semckd [196], hkd [197], review [198], dkd [199]

**weight remapping**: net2net [200], eas [201], n2n learning [202], nash [203], path-level eas [204], fna [205], fna++ [206]

**architecture remapping**: darts [207], data [208], data-gs [209], p-darts [210], darts+ [211], sgas [212], snas [213], milenas [214], darts- [215]

\begin{table}
\begin{tabular}{c c} \hline \hline
**category** & **description** & **method** \\ \hline \multirow{6}{*}{fine-tuning} & all parameters in the pre-trained model & **cnn**: vggnet [48], inception [49], resnet [24], efficientnet [50], c3d [55], i3d [56], s3d [57], x3d [58] \\  & **transformer**: vit [63], doit [64], tnt [65], tzt [66], pvt [67], swin-vit [68], video swin transformer [69], cpvt [70] \\  & **cnn and transformer**: shuffle [72], cmt [73], volo [74] \\ \hline \multirow{6}{*}{prompt tuning} & **vision-driven prompt**: vpt [82], s-prompting[83], dept [84], zegclip [85], act [86], pvit [87], tecoa [88], evp [89], prosfda [90], apt [91], pat [92], lpt [93], pointclip [94], p2p [95], promptgen [96], noair [97], pgn [98], fptrans [99], frpt [100], repro [101], vilo [102], lion [103] \\  & **language-driven prompt**: coop [104], subpt [105], mpa [106], zegot [107], x-clip [108], prograd [109], berg et.",extends,,{'conclusion'},905_vit-huge_14,an image is worth 16x16 words: transformers for image recognition at scale,268d347e8a55b5eb82fb5e7d2f800e33c75ab18a,2020.0,0340c850e033abbf71c7214e403c8fe2be5ef91f,2023.0,vit-huge_14,"[0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0
 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0]",
"swinir-light [38] and hnct [22] appended cnn to swin transformer [44]. srpn-lite [81] applied the network pruning technique [61] on edsr-baseline <cite>[39]</cite>, a cnn-based lightweight sr model. most recently, elan-light [79] utilized group-wise multi-scale self-attention.",extends,,{'background'},63_edsr,enhanced deep residual networks for single image super-resolution,7ba5d3808e117e7a68dc40331ce1d483ceeedcb2,2017.0,f1939f5c17b1e9dc8229cde49afc181d9dccf215,2022.0,edsr,"[0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0
 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0]",
"all the experiments are run for 50 realizations, where each realization involves a random sample of 50/10 patients from the synthea generated data.10 the scoring function is distmult [6] with weights initialized at uniformly at random, the loss function is logistic regression, and the gnn architecture uses rgcnconv layers [6]. the training is carried out with adam <cite>[18]</cite> for 1000 epochs with variable learning rate11 and weight decay 0.0005.

footnote 10: a training sample has, on average, 35k edges and 1.9k nodes (50 patients, 1603 encounters, 153 observations, 107 conditions, and 5 care actions). footnote 11: the learning rate is equal to 0.1 for the first 100 epochs, 0.01 for the following 600 epochs, and 0.001 for the last 300 epochs.",extends,,set(),1093_adam_(cifar-10),adam: a method for stochastic optimization,a6cb366736791bcccc5c8639de5a8f9636bf87e8,2014.0,94a443efe70955abf9831e0afd6cc44b18e9af39,2023.0,adam_(cifar-10),"[0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 1 0
 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0]",
"** we report details related to the choice of hyper-parameters, expanding on section 5 from the main manuscript. we train our models with a deeplab-v2 <cite>[12]</cite> architecture, implemented in pytorch [60]. we pre-train our models on gta-5 [62] for 6 epochs, using sgd optimizer with learning rate \(\eta=2.5\cdot 10^{-4}\), momentum \(\alpha=0.9\) and weight decay \(\lambda=5*10^{-4}\).",extends,,{'appendix'},9_deeplab_(2017),"deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",cab372bc3824780cce20d9dd1c22d4df39ed081a,2016.0,de12ecd97131c51476e0628274e5a0f5ee2474cc,,deeplab_(2017),"[1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0
 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]",
"footnote 7: 30 popqa and 26 entityquestions questions had popularity less than the smallest popularity bin, and are excluded to avoid showing results for small sample sizes. footnote 7: we use wikipedia dump from december 2018.

retrieval models.we use two widely-used retrieval systems: **bm25**robertson et al (2009) and **contriever**<cite>izacard et al (2022)</cite>. bm25 is a static term-based retriever without training, while contriever is pretrained on large unlabeled corpora, followed by fine-tuning on ms marco bajaj et al (2016).",extends,,"{'method', 'result'}",336_contriever,unsupervised dense information retrieval with contrastive learning,4f4a409f701f7552d45c46a5b0fea69dca6f8e84,2021.0,7b0f98f51040700aae3cd9f0e3432dedcd69fb30,2022.0,contriever,"[0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0
 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0]",
"our method does not rely on any existing mim

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline method & pre-text task & \#views & \#epochs & top-1 acc (\%) & training costs \\ \hline ibot & mim+cl & 2 & 300 & 82.0 & 2.14\(\times\) \\ data2vec & mim+cl & 2 & 300 & 83.0 & 1.60\(\times\) \\ \hline mfm & mfm & 1 & 300 & 83.1 & 1.00\(\times\) \\ \hline \hline \end{tabular}
\end{table}
table 10: **system-level comparison with siamese-based hybrid mim methods (_e.g._, ibot (zhou et al, 2022) and data2vec (baevski et al, 2022)) using vit-b/16 on imagenet-1k. for a fair comparison, we re-implement ibot without multi-crop augmentation (but keeping the two global views) and data2vec (<cite>baevski et al, 2022</cite>) without additional losses of intermediate transformer layers using their official code. training costs are counted in relative to our approach.",extends,,{'appendix'},885_data2vec_(language),"data2vec: a general framework for self-supervised learning in speech, vision and language",8f2bca9d684005675e294b33c26481e36f528cdb,2022.0,2a7aee4ff519a7c0938bd397b6610707002836b3,2022.0,data2vec_(language),"[1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0
 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0]",
"[86]. \begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline
**model** & **architecture** & **max len** & **dim** & **\# para** & **pretrained data** & **time\({}^{1}\)** \\ \cline{4-8}  & & & & & **source** & **size** \\ \hline
**local models** & & & & & & & \\ profile hmms [37] & hidden markov & – & – & – & msas & – & oct 2012 \\ evmutation [38] & potts models & – & – & – & msas & – & jan 2017 \\ msa transformer <cite>[39]</cite> & transformer & 1024 & 768 & 100m & uniref50 [14] & 26m & feb 2021 \\ deepsequence [22] & vaes & – & – & – & msas & – & dec 2017 \\ eve [40] & bayesian vaes & – & – & – & msas & – & oct 2021 \\
**global models** & & & & & & & \\ tape resnet [41] & resnet & 1024 & 256 & 38m & pfam [36] & 31m & jun 2019 \\ tape lstm [41] & lstm & 1024 & 2048 & 38m & pfam [36] & 31m & jun 2019 \\ tape transformer [41] & transformer & 1024 & 512 & 38m & pfam [36] & 31m & jun 2019 \\ bepler [42] & lstm & 512 & 100 & 22m & pfam [36] & 31m & feb 2019 \\ unirep [21] & lstm & 512 & 1900 & 18m & uniref50 [14] & 24m & mar 2019 \\ eunirep [43] & lstm & 512 & 1900 & 18m & uniref50 [14]; msas & 24m & jan 2020 \\ esm-1b [23] & transformer & 1024 & 1280 & 650m & uniref50 [14] & 250m & dec 2020 \\ esm-1v [44] & transformer & 1024 & 1280 & 650m & uniref90 [14] & 98m & jul 2021 \\ esm-if1 [45] & transformer & – & 512 & 124m & uniref50 [14]; cath [46] & 12m sequences; sep 2022 & 16k structures \\ progen [47] & transformer & 512 & – & 1.2b & uniparc [14]; uniprotkb & 281m & jul 2021 \\  & & & & & & & \\  & & & & & & & \\  & & & & & & & \\ proteinbert [49] & transformer & 1024 & – & 16m & uniref90 [14] & 106m & may 2021 \\ tranception [15] & transformer & 1024 & 1280 & 700m & uniref100 [14] & 250m & may 2022 \\ esm-2 [50] & transformer & 1024 & 5120 & 15b & uniref90 [14] & 65m & oct 2022 \\ \hline \hline \end{tabular}
\end{table}
table 1: summary of protein language models. # para: number of parameters which are only provided for deep learning models.",extends,,set(),1153_msa_transformer,msa transformer,deee48c5e0ac0407a1e002905caaf2b174bdb0e6,2021.0,89f5edb42eb675c5ed80198b0127e3537e420b7d,2023.0,msa_transformer,"[1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0
 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0]",
"first, two segmentation masks per object were generated by human annotators (which also had to chose the objects based on diversity in the first place). then, deeplabv3+ <cite>[6]</cite>, a state-of-the-art semantic segmentation network, was trained on the initially generated masks in a way to overfit these objects in a sequence-specific way, thus yielding reasonable segmentation masks for the remaining objects in each track. the resulting segmentation masks then underwent another manual correction step to fix remaining errors.",extends,,{'introduction'},161_deeplabv3+,encoder-decoder with atrous separable convolution for semantic image segmentation,9217e28b2273eb3b26e4e9b7b498b4661e6e09f5,2018.0,0fd731b4f5802b7177c45607dcfb8b8c45d418bf,,deeplabv3+,"[1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0
 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0]",
"the free-space annotation was done fully automatically on the camera images. a deeplabv3+ <cite>[5]</cite>, pre-trained on cityscape, has been fine-tuned with 2 classes (_free space_ and _occupied_) on a small manually-annotated part of our dataset. this model segmented each video frame and the obtained segmentation mask was projected from the camera's coordinate system to the radar's one thanks to known calibration.",extends,,set(),161_deeplabv3+,encoder-decoder with atrous separable convolution for semantic image segmentation,9217e28b2273eb3b26e4e9b7b498b4661e6e09f5,2018.0,bfc2d694e44bb726eef02e128df3babd2bbf1c21,,deeplabv3+,"[1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0
 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0]",
