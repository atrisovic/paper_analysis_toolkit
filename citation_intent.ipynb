{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from config import data_path\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all MMDs\n",
    "\n",
    "def grep_from_folder(folder_path, pattern):\n",
    "    matched_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if re.search(pattern, file):\n",
    "                matched_files.append(os.path.join(root, file))\n",
    "    return matched_files\n",
    "\n",
    "\n",
    "txt_files = grep_from_folder(data_path, \".mmd\")\n",
    "# print(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citation_sentences(text, ref) -> list:\n",
    "    # Split text into sentences using NLTK\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Filter sentences that contain ref\n",
    "    sentences_with_ref = [sentence for sentence in sentences if ref in sentence]\n",
    "    return sentences_with_ref\n",
    "\n",
    "\n",
    "def get_citation_sentences_re(text, ref) -> list:\n",
    "    # Split text into sentences using regular expression\n",
    "    # The pattern looks for periods followed by a space and a capital letter, which is a simple heuristic for sentence boundaries.\n",
    "    sentences = re.split(r\"(?<=\\.)\\s+\", text)\n",
    "\n",
    "    # Filter sentences that contain '[8]'\n",
    "    sentences_with_ref = [sentence for sentence in sentences if ref in sentence]\n",
    "    return sentences_with_ref\n",
    "\n",
    "\n",
    "def get_citation_ref_number(line) -> str:\n",
    "    \"\"\"Returns the citation reference number if found, otherwise None.\"\"\"\n",
    "    # Regular expression to find \"[ number ]\"\n",
    "    match = re.search(r\"\\[\\d+\\]\", line)\n",
    "\n",
    "    # Extracting and printing the match\n",
    "    if match:\n",
    "        extracted_string = match.group()\n",
    "        return extracted_string\n",
    "    return None\n",
    "\n",
    "\n",
    "def format_citation_ref(ref) -> str:\n",
    "    \"\"\"Solves formatting issue for string references from get_citation_ref_str.\"\"\"\n",
    "    # Replace the '[' with ', ' and remove the ']'\n",
    "    converted_string = ref.replace(\" [\", \", \").replace(\"]\", \"\")\n",
    "    converted_string = converted_string.replace(\" (\", \", \").replace(\")\", \"\")\n",
    "    return converted_string\n",
    "\n",
    "\n",
    "def get_citation_ref_str(line) -> str:\n",
    "    \"\"\"Returns the citation reference string if found, otherwise None.\"\"\"\n",
    "    # Regular expression pattern to match the described criteria\n",
    "    pattern = r\"[A-Z][^)]+[\\)|\\]]\"\n",
    "\n",
    "    # Search for the pattern in the given line\n",
    "    match = re.search(pattern, line)\n",
    "\n",
    "    # Extract and print the matching string if found\n",
    "    if match:\n",
    "        extracted_string = match.group()\n",
    "        return format_citation_ref(extracted_string)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_citation_ref(\n",
    "    text, target_phrase=\"Llama: Open and efficient foundation language models\"\n",
    ") -> list:\n",
    "    \"\"\"This function looks in the references and returns\n",
    "    the reference number or string how the model is cited.\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    # Filter lines that contain the specific phrase\n",
    "    matching_lines = [line for line in lines if target_phrase in line]\n",
    "\n",
    "    for line in matching_lines:\n",
    "        ref_number = get_citation_ref_number(line)\n",
    "        if ref_number:\n",
    "            return ref_number\n",
    "        ref_str = get_citation_ref_str(line)\n",
    "        if ref_str:\n",
    "            return ref_str\n",
    "    return matching_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get citation sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Touvron et al., 2023a : We mainly consider foundation models including LLAMA-65B (Touvron et al., 2023a), text-davinvi-003 (Brown et al., 2020), ChatGPT and GPT-4 (OpenAI, 2023).\n",
      "Touvron et al., 2023a : We compare our SKiC with zero/few-shot standard prompting (4-shot) (Brown et al., 2020), CoT (Wei et al., 2022b) and Least-to-Most prompting (LtM) (Zhou et al., 2022) on different large language models, including LLAMA-65B (Touvron et al., 2023a), text-davinvi-003 (Brown et al., 2020; Ouyang et al., 2022), and ChatGPT.\n",
      "Touvron et al., 2023 : However, a large number of predictive tasks fail to naively fit into the existing supervised data distillation framework, _e.g._, image-generation (Ramesh et al., 2022; Rombach et al., 2022), language modeling (Brown et al., 2020; Devlin et al., 2019; Touvron et al., 2023), representation learning (Chen et al., 2020; Grill et al., 2020), _etc_.\n",
      "Touvron et al., 2023 : ### Extending to LLaMA-Based Models\n",
      "\n",
      "To further investigate the roles played by size and training method in the model's performance, we carry out our experimental procedure on three versions with different sizes (7B, 13B, and 30B) of the LLaMA model (Touvron et al., 2023), and on Stanford Alpaca (which applies instruction tuning on LLaMA 7B) (Taori et al., 2023).\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "# Loop through the files and collect citation sentences\n",
    "for file in txt_files[:10]:  # Limiting to 10 files for now\n",
    "    with open(file, \"r\") as file:\n",
    "        content = file.read()\n",
    "    matching_lines = get_citation_ref(content)\n",
    "\n",
    "    if type(matching_lines) == str:\n",
    "        ref = matching_lines\n",
    "\n",
    "    for sentence in get_citation_sentences(content, ref):\n",
    "        if sentence[0] != \"*\":\n",
    "            print(f\"{ref} : {sentence}\")\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    # if type(matching_lines) == str:\n",
    "    #     print(matching_lines)\n",
    "    # else:\n",
    "    #     # Print matching lines\n",
    "    #     for line in matching_lines:\n",
    "    #         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Improve citation sentence extraction and matching to the reference (foundation model) paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get citation intent with MultiCite\n",
    "\n",
    "Paper: https://arxiv.org/abs/2107.00414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(model_checkpoint):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "    classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "    return classifier\n",
    "multicite = get_classifier('allenai/multicite-multilabel-scibert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = multicite(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'uses', 'score': 0.987091064453125},\n",
       " {'label': 'uses', 'score': 0.9294595122337341},\n",
       " {'label': 'background', 'score': 0.8526414632797241},\n",
       " {'label': 'uses', 'score': 0.9757737517356873}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
